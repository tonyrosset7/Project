id,title,abstract,year,journal,citations,abstract_lem
0,DECAB-LSTM: Deep Contextualized Attentional Bidirectional LSTM for cancer hallmark classification,"The great number of online scientific publications on cancer research makes large scale data mining possible. The hallmarks or characteristics of cancer can be used to distinguish cancerous cells from normal cells. Therefore, it is extremely necessary to organize and categorize a sea of scientific articles into the corresponding hallmarks by predicting whether or not they contain the information of interest. In the past, many research works tended to employ traditional machine learning methods that characterize feature engineering. Deep learning-based methods have achieved state-of-the-art performance in a wide range of Natural Language Processing (NLP) tasks. However, there is only a limited number of work with a focus on deep learning techniques for the task of cancer hallmark text classification. To advance this task, a novel neural architecture DEep Contextualized Attentional Bidirectional LSTM (DECAB-LSTM) was proposed, capable of learning to attend to the valuable information in a sentence by introducing contextual attention mechanism. We also investigated the effect of a good word embedding for the cancer hallmark text classification. We trained our model on a benchmark dataset and reported the accuracy, f score, and AUC metrics. Compared to several baselines like Logistic regression, Support Vector Machines, Convolutional Neural Networks, fastText, etc., the proposed model have achieved state-of-the-art performance over baselines, demonstrating its great potential in the empirical application to cancer research. © 2020 Elsevier B.V.",2020,Knowledge-Based Systems,0,@ great number of online scientific publication on cancer research make @ scale data mining possible @ @ hallmark @ characteristic of cancer @ @ used to distinguish cancerous cell @ normal cell @ therefore @ is extremely necessary to organize and categorize a sea of scientific article @ @ corresponding hallmark by predicting whether @ not @ contain @ information of interest @ in @ past many research work tended to employ traditional machine learning method @ characterize feature engineering @ deep learning-based method @ achieved state-of-the-art performance in a wide range of natural language processing @ nlp @ task @ however @ is only a limited number of work @ a focus on deep learning technique @ @ task of cancer hallmark text classification @ to advance @ task a novel neural architecture deep contextualized attentional bidirectional lstm @ decab-lstm @ wa proposed capable of learning to attend to @ valuable information in a sentence by introducing contextual attention mechanism @ @ @ investigated @ effect of a good word embedding @ @ cancer hallmark text classification @ @ trained @ model on a benchmark dataset and reported @ accuracy f score and auc metric @ compared to several baseline like logistic regression support vector machine convolutional neural network fasttext etc @ @ proposed model @ achieved state-of-the-art performance @ baseline demonstrating @ great potential in @ empirical application to cancer research @ @ b @ v @ 
1,Digital transformation: challenges faced by organizations and their potential solutions,"Purpose: Digital transformation is the way forward for all businesses. The technology is advancing at a rapid pace and the companies need to adapt to the change, not just to take advantage of the enormous opportunities it provides but even to stay relevant in this volatility, uncertainty, complexity, and ambiguity world. This study aims to define the concept of digital transformation and what it means in today’s business scenario. It helps to understand the different stages of digital maturity, identify the barriers in adopting different technologies and provide solutions to overcome those challenges. Design/methodology/approach: This is a qualitative study in which opinions of the digital transformation experts were collected using a qualitative questionnaire. Natural language processing (NLP) and text mining techniques were applied along with a thorough analysis of the text to generate the results. Findings: The study was able to uncover – what it means to be digitally transformed, different challenges an organization faces during the digital transformation journey and their potential solutions. Originality/value: The existing literature on the topic is scattered and does not provide a roadmap for a company to adopt digital transformation. This study aims to fill up the gap and cover various aspects of the whole transformation process. The uniqueness of the study lies in the use of NLP techniques to perform text analytics on the data. © 2020, Emerald Publishing Limited.",2020,International Journal of Innovation Science,0,purpose @ digital transformation is @ way forward @ @ @ @ @ technology is advancing at a rapid pace and @ company need to adapt to @ change not @ to take advantage of @ enormous opportunity @ provides @ even to stay relevant in @ volatility uncertainty complexity and ambiguity world @ @ study aim to define @ concept of digital transformation and @ @ mean in today s @ scenario @ @ help to understand @ different stage of digital maturity identify @ barrier in adopting different technology and provide solution to overcome @ challenge @ design methodology approach @ @ is a qualitative study in @ opinion of @ digital transformation expert @ collected @ a qualitative questionnaire @ natural language processing @ nlp @ and text mining technique @ applied along @ a thorough analysis of @ text to generate @ @ @ finding @ @ study wa able to uncover @ @ mean to @ digitally transformed different challenge @ organization face @ @ digital transformation journey and @ potential solution @ originality value @ @ existing literature on @ topic is scattered and doe not provide a roadmap @ a company to adopt digital transformation @ @ study aim to fill up @ gap and cover various aspect of @ whole transformation process @ @ uniqueness of @ study lie in @ use of nlp technique to perform text analytics on @ data @ emerald publishing limited @ 
17,Using argument mining for legal text summarization,"Argument mining, a subfield of natural language processing and text mining, is a process of extracting argumentative text portions and identifying the role the selected texts play. Legal argument mining targets the argumentative parts of a legal text. In order to better understand how to apply legal argument mining as a step toward improving case summarization, we have assembled a sizeable set of cases and human-expert-prepared summaries annotated in terms of legal argument triples that capture the most important skeletal argument structures in a case. We report the results of applying multiple machine learning techniques to demonstrate and analyze the advantages and disadvantages of different methods to identify sentence components of these legal argument triples. © 2020 The Authors, Faculty of Law, Masaryk University and IOS Press.",2020,Frontiers in Artificial Intelligence and Applications,0,argument mining a subfield of natural language processing and text mining is a process of extracting argumentative text portion and identifying @ role @ selected text play @ legal argument mining target @ argumentative part of a legal text @ in order to better understand @ to apply legal argument mining a a step toward improving case summarization @ @ assembled a sizeable set of case and human-expert-prepared summary annotated in term of legal argument triple @ capture @ @ important skeletal argument structure in a case @ @ report @ @ of applying multiple machine learning technique to demonstrate and analyze @ advantage and disadvantage of different method to identify sentence component of @ legal argument triple @ @ author faculty of law masaryk university and io @ @ 
24,Efficient text summarization method for blind people using text mining techniques,"Owing to the phenomenal growth in communication technology, most of us hardly have time to read books. This habit of reading is slowly diminishing because of the busy lives of people. For visually challenged people, the situation is even worse. In order to address this impedes, we develop a better and more accurate methodology than the existing ones. In this work, in order to save the efforts for reading the complete text every time, we modify the Weighted TF_IDF (Term Frequency Inverse Document Frequency) algorithm to summarize books into relevant keywords. Then, we compare the modified algorithm with that of the existing algorithms of TextRank Algorithm, Luhn’s Algorithm, LexRank Algorithm, Latent Semantic Analysis(LSA). From the comparative analysis, we find that Weighted TF_IDF is an efficient algorithm to automate text summarization and produce an effective summary which is then converted from text to speech. Thus, the proposed algorithm would highly be useful for blind people. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2020,International Journal of Speech Technology,0,owing to @ phenomenal growth in communication technology @ of u hardly @ time to read book @ @ habit of reading is slowly diminishing @ of @ busy life of people @ @ visually challenged people @ situation is even worse @ in order to address @ impedes @ develop a better and more accurate methodology @ @ existing @ @ in @ work in order to save @ effort @ reading @ complete text every time @ modify @ weighted tf idf @ term frequency inverse document frequency @ algorithm to summarize book @ relevant keywords @ @ @ compare @ modified algorithm @ @ of @ existing algorithm of textrank algorithm luhn s algorithm lexrank algorithm latent semantic analysis @ lsa @ @ @ @ comparative analysis @ find @ weighted tf idf is @ efficient algorithm to automate text summarization and produce @ effective summary @ is @ converted @ text to speech @ thus @ proposed algorithm would highly @ useful @ blind people @ @ science @ medium llc part of @ nature @ 
28,MAGPEL: an autoMated pipeline for inferring vAriant-driven Gene PanEls from the full-length biomedical literature,"In spite of the efforts in developing and maintaining accurate variant databases, a large number of disease-associated variants are still hidden in the biomedical literature. Curation of the biomedical literature in an effort to extract this information is a challenging task due to: (i) the complexity of natural language processing, (ii) inconsistent use of standard recommendations for variant description, and (iii) the lack of clarity and consistency in describing the variant-genotype-phenotype associations in the biomedical literature. In this article, we employ text mining and word cloud analysis techniques to address these challenges. The proposed framework extracts the variant-gene-disease associations from the full-length biomedical literature and designs an evidence-based variant-driven gene panel for a given condition. We validate the identified genes by showing their diagnostic abilities to predict the patients’ clinical outcome on several independent validation cohorts. As representative examples, we present our results for acute myeloid leukemia (AML), breast cancer and prostate cancer. We compare these panels with other variant-driven gene panels obtained from Clinvar, Mastermind and others from literature, as well as with a panel identified with a classical differentially expressed genes (DEGs) approach. The results show that the panels obtained by the proposed framework yield better results than the other gene panels currently available in the literature. © 2020, The Author(s).",2020,Scientific Reports,0,in spite of @ effort in developing and maintaining accurate variant database a @ number of disease-associated variant @ still hidden in @ biomedical literature @ curation of @ biomedical literature in @ effort to extract @ information is a challenging task due to @ @ i @ @ complexity of natural language processing @ ii @ inconsistent use of standard recommendation @ variant description and @ iii @ @ lack of clarity and consistency in describing @ variant-genotype-phenotype association in @ biomedical literature @ in @ article @ employ text mining and word cloud analysis technique to address @ challenge @ @ proposed framework extract @ variant-gene-disease association @ @ full-length biomedical literature and design @ evidence-based variant-driven gene panel @ a given condition @ @ validate @ identified gene by showing @ diagnostic ability to predict @ patient clinical outcome on several independent validation cohort @ a representative example @ @ @ @ @ acute myeloid leukemia @ aml @ breast cancer and prostate cancer @ @ compare @ panel @ @ variant-driven gene panel obtained @ clinvar mastermind and others @ literature a well a @ a panel identified @ a classical differentially expressed gene @ degs @ approach @ @ @ @ @ @ panel obtained by @ proposed framework yield better @ @ @ @ gene panel currently available in @ literature @ @ author @ s @ @ 
29,A survey on intention analysis: successful approaches and open challenges,"Intention Analysis is a computational task that analyzes people’s desires, wishes, and attitudes from user-generated texts. This sub-field of text mining has recently attracted research interest. This research paper provides an overview and an analysis of the latest studies in this field. These studies were categorized and summarized according to their contributions and the techniques they used. Several proposed approaches and some real applications were investigated in depth and presented in detail. Moreover, some related fields to intention analysis such as Transfer Learning (TL), Spam Detection (SD), and Building Resources (BR) were discussed in this survey of the literature dedicated to Intention Analysis. The aim of this survey is to give a comprehensive view of the intention analysis field supported by a number of graphics and summary tables about the literature. The paper concludes by identifying a number of research topics that can be promising for future research. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Journal of Intelligent Information Systems,1,intention analysis is a computational task @ analyzes people s desire wish and attitude @ user-generated text @ @ sub-field of text mining ha recently attracted research interest @ @ research @ provides @ overview and @ analysis of @ latest study in @ field @ @ study @ categorized and summarized according to @ contribution and @ technique @ used @ several proposed approach and some real application @ investigated in depth and presented in detail @ moreover some related field to intention analysis @ a transfer learning @ tl @ spam detection @ sd @ and building resource @ br @ @ discussed in @ survey of @ literature dedicated to intention analysis @ @ aim of @ survey is to give a comprehensive view of @ intention analysis field supported by a number of graphic and summary table @ @ literature @ @ @ concludes by identifying a number of research topic @ @ @ promising @ future research @ @ science @ medium llc part of @ nature @ 
30,Automatic paper writing based on a RNN and the TextRank algorithm,"Academic research is crucial to the development of science and technology and is an important factor that affects national strength. When writing an academic research paper, a rhetorical structure is typically used to present the paper's ideas, but this task is quite difficult for junior researchers. To solve this problem, some studies have adopted text mining to assist with the writing, but the existing methods still require human intervention to generate sentences. Recently, due to the increasing maturity of deep learning technology and the ability to address the problem of automatic text generation, progress has been made in this area. The highly complex deep learning operations can correctly generate sequences and find correlations between sequences. When a user provides a few keywords and key sentences, the proposed algorithm can generate an introduction section for the user. The results show that the generated introduction is more coherent, clearer, and more fluent than existing summarization methods. In addition, the method proposed in this study improves the accuracy compared with traditional text extraction methods. The manuscript produced by this study has been evaluated to show that the study can produce a comprehensive introduction compared with previous studies. © 2020 Elsevier B.V.",2020,Applied Soft Computing Journal,1,@ research is crucial to @ development of science and technology and is @ important factor @ affect national strength @ @ writing @ @ research @ a rhetorical structure is typically used to @ @ @ @ s idea @ @ task is quite difficult @ junior researcher @ to solve @ problem some study @ adopted text mining to assist @ @ writing @ @ existing method still require human intervention to generate sentence @ recently due to @ increasing maturity of deep learning technology and @ ability to address @ problem of automatic text generation progress ha @ made in @ area @ @ highly complex deep learning operation @ correctly generate sequence and find correlation @ sequence @ @ a user provides a @ keywords and key sentence @ proposed algorithm @ generate @ introduction section @ @ user @ @ @ @ @ @ generated introduction is more coherent clearer and more fluent @ existing summarization method @ in addition @ method proposed in @ study improves @ accuracy compared @ traditional text extraction method @ @ manuscript produced by @ study ha @ evaluated to @ @ @ study @ produce a comprehensive introduction compared @ previous study @ @ b @ v @ 
31,Good and bad events: combining network-based event detection with sentiment analysis,"The huge volume and velocity of media content published on the Web presents a substantial challenge to human analysts. In prior work, we developed a system (network event detection, NED) to assist analysts by detecting events within high-volume news streams in real time. NED can process a heterogeneous stream of news articles or social media user posts, combining text mining and network analysis to detect breaking news stories and generate an easy-to-understand event summary. In this paper, we expand the NED event detection and summarisation approach in two ways. First, we introduce a new approach to named entity disambiguation for tweets, which contain minimal information due to brevity. Second, we apply sentiment analysis techniques to documents associated with a detected event to characterise the event as either broadly ‘positive’ or ‘negative’ based on media portrayal. Our expansion focuses on Twitter streams since Twitter has become an important news dissemination platform and is often the site where emerging events are first seen. To test the extended methodology, we apply it here to three data sets related to political elections in the UK and the USA. The addition of sentiment analysis to the NED event detection methodology improves the insight gained by the user by allowing quick evaluation of the perceived impact of an event. This approach may have potential applications in domains where public sentiment is relevant to decision-making around events, such as financial markets and politics. © 2020, The Author(s).",2020,Social Network Analysis and Mining,0,@ huge volume and velocity of medium content published on @ web @ a substantial challenge to human analyst @ in prior work @ developed a system @ network event detection ned @ to assist analyst by detecting event within high-volume news stream in real time @ ned @ process a heterogeneous stream of news article @ social medium user post combining text mining and network analysis to detect breaking news story and generate @ easy-to-understand event summary @ in @ @ @ expand @ ned event detection and summarisation approach in @ way @ first @ introduce a @ approach to named entity disambiguation @ tweet @ contain minimal information due to brevity @ second @ apply sentiment analysis technique to document associated @ a detected event to characterise @ event a either broadly positive @ negative based on medium portrayal @ @ expansion focus on twitter stream since twitter ha become @ important news dissemination platform and is often @ site @ emerging event @ first seen @ to test @ extended methodology @ apply @ @ to three data set related to political election in @ uk and @ usa @ @ addition of sentiment analysis to @ ned event detection methodology improves @ insight gained by @ user by allowing quick evaluation of @ perceived impact of @ event @ @ approach may @ potential application in domain @ public sentiment is relevant to decision-making around event @ a financial market and politics @ @ author @ s @ @ 
32,Reducing efforts of software engineering systematic literature reviews updates using text classification,"Context: Systematic Literature Reviews (SLRs) are frequently used to synthesize evidence in Software Engineering (SE), however replicating and keeping SLRs up-to-date is a major challenge. The activity of studies selection in SLR is labor intensive due to the large number of studies that must be analyzed. Different approaches have been investigated to support SLR processes, such as: Visual Text Mining or Text Classification. But acquiring the initial dataset is time-consuming and labor intensive. Objective: In this work, we proposed and evaluated the use of Text Classification to support the studies selection activity of new evidences to update SLRs in SE. Method: We applied Text Classification techniques to investigate how effective and how much effort could be spared during the studies selection phase of an SLR update. Considering the SLRs update scenario, the studies analyzed in the primary SLR could be used as a classified dataset to train Supervised Machine Learning algorithms. We conducted an experiment with 8 Software Engineering SLRs. In the experiments, we investigated the use of multiple preprocessing and feature extraction tasks such as tokenization, stop words removal, word lemmatization, TF-IDF (Term-Frequency/Inverse-Document-Frequency) with Decision Tree and Support Vector Machines as classification algorithms. Furthermore, we configured the classifier activation threshold for maximizing Recall, hence reducing the number of Missed selected studies. Results: The techniques accuracies were measured and the results achieved on average a F-Score of 0.92 and 62% of exclusion rate when varying the activation threshold of the classifiers, with a 4% average number of Missed selected studies. Both the Exclusion rate and number of Missed selected studies were significantly different when compared to classifier which did not use the configuration of the activation threshold. Conclusion: The results showed the potential of the techniques in reducing the effort required of SLRs updates. © 2020 Elsevier B.V.",2020,Information and Software Technology,0,context @ systematic literature review @ slrs @ @ frequently used to synthesize evidence in software engineering @ se @ however replicating and keeping slrs up-to-date is a major challenge @ @ activity of study selection in slr is labor intensive due to @ @ number of study @ must @ analyzed @ different approach @ @ investigated to support slr process @ a @ visual text mining @ text classification @ @ acquiring @ initial dataset is time-consuming and labor intensive @ objective @ in @ work @ proposed and evaluated @ use of text classification to support @ study selection activity of @ evidence to update slrs in se @ method @ @ applied text classification technique to investigate @ effective and @ much effort could @ spared @ @ study selection phase of @ slr update @ considering @ slrs update scenario @ study analyzed in @ primary slr could @ used a a classified dataset to train supervised machine learning algorithm @ @ conducted @ experiment @ software engineering slrs @ in @ experiment @ investigated @ use of multiple preprocessing and feature extraction task @ a tokenization stop word removal word lemmatization tf-idf @ term-frequency inverse-document-frequency @ @ decision tree and support vector machine a classification algorithm @ furthermore @ configured @ classifier activation threshold @ maximizing recall hence reducing @ number of missed selected study @ @ @ @ technique accuracy @ measured and @ @ achieved on average a f-score of @ and of exclusion rate @ varying @ activation threshold of @ classifier @ a average number of missed selected study @ @ @ exclusion rate and number of missed selected study @ significantly different @ compared to classifier @ @ not use @ configuration of @ activation threshold @ conclusion @ @ @ showed @ potential of @ technique in reducing @ effort required of slrs update @ @ b @ v @ 
33,Customer reviews analytics on food delivery services in social media: A review,"Food delivery services have gained attention and become a top priority in developed cities by reducing travel time and waiting time by offering online food delivery options for a variety of dishes from a wide variety of restaurants. Therefore, customer analytics have been considered in business analysis by enabling businesses to collect and analyse customer feedback to make business decisions to be more advanced in the future. This paper aims to study the techniques used in customer analytics for food delivery services and identify the factors of customers’ reviews for food delivery services especially in social media. A total of 53 papers reviewed, several techniques and algorithms on customer analytics for food delivery services in social media are Lexicon, machine learning, natural language processing (NLP), support vector machine (SVM), and text mining. The paper further analyse the challenges and factors that give impacts to the customers’ reviews for food delivery services. These findings would be appropriate for development and enhancement of food delivery services in future works. © 2020, Institute of Advanced Engineering and Science. All rights reserved.",2020,IAES International Journal of Artificial Intelligence,0,food delivery service @ gained attention and become a top priority in developed city by reducing travel time and waiting time by offering online food delivery option @ a variety of dish @ a wide variety of restaurant @ therefore customer analytics @ @ considered in @ analysis by enabling @ to collect and analyse customer feedback to make @ decision to @ more advanced in @ future @ @ @ aim to study @ technique used in customer analytics @ food delivery service and identify @ factor of customer review @ food delivery service especially in social medium @ a total of @ reviewed several technique and algorithm on customer analytics @ food delivery service in social medium @ lexicon machine learning natural language processing @ nlp @ support vector machine @ svm @ and text mining @ @ @ @ analyse @ challenge and factor @ give impact to @ customer review @ food delivery service @ @ finding would @ appropriate @ development and enhancement of food delivery service in future work @ institute of advanced engineering and science @ @ right reserved @ 
35,A New Hybrid Technique for Detection of Plagiarism from Text Documents,"Plagiarism occurs when we use the ideas, expressions, work, and words of other authors and do not give them the required attribution. The major contributing factor in plagiarism is the availability of a high amount of data and information on the internet that can be swiftly accessed. The proposed system introduces an extrinsic plagiarism detection approach inspired by cognition because it utilizes semantic knowledge to detect the plagiarized part from the text without human involvement. A lexical database like WordNet assists the computers to perceive the data and information. These days most of the plagiarism detection systems fail to detect highly complex cases of plagiarism. The proposed system uses Dice measure as similarity measure for finding the semantic resemblance between the pair of sentences. It also uses linguistic features like path similarity, depth estimation measure to compute the resemblance between the pair of words and these features are combined by assigning different weights to them. It is capable of identifying cases like restructuring, paraphrasing, verbatim copy, and synonymized plagiarism. It has been evaluated on the PAN-PC-11 corpus. The results obtained from the proposed system signify that it has outperformed other existing systems on PAN-PC-11 in terms of precision, recall, F-measure, and PlagDet score. The proposed system has innovative approach, but the results are somehow close and reasonably better than the existing systems. © 2020, King Fahd University of Petroleum & Minerals.",2020,Arabian Journal for Science and Engineering,0,plagiarism occurs @ @ use @ idea expression work and word of @ author and @ not give @ @ required attribution @ @ major contributing factor in plagiarism is @ availability of a high amount of data and information on @ internet @ @ @ swiftly accessed @ @ proposed system introduces @ extrinsic plagiarism detection approach inspired by cognition @ @ utilizes semantic knowledge to detect @ plagiarized part @ @ text without human involvement @ a lexical database like wordnet assist @ computer to perceive @ data and information @ @ day @ of @ plagiarism detection system fail to detect highly complex case of plagiarism @ @ proposed system us dice measure a similarity measure @ finding @ semantic resemblance @ @ pair of sentence @ @ @ us linguistic feature like path similarity depth estimation measure to compute @ resemblance @ @ pair of word and @ feature @ combined by assigning different weight to @ @ @ is capable of identifying case like restructuring paraphrasing verbatim copy and synonymized plagiarism @ @ ha @ evaluated on @ pan-pc corpus @ @ @ obtained @ @ proposed system signify @ @ ha outperformed @ existing system on pan-pc in term of precision recall f-measure and plagdet score @ @ proposed system ha innovative approach @ @ @ @ somehow close and reasonably better @ @ existing system @ king fahd university of petroleum mineral @ 
36,Exploiting pivot words to classify and summarize discourse facets of scientific papers,"The ever-increasing number of published scientific articles has prompted the need for automated, data-driven approaches to summarizing the content of scientific articles. The Computational Linguistics Scientific Document Summarization Shared Task (CL-SciSumm 2019) has recently fostered the study and development of new text mining and machine learning solutions to the summarization problem customized to the academic domain. In CL-SciSumm, a Reference Paper (RP) is associated with a set of Citing Papers (CPs), all containing citations to the RP. In each CP, the text spans (i.e., citances) have been identified that pertain to a particular citation to the RP. The task of identifying the spans of text in the RP that most accurately reflect the citance is addressed using supervised approaches. This paper proposes a new, more effective solution to the CL-SciSumm discourse facet classification task, which entails identifying for each cited text span what facet of the paper it belongs to from a predefined set of facets. It proposes also to extend the set of traditional CL-SciSumm tasks with a new one, namely the discourse facet summarization task. The idea behind is to extract facet-specific descriptions of each RP consisting of a fixed-length collection of RP’s text spans. To tackle both the standard and the new tasks, we propose machine learning supported solutions based on the extraction of a selection of discriminating words, called pivot words. Predictive features based on pivot words are shown to be of great importance to rate the pertinence and relevance of a text span to a given facet. The newly proposed facet classification method performs significantly better than the best performing CL-SciSumm 2019 participant (i.e., the classification accuracy has increased by + 8%), whereas regression methods achieved promising results for the newly proposed summarization task. © 2020, Akadémiai Kiadó, Budapest, Hungary.",2020,Scientometrics,0,@ ever-increasing number of published scientific article ha prompted @ need @ automated data-driven approach to summarizing @ content of scientific article @ @ computational linguistics scientific document summarization shared task @ cl-scisumm @ ha recently fostered @ study and development of @ text mining and machine learning solution to @ summarization problem customized to @ @ domain @ in cl-scisumm a reference @ @ rp @ is associated @ a set of citing @ @ cps @ @ containing citation to @ rp @ in @ cp @ text span @ i @ e @ citances @ @ @ identified @ pertain to a particular citation to @ rp @ @ task of identifying @ span of text in @ rp @ @ accurately reflect @ citance is addressed @ supervised approach @ @ @ proposes a @ more effective solution to @ cl-scisumm discourse facet classification task @ entail identifying @ @ cited text span @ facet of @ @ @ belongs to @ a predefined set of facet @ @ proposes @ to extend @ set of traditional cl-scisumm task @ a @ @ namely @ discourse facet summarization task @ @ idea behind is to extract facet-specific description of @ rp consisting of a fixed-length collection of rp s text span @ to tackle @ @ standard and @ @ task @ propose machine learning supported solution based on @ extraction of a selection of discriminating word called pivot word @ predictive feature based on pivot word @ @ to @ of great importance to rate @ pertinence and relevance of a text span to a given facet @ @ newly proposed facet classification method performs significantly better @ @ best performing cl-scisumm participant @ i @ e @ @ classification accuracy ha increased by @ whereas regression method achieved promising @ @ @ newly proposed summarization task @ akadémiai kiadó budapest hungary @ 
37,A dockerized framework for hierarchical frequency-based document clustering on cloud computing infrastructures,"Scalable big data analysis frameworks are of paramount importance in the modern web society, which is characterized by a huge number of resources, including electronic text documents. Document clustering is an important field in text mining and is commonly used for document organization, browsing, summarization and classification. Hierarchical clustering methods construct a hierarchy structure that, combined with the produced clusters, can be useful in managing documents, thus making the browsing and navigation process easier and quicker, and providing only relevant information to the users’ queries by leveraging the structure relationships. Nevertheless, the high computational cost and memory usage of baseline hierarchical clustering algorithms render them inappropriate for the vast number of documents that must be handled daily. In this paper, we propose a new scalable hierarchical clustering framework, which uses the frequency of the topics in the documents to overcome these limitations. Our work consists of a binary tree construction algorithm that creates a hierarchy of the documents using three metrics (Identity, Entropy, Bin Similarity), and a branch breaking algorithm which composes the final clusters by applying thresholds to each branch of the tree. The clustering algorithm is followed by a meta-clustering module which makes use of graph theory to obtain insights in the leaf clusters’ connections. The feature vectors representing each document derive from topic modeling. At the implementation level, the clustering method has been dockerized in order to facilitate its deployment on cloud computing infrastructures. Finally, the proposed framework is evaluated on several datasets of varying size and content, achieving significant reduction in both memory consumption and computational time over existing hierarchical clustering algorithms. The experiments also include performance testing on cloud resources using different setups and the results are promising. © 2020, The Author(s).",2020,Journal of Cloud Computing,2,scalable big data analysis framework @ of paramount importance in @ modern web society @ is characterized by a huge number of resource including electronic text document @ document clustering is @ important field in text mining and is commonly used @ document organization browsing summarization and classification @ hierarchical clustering method construct a hierarchy structure @ combined @ @ produced cluster @ @ useful in managing document thus making @ browsing and navigation process easier and quicker and providing only relevant information to @ user query by leveraging @ structure relationship @ nevertheless @ high computational cost and memory usage of baseline hierarchical clustering algorithm render @ inappropriate @ @ vast number of document @ must @ handled daily @ in @ @ @ propose a @ scalable hierarchical clustering framework @ us @ frequency of @ topic in @ document to overcome @ limitation @ @ work consists of a binary tree construction algorithm @ creates a hierarchy of @ document @ three metric @ identity entropy bin similarity @ and a branch breaking algorithm @ composes @ final cluster by applying threshold to @ branch of @ tree @ @ clustering algorithm is followed by a meta-clustering module @ make use of graph theory to obtain insight in @ leaf cluster connection @ @ feature vector representing @ document derive @ topic modeling @ at @ implementation level @ clustering method ha @ dockerized in order to facilitate @ deployment on cloud computing infrastructure @ finally @ proposed framework is evaluated on several datasets of varying size and content achieving significant reduction in @ memory consumption and computational time @ existing hierarchical clustering algorithm @ @ experiment @ include performance testing on cloud resource @ different setup and @ @ @ promising @ @ author @ s @ @ 
51,Automatic question generation,"Automatic generation of semantically well-formed questions from a given text can contribute to various domains, including education, dialogues/interactive question answering systems, search engines, and more. It is well-known as a challenging task, which involves the common obstacles of other natural language processing (NLP) activities. We start this advanced review with a brief overview of the most common automatic question generation (AQG) applications. Then we describe the main steps of a typical AQG pipeline, namely question construction, ranking, and evaluation. Finally, we discuss the open challenges of the AQG field that still need to be addressed by NLP researchers. This article is categorized under: Algorithmic Development > Text Mining. © 2020 Wiley Periodicals LLC.",2020,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,0,automatic generation of semantically well-formed question @ a given text @ contribute to various domain including education dialogue interactive question answering system search engine and more @ @ is well-known a a challenging task @ involves @ common obstacle of @ natural language processing @ nlp @ activity @ @ start @ advanced review @ a brief overview of @ @ common automatic question generation @ aqg @ application @ @ @ describe @ main step of a typical aqg pipeline namely question construction ranking and evaluation @ finally @ discus @ open challenge of @ aqg field @ still need to @ addressed by nlp researcher @ @ article is categorized @ @ algorithmic development text mining @ wiley periodical llc @ 
52,A deep neural network model for speakers coreference resolution in legal texts,"Coreference resolution is one of the fundamental tasks in natural language processing (NLP), and is of great significance to understand the semantics of texts. Meanwhile, resolving coreference is essential for many NLP downstream applications. Existing methods largely focus on pronouns, possessives and noun phrases resolution in the general domain, while little work is proposed for professional domains such as the legal field. Different from general texts, how to code legal texts and capture the relationship between entities in the text, and then resolve coreference is a challenging problem. For better understanding the legal text, and facilitating a series of downstream tasks in legal text mining, we propose a deep neural network model for coreference resolution in court record documents. Specifically, the pre-trained language model and bi-directional long short-term memory networks are first utilized to encode legal texts. Second, graph neural networks are applied to incorporate reference relations between entities. Finally, two distinct classifiers are used to score the candidate pairs. Results on the dataset show that our model achieves 87.53% F1 score on court record documents, outperforming neural baseline models by a large margin. Further analysis shows that the proposed method can effectively identify the reference relations between entities and model the entity dependencies. © 2020 Elsevier Ltd",2020,Information Processing and Management,0,coreference resolution is @ of @ fundamental task in natural language processing @ nlp @ and is of great significance to understand @ semantics of text @ meanwhile resolving coreference is essential @ many nlp downstream application @ existing method largely focus on pronoun possessive and noun phrase resolution in @ general domain @ little work is proposed @ professional domain @ a @ legal field @ different @ general text @ to code legal text and capture @ relationship @ entity in @ text and @ resolve coreference is a challenging problem @ @ better understanding @ legal text and facilitating a series of downstream task in legal text mining @ propose a deep neural network model @ coreference resolution in court record document @ specifically @ pre-trained language model and bi-directional long short-term memory network @ first utilized to encode legal text @ second graph neural network @ applied to incorporate reference relation @ entity @ finally @ distinct classifier @ used to score @ candidate pair @ @ on @ dataset @ @ @ model achieves @ f score on court record document outperforming neural baseline model by a @ margin @ @ analysis @ @ @ proposed method @ effectively identify @ reference relation @ entity and model @ entity dependency @ @ ltd
54,Enriching domain concepts with qualitative attributes: A text mining based approach,"Attributes, whether qualitative or non-qualitative are the formal description of any real-world entity and are crucial in modern knowledge representation models like ontology. Though ample evidence for the amount of research done for mining non-qualitative attributes (like part-of relation) extraction from text as well as the Web is available in the wealth of literature, on the other side limited research can be found relating to qualitative attribute (i.e., size, color, taste etc.,) mining. Herein this research article an analytical framework has been proposed to retrieve qualitative attribute values from unstructured domain text. The research objective covers two aspects of information retrieval (1) acquiring quality values from unstructured text and (2) then assigning attribute to them by comparing the Google derived meaning or context of attributes as well as quality value (adjectives). The goal has been accomplished by using a framework which integrates Vector Space Modelling (VSM) with a probabilistic Multinomial Naive Bayes (MNB) classifier. Performance Evaluation has been carried out on two data sets (1) HeiPLAS Development Data set (106 adjective-noun exemplary phrases) and (2) a text data set in Medicinal Plant Domain (MPD). System is found to perform better with probabilistic approach compared to the existing pattern-based framework in the state of art. © 2020, Zarka Private University. All rights reserved.",2020,International Arab Journal of Information Technology,0,attribute whether qualitative @ non-qualitative @ @ formal description of @ real-world entity and @ crucial in modern knowledge representation model like ontology @ though ample evidence @ @ amount of research done @ mining non-qualitative attribute @ like part-of relation @ extraction @ text a well a @ web is available in @ wealth of literature on @ @ side limited research @ @ found relating to qualitative attribute @ i @ e @ size color taste etc @ @ mining @ herein @ research article @ analytical framework ha @ proposed to retrieve qualitative attribute value @ unstructured domain text @ @ research objective cover @ aspect of information retrieval @ @ acquiring quality value @ unstructured text and @ @ @ assigning attribute to @ by comparing @ google derived meaning @ context of attribute a well a quality value @ adjective @ @ @ goal ha @ accomplished by @ a framework @ integrates vector space modelling @ vsm @ @ a probabilistic multinomial naive bayes @ mnb @ classifier @ performance evaluation ha @ carried @ on @ data set @ @ heiplas development data set @ adjective-noun exemplary phrase @ and @ @ a text data set in medicinal plant domain @ mpd @ @ system is found to perform better @ probabilistic approach compared to @ existing pattern-based framework in @ state of art @ zarka private university @ @ right reserved @ 
55,Ontology-driven aspect-based sentiment analysis classification: An infodemiological case study regarding infectious diseases in Latin America,"Infodemiology is the process of mining unstructured and textual data so as to provide public health officials and policymakers with valuable information regarding public health. The appearance of this new data source, which was previously unimaginable, has opened up a new way in which to improve public health systems, resulting in better communication policies and better detection systems. However, the unstructured nature of the Internet, along with the complexity of the infectious disease domain, prevents the information extracted from being easily understood. Moreover, when dealing with languages other than English, for which some of the most common Natural Language Processing resources are not available, the correct exploitation of this data becomes even more difficult. We intend to fill these gaps proposing an ontology-driven aspect-based sentiment analysis with which to measure the general public's opinions as regards infectious diseases when expressed in Spanish by employing a case study of tweets concerning the Zika, Dengue and Chikungunya viruses in Latin America. Our proposal is based on two technologies. We first use ontologies in order to model the infectious disease domain with concepts such as risks, symptoms, transmission methods or drugs, among other concepts. We then measure the relationship between these concepts in order to determine the degree to which one concept influences other concepts. This new information is subsequently applied in order to build an aspect-based sentiment analysis model based on statistical and linguistic features. This is done by applying deep-learning models. Our proposal is available on a web platform, where users can see the sentiment for each concept at a glance and analyse how each concept influences the sentiment of the others. © 2020 Elsevier B.V.",2020,Future Generation Computer Systems,9,infodemiology is @ process of mining unstructured and textual data @ a to provide public health official and policymakers @ valuable information regarding public health @ @ appearance of @ @ data source @ wa @ unimaginable ha opened up a @ way in @ to improve public health system resulting in better communication policy and better detection system @ however @ unstructured nature of @ internet along @ @ complexity of @ infectious disease domain prevents @ information extracted @ @ easily understood @ moreover @ dealing @ language @ @ english @ @ some of @ @ common natural language processing resource @ not available @ correct exploitation of @ data becomes even more difficult @ @ intend to fill @ gap proposing @ ontology-driven aspect-based sentiment analysis @ @ to measure @ general public @ s opinion a regard infectious disease @ expressed in spanish by employing a case study of tweet concerning @ zika dengue and chikungunya virus in latin america @ @ proposal is based on @ technology @ @ first use ontology in order to model @ infectious disease domain @ concept @ a risk symptom transmission method @ drug among @ concept @ @ @ measure @ relationship @ @ concept in order to determine @ degree to @ @ concept influence @ concept @ @ @ information is subsequently applied in order to build @ aspect-based sentiment analysis model based on statistical and linguistic feature @ @ is done by applying deep-learning model @ @ proposal is available on a web platform @ user @ see @ sentiment @ @ concept at a glance and analyse @ @ concept influence @ sentiment of @ others @ @ b @ v @ 
57,Topic Evolution and Emerging Topic Analysis Based on Open Source Software,"We present an analytical, open source and flexible natural language processing and text mining method for topic evolution, emerging topic detection and research trend forecasting for all kinds of data-tagged text. We make full use of the functions provided by the open source VOSviewer and Microsoft Office, including a thesaurus for data clean-up and a LOOKUP function for comparative analysis. Through application and verification in the domain of perovskite solar cells research, this method proves to be effective. A certain amount of manual data processing and a specific research domain background are required for better, more illustrative analysis results. Adequate time for analysis is also necessary. We try to set up an easy, useful, and flexible interdisciplinary text analyzing procedure for researchers, especially those without solid computer programming skills or who cannot easily access complex software. This procedure can also serve as a wonderful example for teaching information literacy. This text analysis approach has not been reported before. © 2020 2020 Xiang Shen et al., published by Sciendo.",2020,Journal of Data and Information Science,1,@ @ @ analytical open source and flexible natural language processing and text mining method @ topic evolution emerging topic detection and research trend forecasting @ @ kind of data-tagged text @ @ make full use of @ function provided by @ open source vosviewer and microsoft office including a thesaurus @ data clean-up and a lookup function @ comparative analysis @ @ application and verification in @ domain of perovskite solar cell research @ method prof to @ effective @ a certain amount of manual data processing and a specific research domain background @ required @ better more illustrative analysis @ @ adequate time @ analysis is @ necessary @ @ try to set up @ easy useful and flexible interdisciplinary text analyzing procedure @ researcher especially @ without solid computer programming skill @ @ cannot easily access complex software @ @ procedure @ @ serve a a wonderful example @ teaching information literacy @ @ text analysis approach ha not @ reported @ @ xiang shen et al @ published by sciendo @ 
62,"Semantic analysis-based relevant data retrieval model using feature selection, summarization and CNN","Semantic analysis is playing a major role and task in text mining process caused by the presence of huge number of relevant and irrelevant data in Internet and other resources. Here, the semantic-based text summarization must be incorporated for the successful relevant data extraction by using data classification. The accurate classification process is done by using deep learning techniques recently. However, no existing model is achieved reasonable relevancy accuracy. For overcoming the drawbacks, we propose an effective semantic analysis-based relevant data retrieval model for retrieving the relevant data from local repository or web applications in Internet. This new model consists of (i) semantic similarity-based feature selection and (ii) enrichment technique, (iii) data summarization technique and iv) text relationship-based deep neural network classifier. Here, we propose a new semantic analysis-based feature selection algorithm to select the similarity indexed relevant data from local repositories or web applications. In addition, a new semantic-based data summarization technique is also introduced for summarizing the text that is available in the online resources. Finally, a new semantic similarity-based deep neural network-based classifier is also introduced for categorizingthe data according to the semantic relation. The proposed model is proved the effectiveness of the data retrieval process by conducting various experiments based on the relevant data extraction from Internet resources, and it also tested with the recognized datasets. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",2020,Soft Computing,0,semantic analysis is playing a major role and task in text mining process caused by @ presence of huge number of relevant and irrelevant data in internet and @ resource @ @ @ semantic-based text summarization must @ incorporated @ @ successful relevant data extraction by @ data classification @ @ accurate classification process is done by @ deep learning technique recently @ however no existing model is achieved reasonable relevancy accuracy @ @ overcoming @ drawback @ propose @ effective semantic analysis-based relevant data retrieval model @ retrieving @ relevant data @ local repository @ web application in internet @ @ @ model consists of @ i @ semantic similarity-based feature selection and @ ii @ enrichment technique @ iii @ data summarization technique and @ @ text relationship-based deep neural network classifier @ @ @ propose a @ semantic analysis-based feature selection algorithm to select @ similarity indexed relevant data @ local repository @ web application @ in addition a @ semantic-based data summarization technique is @ introduced @ summarizing @ text @ is available in @ online resource @ finally a @ semantic similarity-based deep neural network-based classifier is @ introduced @ categorizingthe data according to @ semantic relation @ @ proposed model is proved @ effectiveness of @ data retrieval process by conducting various experiment based on @ relevant data extraction @ internet resource and @ @ tested @ @ recognized datasets @ springer-verlag gmbh germany part of @ nature @ 
64,Building a morpho-semantic knowledge graph for Arabic information retrieval,"In this paper, we propose to build a morpho-semantic knowledge graph from Arabic vocalized corpora. Our work focuses on classical Arabic as it has not been deeply investigated in related works. We use a tool suite which allows analyzing and disambiguating Arabic texts, taking into account short diacritics to reduce ambiguities. At the morphological level, we combine Ghwanmeh stemmer and MADAMIRA which are adapted to extract a multi-level lexicon from Arabic vocalized corpora. At the semantic level, we infer semantic dependencies between tokens by exploiting contextual knowledge extracted by a concordancer. Both morphological and semantic links are represented through compressed graphs, which are accessed through lazy methods. These graphs are mined using a measure inspired from BM25 to compute one-to-many similarity. Indeed, we propose to evaluate the morpho-semantic Knowledge Graph in the context of Arabic Information Retrieval (IR). Several scenarios of document indexing and query expansion are assessed. That is, we vary indexing units for Arabic IR based on different levels of morphological knowledge, a challenging issue which is not yet resolved in previous research. We also experiment several combinations of morpho-semantic query expansion. This permits to validate our resource and to study its impact on IR based on state-of-the art evaluation metrics. © 2019 Elsevier Ltd",2020,Information Processing and Management,6,in @ @ @ propose to build a morpho-semantic knowledge graph @ arabic vocalized corpus @ @ work focus on classical arabic a @ ha not @ deeply investigated in related work @ @ use a tool suite @ allows analyzing and disambiguating arabic text taking @ account short diacritic to reduce ambiguity @ at @ morphological level @ combine ghwanmeh stemmer and madamira @ @ adapted to extract a multi-level lexicon @ arabic vocalized corpus @ at @ semantic level @ infer semantic dependency @ token by exploiting contextual knowledge extracted by a concordancer @ @ morphological and semantic link @ represented @ compressed graph @ @ accessed @ lazy method @ @ graph @ mined @ a measure inspired @ bm to compute one-to-many similarity @ indeed @ propose to evaluate @ morpho-semantic knowledge graph in @ context of arabic information retrieval @ ir @ @ several scenario of document indexing and query expansion @ assessed @ @ is @ vary indexing unit @ arabic ir based on different level of morphological knowledge a challenging issue @ is not yet resolved in previous research @ @ @ experiment several combination of morpho-semantic query expansion @ @ permit to validate @ resource and to study @ impact on ir based on state-of-the art evaluation metric @ @ ltd
75,A Framework for Understanding the Relationship between Social Media Discourse and Mental Health,"Over 35% of the world's population uses social media. Platforms like Facebook, Twitter, and Instagram have radically influenced the way individuals interact and communicate. These platforms facilitate both public and private communication with strangers and friends alike, providing rich insight into an individual's personality, health, and wellbeing. To date, many researchers have employed a variety of methods for extracting mental health-centric features from digital text communication (DTC) data, including natural language processing, social network analysis, and extraction of temporal discourse patterns. However, none have explored a hierarchical framework for extracting features from private messages with the goal of unifying approaches across methodological domains. Furthermore, while analyses of large, public corpora abound in existing literature, limited work has been done to explore the relationship between of private textual communications, personality traits, and symptoms of mental illness. We present a framework for constructing rich feature spaces from digital text communications. We then demonstrate the efficacy of our framework by applying it to a dataset of private Facebook messages in a college student population (N=103). Our results reveal key individual differences in temporal and relational behaviors, as well as language usage in relation to validated measures of trait-level anxiety, loneliness, and personality. This work represents a critical step forward in linking features of private social media messages to validated measures of mental health, wellbeing, and personality. © 2020 ACM.",2020,Proceedings of the ACM on Human-Computer Interaction,0,@ of @ world @ s population us social medium @ platform like facebook twitter and instagram @ radically influenced @ way individual interact and communicate @ @ platform facilitate @ public and private communication @ stranger and friend alike providing rich insight @ @ individual @ s personality health and wellbeing @ to date many researcher @ employed a variety of method @ extracting mental health-centric feature @ digital text communication @ dtc @ data including natural language processing social network analysis and extraction of temporal discourse pattern @ however none @ explored a hierarchical framework @ extracting feature @ private message @ @ goal of unifying approach across methodological domain @ furthermore @ analysis of @ public corpus abound in existing literature limited work ha @ done to explore @ relationship @ of private textual communication personality trait and symptom of mental illness @ @ @ a framework @ constructing rich feature space @ digital text communication @ @ @ demonstrate @ efficacy of @ framework by applying @ to a dataset of private facebook message in a college student population @ n @ @ @ @ reveal key individual difference in temporal and relational behavior a well a language usage in relation to validated measure of trait-level anxiety loneliness and personality @ @ work represents a critical step forward in linking feature of private social medium message to validated measure of mental health wellbeing and personality @ acm @ 
81,A prior case study of natural language processing on different domain,"In the present state of digital world, computer machine do not understand the human's ordinary language. This is the great barrier between humans and digital systems. Hence, researchers found an advanced technology that provides information to the users from the digital machine. However, natural language processing (i.e. NLP) is a branch of AI that has significant implication on the ways that computer machine and humans can interact. NLP has become an essential technology in bridging the communication gap between humans and digital data. Thus, this study provides the necessity of the NLP in the current computing world along with different approaches and their applications. It also, highlights the key challenges in the development of new NLP model. Copyright © 2020Institute of Advanced Engineering and Science. All rights reserved.",2020,International Journal of Electrical and Computer Engineering,0,in @ @ state of digital world computer machine @ not understand @ human @ s ordinary language @ @ is @ great barrier @ human and digital system @ hence researcher found @ advanced technology @ provides information to @ user @ @ digital machine @ however natural language processing @ i @ e @ nlp @ is a branch of ai @ ha significant implication on @ way @ computer machine and human @ interact @ nlp ha become @ essential technology in bridging @ communication gap @ human and digital data @ thus @ study provides @ necessity of @ nlp in @ current computing world along @ different approach and @ application @ @ @ highlight @ key challenge in @ development of @ nlp model @ @ institute of advanced engineering and science @ @ right reserved @ 
86,Research on Text Classification Based on Automatically Extracted Keywords,"Automatic keywords extraction and classification tasks are important research directions in the domains of NLP (natural language processing), information retrieval, and text mining. As the fine granularity abstracted from text data, keywords are also the most important feature of text data, which has great practical and potential value in document classification, topic modeling, information retrieval, and other aspects. The compact representation of documents can be achieved through keywords, which contains massive significant information. Therefore, it may be quite advantageous to realize text classification with high-dimensional feature space. For this reason, this study designed a supervised keyword classification method based on TextRank keyword automatic extraction technology and optimize the model with the genetic algorithm to contribute to modeling the keywords of the topic for text classification. Copyright © 2020, IGI Global.",2020,International Journal of Enterprise Information Systems,0,automatic keywords extraction and classification task @ important research direction in @ domain of nlp @ natural language processing @ information retrieval and text mining @ a @ fine granularity abstracted @ text data keywords @ @ @ @ important feature of text data @ ha great practical and potential value in document classification topic modeling information retrieval and @ aspect @ @ compact representation of document @ @ achieved @ keywords @ contains massive significant information @ therefore @ may @ quite advantageous to realize text classification @ high-dimensional feature space @ @ @ reason @ study designed a supervised keyword classification method based on textrank keyword automatic extraction technology and optimize @ model @ @ genetic algorithm to contribute to modeling @ keywords of @ topic @ text classification @ @ igi global @ 
92,Chinese Clinical Named Entity Recognition Based on Stroke ELMo and Multi-Task Learning,"In recent years, the number of electronic medical record text has grown substantially, which provides a rich source of knowledge for medical research. According to the medical domain demand, effective text mining technology can obtain medical related information from the massive electronic medical records efficiently and accurately, which will greatly promote the research in the medical health field. Chinese Clinical Named Entity Recognition (CNER) is a fundamental task for Chinese medical information extraction, which has received much attention. However, most of the existing Chinese CNER works are based on traditional text representation embeddings (i.e., context-independent representation for each word) and depend on effective feature engineering to improve the performance of models in the medical field. There is less related work in Chinese biomedical pretrained text embeddings. In addition, the existing Chinese CNER dataset size is small, and medical entity annotation requires medical background knowledge, which is time-consuming and labor-intensive. To address the problems, this paper proposes a Chinese CNER method based on stroke ELMo and multi-task learning. Firstly, a stroke ELMo (Embeddings from Language Models) model is proposed to obtain Chinese pretrained text representation. The ELMo method is improved by taking the stroke sequence as input. It is a context-dependent representation method and can learn rich structure information of the Chinese characters from the large Chinese biomedical text corpus. To learn high quality Chinese biomedical text representations, the massive Chinese medical abstracts were downloaded from the CNKI website. Then these abstracts and the Chinese electronic medical record texts provided by the China Conference on Knowledge Graph and Semantic Computing (CCKS) challenge were used to train the stroke ELMo embeddings. The experimental results show that stroke ELMo embeddings achieve the better performance than the traditional word2vec embeddings. When the concatenation of the word2vec and stroke ELMo embeddings as input is fed into the model, the model obtains the best performance. Secondly, we explored the effect of multi-task learning on the Chinese CNER task. The single task model, fully-shared multi-task learning model and shared-private multi-task learning model are compared on the CCKS17 and CCKS18 data sets. The experimental results show that the shared-private multi-task learning model achieves the best F-score. It can utilize the correlation of the tasks to improve the model performance and make full use of the existing datasets. We also tested the performance of the multi-task learning model on the different sizes training data sets. The shared-private multi-task learning model trained on only 60% of the training data can achieve better performances than the single task model trained on the complete training data on the CCKS17 and CCKS18 CNER datasets. Moreover, the effects of common NER features (i.e., word embedding, dictionary and radical features) and neural network models (i.e., CNN, BiLSTM, CNN-CRF and BiLSTM-CRF models) were investigated for the Chinese CNER task. The experimental results show that the BiLSTM-CRF model outperforms the other models. Among other features, the dictionary feature is most effective. Finally, compared with other existing methods, our neural network model based on stroke ELMo and multi-task learning achieves better performances on the CCKS17 and CCKS18 CNER datasets (the F-scores of 91.75% and 90.05%, respectively). © 2020, Science Press. All right reserved.",2020,Jisuanji Xuebao/Chinese Journal of Computers,0,in recent year @ number of electronic medical record text ha grown substantially @ provides a rich source of knowledge @ medical research @ according to @ medical domain demand effective text mining technology @ obtain medical related information @ @ massive electronic medical record efficiently and accurately @ @ greatly promote @ research in @ medical health field @ chinese clinical named entity recognition @ cner @ is a fundamental task @ chinese medical information extraction @ ha received much attention @ however @ of @ existing chinese cner work @ based on traditional text representation embeddings @ i @ e @ context-independent representation @ @ word @ and depend on effective feature engineering to improve @ performance of model in @ medical field @ @ is le related work in chinese biomedical pretrained text embeddings @ in addition @ existing chinese cner dataset size is small and medical entity annotation requires medical background knowledge @ is time-consuming and labor-intensive @ to address @ problem @ @ proposes a chinese cner method based on stroke elmo and multi-task learning @ firstly a stroke elmo @ embeddings @ language model @ model is proposed to obtain chinese pretrained text representation @ @ elmo method is improved by taking @ stroke sequence a input @ @ is a context-dependent representation method and @ learn rich structure information of @ chinese character @ @ @ chinese biomedical text corpus @ to learn high quality chinese biomedical text representation @ massive chinese medical abstract @ downloaded @ @ cnki website @ @ @ abstract and @ chinese electronic medical record text provided by @ china conference on knowledge graph and semantic computing @ ccks @ challenge @ used to train @ stroke elmo embeddings @ @ experimental @ @ @ stroke elmo embeddings achieve @ better performance @ @ traditional word vec embeddings @ @ @ concatenation of @ word vec and stroke elmo embeddings a input is fed @ @ model @ model obtains @ best performance @ secondly @ explored @ effect of multi-task learning on @ chinese cner task @ @ single task model fully-shared multi-task learning model and shared-private multi-task learning model @ compared on @ ccks and ccks data set @ @ experimental @ @ @ @ shared-private multi-task learning model achieves @ best f-score @ @ @ utilize @ correlation of @ task to improve @ model performance and make full use of @ existing datasets @ @ @ tested @ performance of @ multi-task learning model on @ different size training data set @ @ shared-private multi-task learning model trained on only of @ training data @ achieve better performance @ @ single task model trained on @ complete training data on @ ccks and ccks cner datasets @ moreover @ effect of common ner feature @ i @ e @ word embedding dictionary and radical feature @ and neural network model @ i @ e @ cnn bilstm cnn-crf and bilstm-crf model @ @ investigated @ @ chinese cner task @ @ experimental @ @ @ @ bilstm-crf model outperforms @ @ model @ among @ feature @ dictionary feature is @ effective @ finally compared @ @ existing method @ neural network model based on stroke elmo and multi-task learning achieves better performance on @ ccks and ccks cner datasets @ @ f-scores of @ and @ respectively @ @ science @ @ @ right reserved @ 
93,Role of Educational Data Mining in Student Learning Processes with Sentiment Analysis: A Survey,"Educational data mining is a research field that is used to enhance education system. Research studies using educational data mining are in increase because of the knowledge acquired for decision making to enhance the education process by the information retrieved by machine learning processes. Sentiment analysis is one of the most involved research fields of data mining in natural language processing, web mining, and text mining. It plays a vital role in many areas such as management sciences and social sciences, including education. In education, investigating students’ opinions, emotions using techniques of sentiment analysis can understand the students’ feelings that students experience in academic, personal, and societal environments. This investigation with sentiment analysis helps the academicians and other stakeholders to understand their motive on education is online. This article intends to explore different theories on education, students’ learning process, and to study different approaches of sentiment analysis academics. Copyright © 2020, IGI Global.",2020,International Journal of Knowledge and  Systems Science,0,educational data mining is a research field @ is used to enhance education system @ research study @ educational data mining @ in increase @ of @ knowledge acquired @ decision making to enhance @ education process by @ information retrieved by machine learning process @ sentiment analysis is @ of @ @ involved research field of data mining in natural language processing web mining and text mining @ @ play a vital role in many area @ a management science and social science including education @ in education investigating student opinion emotion @ technique of sentiment analysis @ understand @ student feeling @ student experience in @ personal and societal environment @ @ investigation @ sentiment analysis help @ academician and @ stakeholder to understand @ motive on education is online @ @ article intends to explore different theory on education student learning process and to study different approach of sentiment analysis @ @ @ igi global @ 
95,Feature-Level Rating System Using Customer Reviews and Review Votes,"This work studies how we can obtain feature-level ratings of the mobile products from the customer reviews and review votes to influence decision-making, both for new customers and manufacturers. Such a rating system gives a more comprehensive picture of the product than what a product-level rating system offers. While product-level ratings are too generic, feature-level ratings are particular; we exactly know what is good or bad about the product. There has always been a need to know which features fall short or are doing well according to the customer's perception. It keeps both the manufacturer and the customer well-informed in the decisions to make in improving the product and buying, respectively. Different customers are interested in different features. Thus, feature-level ratings can make buying decisions personalized. We analyze the customer reviews collected on an online shopping site (Amazon) about various mobile products and the review votes. Explicitly, we carry out a feature-focused sentiment analysis for this purpose. Eventually, our analysis yields ratings to 108 features for 4000+ mobiles sold online. It helps in decision-making on how to improve the product (from the manufacturer's perspective) and in making the personalized buying decisions (from the buyer's perspective) a possibility. Our analysis has applications in recommender systems, consumer research, and so on. © 2014 IEEE.",2020,IEEE Transactions on Computational Social Systems,2,@ work study @ @ @ obtain feature-level rating of @ mobile product @ @ customer review and review vote to influence decision-making @ @ @ customer and manufacturer @ @ a rating system give a more comprehensive picture of @ product @ @ a product-level rating system offer @ @ product-level rating @ too generic feature-level rating @ particular @ @ exactly know @ is good @ bad @ @ product @ @ ha always @ a need to know @ feature fall short @ @ @ well according to @ customer @ s perception @ @ keep @ @ manufacturer and @ customer well-informed in @ decision to make in improving @ product and buying respectively @ different customer @ interested in different feature @ thus feature-level rating @ make buying decision personalized @ @ analyze @ customer review collected on @ online shopping site @ amazon @ @ various mobile product and @ review vote @ explicitly @ carry @ a feature-focused sentiment analysis @ @ purpose @ eventually @ analysis yield rating to feature @ mobile sold online @ @ help in decision-making on @ to improve @ product @ @ @ manufacturer @ s perspective @ and in making @ personalized buying decision @ @ @ buyer @ s perspective @ a possibility @ @ analysis ha application in recommender system consumer research and @ on @ @ @ 
109,Extracting supply chain maps from news articles using deep neural networks,"Supply chains are increasingly global, complex and multi-tiered. Consequently, companies often struggle to maintain complete visibility of their supply network. This poses a problem as visibility of the network structure is required for tasks like effectively managing supply chain risk. In this paper, we discuss automated supply chain mapping as a means of maintaining structural visibility of a company's supply chain, and we use Deep Learning to automatically extract buyer–supplier relations from natural language text. Early results show that supply chain mapping solutions using Natural Language Processing and Deep Learning could enable companies to (a) automatically generate rudimentary supply chain maps, (b) verify existing supply chain maps, or (c) augment existing maps with additional supplier information. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2020,International Journal of Production Research,2,supply chain @ increasingly global complex and multi-tiered @ consequently company often struggle to maintain complete visibility of @ supply network @ @ pose a problem a visibility of @ network structure is required @ task like effectively managing supply chain risk @ in @ @ @ discus automated supply chain mapping a a mean of maintaining structural visibility of a company @ s supply chain and @ use deep learning to automatically extract buyer supplier relation @ natural language text @ early @ @ @ supply chain mapping solution @ natural language processing and deep learning could enable company to @ a @ automatically generate rudimentary supply chain map @ b @ verify existing supply chain map @ @ c @ augment existing map @ additional supplier information @ informa uk limited trading a taylor francis group @ 
110,An ontological artifact for classifying social media: Text mining analysis for financial data,"In this paper we utilize a structured natural language processing implementation of the Financial Industry Business Ontology (FIBO) to extract financial information from the unstructured textual data of the social media platform Twitter regarding financial and budget information in the public sector, namely the two public-private agencies of the Port Authority of NY and NJ (PANYNJ), and the NY Metropolitan Transportation Agency (MTA). This research initiative uses the Design Science Research (DSR) perspective to develop an artifact to classify tweets as being either relevant to financial bonds or not. We apply a frame and slot approach from the artificial intelligence and natural language processing literature to operationalize this artifact. FIBO provides standards for defining the facts, terms, and relationships associated with financial concepts. We show that FIBO grammar can be used to mine semantic meaning from unstructured textual data and that it provides a nuanced representation of structured financial data. With this artifact, social media such as Twitter may be accessed for the knowledge that its text contains about financial concepts using the FIBO ontology. This process is anticipated to be of interest to bond issuers, regulators, analysts, investors, and academics. It may also be extended towards other financial domains such as securities, derivatives, commodities, and banking that relate to FIBO ontologies, as well as more generally to develop a structured knowledge representation of unstructured data through the application of an ontology. © 2020 Elsevier Inc.",2020,International Journal of Accounting Information Systems,0,in @ @ @ utilize a structured natural language processing implementation of @ financial industry @ ontology @ fibo @ to extract financial information @ @ unstructured textual data of @ social medium platform twitter regarding financial and budget information in @ public sector namely @ @ public-private agency of @ port authority of ny and nj @ panynj @ and @ ny metropolitan transportation agency @ mta @ @ @ research initiative us @ design science research @ dsr @ perspective to develop @ artifact to classify tweet a @ either relevant to financial bond @ not @ @ apply a frame and slot approach @ @ artificial intelligence and natural language processing literature to operationalize @ artifact @ fibo provides standard @ defining @ fact term and relationship associated @ financial concept @ @ @ @ fibo grammar @ @ used to mine semantic meaning @ unstructured textual data and @ @ provides a nuanced representation of structured financial data @ @ @ artifact social medium @ a twitter may @ accessed @ @ knowledge @ @ text contains @ financial concept @ @ fibo ontology @ @ process is anticipated to @ of interest to bond issuer regulator analyst investor and @ @ @ may @ @ extended towards @ financial domain @ a security derivative commodity and banking @ relate to fibo ontology a well a more generally to develop a structured knowledge representation of unstructured data @ @ application of @ ontology @ @ inc @ 
111,A Discriminative Convolutional Neural Network with Context-Aware Attention,"Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting n-gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual n-gram features adaptively. In this article, we propose a discriminative CNN with context-Aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model. © 2020 ACM.",2020,ACM Transactions on Intelligent Systems and Technology,0,feature representation and feature extraction @ @ crucial procedure in text mining @ convolutional neural network @ cnn @ @ @ overwhelming success @ text-mining task since @ @ capable of efficiently extracting n-gram feature @ source data @ however vanilla cnn ha @ @ weakness on feature representation and feature extraction @ a certain amount of filter in cnn @ inevitably duplicate and thus hinder to discriminatively represent a given text @ in addition @ existing cnn model extract feature in a fixed way @ i @ e @ max pooling @ @ either limit @ cnn to local optimum @ without considering @ relation @ @ feature thereby unable to learn a contextual n-gram feature adaptively @ in @ article @ propose a discriminative cnn @ context-aware attention to solve @ challenge of vanilla cnn @ specifically @ model mainly encourages discrimination across different filter via maximizing @ earth mover distance and estimate @ salience of feature candidate by considering @ relation @ context feature @ @ validate carefully @ finding @ baseline on five benchmark datasets of classification and @ datasets of summarization @ @ @ of @ experiment verify @ competitive performance of @ proposed model @ acm @ 
114,Identification of malignancies from free-text histopathology reports using a multi-model supervised machine learning approach,"We explored various Machine Learning (ML) models to evaluate how each model performs in the task of classifying histopathology reports. We trained, optimized, and performed classification with Stochastic Gradient Descent (SGD), Support Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbor (KNN), Adaptive Boosting (AB), Decision Trees (DT), Gaussian Naïve Bayes (GNB), Logistic Regression (LR), and Dummy classifier. We started with 60,083 histopathology reports, which reduced to 60,069 after pre-processing. The F1-scores for SVM, SGD KNN, RF, DT, LR, AB, and GNB were 97%, 96%, 96%, 96%, 92%, 96%, 84%, and 88%, respectively, while the misclassification rates were 3.31%, 5.25%, 4.39%, 1.75%, 3.5%, 4.26%, 23.9%, and 19.94%, respectively. The approximate run times were 2 h, 20 min, 40 min, 8 h, 40 min, 10 min, 50 min, and 4 min, respectively. RF had the longest run time but the lowest misclassification rate on the labeled data. Our study demonstrated the possibility of applying ML techniques in the processing of free-text pathology reports for cancer registries for cancer incidence reporting in a Sub-Saharan Africa setting. This is an important consideration for the resource-constrained environments to leverage ML techniques to reduce workloads and improve the timeliness of reporting of cancer statistics. © 2020 by the authors.",2020,Information (Switzerland),0,@ explored various machine learning @ ml @ model to evaluate @ @ model performs in @ task of classifying histopathology report @ @ trained optimized and performed classification @ stochastic gradient descent @ sgd @ support vector machine @ svm @ random forest @ rf @ k-nearest neighbor @ knn @ adaptive boosting @ ab @ decision tree @ dt @ gaussian naïve bayes @ gnb @ logistic regression @ lr @ and dummy classifier @ @ started @ histopathology report @ reduced to @ pre-processing @ @ f score @ svm sgd knn rf dt lr ab and gnb @ and respectively @ @ misclassification rate @ @ @ @ @ @ @ @ and @ respectively @ @ approximate run time @ h min min h min min min and min respectively @ rf @ @ longest run time @ @ lowest misclassification rate on @ labeled data @ @ study demonstrated @ possibility of applying ml technique in @ processing of free-text pathology report @ cancer registry @ cancer incidence reporting in a sub-saharan africa setting @ @ is @ important consideration @ @ resource-constrained environment to leverage ml technique to reduce workload and improve @ timeliness of reporting of cancer statistic @ by @ author @ 
116,An adaptation of a F-measure for automatic text summarization by extraction,"In this paper, we propose to adapt the F-measure to evaluate an automatic summaries of texts; we the main key to our proposal is to prove that the automatic summary task can be modeled in supervised classification. First, we will start the research by to make a comparison between the automatic summary task and the supervised classification. After that, we are going to define how to draw a confusion matrix which classification evaluation base, and from it we calculate the F-Measure. In this vein, we must prove that the new measure is valid and trustworthy, And for this we will calculates the correlation with ROUGE Evaluation. Before ending we analyze and interpret the main results in order to answer the questions put forward in this research. At the end, we are going to conclude our study with a set of facts based on the collected data. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Cluster Computing,0,in @ @ @ propose to adapt @ f-measure to evaluate @ automatic summary of text @ @ @ main key to @ proposal is to prove @ @ automatic summary task @ @ modeled in supervised classification @ first @ @ start @ research by to make a comparison @ @ automatic summary task and @ supervised classification @ @ @ @ @ going to define @ to draw a confusion matrix @ classification evaluation base and @ @ @ calculate @ f-measure @ in @ vein @ must prove @ @ @ measure is valid and trustworthy and @ @ @ @ calculates @ correlation @ rouge evaluation @ @ ending @ analyze and interpret @ main @ in order to answer @ question put forward in @ research @ at @ end @ @ going to conclude @ study @ a set of fact based on @ collected data @ @ science @ medium llc part of @ nature @ 
119,Mining insights from large-scale corpora using fine-tuned language models,"Mining insights from large volume of social media texts with minimal supervision is a highly challenging Natural Language Processing (NLP) task. While Language Models' (LMs) efficacy in several downstream tasks is well-studied, assessing their applicability in answering relational questions, tracking perception or mining deeper insights is under-explored. Few recent lines of work have scratched the surface by studying pre-trained LMs' (e.g., BERT) capability in answering relational questions through 'fill-in-the-blank' cloze statements (e.g., [Dante was born in MASK]). BERT predicts the MASK-ed word with a list of words ranked by probability (in this case, BERT successfully predicts Florence with the highest probability). In this paper, we conduct a feasibility study of fine-tuned LMs with a different focus on tracking polls, tracking community perception and mining deeper insights typically obtained through costly surveys. Our main focus is on a substantial corpus of video comments extracted from YouTube videos (6,182,868 comments on 130,067 videos by 1,518,077 users) posted within 100 days prior to the 2019 Indian General Election. Using fill-in-the-blank cloze statements against a recent high-performance language modeling algorithm, BERT, we present a novel application of this family of tools that is able to (1) aggregate political sentiment (2) reveal community perception and (3) track evolving national priorities and issues of interest. © 2020 The authors and IOS Press.",2020,Frontiers in Artificial Intelligence and Applications,1,mining insight @ @ volume of social medium text @ minimal supervision is a highly challenging natural language processing @ nlp @ task @ @ language model @ @ lm @ efficacy in several downstream task is well-studied assessing @ applicability in answering relational question tracking perception @ mining deeper insight is under-explored @ @ recent line of work @ scratched @ surface by studying pre-trained lm @ @ e @ g @ bert @ capability in answering relational question @ @ fill-in-the-blank @ cloze statement @ e @ g @ dante wa born in mask @ @ bert predicts @ mask-ed word @ a list of word ranked by probability @ in @ case bert successfully predicts florence @ @ highest probability @ @ in @ @ @ conduct a feasibility study of fine-tuned lm @ a different focus on tracking poll tracking community perception and mining deeper insight typically obtained @ costly survey @ @ main focus is on a substantial corpus of video comment extracted @ youtube video @ comment on video by user @ posted within day prior to @ indian general election @ @ fill-in-the-blank cloze statement @ a recent high-performance language modeling algorithm bert @ @ a novel application of @ family of tool @ is able to @ @ aggregate political sentiment @ @ reveal community perception and @ @ track evolving national priority and issue of interest @ @ author and io @ @ 
128,A Method of Interest Degree Mining Based on Behavior Data Analysis,"Based on big data, this paper starts from the behavior data of users on social media, and studies and explores the core issues of user modeling under personalized services. Focusing on the goal of user interest modeling, this paper proposes corresponding improvement measures for the existing interest model, which has great difference in interest description among different users and it is difficult to find the user interest change in time. For the above problems, this paper takes user-generated content and user behavior information as the analysis object, and uses natural language processing, knowledge warehouse, data fusion and other methods and techniques to numerically analyze user interest mining based on text mining and multi-source data fusion. We propose a user interest label space mapping method to avoid data sparse problem caused by too many dimensions in interest analysis. At the same time, we propose a method to extract and blend the long-Term and short-Term interests, and realize the comprehensive evaluation of interests. In the analysis of the big data phase, the user preference social property application preference value law, it is expected to achieve user Internet social media application preference data mining from the perspective of big data. © 2020 World Scientific Publishing Company.",2020,International Journal of Pattern Recognition and Artificial Intelligence,0,based on big data @ @ start @ @ behavior data of user on social medium and study and explores @ core issue of user modeling @ personalized service @ focusing on @ goal of user interest modeling @ @ proposes corresponding improvement measure @ @ existing interest model @ ha great difference in interest description among different user and @ is difficult to find @ user interest change in time @ @ @ @ problem @ @ take user-generated content and user behavior information a @ analysis object and us natural language processing knowledge warehouse data fusion and @ method and technique to numerically analyze user interest mining based on text mining and multi-source data fusion @ @ propose a user interest label space mapping method to avoid data sparse problem caused by too many dimension in interest analysis @ at @ @ time @ propose a method to extract and blend @ long-term and short-term interest and realize @ comprehensive evaluation of interest @ in @ analysis of @ big data phase @ user preference social property application preference value law @ is expected to achieve user internet social medium application preference data mining @ @ perspective of big data @ world scientific publishing company @ 
132,Indonesian news classification using convolutional neural network,"Every language has unique characteristics, structures, and grammar. Thus, different styles will have different processes and result in processed in Natural Language Processing (NLP) research area. In the current NLP research area, Data Mining (DM) or Machine Learning (ML) technique is popular, especially for Deep Learning (DL) method. This research aims to classify text data in the Indonesian language using Convolutional Neural Network (CNN) as one of the DL algorithms. The CNN algorithm used modified following the Indonesian language characteristics. Thereby, in the text pre-processing phase, stopword removal and stemming are particularly suitable for the Indonesian language. The experiment conducted using 472 Indonesian News text data from various sources with four categories: .,hiburan? (entertainment), .,olahraga? (sport), .,tajuk utama? (headline news), and .,teknologi? (technology). Based on the experiment and evaluation using 377 training data and 95 testing data, producing five models with ten epoch for each model, CNN has the best percentage of accuracy around 90,74% and loss value around 29,05% for 300 hidden layers in classifying the Indonesian News data. © Advanced Scientific Research. All rights reserved.",2020,Indonesian Journal of Electrical Engineering and Computer Science,0,every language ha unique characteristic structure and grammar @ thus different style @ @ different process and @ in processed in natural language processing @ nlp @ research area @ in @ current nlp research area data mining @ dm @ @ machine learning @ ml @ technique is popular especially @ deep learning @ dl @ method @ @ research aim to classify text data in @ indonesian language @ convolutional neural network @ cnn @ a @ of @ dl algorithm @ @ cnn algorithm used modified following @ indonesian language characteristic @ thereby in @ text pre-processing phase stopword removal and stemming @ particularly suitable @ @ indonesian language @ @ experiment conducted @ indonesian news text data @ various source @ four category @ @ hiburan @ @ entertainment @ @ olahraga @ @ sport @ @ tajuk utama @ @ headline news @ and @ teknologi @ @ technology @ @ based on @ experiment and evaluation @ training data and testing data producing five model @ ten epoch @ @ model cnn ha @ best percentage of accuracy around and loss value around @ hidden layer in classifying @ indonesian news data @ advanced scientific research @ @ right reserved @ 
147,Investigating text mining features and classifiers: An experimental analysis,"The proposed work presents the detection of hate speech in social media. In this context, the text mining techniques are valuable. This, in turn, requires an accurate classification algorithm as well as text feature selection technique which works well with the sentiment classification. Therefore, a review of existing techniques is performed first. According to literature, amongst the techniques suggested by the researchers, the GI (Gini Index), DF (document frequency), POS (part of speech) tagging, and IG (information gain)are popular and frequently used techniques for sentiment classification. Thus, these techniques are chosen to implement the text feature selection. Further, the SVM (Support Vector Machine) and KNN (k-Nearest Neighbour) algorithms are applied to classify the selected features. The experimental outcomes show that the SVM is accurate and efficient algorithm for classifying measured features. Additionally, it is seen that the combination of SVM with the feature extraction techniques POS and IG is also time efficient. The proposed work may be extended by use of the prevalent algorithms. © 2020, World Academy of Research in Science and Engineering. All rights reserved.",2020,International Journal of Advanced Trends in Computer Science and Engineering,0,@ proposed work @ @ detection of hate speech in social medium @ in @ context @ text mining technique @ valuable @ @ in turn requires @ accurate classification algorithm a well a text feature selection technique @ work well @ @ sentiment classification @ therefore a review of existing technique is performed first @ according to literature amongst @ technique suggested by @ researcher @ gi @ gini index @ df @ document frequency @ po @ part of speech @ tagging and ig @ information gain @ @ popular and frequently used technique @ sentiment classification @ thus @ technique @ chosen to implement @ text feature selection @ @ @ svm @ support vector machine @ and knn @ k-nearest neighbour @ algorithm @ applied to classify @ selected feature @ @ experimental outcome @ @ @ svm is accurate and efficient algorithm @ classifying measured feature @ additionally @ is seen @ @ combination of svm @ @ feature extraction technique po and ig is @ time efficient @ @ proposed work may @ extended by use of @ prevalent algorithm @ world academy of research in science and engineering @ @ right reserved @ 
151,Emotion Recognition on Twitter: Comparative Study and Training a Unison Model,"Despite recent successes of deep learning in many fields of natural language processing, previous studies of emotion recognition on Twitter mainly focused on the use of lexicons and simple classifiers on bag-of-words models. The central question of our study is whether we can improve their performance using deep learning. To this end, we exploit hashtags to create three large emotion-labeled data sets corresponding to different classifications of emotions. We then compare the performance of several word- A nd character-based recurrent and convolutional neural networks with the performance on bag-of-words and latent semantic indexing models. We also investigate the transferability of the final hidden state representations between different classifications of emotions, and whether it is possible to build a unison model for predicting all of them using a shared representation. We show that recurrent neural networks, especially character-based ones, can improve over bag-of-words and latent semantic indexing models. Although the transfer capabilities of these models are poor, the newly proposed training heuristic produces a unison model with performance comparable to that of the three single models. © 2018 IEEE.",2020,IEEE Transactions on Affective Computing,10,despite recent success of deep learning in many field of natural language processing previous study of emotion recognition on twitter mainly focused on @ use of lexicon and simple classifier on bag-of-words model @ @ central question of @ study is whether @ @ improve @ performance @ deep learning @ to @ end @ exploit hashtags to create three @ emotion-labeled data set corresponding to different classification of emotion @ @ @ compare @ performance of several word a nd character-based recurrent and convolutional neural network @ @ performance on bag-of-words and latent semantic indexing model @ @ @ investigate @ transferability of @ final hidden state representation @ different classification of emotion and whether @ is possible to build a unison model @ predicting @ of @ @ a shared representation @ @ @ @ recurrent neural network especially character-based @ @ improve @ bag-of-words and latent semantic indexing model @ although @ transfer capability of @ model @ poor @ newly proposed training heuristic produce a unison model @ performance comparable to @ of @ three single model @ @ @ 
153,SimiT: A Text Similarity Method Using Lexicon and Dependency Representations,"Semantic textual similarity methods are becoming increasingly crucial in text mining research areas such as text retrieval and summarization. Existing methods of text similarity have often been computed by their shallow or syntactic representation rather than considering their semantic content and meanings. This paper focuses mainly on computing the similarity between sentences without a supervised learning approach, only considering their word-level coherence which is calculated by a hybrid method of dependency parser and lexicon embeddings. Hence, we concentrate on structural similarity between text pairs by regarding their dependency parser embeddings. Our hybrid method also pays attention to the semantic information of words implied in the sentences. In the evaluation, we compare our method with the state-of-the-art semantic similarity measures in a well-known dataset. Our method outperforms most of the studies in the literature and the overall performance achieves better results when combining the similarity scores of both embedding models. © 2020, Ohmsha, Ltd. and Springer Japan KK, part of Springer Nature.",2020,New Generation Computing,0,semantic textual similarity method @ becoming increasingly crucial in text mining research area @ a text retrieval and summarization @ existing method of text similarity @ often @ computed by @ shallow @ syntactic representation rather @ considering @ semantic content and meaning @ @ @ focus mainly on computing @ similarity @ sentence without a supervised learning approach only considering @ word-level coherence @ is calculated by a hybrid method of dependency parser and lexicon embeddings @ hence @ concentrate on structural similarity @ text pair by regarding @ dependency parser embeddings @ @ hybrid method @ pay attention to @ semantic information of word implied in @ sentence @ in @ evaluation @ compare @ method @ @ state-of-the-art semantic similarity measure in a well-known dataset @ @ method outperforms @ of @ study in @ literature and @ overall performance achieves better @ @ combining @ similarity score of @ embedding model @ ohmsha ltd @ and @ japan kk part of @ nature @ 
155,NET-LDA: A novel topic modeling method based on semantic document similarity,"Topic models, such as latent Dirichlet allocation (LDA), allow us to categorize each document based on the topics. It builds a document as a mixture of topics and a topic is modeled as a probability distribution over words. However, the key drawback of the traditional topic model is that it cannot handle the semantic knowledge hidden in the documents. Therefore, semantically related, coherent and meaningful topics cannot be obtained. However, semantic inference plays a significant role in topic modeling as well as in other text mining tasks. In this paper, in order to tackle this problem, a novel NET-LDA model is proposed. In NET-LDA, semantically similar documents are merged to bring all semantically related words together and the obtained semantic similarity knowledge is incorporated into the model with a new adaptive semantic parameter. The motivation of the study is to reveal the impact of semantic knowledge in the topic model researches. Therefore, in a given corpus, different documents may contain different words but may speak about the same topic. For such documents to be correctly identified, the feature space of the documents must be elaborated with more powerful features. In order to accomplish this goal, the semantic space of documents is constructed with concepts and named entities. Two datasets in the English and Turkish languages and 12 different domains have been evaluated to show the independence of the model from both language and domain. The proposed NET-LDA, compared to the baselines, outperforms in terms of topic coherence, F-measure, and qualitative evaluation. © 2020 Turkiye Klinikleri. All rights reserved.",2020,Turkish Journal of Electrical Engineering and Computer Sciences,0,topic model @ a latent dirichlet allocation @ lda @ allow u to categorize @ document based on @ topic @ @ build a document a a mixture of topic and a topic is modeled a a probability distribution @ word @ however @ key drawback of @ traditional topic model is @ @ cannot handle @ semantic knowledge hidden in @ document @ therefore semantically related coherent and meaningful topic cannot @ obtained @ however semantic inference play a significant role in topic modeling a well a in @ text mining task @ in @ @ in order to tackle @ problem a novel net-lda model is proposed @ in net-lda semantically similar document @ merged to bring @ semantically related word together and @ obtained semantic similarity knowledge is incorporated @ @ model @ a @ adaptive semantic parameter @ @ motivation of @ study is to reveal @ impact of semantic knowledge in @ topic model research @ therefore in a given corpus different document may contain different word @ may speak @ @ @ topic @ @ @ document to @ correctly identified @ feature space of @ document must @ elaborated @ more powerful feature @ in order to accomplish @ goal @ semantic space of document is constructed @ concept and named entity @ @ datasets in @ english and turkish language and different domain @ @ evaluated to @ @ independence of @ model @ @ language and domain @ @ proposed net-lda compared to @ baseline outperforms in term of topic coherence f-measure and qualitative evaluation @ turkiye klinikleri @ @ right reserved @ 
161,ACRank: a multi-evidence text-mining model for alliance discovery from news articles,"Purpose: Strategic alliances among organizations are some of the central drivers of innovation and economic growth. However, the discovery of alliances has relied on pure manual search and has limited scope. This paper proposes a text-mining framework, ACRank, that automatically extracts alliances from news articles. ACRank aims to provide human analysts with a higher coverage of strategic alliances compared to existing databases, yet maintain a reasonable extraction precision. It has the potential to discover alliances involving less well-known companies, a situation often neglected by commercial databases. Design/methodology/approach: The proposed framework is a systematic process of alliance extraction and validation using natural language processing techniques and alliance domain knowledge. The process integrates news article search, entity extraction, and syntactic and semantic linguistic parsing techniques. In particular, Alliance Discovery Template (ADT) identifies a number of linguistic templates expanded from expert domain knowledge and extract potential alliances at sentence-level. Alliance Confidence Ranking (ACRank)further validates each unique alliance based on multiple features at document-level. The framework is designed to deal with extremely skewed, noisy data from news articles. Findings: In evaluating the performance of ACRank on a gold standard data set of IBM alliances (2006–2008) showed that: Sentence-level ADT-based extraction achieved 78.1% recall and 44.7% precision and eliminated over 99% of the noise in news articles. ACRank further improved precision to 97% with the top20% of extracted alliance instances. Further comparison with Thomson Reuters SDC database showed that SDC covered less than 20% of total alliances, while ACRank covered 67%. When applying ACRank to Dow 30 company news articles, ACRank is estimated to achieve a recall between 0.48 and 0.95, and only 15% of the alliances appeared in SDC. Originality/value: The research framework proposed in this paper indicates a promising direction of building a comprehensive alliance database using automatic approaches. It adds value to academic studies and business analyses that require in-depth knowledge of strategic alliances. It also encourages other innovative studies that use text mining and data analytics to study business relations. © 2020, Emerald Publishing Limited.",2020,Information Technology and People,0,purpose @ strategic alliance among organization @ some of @ central driver of innovation and economic growth @ however @ discovery of alliance ha relied on pure manual search and ha limited scope @ @ @ proposes a text-mining framework acrank @ automatically extract alliance @ news article @ acrank aim to provide human analyst @ a higher coverage of strategic alliance compared to existing database yet maintain a reasonable extraction precision @ @ ha @ potential to discover alliance involving le well-known company a situation often neglected by commercial database @ design methodology approach @ @ proposed framework is a systematic process of alliance extraction and validation @ natural language processing technique and alliance domain knowledge @ @ process integrates news article search entity extraction and syntactic and semantic linguistic parsing technique @ in particular alliance discovery template @ adt @ identifies a number of linguistic template expanded @ expert domain knowledge and extract potential alliance at sentence-level @ alliance confidence ranking @ acrank @ @ validates @ unique alliance based on multiple feature at document-level @ @ framework is designed to deal @ extremely skewed noisy data @ news article @ finding @ in evaluating @ performance of acrank on a gold standard data set of ibm alliance @ @ showed @ @ sentence-level adt-based extraction achieved @ recall and @ precision and eliminated @ of @ noise in news article @ acrank @ improved precision to @ @ top of extracted alliance instance @ @ comparison @ thomson reuters sdc database showed @ sdc covered le @ of total alliance @ acrank covered @ @ applying acrank to dow company news article acrank is estimated to achieve a recall @ @ and @ and only of @ alliance appeared in sdc @ originality value @ @ research framework proposed in @ @ indicates a promising direction of building a comprehensive alliance database @ automatic approach @ @ add value to @ study and @ analysis @ require in-depth knowledge of strategic alliance @ @ @ encourages @ innovative study @ use text mining and data analytics to study @ relation @ emerald publishing limited @ 
173,Forecasting net income estimate and stock price using text mining from economic reports,"This paper proposes and analyzes a methodology of forecasting movements of the analysts' net income estimates and those of stock prices. We achieve this by applying natural language processing and neural networks in the context of analyst reports. In the pre-experiment, we applied our method to extract opinion sentences from the analyst report while classifying the remaining parts as non-opinion sentences. Then, we performed two additional experiments. First, we employed our proposed method for forecasting the movements of analysts' net income estimates by inputting the opinion and non-opinion sentences into separate neural networks. Besides the reports, we inputted the trend of the net income estimate to the networks. Second, we employed our proposed method for forecasting the movements of stock prices. Consequently, we found differences between security firms, which depend on whether analysts' net income estimates tend to be forecasted by opinions or facts in the context of analyst reports. Furthermore, the trend of the net income estimate was found to be effective for the forecast as well as an analyst report. However, in experiments of forecasting movements of stock prices, the difference between opinion sentences and non-opinion sentences was not effective. © 2020 by the authors.",2020,Information (Switzerland),1,@ @ proposes and analyzes a methodology of forecasting movement of @ analyst @ net income estimate and @ of stock price @ @ achieve @ by applying natural language processing and neural network in @ context of analyst report @ in @ pre-experiment @ applied @ method to extract opinion sentence @ @ analyst report @ classifying @ remaining part a non-opinion sentence @ @ @ performed @ additional experiment @ first @ employed @ proposed method @ forecasting @ movement of analyst @ net income estimate by inputting @ opinion and non-opinion sentence @ separate neural network @ besides @ report @ inputted @ trend of @ net income estimate to @ network @ second @ employed @ proposed method @ forecasting @ movement of stock price @ consequently @ found difference @ security firm @ depend on whether analyst @ net income estimate tend to @ forecasted by opinion @ fact in @ context of analyst report @ furthermore @ trend of @ net income estimate wa found to @ effective @ @ forecast a well a @ analyst report @ however in experiment of forecasting movement of stock price @ difference @ opinion sentence and non-opinion sentence wa not effective @ by @ author @ 
176,Malicious text identification: Deep learning from public comments and emails,"Identifying internet spam has been a challenging problem for decades. Several solutions have succeeded to detect spam comments in social media or fraudulent emails. However, an adequate strategy for filtering messages is difficult to achieve, as these messages resemble real communications. From the Natural Language Processing (NLP) perspective, Deep Learning models are a good alternative for classifying text after being preprocessed. In particular, Long Short-Term Memory (LSTM) networks are one of the models that perform well for the binary and multi-label text classification problems. In this paper, an approach merging two different data sources, one intended for Spam in social media posts and the other for Fraud classification in emails, is presented. We designed a multi-label LSTM model and trained it on the joint datasets including text with common bigrams, extracted from each independent dataset. The experiment results show that our proposed model is capable of identifying malicious text regardless of the source. The LSTM model trained with the merged dataset outperforms the models trained independently on each dataset. © 2020 by the authors.",2020,Information (Switzerland),1,identifying internet spam ha @ a challenging problem @ decade @ several solution @ succeeded to detect spam comment in social medium @ fraudulent email @ however @ adequate strategy @ filtering message is difficult to achieve a @ message resemble real communication @ @ @ natural language processing @ nlp @ perspective deep learning model @ a good alternative @ classifying text @ @ preprocessed @ in particular long short-term memory @ lstm @ network @ @ of @ model @ perform well @ @ binary and multi-label text classification problem @ in @ @ @ approach merging @ different data source @ intended @ spam in social medium post and @ @ @ fraud classification in email is presented @ @ designed a multi-label lstm model and trained @ on @ joint datasets including text @ common bigram extracted @ @ independent dataset @ @ experiment @ @ @ @ proposed model is capable of identifying malicious text regardless of @ source @ @ lstm model trained @ @ merged dataset outperforms @ model trained independently on @ dataset @ by @ author @ 
183,Intelligent User Assistance for Automated Data Mining Method Selection,"In any data science and analytics project, the task of mapping a domain-specific problem to an adequate set of data mining methods by experts of the field is a crucial step. However, these experts are not always available and data mining novices may be required to perform the task. While there are several research efforts for automated method selection as a means of support, only a few approaches consider the particularities of problems expressed in the natural and domain-specific language of the novice. The study proposes the design of an intelligent assistance system that takes problem descriptions articulated in natural language as an input and offers advice regarding the most suitable class of data mining methods. Following a design science research approach, the paper (i) outlines the problem setting with an exemplary scenario from industrial practice, (ii) derives design requirements, (iii) develops design principles and proposes design features, (iv) develops and implements the IT artifact using several methods such as embeddings, keyword extractions, topic models, and text classifiers, (v) demonstrates and evaluates the implemented prototype based on different classification pipelines, and (vi) discusses the results’ practical and theoretical contributions. The best performing classification pipelines show high accuracies when applied to validation data and are capable of creating a suitable mapping that exceeds the performance of joint novice assessments and simpler means of text mining. The research provides a promising foundation for further enhancements, either as a stand-alone intelligent assistance system or as an add-on to already existing data science and analytics platforms. © 2020, The Author(s).",2020,Business and Information Systems Engineering,1,in @ data science and analytics project @ task of mapping a domain-specific problem to @ adequate set of data mining method by expert of @ field is a crucial step @ however @ expert @ not always available and data mining novice may @ required to perform @ task @ @ @ @ several research effort @ automated method selection a a mean of support only a @ approach consider @ particularity of problem expressed in @ natural and domain-specific language of @ novice @ @ study proposes @ design of @ intelligent assistance system @ take problem description articulated in natural language a @ input and offer advice regarding @ @ suitable class of data mining method @ following a design science research approach @ @ @ i @ outline @ problem setting @ @ exemplary scenario @ industrial practice @ ii @ derives design requirement @ iii @ develops design principle and proposes design feature @ @ @ develops and implement @ @ artifact @ several method @ a embeddings keyword extraction topic model and text classifier @ v @ demonstrates and evaluates @ implemented prototype based on different classification pipeline and @ vi @ discus @ @ practical and theoretical contribution @ @ best performing classification pipeline @ high accuracy @ applied to validation data and @ capable of creating a suitable mapping @ exceeds @ performance of joint novice assessment and simpler mean of text mining @ @ research provides a promising foundation @ @ enhancement either a a stand-alone intelligent assistance system @ a @ add-on to already existing data science and analytics platform @ @ author @ s @ @ 
184,Charismatic Document Clustering Through Novel K-Means Non-negative Matrix Factorization (KNMF) Algorithm Using Key Phrase Extraction,"The tedious challenging of Big Data is to store and retrieve of required data from the search engines. Problem Defined There is an obligation for the quick and efficient retrieval of useful information for the many organizations. The elementary idea is to arrange these computing files of organization into individual folders in an hierarchical order of folders. Manually, to order these files into folders, there is an ardent need to know about the file contents and name of the files to give impression of files, so that it provides an alignment of certain set of files as a bunch. Problem Statement Manual grouping of files has its own complications, for example when these files are in numerous amounts and also their contents cannot be illustrious by their labels. Therefore, it’s an intense requirement for Document clustering with data processing machines for enthusiastic results. Existing System A couple of analyzers are impending with dynamic algorithms and comprehensive analogy of extant algorithms, but, yet, these have been restricted to organizations and colleges. After recent updated rules of NMF their raised a self interest in document clustering. These rules gave trust in its performances with better results when compared to Latent Semantic Indexing with Singular Value Decomposition. Proposed System A new working miniature called Novel K-means Non-Negative Matrix Factorization (KNMF) is implemented using renovated guidelines of NMF which has been diagnosed for clustering documents consequently. A new data set called Newsgroup20 is considered for the exploratory purpose. Removal of common clutter/stop words using keywords from Key Phrase Extraction Algorithm and a new proposed Iterated Lovin stemming will be utilized in preprocessing step inassisting to KNMF. Compared to the Porter stemmer and Lovins stemmer algorithms, Iterative Lovins algorithm is providing 5% more reduction. 60% of the document terms are been minimized to root as remaining terms are already root words. Eventually, an appeal to these processes named “Progressive Text mining radical” is developed inlateral exertion of K-Means algorithm from the defined Apache Mahout Project which is used to analyze the performance of the MapReduce framework in Hadoop. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2020,International Journal of Parallel Programming,4,@ tedious challenging of big data is to store and retrieve of required data @ @ search engine @ problem defined @ is @ obligation @ @ quick and efficient retrieval of useful information @ @ many organization @ @ elementary idea is to arrange @ computing file of organization @ individual folder in @ hierarchical order of folder @ manually to order @ file @ folder @ is @ ardent need to know @ @ file content and name of @ file to give impression of file @ @ @ provides @ alignment of certain set of file a a bunch @ problem statement manual grouping of file ha @ @ complication @ example @ @ file @ in numerous amount and @ @ content cannot @ illustrious by @ label @ therefore @ s @ intense requirement @ document clustering @ data processing machine @ enthusiastic @ @ existing system a couple of analyzer @ impending @ dynamic algorithm and comprehensive analogy of extant algorithm @ yet @ @ @ restricted to organization and college @ @ recent updated rule of nmf @ raised a self interest in document clustering @ @ rule gave trust in @ performance @ better @ @ compared to latent semantic indexing @ singular value decomposition @ proposed system a @ working miniature called novel k-means non-negative matrix factorization @ knmf @ is implemented @ renovated guideline of nmf @ ha @ diagnosed @ clustering document consequently @ a @ data set called newsgroup is considered @ @ exploratory purpose @ removal of common clutter stop word @ keywords @ key phrase extraction algorithm and a @ proposed iterated lovin stemming @ @ utilized in preprocessing step inassisting to knmf @ compared to @ porter stemmer and lovins stemmer algorithm iterative lovins algorithm is providing more reduction @ of @ document term @ @ minimized to root a remaining term @ already root word @ eventually @ appeal to @ process named progressive text mining radical is developed inlateral exertion of k-means algorithm @ @ defined apache mahout project @ is used to analyze @ performance of @ mapreduce framework in hadoop @ @ science @ medium llc part of @ nature @ 
185,A new emergency management dynamic value assessment model based on social media data: a multiphase decision-making perspective,"User-Generated Content (UGC) is becoming a powerful data source to support emergency management. Managers usually face two difficulties in practical emergency management. First, the requirement topics for emergency management are changing over time. Second, the value of the same microblog is changing over different emergency phases. The contributions of this study lie in the following aspects. First, this paper develops a multiphase dynamic assessment model. Second, an idea for the dynamic evaluation of UGC is proposed. Third, this paper presents an effective quantification method to assess the dynamic value of social media data. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2020,Enterprise Information Systems,7,user-generated content @ ugc @ is becoming a powerful data source to support emergency management @ manager usually face @ difficulty in practical emergency management @ first @ requirement topic @ emergency management @ changing @ time @ second @ value of @ @ microblog is changing @ different emergency phase @ @ contribution of @ study lie in @ following aspect @ first @ @ develops a multiphase dynamic assessment model @ second @ idea @ @ dynamic evaluation of ugc is proposed @ third @ @ @ @ effective quantification method to ass @ dynamic value of social medium data @ informa uk limited trading a taylor francis group @ 
188,A constrained optimization algorithm for learning GloVe embeddings with semantic lexicons,"GloVe representations of words as vector embeddings in continuous spaces are learned from matrix factorization of the words’ co-occurrences matrix constructed from large corpora. Due to their high quality as textual features, GloVe embeddings have been extensively utilized for many text mining and natural language processing tasks with considerable success. Further improvements of these word representations can be obtained by also taking into account the valuable information of the semantic properties of the words and the complex relationships between them as provided by semantic lexicons. In this paper we adopt optimization techniques from the domain of machine learning with constrained optimization in order to leverage the relational knowledge between words, and we propose an efficient algorithm that produces word embeddings enhanced by the semantic information. The proposed algorithm outperforms other related approaches that utilize semantic information either during training or as a post-processing step. Our claims are validated by experiments on popular text mining and natural language processing tasks, including word similarities, word analogies, and sentiment analysis, which demonstrate that our proposed model can significantly improve the quality of word vector representations. © 2020 Elsevier B.V.",2020,Knowledge-Based Systems,3,glove representation of word a vector embeddings in continuous space @ learned @ matrix factorization of @ word co-occurrence matrix constructed @ @ corpus @ due to @ high quality a textual feature glove embeddings @ @ extensively utilized @ many text mining and natural language processing task @ considerable success @ @ improvement of @ word representation @ @ obtained by @ taking @ account @ valuable information of @ semantic property of @ word and @ complex relationship @ @ a provided by semantic lexicon @ in @ @ @ adopt optimization technique @ @ domain of machine learning @ constrained optimization in order to leverage @ relational knowledge @ word and @ propose @ efficient algorithm @ produce word embeddings enhanced by @ semantic information @ @ proposed algorithm outperforms @ related approach @ utilize semantic information either @ training @ a a post-processing step @ @ claim @ validated by experiment on popular text mining and natural language processing task including word similarity word analogy and sentiment analysis @ demonstrate @ @ proposed model @ significantly improve @ quality of word vector representation @ @ b @ v @ 
189,Words are important: A textual content based identity resolution scheme across multiple online social networks,"Identity resolution of a person using various online social networks can enable an interested party to have a better and holistic understanding of former's behavior and personality. Major challenges in developing a reliable and scalable matching scheme for online identities include non-availability of required information or having contradictory information for the same user across these networks. In this study, we present a scheme for identity matching which utilizes important features extracted from contents generated by or shared with users across one's online social networks. With the help of natural language processing and text mining techniques, we extract and process parts-of-speech, symbols, emoticons, numbers, and high frequency words in user's posts, tweets, retweets, and URLs. On the basis of experiments with ground truth Twitter–Facebook real datasets, this method achieved 91.2 percent accuracy in matching user's identity across the user's profiles. The main contribution of this paper is that this proposes a novel method for identity matching, which utilizes only the publicly available content information of online social network users. This method can be used alone for identity matching, or can be used along with other identity resolution frameworks to enhance their accuracy. © 2020 Elsevier B.V.",2020,Knowledge-Based Systems,2,identity resolution of a person @ various online social network @ enable @ interested party to @ a better and holistic understanding of former @ s behavior and personality @ major challenge in developing a reliable and scalable matching scheme @ online identity include non-availability of required information @ @ contradictory information @ @ @ user across @ network @ in @ study @ @ a scheme @ identity matching @ utilizes important feature extracted @ content generated by @ shared @ user across @ @ s online social network @ @ @ help of natural language processing and text mining technique @ extract and process parts-of-speech symbol emoticon number and high frequency word in user @ s post tweet retweets and url @ on @ basis of experiment @ ground truth twitter facebook real datasets @ method achieved @ percent accuracy in matching user @ s identity across @ user @ s profile @ @ main contribution of @ @ is @ @ proposes a novel method @ identity matching @ utilizes only @ publicly available content information of online social network user @ @ method @ @ used alone @ identity matching @ @ @ used along @ @ identity resolution framework to enhance @ accuracy @ @ b @ v @ 
195,Keyword extraction: Issues and methods,"Due to the considerable growth of the volume of text documents on the Internet and in digital libraries, manual analysis of these documents is no longer feasible. Having efficient approaches to keyword extraction in order to retrieve the 'key' elements of the studied documents is now a necessity. Keyword extraction has been an active research field for many years, covering various applications in Text Mining, Information Retrieval, and Natural Language Processing, and meeting different requirements. However, it is not a unified domain of research. In spite of the existence of many approaches in the field, there is no single approach that effectively extracts keywords from different data sources. This shows the importance of having a comprehensive review, which discusses the complexity of the task and categorizes the main approaches of the field based on the features and methods of extraction that they use. This paper presents a general introduction to the field of keyword/keyphrase extraction. Unlike the existing surveys, different aspects of the problem along with the main challenges in the field are discussed. This mainly includes the unclear definition of 'keyness', complexities of targeting proper features for capturing desired keyness properties and selecting efficient extraction methods, and also the evaluation issues. By classifying a broad range of state-of-the-art approaches and analysing the benefits and drawbacks of different features and methods, we provide a clearer picture of them. This review is intended to help readers find their way around all the works related to keyword extraction and guide them in choosing or designing a method that is appropriate for the application they are targeting. © 2019 Cambridge University Press..",2020,Natural Language Engineering,1,due to @ considerable growth of @ volume of text document on @ internet and in digital library manual analysis of @ document is no longer feasible @ @ efficient approach to keyword extraction in order to retrieve @ @ key @ element of @ studied document is now a necessity @ keyword extraction ha @ @ active research field @ many year covering various application in text mining information retrieval and natural language processing and meeting different requirement @ however @ is not a unified domain of research @ in spite of @ existence of many approach in @ field @ is no single approach @ effectively extract keywords @ different data source @ @ @ @ importance of @ a comprehensive review @ discus @ complexity of @ task and categorizes @ main approach of @ field based on @ feature and method of extraction @ @ use @ @ @ @ a general introduction to @ field of keyword keyphrase extraction @ unlike @ existing survey different aspect of @ problem along @ @ main challenge in @ field @ discussed @ @ mainly includes @ unclear definition of @ keynes @ complexity of targeting proper feature @ capturing desired keynes property and selecting efficient extraction method and @ @ evaluation issue @ by classifying a broad range of state-of-the-art approach and analysing @ benefit and drawback of different feature and method @ provide a clearer picture of @ @ @ review is intended to help reader find @ way around @ @ work related to keyword extraction and guide @ in choosing @ designing a method @ is appropriate @ @ application @ @ targeting @ cambridge university @ @ @ 
197,Emo2Vec: Learning Emotional Embeddings via Multi-Emotion Category,"Sentiment analysis or opinion mining for subject information extraction from the text has become more and more dependent on natural language processing, especially for business and healthcare, since the online products and service reviews affect the consuming behaviors. Word embeddings that can map the words to low-dimensional vector representations have been widely used in natural language processing tasks. But the word embeddings based on context such as Word2Vec and GloVe fail to capture the sentiment information. Most of existing sentiment analysis methods incorporate emotional polarity (positive and negative) to improve the sentiment embeddings for the emotion classification. This article takes advantage of an emotional psychology model to learn the emotional embeddings in Chinese first. In order to combine the semantic space and an emotional space, we present two different purifying models from local (LPM) and global (GPM) perspectives based on Plutchik's wheel of emotions to add the emotional information into word vectors. The two models aim to improve the word vectors so that not only the semantically similar words but also the sentimentally similar words can be closer than before. The Plutchik's wheel of emotions model can give eight-dimensional vector for one word in emotional space that can capture more sentiment information than the binary polarity labels. The obvious advantage of the local purifying model is that it can be fit for any pretrained word embeddings. For the global purifying model, we can get the final emotional embeddings at once. These models have been extended to handle English texts. The experimental results on Chinese and English datasets show that our purifying model can improve the conventional word embeddings and some proposed sentiment embeddings for sentiment classification and multi-emotion classification. © 2020 ACM.",2020,ACM Transactions on Internet Technology,1,sentiment analysis @ opinion mining @ subject information extraction @ @ text ha become more and more dependent on natural language processing especially @ @ and healthcare since @ online product and service review affect @ consuming behavior @ word embeddings @ @ map @ word to low-dimensional vector representation @ @ widely used in natural language processing task @ @ @ word embeddings based on context @ a word vec and glove fail to capture @ sentiment information @ @ of existing sentiment analysis method incorporate emotional polarity @ positive and negative @ to improve @ sentiment embeddings @ @ emotion classification @ @ article take advantage of @ emotional psychology model to learn @ emotional embeddings in chinese first @ in order to combine @ semantic space and @ emotional space @ @ @ different purifying model @ local @ lpm @ and global @ gpm @ perspective based on plutchik @ s wheel of emotion to add @ emotional information @ word vector @ @ @ model aim to improve @ word vector @ @ not only @ semantically similar word @ @ @ sentimentally similar word @ @ closer @ @ @ @ plutchik @ s wheel of emotion model @ give eight-dimensional vector @ @ word in emotional space @ @ capture more sentiment information @ @ binary polarity label @ @ obvious advantage of @ local purifying model is @ @ @ @ fit @ @ pretrained word embeddings @ @ @ global purifying model @ @ get @ final emotional embeddings at @ @ @ model @ @ extended to handle english text @ @ experimental @ on chinese and english datasets @ @ @ purifying model @ improve @ conventional word embeddings and some proposed sentiment embeddings @ sentiment classification and multi-emotion classification @ acm @ 
200,Corpus-based paraphrase detection experiments and review,"Paraphrase detection is important for a number of applications, including plagiarism detection, authorship attribution, question answering, text summarization, text mining in general, etc. In this paper, we give a performance overview of various types of corpus-based models, especially deep learning (DL) models, with the task of paraphrase detection. We report the results of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO, and USE) evaluated on three different public available corpora: Microsoft Research Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase Corpus 2011. Through a great number of experiments, we decided on the most appropriate approaches for text pre-processing: hyper-parameters, sub-model selection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and semantic similarity/paraphrase detection threshold. Our findings and those of other researchers who have used deep learning models show that DL models are very competitive with traditional state-of-theart approaches and have potential that should be further developed. © 2020 by the authors.",2020,Information (Switzerland),1,paraphrase detection is important @ a number of application including plagiarism detection authorship attribution question answering text summarization text mining in general etc @ in @ @ @ give a performance overview of various type of corpus-based model especially deep learning @ dl @ model @ @ task of paraphrase detection @ @ report @ @ of eight model @ lsi tf-idf word vec doc vec glove fasttext elmo and use @ evaluated on three different public available corpus @ microsoft research paraphrase corpus clough and stevenson and webis crowd paraphrase corpus @ @ a great number of experiment @ decided on @ @ appropriate approach @ text pre-processing @ hyper-parameters sub-model selection-where @ exist @ e @ g @ skipgram v @ cbow @ distance measure and semantic similarity paraphrase detection threshold @ @ finding and @ of @ researcher @ @ used deep learning model @ @ dl model @ @ competitive @ traditional state-of-theart approach and @ potential @ @ @ @ developed @ by @ author @ 
201,Word Embedding Projection Models for Hypernymy Relation Prediction,"A hypernymy (""is-a"") relation is an important concept in the field of Natural Language Processing (NLP) and computational linguistics. This type of semantic relations is often used to describe the subordination relation between two semantic concepts, such as ""(dog, animal)"", ""(rose, plant)"" and ""(sofa, furniture)"". The accurate extraction and prediction of hypernymy relations from massive text corpora is extremely important for mining the inherent hierarchy among semantic concepts and entities, as well as building large-scale semantic networks, ontologies, knowledge graphs and other knowledge-intensive information systems. This task is also beneficial to a variety of downstream NLP tasks, including natural language inference, personalized recommendation, query understanding and so on. Most traditional hypernymy prediction algorithms rely on relatively fixed language patterns, such as the Hearst patterns in English. These approaches have several potential drawbacks such as the low coverage of relations in texts and the high degree of manual intervention required to train these machine learning models. In addition, the textual patterns used for hypernymy extraction are highly correlated with the characteristics of the target language itself. For languages with low regularity in text expressions such as Chinese, pattern-based methods are not sufficiently accurate. Distributional models for hypernymy prediction are more precise and can avoid the occurrence sparsity problem of concepts, but likely to suffer from the ""lexical memorization"" problem. With the rapid development of deep learning techniques in NLP, word embeddings which learned from neural language models are frequently employed to model the semantic relations between words, without a lot of linguistic knowledge. Especially, word embedding projection models learn how to map the embeddings of hyponyms to those of their hypernyms, modeling the representations of hypernymy relations in the embedding space explicitly. In view of existing classical and latest research, this paper introduces the development process and the latest breakthrough of word embedding projection models, in order to predict hypernymy relations accurately. We give a unified mathematical framework of these models and discuss how these models are developed, including the improvements of projection learning based on deep iterative, transductive and adversarial learning. Specifically, iterative learning methods consider the situation where hypernymy relations from different domains have diverse representations, and employ iterative, semi-supervised learning technique to learn multiple projection matrices from the embeddings of hyponyms to hypernyms. Transductive models learn the projection matrices of hypernymy and non-hypernymy relations at the same time, and consider the semantic differences between concepts in the training and testing sets. Because there are a large number of hypernymy relations in modern taxonomies, deep adversarial models learn neural network-based projection models over taxonomies and training sets, and train adversarial classifiers to make the two neural networks to learn from each other. In the experiments, we evaluate all these projection learning models under a unified framework, including multiple general-domain and domain-specific benchmark datasets in English and Chinese languages. We also compare the advantages and disadvantages of these projection learning models under different learning circumstances. Finally, the future research directions of this work are discussed, which focus on domain-specific and long-tail hypernymy prediction. © 2020, Science Press. All right reserved.",2020,Jisuanji Xuebao/Chinese Journal of Computers,1,a hypernymy @ @ is-a @ @ relation is @ important concept in @ field of natural language processing @ nlp @ and computational linguistics @ @ type of semantic relation is often used to describe @ subordination relation @ @ semantic concept @ a @ @ dog animal @ @ @ @ rose plant @ @ and @ @ sofa furniture @ @ @ @ accurate extraction and prediction of hypernymy relation @ massive text corpus is extremely important @ mining @ inherent hierarchy among semantic concept and entity a well a building large-scale semantic network ontology knowledge graph and @ knowledge-intensive information system @ @ task is @ beneficial to a variety of downstream nlp task including natural language inference personalized recommendation query understanding and @ on @ @ traditional hypernymy prediction algorithm rely on relatively fixed language pattern @ a @ hearst pattern in english @ @ approach @ several potential drawback @ a @ low coverage of relation in text and @ high degree of manual intervention required to train @ machine learning model @ in addition @ textual pattern used @ hypernymy extraction @ highly correlated @ @ characteristic of @ target language @ @ @ language @ low regularity in text expression @ a chinese pattern-based method @ not sufficiently accurate @ distributional model @ hypernymy prediction @ more precise and @ avoid @ occurrence sparsity problem of concept @ likely to suffer @ @ @ lexical memorization @ problem @ @ @ rapid development of deep learning technique in nlp word embeddings @ learned @ neural language model @ frequently employed to model @ semantic relation @ word without a lot of linguistic knowledge @ especially word embedding projection model learn @ to map @ embeddings of hyponym to @ of @ hypernym modeling @ representation of hypernymy relation in @ embedding space explicitly @ in view of existing classical and latest research @ @ introduces @ development process and @ latest breakthrough of word embedding projection model in order to predict hypernymy relation accurately @ @ give a unified mathematical framework of @ model and discus @ @ model @ developed including @ improvement of projection learning based on deep iterative transductive and adversarial learning @ specifically iterative learning method consider @ situation @ hypernymy relation @ different domain @ diverse representation and employ iterative semi-supervised learning technique to learn multiple projection matrix @ @ embeddings of hyponym to hypernym @ transductive model learn @ projection matrix of hypernymy and non-hypernymy relation at @ @ time and consider @ semantic difference @ concept in @ training and testing set @ @ @ @ a @ number of hypernymy relation in modern taxonomy deep adversarial model learn neural network-based projection model @ taxonomy and training set and train adversarial classifier to make @ @ neural network to learn @ @ @ @ in @ experiment @ evaluate @ @ projection learning model @ a unified framework including multiple general-domain and domain-specific benchmark datasets in english and chinese language @ @ @ compare @ advantage and disadvantage of @ projection learning model @ different learning circumstance @ finally @ future research direction of @ work @ discussed @ focus on domain-specific and long-tail hypernymy prediction @ science @ @ @ right reserved @ 
202,Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches,"This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",2020,Neural Computing and Applications,0,@ @ concern @ application of recurrent neural network to text synthesis in @ word level @ @ help of keywords @ first a part of speech tagging library is employed to extract verb and noun @ @ text used in @ work a part of @ @ @ considered @ automatic elimination a @ aforementioned keywords @ @ ultimate aim is to train a recurrent neural network to map @ keyword sequence of a text to @ entire text @ successive reformulations of @ keyword and full-text word sequence @ performed @ @ @ @ serve a @ input and target of @ network a efficiently a possible @ @ predicted text @ understandable enough and @ model performance depends on @ problem difficulty determined by @ percentage of full-text word @ @ considered a keywords @ range @ to approximately @ training memory cost mainly affected by @ network architecture a well a @ similarity @ different text @ determines @ best architecture @ springer-verlag london ltd @ part of @ nature @ 
203,A multi-view similarity measure framework for trouble ticket mining,"Text similarity measures play a very important role in several text mining applications. Although there is an extensive literature on measuring the similarity between long texts, there is less work related to the measurement of similarity between short texts. And most of these works on short text similarity are based on adaptations of long-text similarity methods. Unfortunately, the description of a trouble ticket is just a kind of short texts. Thus, ticket mining applications such as ticket classification, ticket clustering, and ticket resolution recommendation often suffer from poor performance because of tickets’ particular characteristics of unstructured, short free-text with large vocabulary size, large volume, non-English dictionary words, and so on. Therefore, the ability to accurately measure the similarity between two tickets is critical to the performance of ticket mining. To address this performance issue, this paper proposes a multi-view similarity measure framework that easily integrates several kinds of existing similarity measures including surface matching based measures, semantic similarity measures and syntax based measures. Further, in order to make full use of the strengths of different similarity measures, our framework adopts four different policies to combine them. In particular, we consider a machine learning based policy that can be applied to integrate various similarity measures in a more general way, which makes our framework flexible and extensible. To demonstrate the effectiveness of measures generated from our framework, we empirically validate them on a publicly available short text data set and apply them to a real-world ticket data set from a large enterprise IT infrastructure. Some important findings obtained via the result analysis will be helpful to further improve performance. © 2020 Elsevier B.V.",2020,Data and Knowledge Engineering,0,text similarity measure play a @ important role in several text mining application @ although @ is @ extensive literature on measuring @ similarity @ long text @ is le work related to @ measurement of similarity @ short text @ and @ of @ work on short text similarity @ based on adaptation of long-text similarity method @ unfortunately @ description of a trouble ticket is @ a kind of short text @ thus ticket mining application @ a ticket classification ticket clustering and ticket resolution recommendation often suffer @ poor performance @ of ticket particular characteristic of unstructured short free-text @ @ vocabulary size @ volume non-english dictionary word and @ on @ therefore @ ability to accurately measure @ similarity @ @ ticket is critical to @ performance of ticket mining @ to address @ performance issue @ @ proposes a multi-view similarity measure framework @ easily integrates several kind of existing similarity measure including surface matching based measure semantic similarity measure and syntax based measure @ @ in order to make full use of @ strength of different similarity measure @ framework adopts four different policy to combine @ @ in particular @ consider a machine learning based policy @ @ @ applied to integrate various similarity measure in a more general way @ make @ framework flexible and extensible @ to demonstrate @ effectiveness of measure generated @ @ framework @ empirically validate @ on a publicly available short text data set and apply @ to a real-world ticket data set @ a @ enterprise @ infrastructure @ some important finding obtained via @ @ analysis @ @ helpful to @ improve performance @ @ b @ v @ 
205,Karcı summarization: A simple and effective approach for automatic text summarization using Karcı entropy,"Increases in the amount of text resources available via the Internet has amplified the need for automated document summarizing tools. However, further efforts are needed in order to improve the quality of the existing summarization tools currently available. The current study proposes Karcı Summarization, a novel methodology for extractive, generic summarization of text documents. Karcı Entropy was used for the first time in a document summarization method within a unique approach. An important feature of the proposed system is that it does not require any kind of information source or training data. At the stage of presenting the input text, a tool for text processing was introduced; known as KUSH (named after its authors; Karcı, Uçkan, Seyyarer, and Hark), and is used to protect semantic consistency between sentences. The Karcı Entropy-based solution chooses the most effective, generic and most informational sentences within a paragraph or unit of text. Experimentation with the Karcı Summarization approach was tested using open-access document text (Document Understanding Conference; DUC-2002, DUC-2004) datasets. Performance achievement of the Karcı Summarization approach was calculated using metrics known as Recall-Oriented Understudy for Gisting Evaluation (ROUGE). The experimental results showed that the proposed summarizer outperformed all current state-of-the-art methods in terms of 200-word summaries in the metrics of ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-W-1.2. In addition, the proposed summarizer outperformed the nearest competitive summarizers by a factor of 6.4% for ROUGE-1 Recall on the DUC-2002 dataset. These results demonstrate that Karcı Summarization is a promising technique and it is therefore expected to attract interest from researchers in the field. Our approach was shown to have a high potential for adoptability. Moreover, the method was assessed as quite insensitive to disorderly and missing texts due to its KUSH text processing module. © 2019 Elsevier Ltd",2020,Information Processing and Management,6,increase in @ amount of text resource available via @ internet ha amplified @ need @ automated document summarizing tool @ however @ effort @ needed in order to improve @ quality of @ existing summarization tool currently available @ @ current study proposes karcı summarization a novel methodology @ extractive generic summarization of text document @ karcı entropy wa used @ @ first time in a document summarization method within a unique approach @ @ important feature of @ proposed system is @ @ doe not require @ kind of information source @ training data @ at @ stage of presenting @ input text a tool @ text processing wa introduced @ known a kush @ named @ @ author @ karcı uçkan seyyarer and hark @ and is used to protect semantic consistency @ sentence @ @ karcı entropy-based solution chooses @ @ effective generic and @ informational sentence within a paragraph @ unit of text @ experimentation @ @ karcı summarization approach wa tested @ open-access document text @ document understanding conference @ duc duc @ datasets @ performance achievement of @ karcı summarization approach wa calculated @ metric known a recall-oriented understudy @ gisting evaluation @ rouge @ @ @ experimental @ showed @ @ proposed summarizer outperformed @ current state-of-the-art method in term of word summary in @ metric of rouge rouge rouge-l and rouge-w @ @ in addition @ proposed summarizer outperformed @ nearest competitive summarizers by a factor of @ @ rouge recall on @ duc dataset @ @ @ demonstrate @ karcı summarization is a promising technique and @ is therefore expected to attract interest @ researcher in @ field @ @ approach wa @ to @ a high potential @ adoptability @ moreover @ method wa assessed a quite insensitive to disorderly and missing text due to @ kush text processing module @ @ ltd
206,Phrase2Vec: Phrase embedding based on parsing,"Text is one of the most common unstructured data, and usually, the most primary task in text mining is to transfer the text into a structured representation. However, the existing text representation models split the complete semantic unit and neglect the order of words, finally lead to understanding bias. In this paper, we propose a novel phrase-based text representation method that takes into account the integrity of semantic units and utilizes vectors to represent the similarity relationship between texts. First, we propose HPMBP (Hierarchical Phrase Mining Based on Parsing) which mines hierarchical phrases by parsing and uses BOP (Bag Of Phrases) to represent text. Then, we put forward three phrase embedding models, called Phrase2Vec, including Skip-Phrase, CBOP (Continuous Bag Of Phrases), and GloVeFP (Global Vectors For Phrase Representation). They learn the phrase vector with semantic similarity, further obtain the vector representation of the text. Based on Phrase2Vec, we propose PETC (Phrase Embedding based Text Classification) and PETCLU (Phrase Embedding based Text Clustering). PETC utilizes the phrase embedding to get the text vector, which is fed to a neural network for text classification. PETCLU gets the vectorization expression of text and cluster center by Phrase2Vec, furthermore extends the K-means model for text clustering. To the best of our knowledge, it is the first work that focuses on the phrase-based English text representation. Experiments show that the introduced Phrase2Vec outperforms state-of-the-art phrase embedding models in the similarity task and the analogical reasoning task on Enwiki, DBLP, and Yelp dataset. PETC is superior to the baseline text classification methods in the F1-value index by about 4%. PETCLU is also ahead of the prevalent text clustering methods in entropy and purity indicators. In summary, Phrase2Vec is a promising approach to text mining. © 2019",2020,Information Sciences,6,text is @ of @ @ common unstructured data and usually @ @ primary task in text mining is to transfer @ text @ a structured representation @ however @ existing text representation model split @ complete semantic unit and neglect @ order of word finally lead to understanding bias @ in @ @ @ propose a novel phrase-based text representation method @ take @ account @ integrity of semantic unit and utilizes vector to represent @ similarity relationship @ text @ first @ propose hpmbp @ hierarchical phrase mining based on parsing @ @ mine hierarchical phrase by parsing and us bop @ bag of phrase @ to represent text @ @ @ put forward three phrase embedding model called phrase vec including skip-phrase cbop @ continuous bag of phrase @ and glovefp @ global vector @ phrase representation @ @ @ learn @ phrase vector @ semantic similarity @ obtain @ vector representation of @ text @ based on phrase vec @ propose petc @ phrase embedding based text classification @ and petclu @ phrase embedding based text clustering @ @ petc utilizes @ phrase embedding to get @ text vector @ is fed to a neural network @ text classification @ petclu get @ vectorization expression of text and cluster center by phrase vec furthermore extends @ k-means model @ text clustering @ to @ best of @ knowledge @ is @ first work @ focus on @ phrase-based english text representation @ experiment @ @ @ introduced phrase vec outperforms state-of-the-art phrase embedding model in @ similarity task and @ analogical reasoning task on enwiki dblp and yelp dataset @ petc is superior to @ baseline text classification method in @ f value index by @ @ petclu is @ ahead of @ prevalent text clustering method in entropy and purity indicator @ in summary phrase vec is a promising approach to text mining @ 
207,Query-based unsupervised learning for improving social media search,"In the current information era over the internet, social media has become one of the essential information sources for users. While the text is the primary information representation, finding relevant information is a challenging mission for researchers due to its nature (e.g., short length, sparseness). Acquiring high-quality search results from massive data, such as social media needs a set of representative query terms that are not always available. In this paper, we propose a novel query-based unsupervised learning model to represent the implicit relationships in the short text from social media. This bridges the gap of the lack of word co-occurrences without requiring many parameters to be estimated and external evidence to be collected. To confirm the proposed model effectiveness, we compare the proposed model with state-of-the-art lexical, topic model and temporal models on the large-scale TREC microblog 2011-2014 collections. The experimental results show that the proposed model significantly improved overall state-of-the-art lexical, topic model and temporal models with the maximum percentage of increase reaching 33.97% based on MAP value and 21.38% based on Precision at top 30 documents. The proposed model can improve the social media search effectiveness in potential closely retrieval tasks, such as question answering and timeline summarisation. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2020,World Wide Web,0,in @ current information era @ @ internet social medium ha become @ of @ essential information source @ user @ @ @ text is @ primary information representation finding relevant information is a challenging mission @ researcher due to @ nature @ e @ g @ short length sparseness @ @ acquiring high-quality search @ @ massive data @ a social medium need a set of representative query term @ @ not always available @ in @ @ @ propose a novel query-based unsupervised learning model to represent @ implicit relationship in @ short text @ social medium @ @ bridge @ gap of @ lack of word co-occurrence without requiring many parameter to @ estimated and external evidence to @ collected @ to confirm @ proposed model effectiveness @ compare @ proposed model @ state-of-the-art lexical topic model and temporal model on @ large-scale trec microblog collection @ @ experimental @ @ @ @ proposed model significantly improved overall state-of-the-art lexical topic model and temporal model @ @ maximum percentage of increase reaching @ based on map value and @ based on precision at top document @ @ proposed model @ improve @ social medium search effectiveness in potential closely retrieval task @ a question answering and timeline summarisation @ @ science @ medium llc part of @ nature @ 
209,Drug repurposing against Parkinson's disease by text mining the scientific literature,"Purpose: Drug repurposing involves the identification of new applications for existing drugs. Owing to the enormous rise in the costs of pharmaceutical R&D, several pharmaceutical companies are leveraging repurposing strategies. Parkinson's disease is the second most common neurodegenerative disorder worldwide, affecting approximately 1–2 percent of the human population older than 65 years. This study proposes a literature-based drug repurposing strategy in Parkinson's disease. Design/methodology/approach: The literature-based drug repurposing strategy proposed herein combined natural language processing, network science and machine learning methods for analyzing unstructured text data and producing actional knowledge for drug repurposing. The approach comprised multiple computational components, including the extraction of biomedical entities and their relationships, knowledge graph construction, knowledge representation learning and machine learning-based prediction. Findings: The proposed strategy was used to mine information pertaining to the mechanisms of disease treatment from known treatment relationships and predict drugs for repurposing against Parkinson's disease. The F1 score of the best-performing method was 0.97, indicating the effectiveness of the proposed approach. The study also presents experimental results obtained by combining the different components of the strategy. Originality/value: The drug repurposing strategy proposed herein for Parkinson's disease is distinct from those existing in the literature in that the drug repurposing pipeline includes components of natural language processing, knowledge representation and machine learning for analyzing the scientific literature. The results of the study provide important and valuable information to researchers studying different aspects of Parkinson's disease. © 2020, Emerald Publishing Limited.",2020,Library Hi Tech,3,purpose @ drug repurposing involves @ identification of @ application @ existing drug @ owing to @ enormous rise in @ cost of pharmaceutical r @ several pharmaceutical company @ leveraging repurposing strategy @ parkinson @ s disease is @ second @ common neurodegenerative disorder worldwide affecting approximately percent of @ human population older @ year @ @ study proposes a literature-based drug repurposing strategy in parkinson @ s disease @ design methodology approach @ @ literature-based drug repurposing strategy proposed herein combined natural language processing network science and machine learning method @ analyzing unstructured text data and producing actional knowledge @ drug repurposing @ @ approach comprised multiple computational component including @ extraction of biomedical entity and @ relationship knowledge graph construction knowledge representation learning and machine learning-based prediction @ finding @ @ proposed strategy wa used to mine information pertaining to @ mechanism of disease treatment @ known treatment relationship and predict drug @ repurposing @ parkinson @ s disease @ @ f score of @ best-performing method wa @ indicating @ effectiveness of @ proposed approach @ @ study @ @ experimental @ obtained by combining @ different component of @ strategy @ originality value @ @ drug repurposing strategy proposed herein @ parkinson @ s disease is distinct @ @ existing in @ literature in @ @ drug repurposing pipeline includes component of natural language processing knowledge representation and machine learning @ analyzing @ scientific literature @ @ @ of @ study provide important and valuable information to researcher studying different aspect of parkinson @ s disease @ emerald publishing limited @ 
210,Unusual customer response identification and visualization based on text mining and anomaly detection,"The Vehicle Dependability Study (VDS) is a survey study on customer satisfaction for vehicles that have been sold for three years. VDS data analytics plays an important role in the vehicle development process because it can contribute to enhancing the brand image and sales of an automobile company by properly reflecting customer requirements retrieved from the analysis results when developing the vehicle's next model. Conventional approaches to analyzing the voice of customers (VOC) data, such as VDS, have focused on finding the mainstream of customer responses, many of which are already known to the enterprise. However, detecting and visualizing notable opinions from a large amount of VOC data are important in responding to customer complaints. In this study, we propose a framework for identifying unusual but significant customer responses and frequently used words therein based on distributed document representation, local outlier factor, and TF–IDF methods. We also propose a procedure that can provide useful information to vehicle engineers by visualizing the main results of the framework. This unusual customer response detection and visualization framework can accelerate the efficiency and effectiveness of many VOC data analytics. © 2019",2020,Expert Systems with Applications,1,@ vehicle dependability study @ vd @ is a survey study on customer satisfaction @ vehicle @ @ @ sold @ three year @ vd data analytics play @ important role in @ vehicle development process @ @ @ contribute to enhancing @ brand image and sale of @ automobile company by properly reflecting customer requirement retrieved @ @ analysis @ @ developing @ vehicle @ s next model @ conventional approach to analyzing @ voice of customer @ voc @ data @ a vd @ focused on finding @ mainstream of customer response many of @ @ already known to @ enterprise @ however detecting and visualizing notable opinion @ a @ amount of voc data @ important in responding to customer complaint @ in @ study @ propose a framework @ identifying unusual @ significant customer response and frequently used word therein based on distributed document representation local outlier factor and tf idf method @ @ @ propose a procedure @ @ provide useful information to vehicle engineer by visualizing @ main @ of @ framework @ @ unusual customer response detection and visualization framework @ accelerate @ efficiency and effectiveness of many voc data analytics @ 
211,Bag-of-Concepts representation for document classification based on automatic knowledge acquisition from probabilistic knowledge base,"Text representation, a crucial step for text mining and natural language processing, concerns about transforming unstructured textual data into structured numerical vectors to support various machine learning and data mining algorithms. For document classification, one classical and commonly adopted text representation method is Bag-of-Words (BoW) model. BoW represents document as a fixed-length vector of terms, where each term dimension is a numerical value such as term frequency or tf-idf weight. However, BoW simply looks at surface form of words. It ignores the semantic, conceptual and contextual information of texts, and also suffers from high dimensionality and sparsity issues. To address the aforementioned issues, we propose a novel document representation scheme called Bag-of-Concepts (BoC), which automatically acquires useful conceptual knowledge from external knowledge base, then conceptualizes words and phrases in the document into higher level semantics (i.e. concepts) in a probabilistic manner, and eventually represents a document as a distributed vector in the learned concept space. By utilizing background knowledge from knowledge base, BoC representation is able to provide more semantic and conceptual information of texts, as well as better interpretability for human understanding. We also propose Bag-of-Concept-Clusters (BoCCl) model which clusters semantically similar concepts together and performs entity sense disambiguation to further improve BoC representation. In addition, we combine BoCCl and BoW representations using an attention mechanism to effectively utilize both concept-level and word-level information and achieve optimal performance for document classification. © 2019",2020,Knowledge-Based Systems,3,text representation a crucial step @ text mining and natural language processing concern @ transforming unstructured textual data @ structured numerical vector to support various machine learning and data mining algorithm @ @ document classification @ classical and commonly adopted text representation method is bag-of-words @ bow @ model @ bow represents document a a fixed-length vector of term @ @ term dimension is a numerical value @ a term frequency @ tf-idf weight @ however bow simply look at surface form of word @ @ ignores @ semantic conceptual and contextual information of text and @ suffers @ high dimensionality and sparsity issue @ to address @ aforementioned issue @ propose a novel document representation scheme called bag-of-concepts @ boc @ @ automatically acquires useful conceptual knowledge @ external knowledge base @ conceptualizes word and phrase in @ document @ higher level semantics @ i @ e @ concept @ in a probabilistic manner and eventually represents a document a a distributed vector in @ learned concept space @ by utilizing background knowledge @ knowledge base boc representation is able to provide more semantic and conceptual information of text a well a better interpretability @ human understanding @ @ @ propose bag-of-concept-clusters @ boccl @ model @ cluster semantically similar concept together and performs entity sense disambiguation to @ improve boc representation @ in addition @ combine boccl and bow representation @ @ attention mechanism to effectively utilize @ concept-level and word-level information and achieve optimal performance @ document classification @ 
212,A deep-learning approach to mining conditions,"A condition is a constraint that determines when a consequent holds. Mining them in text is paramount to understand many sentences properly. In the literature, there are a few pattern-based proposals that fall short regarding recall because it is not easy to characterise unusual ways to express conditions with hand-crafted patterns; there is one machine-learning proposal that is bound to the Japanese language, requires specific-purpose dictionaries, taxonomies, and heuristics, works on opinion sentences only, and was evaluated very shallowly. In this article, we present a deep-learning proposal to mine conditions that does not have any of the previous drawbacks; furthermore, we have performed a comprehensive experimental study on a large multi-lingual dataset on many common topics; our conclusion is that our proposals are similar to the state of the art in terms of precision, but improve recall enough to beat them in terms of F1 score. © 2020 Elsevier B.V.",2020,Knowledge-Based Systems,1,a condition is a constraint @ determines @ a consequent hold @ mining @ in text is paramount to understand many sentence properly @ in @ literature @ @ a @ pattern-based proposal @ fall short regarding recall @ @ is not easy to characterise unusual way to express condition @ hand-crafted pattern @ @ is @ machine-learning proposal @ is bound to @ japanese language requires specific-purpose dictionary taxonomy and heuristic work on opinion sentence only and wa evaluated @ shallowly @ in @ article @ @ a deep-learning proposal to mine condition @ doe not @ @ of @ previous drawback @ furthermore @ @ performed a comprehensive experimental study on a @ multi-lingual dataset on many common topic @ @ conclusion is @ @ proposal @ similar to @ state of @ art in term of precision @ improve recall enough to beat @ in term of f score @ @ b @ v @ 
215,Automatic keyphrase extraction: a survey and trends,"Due to the exponential growth of textual data and web sources, an automatic mechanism is required to identify relevant information embedded within them. The utility of Automatic Keyphrase Extraction (AKPE) cannot be overstated, given its widespread adoption in many Information Retrieval (IR), Natural Language Processing (NLP) and Text Mining (TM) applications, and its potential ability to solve difficulties related to extracting valuable information. In recent years, a wide range of AKPE techniques have been proposed. However, they are still impaired by low accuracy rates and moderate performance. This paper provides a comprehensive review of recent research efforts on the AKPE task and its related techniques. More concretely, we highlight the common process of this task, while also illustrating the various approaches used (supervised, unsupervised, and Deep Learning) and released techniques. We investigate the major challenges that such techniques face and depict the specific complexities they address. Besides, we provide a comparison study of the best performing techniques, discuss why some perform better than others and propose recommendations to improve each stage of the AKPE process. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Journal of Intelligent Information Systems,0,due to @ exponential growth of textual data and web source @ automatic mechanism is required to identify relevant information embedded within @ @ @ utility of automatic keyphrase extraction @ akpe @ cannot @ overstated given @ widespread adoption in many information retrieval @ ir @ natural language processing @ nlp @ and text mining @ tm @ application and @ potential ability to solve difficulty related to extracting valuable information @ in recent year a wide range of akpe technique @ @ proposed @ however @ @ still impaired by low accuracy rate and moderate performance @ @ @ provides a comprehensive review of recent research effort on @ akpe task and @ related technique @ more concretely @ highlight @ common process of @ task @ @ illustrating @ various approach used @ supervised unsupervised and deep learning @ and released technique @ @ investigate @ major challenge @ @ technique face and depict @ specific complexity @ address @ besides @ provide a comparison study of @ best performing technique discus @ some perform better @ others and propose recommendation to improve @ stage of @ akpe process @ @ science @ medium llc part of @ nature @ 
217,ArA*summarizer: An Arabic text summarization system based on subtopic segmentation and using an A* algorithm for reduction,"Automatic text summarization is a field situated at the intersection of natural language processing and information retrieval. Its main objective is to automatically produce a condensed representative form of documents. This paper presents ArA*summarizer, an automatic system for Arabic single document summarization. The system is based on an unsupervised hybrid approach that combines statistical, cluster-based, and graph-based techniques. The main idea is to divide text into subtopics then select the most relevant sentences in the most relevant subtopics. The selection process is done by an A* algorithm executed on a graph representing the different lexical–semantic relationships between sentences. Experimentation is conducted on Essex Arabic summaries corpus and using recall-oriented understudy for gisting evaluation, automatic summarization engineering, merged model graphs, and n-gram graph powered evaluation via regression evaluation metrics. The evaluation results showed the good performance of our system compared with existing works. © 2020 John Wiley & Sons, Ltd",2020,Expert Systems,0,automatic text summarization is a field situated at @ intersection of natural language processing and information retrieval @ @ main objective is to automatically produce a condensed representative form of document @ @ @ @ ara summarizer @ automatic system @ arabic single document summarization @ @ system is based on @ unsupervised hybrid approach @ combine statistical cluster-based and graph-based technique @ @ main idea is to divide text @ subtopics @ select @ @ relevant sentence in @ @ relevant subtopics @ @ selection process is done by @ a algorithm executed on a graph representing @ different lexical semantic relationship @ sentence @ experimentation is conducted on essex arabic summary corpus and @ recall-oriented understudy @ gisting evaluation automatic summarization engineering merged model graph and n-gram graph powered evaluation via regression evaluation metric @ @ evaluation @ showed @ good performance of @ system compared @ existing work @ john wiley son ltd
221,Exploring disorder-aware attention for clinical event extraction,"Event extraction is one of the crucial tasks in biomedical text mining that aims to extract specific information concerning incidents embedded in the texts. In this article, we propose a deep learning framework that aims to identify the attributes (severity, course, temporal expression, and document creation time) associated with the medical concepts extracted from electronic medical records. The bi-directional long short-term memory network assisted by the attention mechanism is utilized to uncover the important aspects of the patient's medical conditions. The attention mechanism specific to the medical disorder mention can focus on various parts of the sentence when different disorders are considered as input. The proposed methodology is evaluated on benchmark ShARe/CLEF eHealth Evaluation Lab 2014 shared task 2 datasets. In addition to the CLEF dataset, we also used the social media text, especially the medical blog posts. Experimental results of the proposed approach illustrate that our proposed approach achieves significant performance improvements over the state-of-the-art techniques and the highly competitive deep learning - based baseline methods. © 2020 ACM.",2020,"ACM Transactions on Multimedia Computing, Communications and Applications",1,event extraction is @ of @ crucial task in biomedical text mining @ aim to extract specific information concerning incident embedded in @ text @ in @ article @ propose a deep learning framework @ aim to identify @ attribute @ severity course temporal expression and document creation time @ associated @ @ medical concept extracted @ electronic medical record @ @ bi-directional long short-term memory network assisted by @ attention mechanism is utilized to uncover @ important aspect of @ patient @ s medical condition @ @ attention mechanism specific to @ medical disorder mention @ focus on various part of @ sentence @ different disorder @ considered a input @ @ proposed methodology is evaluated on benchmark share clef ehealth evaluation lab shared task datasets @ in addition to @ clef dataset @ @ used @ social medium text especially @ medical blog post @ experimental @ of @ proposed approach illustrate @ @ proposed approach achieves significant performance improvement @ @ state-of-the-art technique and @ highly competitive deep learning based baseline method @ acm @ 
222,Textual entailment - Based figure summarization for biomedical articles,"This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure's caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure's caption, and the number of overlapping words between sentences and a figure's caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F-measure metric over two datasets, compared to the state-of-the-art unsupervised methods. © 2020 ACM.",2020,"ACM Transactions on Multimedia Computing, Communications and Applications",3,@ article proposes a novel unsupervised approach @ figsum @ @ automatic figure summarization in biomedical scientific article @ a multi-objective evolutionary algorithm @ @ problem is treated a @ optimization problem @ relevant sentence in @ summary @ a given figure @ selected based on various sentence scoring feature @ @ objective function @ @ a @ textual entailment score @ sentence in @ summary and a figure @ s caption @ number of sentence referring to @ figure semantic similarity @ sentence and a figure @ s caption and @ number of overlapping word @ sentence and a figure @ s caption @ @ objective function @ optimized simultaneously @ multi-objective binary differential evolution @ mbde @ @ mbde consists of a set of solution and @ solution represents a subset of sentence to @ selected in @ summary @ mbde generally us a single differential evolution variant @ in @ current study @ ensemble of @ different differential evolution variant measuring diversity among solution and convergence toward global optimal solution respectively is employed @ efficient search @ usually in @ summarization system diversity among sentence @ called anti-redundancy @ in @ summary is a @ critical feature and @ is calculated in term of similarity @ like cosine similarity @ among sentence @ in @ article a @ way of measuring diversity in term of textual entailment is proposed @ to represent @ sentence of @ article in @ form of numeric vector @ recently proposed biobert pre-trained language model in biomedical text mining is utilized @ @ ablation study ha @ @ presented to determine @ importance of different objective function @ @ evaluation of @ proposed technique @ benchmark biomedical datasets containing and figure @ considered @ @ proposed system obtains and improvement in term of @ f-measure metric @ @ datasets compared to @ state-of-the-art unsupervised method @ acm @ 
225,A deep learning analysis on question classification task using Word2vec representations,"Question classification is a primary essential study for automatic question answering implementations. Linguistic features take a significant role to develop an accurate question classifier. Recently, deep learning systems have achieved remarkable success in various text-mining problems such as sentiment analysis, document classification, spam filtering, document summarization, and web mining. In this study, we explain our study on investigating some deep learning architectures for a question classification task in a highly inflectional language Turkish that is an agglutinative language where word structure is produced by adding suffixes (morphemes) to root word. As a non-Indo-European language, languages like Turkish have some unique features, which make it challenging for natural language processing. For instance, Turkish has no grammatical gender and noun classes. In this study, user questions in Turkish are used to train and test the deep learning architectures. In addition to this, the details of the deep learning architectures are compared in terms of test and 10-cross fold validation accuracy. We use two major deep learning models in our paper: long short-term memory (LSTM), Convolutional Neural Networks (CNN), and we also implemented the combination of CNN-LSTM, CNN-SVM structures and a number of various those architectures by changing vector sizes and the embedding types. As well as this, we have built word embeddings using the Word2vec method with a CBOW and skip gram models with different vector sizes on a large corpus composed of user questions. Our another investigation is the effect of using different Word2vec pre-trained word embeddings on these deep learning architectures. Experiment results show that the use of different Word2vec models has a significant impact on the accuracy rate on different deep learning models. Additionally, there is no Turkish question dataset labeled and so another contribution in this study is that we introduce new Turkish question dataset which is translated from UIUC English question dataset. By using these techniques, we have reached an accuracy of 94% on the question dataset. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",2020,Neural Computing and Applications,5,question classification is a primary essential study @ automatic question answering implementation @ linguistic feature take a significant role to develop @ accurate question classifier @ recently deep learning system @ achieved remarkable success in various text-mining problem @ a sentiment analysis document classification spam filtering document summarization and web mining @ in @ study @ explain @ study on investigating some deep learning architecture @ a question classification task in a highly inflectional language turkish @ is @ agglutinative language @ word structure is produced by adding suffix @ morpheme @ to root word @ a a non-indo-european language language like turkish @ some unique feature @ make @ challenging @ natural language processing @ @ instance turkish ha no grammatical gender and noun class @ in @ study user question in turkish @ used to train and test @ deep learning architecture @ in addition to @ @ detail of @ deep learning architecture @ compared in term of test and cross fold validation accuracy @ @ use @ major deep learning model in @ @ @ long short-term memory @ lstm @ convolutional neural network @ cnn @ and @ @ implemented @ combination of cnn-lstm cnn-svm structure and a number of various @ architecture by changing vector size and @ embedding type @ a well a @ @ @ built word embeddings @ @ word vec method @ a cbow and skip gram model @ different vector size on a @ corpus composed of user question @ @ another investigation is @ effect of @ different word vec pre-trained word embeddings on @ deep learning architecture @ experiment @ @ @ @ use of different word vec model ha a significant impact on @ accuracy rate on different deep learning model @ additionally @ is no turkish question dataset labeled and @ another contribution in @ study is @ @ introduce @ turkish question dataset @ is translated @ uiuc english question dataset @ by @ @ technique @ @ reached @ accuracy of on @ question dataset @ springer-verlag london ltd @ part of @ nature @ 
227,Idiom-based features in sentiment analysis: Cutting the gordian knot,"In this paper we describe an automated approach to enriching sentiment analysis with idiom-based features. Specifically, we automated the development of the supporting lexico-semantic resources, which include (1) a set of rules used to identify idioms in text and (2) their sentiment polarity classifications. Our method demonstrates how idiom dictionaries, which are readily available general pedagogical resources, can be adapted into purpose-specific computational resources automatically. These resources were then used to replace the manually engineered counterparts in an existing system, which originally outperformed the baseline sentiment analysis approaches by 17 percentage points on average, taking the F-measure from 40s into 60s. The new fully automated approach outperformed the baselines by 8 percentage points on average taking the F-measure from 40s into 50s. Although the latter improvement is not as high as the one achieved with the manually engineered features, it has got the advantage of being more general in a sense that it can readily utilize an arbitrary list of idioms without the knowledge acquisition overhead previously associated with this task, thereby fully automating the original approach. © 2010-2012 IEEE.",2020,IEEE Transactions on Affective Computing,0,in @ @ @ describe @ automated approach to enriching sentiment analysis @ idiom-based feature @ specifically @ automated @ development of @ supporting lexico-semantic resource @ include @ @ a set of rule used to identify idiom in text and @ @ @ sentiment polarity classification @ @ method demonstrates @ idiom dictionary @ @ readily available general pedagogical resource @ @ adapted @ purpose-specific computational resource automatically @ @ resource @ @ used to replace @ manually engineered counterpart in @ existing system @ originally outperformed @ baseline sentiment analysis approach by percentage point on average taking @ f-measure @ s @ s @ @ @ fully automated approach outperformed @ baseline by percentage point on average taking @ f-measure @ s @ s @ although @ latter improvement is not a high a @ @ achieved @ @ manually engineered feature @ ha got @ advantage of @ more general in a sense @ @ @ readily utilize @ arbitrary list of idiom without @ knowledge acquisition overhead @ associated @ @ task thereby fully automating @ original approach @ @ @ 
228,Textual data dimensionality reduction - a deep learning approach,"The growth of Internet has produced a high volume of natural language textual data. Such data can be sparse and may contain uninformative features which increase the dimensions of the data. This high dimensionality in turn, decreases the efficiency of text mining tasks such as clustering. Transforming the high dimensional data into a lower dimension is an important pre-processing step before applying clustering. In this paper, dimensionality reduction method based on deep Autoencoder neural network named as DRDAE, is proposed to provide optimized and robust features for text clustering. DRDAE selects less correlated and salient feature space from the high dimensional feature space. To evaluate proposed algorithm, k-means is used to cluster text documents. The proposed method is tested on five benchmark text datasets. Simulation results demonstrate that the proposed algorithm clearly outperforms other conventional dimensionality reduction methods in the literature in terms of RI measure. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Multimedia Tools and Applications,0,@ growth of internet ha produced a high volume of natural language textual data @ @ data @ @ sparse and may contain uninformative feature @ increase @ dimension of @ data @ @ high dimensionality in turn decrease @ efficiency of text mining task @ a clustering @ transforming @ high dimensional data @ a lower dimension is @ important pre-processing step @ applying clustering @ in @ @ dimensionality reduction method based on deep autoencoder neural network named a drdae is proposed to provide optimized and robust feature @ text clustering @ drdae selects le correlated and salient feature space @ @ high dimensional feature space @ to evaluate proposed algorithm k-means is used to cluster text document @ @ proposed method is tested on five benchmark text datasets @ simulation @ demonstrate @ @ proposed algorithm clearly outperforms @ conventional dimensionality reduction method in @ literature in term of ri measure @ @ science @ medium llc part of @ nature @ 
233,Generation of topic evolution graphs from short text streams,"Topic evolution mining on short texts is an important research topic in natural language processing. Existing methods have been focused either on the topic evolution of normal documents or on the evolution of topics along a timeline. In this paper, we aim to generate topic evolutionary graphs from short texts, which not only capture the main topic timeline, but also reveal the correlations between related subtopics. Firstly, we propose an Encoder-only Transformer Language Model (ETLM) to quantify the relationship between words. Then we propose a novel topic model, referred as weighted Conditional random field regularized Correlated Topic Model (CCTM), which leverages semantic correlations to discover meaningful topics and topic correlations. Finally, topic evolutionary graphs are generated by an Online version of CCTM (OCCTM) to capture the evolutionary patterns of main topics and related subtopics. Experimental results on real-world datasets demonstrate our method outperforms baselines on quality of topics and presents motivated patterns for topic evolution mining. © 2019",2020,Neurocomputing,6,topic evolution mining on short text is @ important research topic in natural language processing @ existing method @ @ focused either on @ topic evolution of normal document @ on @ evolution of topic along a timeline @ in @ @ @ aim to generate topic evolutionary graph @ short text @ not only capture @ main topic timeline @ @ reveal @ correlation @ related subtopics @ firstly @ propose @ encoder-only transformer language model @ etlm @ to quantify @ relationship @ word @ @ @ propose a novel topic model referred a weighted conditional random field regularized correlated topic model @ cctm @ @ leverage semantic correlation to discover meaningful topic and topic correlation @ finally topic evolutionary graph @ generated by @ online version of cctm @ occtm @ to capture @ evolutionary pattern of main topic and related subtopics @ experimental @ on real-world datasets demonstrate @ method outperforms baseline on quality of topic and @ motivated pattern @ topic evolution mining @ 
236,Improving named entity recognition in noisy user-generated text with local distance neighbor feature,"Recognizing infrequent or emerging named entities in a user-generated text is a challenging task, especially when informal or slang text is used. Some recent works propose to use a gazetteer to solve this problem, but this solution is not general because the gazetteer is task-specific and its maintenance is costly. In this paper, we overcome this drawback by presenting Local Distance Neighbor (LDN), a novel feature that substitutes the gazetteer and makes that the model obtains state-of-the-art results. LDN captures an initial guess for each input token based on the categories of its neighboring tokens within an embedding space. We evaluated the proposed network on the W-NUT-2017 dataset, and we obtained the state-of-the-art F1 score for the Group, Person, and Product categories. We employed our new feature together with the model proposed by Aguilar et al. to recognize named entities in the Tor Darknet related to suspicious activities associated with weapons and drug selling. After increasing the samples of the W-NUT-2017 dataset with 851 manually annotated entries, we repeated our evaluation in this extended version of the dataset, achieving entity and surface F1 scores of 52.96% and 50.57%, respectively. Furthermore, we demonstrate that our proposal can be useful for Law Enforcement Agencies in mining the textual information in the Tor hidden services, being especially adequate for the Group, Person, and Product categories. © 2019 Elsevier B.V.",2020,Neurocomputing,2,recognizing infrequent @ emerging named entity in a user-generated text is a challenging task especially @ informal @ slang text is used @ some recent work propose to use a gazetteer to solve @ problem @ @ solution is not general @ @ gazetteer is task-specific and @ maintenance is costly @ in @ @ @ overcome @ drawback by presenting local distance neighbor @ ldn @ a novel feature @ substitute @ gazetteer and make @ @ model obtains state-of-the-art @ @ ldn capture @ initial guess @ @ input token based on @ category of @ neighboring token within @ embedding space @ @ evaluated @ proposed network on @ w-nut dataset and @ obtained @ state-of-the-art f score @ @ group person and product category @ @ employed @ @ feature together @ @ model proposed by aguilar et al @ to recognize named entity in @ tor darknet related to suspicious activity associated @ weapon and drug selling @ @ increasing @ sample of @ w-nut dataset @ manually annotated entry @ repeated @ evaluation in @ extended version of @ dataset achieving entity and surface f score of @ and @ respectively @ furthermore @ demonstrate @ @ proposal @ @ useful @ law enforcement agency in mining @ textual information in @ tor hidden service @ especially adequate @ @ group person and product category @ @ b @ v @ 
240,"Comparing the Use of Research Resource Identifiers and Natural Language Processing for Citation of Databases, Software, and Other Digital Artifacts","The Research Resource Identifier (RRID) was introduced in 2014 to better identify biomedical research resources and track their use across the literature, including key digital resources such as databases and software. Authors include an RRID after the first mention of any resource used. Here, we provide an overview of RRIDs and analyze their use for digital resource identification. We quantitatively compare the output of our RRID curation workflow with the outputs of automated text mining systems used to identify resource mentions in text. The results show that authors follow RRID reporting guidelines well, and that our natural language processing based text mining was able to identify nearly all of the resources identified by RRIDs as well as thousands more. Finally, we demonstrate how RRIDs and text mining can complement each other to provide a scalable solution to digital resource citation. © 1999-2011 IEEE.",2020,Computing in Science and Engineering,3,@ research resource identifier @ rrid @ wa introduced in to better identify biomedical research resource and track @ use across @ literature including key digital resource @ a database and software @ author include @ rrid @ @ first mention of @ resource used @ @ @ provide @ overview of rrids and analyze @ use @ digital resource identification @ @ quantitatively compare @ output of @ rrid curation workflow @ @ output of automated text mining system used to identify resource mention in text @ @ @ @ @ author follow rrid reporting guideline well and @ @ natural language processing based text mining wa able to identify nearly @ of @ resource identified by rrids a well a thousand more @ finally @ demonstrate @ rrids and text mining @ complement @ @ to provide a scalable solution to digital resource citation @ @ @ 
245,NoteSum: An integrated note summarization system by using text mining algorithms,"This study implemented an integrated system of Note Summarization (NoteSum) that merged with multi-users’ notes and searched for relevant information on the Internet and, slides, and textbooks to create a summary for students to learn effectively. The integrated system's framework consists of four different modules: Topic Identification Module, Supporting Material Finding Module, Content Mapping Module and Learning Material Integrating Module. Five experiments were conducted; these resulted in the following findings. First, translating notes with the assistance of topic terms could enhance translation quality. Second, when mapping contents, NoteSum performed better in a discussion-based course rather than in a technical course. Third, the Jensen-Shannon (JS) Divergence was used to assess the generated summary that performed better for the discussion-based course. Fourth, the three attributes—presence of topic terms, number of non-topic words, and ratio of the words with important parts of speech—had different effects on different subjects. Finally, we compared NoteSum with other existing summarization systems. The results indicated that the NoteSum-generated summary was closer to students’ original notes and thus resulted in better performance in readability, informativeness, and completeness. All the results confirm that our proposed NoteSum is an effective note summarization system for student learning. © 2019",2020,Information Sciences,1,@ study implemented @ integrated system of note summarization @ notesum @ @ merged @ multi-users note and searched @ relevant information on @ internet and slide and textbook to create a summary @ student to learn effectively @ @ integrated system @ s framework consists of four different module @ topic identification module supporting material finding module content mapping module and learning material integrating module @ five experiment @ conducted @ @ resulted in @ following finding @ first translating note @ @ assistance of topic term could enhance translation quality @ second @ mapping content notesum performed better in a discussion-based course rather @ in a technical course @ third @ jensen-shannon @ j @ divergence wa used to ass @ generated summary @ performed better @ @ discussion-based course @ fourth @ three attribute presence of topic term number of non-topic word and ratio of @ word @ important part of speech @ different effect on different subject @ finally @ compared notesum @ @ existing summarization system @ @ @ indicated @ @ notesum-generated summary wa closer to student original note and thus resulted in better performance in readability informativeness and completeness @ @ @ @ confirm @ @ proposed notesum is @ effective note summarization system @ student learning @ 
249,Emoticons & emojis based sentiment analysis: The last two decades!,"Sentiment analysis has changed the way the information is perceived and utilized by demonstrating that the computational recognition of a sentimental expression is feasible. With people continuing to express a variety of sentiments and making assessments online, it has become a challenge to mine sentiments accurately from the ever-multiplying Big Data. These days most of text online consists of both the text and emoticons or Emojis. Many people use the words, Emoticons and Emojis interchangeably. Though they both represent expressions which people miss in non-verbal conversation, there's a slight difference between the two which is discussed in this paper. Emoticons (Emojis) do contribute to the sentiment analysis and have been proven to impact the accuracy. In view of the above, a considerable amount of attention of researchers has been drawn by the Emoticon based approach to human sentiment analysis. In this paper, we shed a light on the research trends in the field by presenting the research work in this domain in the last two decades. This paper presents a systematic division of work of researchers in the field together with the research issues and challenges in the area of Emoticons (Emojis) based Sentiment Analysis. To the best of our knowledge, almost all the literature in the field has been covered in this paper. © 2020 IJSTR.",2020,International Journal of Scientific and Technology Research,0,sentiment analysis ha changed @ way @ information is perceived and utilized by demonstrating @ @ computational recognition of a sentimental expression is feasible @ @ people continuing to express a variety of sentiment and making assessment online @ ha become a challenge to mine sentiment accurately @ @ ever-multiplying big data @ @ day @ of text online consists of @ @ text and emoticon @ emojis @ many people use @ word emoticon and emojis interchangeably @ though @ @ represent expression @ people miss in non-verbal conversation @ @ s a slight difference @ @ @ @ is discussed in @ @ @ emoticon @ emojis @ @ contribute to @ sentiment analysis and @ @ proven to impact @ accuracy @ in view of @ @ a considerable amount of attention of researcher ha @ drawn by @ emoticon based approach to human sentiment analysis @ in @ @ @ shed a light on @ research trend in @ field by presenting @ research work in @ domain in @ last @ decade @ @ @ @ a systematic division of work of researcher in @ field together @ @ research issue and challenge in @ area of emoticon @ emojis @ based sentiment analysis @ to @ best of @ knowledge almost @ @ literature in @ field ha @ covered in @ @ @ ijstr @ 
251,A question answering system in hadith using linguistic knowledge,"Question answering system aims at retrieving precise information from a large collection of documents. This work presents a question answering method to apply on Hadith in order to provide an informative answer corresponding to the user's query. Hadith englobes stories and qualification of the prophet Muhammad (PBSL). It also includes the sayings of his companions and their disciples. The problem with current methods is that they fail to capture the meaning when comparing a sentence and a user's query; hence there is often a conflict between the extracted sentences and user's requirements. However, our proposed method has successfully tackled this problem through: (1) avoiding extract a passage whose similarity with the query is high but whose meaning is different. (2) Computing the semantic and syntactic similarity of the sentence-to-sentence and sentence-to-query. (3) Expanding the words in both the query and sentences to tackle the fundamental problem of term mismatch between sentences and the user's query. Furthermore, in order to reduce redundant Hadith texts, the proposed method uses the greedy algorithm to impose diversity penalty on the sentences. The experimental results display that the proposed method is able to improve performance compared with the existing methods on Hadith datasets. © 2019 Elsevier Ltd",2020,Computer Speech and Language,3,question answering system aim at retrieving precise information @ a @ collection of document @ @ work @ a question answering method to apply on hadith in order to provide @ informative answer corresponding to @ user @ s query @ hadith englobes story and qualification of @ prophet muhammad @ pbsl @ @ @ @ includes @ saying of @ companion and @ disciple @ @ problem @ current method is @ @ fail to capture @ meaning @ comparing a sentence and a user @ s query @ hence @ is often a conflict @ @ extracted sentence and user @ s requirement @ however @ proposed method ha successfully tackled @ problem @ @ @ @ avoiding extract a passage whose similarity @ @ query is high @ whose meaning is different @ @ @ computing @ semantic and syntactic similarity of @ sentence-to-sentence and sentence-to-query @ @ @ expanding @ word in @ @ query and sentence to tackle @ fundamental problem of term mismatch @ sentence and @ user @ s query @ furthermore in order to reduce redundant hadith text @ proposed method us @ greedy algorithm to impose diversity penalty on @ sentence @ @ experimental @ display @ @ proposed method is able to improve performance compared @ @ existing method on hadith datasets @ @ ltd
255,Vocabulary analysis of the archives of design research: Utilizing corpus and text mining techniques,"Background There have been noticeably fewer studies of design terminology in comparison to the studies of the practical design field of South Korea. The study of design terminology, especially the study that looks into the aspect of the use of vocabulary, corresponds to the foundation of design research. This study examines whether the aspect of the use of vocabulary in the design field of South Korea can be technically analyzed using a text mining technique and whether it will be a meaningful method for the study of design terminology. Methods Having constructed the corpus from 2,214 papers written in Korean published in the Archives of Design Research(Korean Society of Design Science), I intended to grasp the aspect of the use of vocabulary with a text mining technique. In the analyzing process, Python libraries and analytical techniques of frequency and distribution(similarities) were used which are widely utilized in natural language processing. Results We looked into a possibility to find a meaningful vocabulary list and similarity among the vocabulary from the design corpus through text mining. We confirmed that the sufficiently interesting findings from the study are related to the design terminology through a series of results. Conclusions This study is the first case to construct a corpus in the design field and to analyze the aspect of the use of vocabulary with a text mining technique. I expect that this study contributes to the vitalization of the study of design terminology with various viewpoints in the future. © Korean Society of Design Science.",2020,Archives of Design Research,0,background @ @ @ noticeably fewer study of design terminology in comparison to @ study of @ practical design field of south korea @ @ study of design terminology especially @ study @ look @ @ aspect of @ use of vocabulary corresponds to @ foundation of design research @ @ study examines whether @ aspect of @ use of vocabulary in @ design field of south korea @ @ technically analyzed @ a text mining technique and whether @ @ @ a meaningful method @ @ study of design terminology @ method @ constructed @ corpus @ @ written in korean published in @ archive of design research @ korean society of design science @ i intended to grasp @ aspect of @ use of vocabulary @ a text mining technique @ in @ analyzing process python library and analytical technique of frequency and distribution @ similarity @ @ used @ @ widely utilized in natural language processing @ @ @ looked @ a possibility to find a meaningful vocabulary list and similarity among @ vocabulary @ @ design corpus @ text mining @ @ confirmed @ @ sufficiently interesting finding @ @ study @ related to @ design terminology @ a series of @ @ conclusion @ study is @ first case to construct a corpus in @ design field and to analyze @ aspect of @ use of vocabulary @ a text mining technique @ i expect @ @ study contributes to @ vitalization of @ study of design terminology @ various viewpoint in @ future @ korean society of design science @ 
264,Analysing performance of text classification models for sentiment analysis of movie reviews,"Text mining is the process of extracting interesting, non-trivial extraction of implicit, previously unknown and useful information from huge volume of textual data. It has become a vibrant research area. It deals with machine supported exploration of text. It uses the techniques from information retrieval, and extraction along with natural language processing. With the increasing amount of data being available on the web, this tremendous volume of data mostly unstructured text, it is a tough task to analyse their content within a short span of time. The crucial challenge in text classification techniques is the overall performance. With the growth of computer technology, there have been many classification algorithms. Each classification algorithms will get different result at speed and efficiency due to the various features of test data. In this paper, we discuss performance measures such as accuracy, recall and precision for text classification model using Random Forest Classifier and Naïve Bayes classifier. We have done pre-processing before applying classification methods. It has been found that Random Forest classifier has a higher accuracy compared with Naïve Bayes classification for sentimental analysis of movie reviews. © IJSTR 2020.",2020,International Journal of Scientific and Technology Research,0,text mining is @ process of extracting interesting non-trivial extraction of implicit @ unknown and useful information @ huge volume of textual data @ @ ha become a vibrant research area @ @ deal @ machine supported exploration of text @ @ us @ technique @ information retrieval and extraction along @ natural language processing @ @ @ increasing amount of data @ available on @ web @ tremendous volume of data mostly unstructured text @ is a tough task to analyse @ content within a short span of time @ @ crucial challenge in text classification technique is @ overall performance @ @ @ growth of computer technology @ @ @ many classification algorithm @ @ classification algorithm @ get different @ at speed and efficiency due to @ various feature of test data @ in @ @ @ discus performance measure @ a accuracy recall and precision @ text classification model @ random forest classifier and naïve bayes classifier @ @ @ done pre-processing @ applying classification method @ @ ha @ found @ random forest classifier ha a higher accuracy compared @ naïve bayes classification @ sentimental analysis of movie review @ ijstr @ 
265,The OpenScience Slovenia metadata dataset,"The OpenScience Slovenia metadata dataset contains metadata entries for Slovenian public domain academic documents which include undergraduate and postgraduate theses, research and professional articles, along with other academic document types. The data within the dataset was collected as a part of the establishment of the Slovenian Open-Access Infrastructure which defined a unified document collection process and cataloguing for universities in Slovenia within the infrastructure repositories. The data was collected from several already established but separate library systems in Slovenia and merged into a single metadata scheme using metadata deduplication and merging techniques. It consists of text and numerical fields, representing attributes that describe documents. These attributes include document titles, keywords, abstracts, typologies, authors, issue years and other identifiers such as URL and UDC. The potential of this dataset lies especially in text mining and text classification tasks and can also be used in development or benchmarking of content-based recommender systems on real-world data. © 2019 The Author(s)",2020,Data in Brief,0,@ openscience slovenia metadata dataset contains metadata entry @ slovenian public domain @ document @ include undergraduate and postgraduate thesis research and professional article along @ @ @ document type @ @ data within @ dataset wa collected a a part of @ establishment of @ slovenian open-access infrastructure @ defined a unified document collection process and cataloguing @ university in slovenia within @ infrastructure repository @ @ data wa collected @ several already established @ separate library system in slovenia and merged @ a single metadata scheme @ metadata deduplication and merging technique @ @ consists of text and numerical field representing attribute @ describe document @ @ attribute include document title keywords abstract typology author issue year and @ identifier @ a url and udc @ @ potential of @ dataset lie especially in text mining and text classification task and @ @ @ used in development @ benchmarking of content-based recommender system on real-world data @ @ author @ s @ 
266,Multidocument Arabic text summarization based on clustering and word2vec to reduce redundancy,"Arabic is one of the most semantically and syntactically complex languages in the world. A key challenging issue in text mining is text summarization, so we propose an unsupervised score-based method which combines the vector space model, continuous bag of words (CBOW), clustering, and a statistically-based method. The problems with multidocument text summarization are the noisy data, redundancy, diminished readability, and sentence incoherency. In this study, we adopt a preprocessing strategy to solve the noise problem and use the word2vec model for two purposes, first, to map the words to fixed-length vectors and, second, to obtain the semantic relationship between each vector based on the dimensions. Similarly, we use a k-means algorithm for two purposes: (1) Selecting the distinctive documents and tokenizing these documents to sentences, and (2) using another iteration of the k-means algorithm to select the key sentences based on the similarity metric to overcome the redundancy problem and generate the initial summary. Lastly, we use weighted principal component analysis (W-PCA) to map the sentences' encoded weights based on a list of features. This selects the highest set of weights, which relates to important sentences for solving incoherency and readability problems. We adopted Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as an evaluation measure to examine our proposed technique and compare it with state-of-the-art methods. Finally, an experiment on the Essex Arabic Summaries Corpus (EASC) using the ROUGE-1 and ROUGE-2 metrics showed promising results in comparison with existing methods. © 2020 by the author.",2020,Information (Switzerland),1,arabic is @ of @ @ semantically and syntactically complex language in @ world @ a key challenging issue in text mining is text summarization @ @ propose @ unsupervised score-based method @ combine @ vector space model continuous bag of word @ cbow @ clustering and a statistically-based method @ @ problem @ multidocument text summarization @ @ noisy data redundancy diminished readability and sentence incoherency @ in @ study @ adopt a preprocessing strategy to solve @ noise problem and use @ word vec model @ @ purpose first to map @ word to fixed-length vector and second to obtain @ semantic relationship @ @ vector based on @ dimension @ similarly @ use a k-means algorithm @ @ purpose @ @ @ selecting @ distinctive document and tokenizing @ document to sentence and @ @ @ another iteration of @ k-means algorithm to select @ key sentence based on @ similarity metric to overcome @ redundancy problem and generate @ initial summary @ lastly @ use weighted principal component analysis @ w-pca @ to map @ sentence @ encoded weight based on a list of feature @ @ selects @ highest set of weight @ relates to important sentence @ solving incoherency and readability problem @ @ adopted recall-oriented understudy @ gisting evaluation @ rouge @ a @ evaluation measure to examine @ proposed technique and compare @ @ state-of-the-art method @ finally @ experiment on @ essex arabic summary corpus @ easc @ @ @ rouge and rouge metric showed promising @ in comparison @ existing method @ by @ author @ 
267,When Digital Trace Data Meet Traditional Communication Theory: Theoretical/Methodological Directions,"This study suggests one direction of theoretical and methodological coupling of communication research with the digital trace data, utilizing its differences from the traditional social science approach (e.g., sampling vs. population, normal distribution vs. power–law distribution, generalization vs. simulation, deductive vs. inductive, and perceived vs. actual). We propose specific examples of (i) combining communication research with trace data methodologically and theoretically; (ii) collaborating with linguistic psychology complemented with the automated content analysis and natural language processing techniques; and (iii) creating new theoretical inquiries by configuring the granular level of interactivity and underlying dynamics, observing the longitudinal change of interactions, and discovering the neglected presence of outliers and the invisibles. We expect the direction suggested by this study contributes to deepening our understanding of human communication behavior. © The Author(s) 2018.",2020,Social Science Computer Review,4,@ study suggests @ direction of theoretical and methodological coupling of communication research @ @ digital trace data utilizing @ difference @ @ traditional social science approach @ e @ g @ sampling v @ population normal distribution v @ power law distribution generalization v @ simulation deductive v @ inductive and perceived v @ actual @ @ @ propose specific example of @ i @ combining communication research @ trace data methodologically and theoretically @ @ ii @ collaborating @ linguistic psychology complemented @ @ automated content analysis and natural language processing technique @ and @ iii @ creating @ theoretical inquiry by configuring @ granular level of interactivity and underlying dynamic observing @ longitudinal change of interaction and discovering @ neglected presence of outlier and @ invisibles @ @ expect @ direction suggested by @ study contributes to deepening @ understanding of human communication behavior @ @ author @ s @ @ 
271,Building a knowledge base shell based on exploring text semantic relations from Arabic text,"This research aims at focusing on building a knowledge base shell which task is to automatically build the knowledge base component from text sources. The proposed research aims at considering the Arabic text sources as one of the main challenges in due to the difficulty in processing Arabic text that suffers from the high inflection, shortcuts, and the varieties in letters' representation. The research restored the confidence in the term frequency method and the research adopts the semantic relations network representation. The research applied the experiment for building the weeds' identification facts' part in the knowledge-based system. The experiment applied the proposed framework on two sources, the precision, recall, and f-score have been measured which resulted in an average of 91.2%, 91.1%, and 91% respectively. The evaluation metrics provide a promising perspective in the proposed approach with more focus on the bottlenecks for enhancements. © 2019 Intelligent Network and Systems Society.",2020,International Journal of Intelligent Engineering and Systems,0,@ research aim at focusing on building a knowledge base shell @ task is to automatically build @ knowledge base component @ text source @ @ proposed research aim at considering @ arabic text source a @ of @ main challenge in due to @ difficulty in processing arabic text @ suffers @ @ high inflection shortcut and @ variety in letter @ representation @ @ research restored @ confidence in @ term frequency method and @ research adopts @ semantic relation network representation @ @ research applied @ experiment @ building @ weed @ identification fact @ part in @ knowledge-based system @ @ experiment applied @ proposed framework on @ source @ precision recall and f-score @ @ measured @ resulted in @ average of @ @ and respectively @ @ evaluation metric provide a promising perspective in @ proposed approach @ more focus on @ bottleneck @ enhancement @ intelligent network and system society @ 
273,Classification of stroke disease using machine learning algorithms,"This paper presents a prototype to classify stroke that combines text mining tools and machine learning algorithms. Machine learning can be portrayed as a significant tracker in areas like surveillance, medicine, data management with the aid of suitably trained machine learning algorithms. Data mining techniques applied in this work give an overall review about the tracking of information with respect to semantic as well as syntactic perspectives. The proposed idea is to mine patients’ symptoms from the case sheets and train the system with the acquired data. In the data collection phase, the case sheets of 507 patients were collected from Sugam Multispecialty Hospital, Kumbakonam, Tamil Nadu, India. Next, the case sheets were mined using tagging and maximum entropy methodologies, and the proposed stemmer extracts the common and unique set of attributes to classify the strokes. Then, the processed data were fed into various machine learning algorithms such as artificial neural networks, support vector machine, boosting and bagging and random forests. Among these algorithms, artificial neural networks trained with a stochastic gradient descent algorithm outperformed the other algorithms with a higher classification accuracy of 95% and a smaller standard deviation of 14.69. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",2020,Neural Computing and Applications,6,@ @ @ a prototype to classify stroke @ combine text mining tool and machine learning algorithm @ machine learning @ @ portrayed a a significant tracker in area like surveillance medicine data management @ @ aid of suitably trained machine learning algorithm @ data mining technique applied in @ work give @ overall review @ @ tracking of information @ respect to semantic a well a syntactic perspective @ @ proposed idea is to mine patient symptom @ @ case sheet and train @ system @ @ acquired data @ in @ data collection phase @ case sheet of patient @ collected @ sugam multispecialty hospital kumbakonam tamil nadu india @ next @ case sheet @ mined @ tagging and maximum entropy methodology and @ proposed stemmer extract @ common and unique set of attribute to classify @ stroke @ @ @ processed data @ fed @ various machine learning algorithm @ a artificial neural network support vector machine boosting and bagging and random forest @ among @ algorithm artificial neural network trained @ a stochastic gradient descent algorithm outperformed @ @ algorithm @ a higher classification accuracy of and a smaller standard deviation of @ @ springer-verlag london ltd @ part of @ nature @ 
274,Sentiment analysis on IMDB using lexicon and neural networks,"To find out what other people think has been an essential part of information-gathering behaviors. And in the case of movies, the movie reviews can provide an intricate insight into the movie and can help decide whether it is worth spending time on. However, with the growing amount of data in reviews, it is quite prudent to automate the process, saving on time. Sentiment analysis is an important field of study in machine learning that focuses on extracting information of subject from the textual reviews. The area of analysis of sentiments is related closely to natural language processing and text mining. It can successfully be used to determine the attitude of the reviewer in regard to various topics or the overall polarity of the review. In the case of movie reviews, along with giving a rating in numeric to a movie, they can enlighten us on the favorableness or the opposite of a movie quantitatively; a collection of those then gives us a comprehensive qualitative insight on different facets of the movie. Opinion mining from movie reviews can be challenging due to the fact that human language is rather complex, leading to situations where a positive word has a negative connotation and vice versa. In this study, the task of opinion mining from movie reviews has been achieved with the use of neural networks trained on the “Movie Review Database” issued by Stanford, in conjunction with two big lists of positive and negative words. The trained network managed to achieve a final accuracy of 91%. © 2020, Springer Nature Switzerland AG.",2020,SN Applied Sciences,6,to find @ @ @ people think ha @ @ essential part of information-gathering behavior @ and in @ case of movie @ movie review @ provide @ intricate insight @ @ movie and @ help decide whether @ is worth spending time on @ however @ @ growing amount of data in review @ is quite prudent to automate @ process saving on time @ sentiment analysis is @ important field of study in machine learning @ focus on extracting information of subject @ @ textual review @ @ area of analysis of sentiment is related closely to natural language processing and text mining @ @ @ successfully @ used to determine @ attitude of @ reviewer in regard to various topic @ @ overall polarity of @ review @ in @ case of movie review along @ giving a rating in numeric to a movie @ @ enlighten u on @ favorableness @ @ opposite of a movie quantitatively @ a collection of @ @ give u a comprehensive qualitative insight on different facet of @ movie @ opinion mining @ movie review @ @ challenging due to @ fact @ human language is rather complex leading to situation @ a positive word ha a negative connotation and vice versa @ in @ study @ task of opinion mining @ movie review ha @ achieved @ @ use of neural network trained on @ movie review database issued by stanford in conjunction @ @ big list of positive and negative word @ @ trained network managed to achieve a final accuracy of @ @ nature switzerland ag @ 
281,"On Computing Entity Relatedness in Wikipedia, with Applications","Many text mining tasks, such as clustering, classification, retrieval, and named entity linking, benefit from a measure of relatedness between entities in a knowledge graph. We present a thorough study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph. To facilitate this study, we introduce a new dataset with human judgments of entity relatedness. No clear dominance is seen between measures based on textual similarity and graph proximity. Some of the better measures involve expensive global graph computations. We propose a new, space-efficient, computationally lightweight, two-stage framework for relatedness computation. In the first stage, a small weighted subgraph is dynamically grown around the two query entities; in the second stage, relatedness is derived based on computations on this subgraph. Our system shows better agreement with human judgment than existing proposals both on the new dataset and on an established one. Our framework also shows improvements with respect to the state-of-the-art on three different extrinsic evaluations in the domains of ranking entity pairs, entity linking, and synonym extraction. © 2019 Elsevier B.V.",2020,Knowledge-Based Systems,0,many text mining task @ a clustering classification retrieval and named entity linking benefit @ a measure of relatedness @ entity in a knowledge graph @ @ @ a thorough study of @ entity relatedness measure in recent literature based on wikipedia a @ knowledge graph @ to facilitate @ study @ introduce a @ dataset @ human judgment of entity relatedness @ no clear dominance is seen @ measure based on textual similarity and graph proximity @ some of @ better measure involve expensive global graph computation @ @ propose a @ space-efficient computationally lightweight two-stage framework @ relatedness computation @ in @ first stage a small weighted subgraph is dynamically grown around @ @ query entity @ in @ second stage relatedness is derived based on computation on @ subgraph @ @ system @ better agreement @ human judgment @ existing proposal @ on @ @ dataset and on @ established @ @ @ framework @ @ improvement @ respect to @ state-of-the-art on three different extrinsic evaluation in @ domain of ranking entity pair entity linking and synonym extraction @ @ b @ v @ 
285,Deconstructing persuasive strategies in mental health apps based on user reviews using natural language processing,"Text Mining is concerned with extracting interesting and significant patterns or knowledge from unstructured text data. In this paper, we applied the text mining approach using natural language processing (NLP) techniques, especially topic modelling (with automated topic labelling), in deconstructing the persuasive strategies implemented or employed by 100 mental health apps based on user reviews. We focus on the persuasive strategies in the primary task support category of the Persuasive Systems Design (PSD) framework. We used the Latent Dirichlet Allocation (LDA) topic modelling algorithm, in conjunction with semantic attributes, to achieve our goal. Our experimental results revealed that self-monitoring is the most employed persuasive strategy. Finally, we compare our findings with that obtained using manual coding method and found significant similarities. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,text mining is concerned @ extracting interesting and significant pattern @ knowledge @ unstructured text data @ in @ @ @ applied @ text mining approach @ natural language processing @ nlp @ technique especially topic modelling @ @ automated topic labelling @ in deconstructing @ persuasive strategy implemented @ employed by mental health apps based on user review @ @ focus on @ persuasive strategy in @ primary task support category of @ persuasive system design @ psd @ framework @ @ used @ latent dirichlet allocation @ lda @ topic modelling algorithm in conjunction @ semantic attribute to achieve @ goal @ @ experimental @ revealed @ self-monitoring is @ @ employed persuasive strategy @ finally @ compare @ finding @ @ obtained @ manual coding method and found significant similarity @ @ @ @ @ by @ author @ 
286,Improved approach to extract knowledge from unstructured data using applied natural language processing techniques,"Extraction of meaningful knowledge from a unstructured data is a complex task. In the literature, efforts have been made using text mining approaches. These approaches employ rich amount of resources in mining the textual datasets. In this paper, we focus on optimization of mining algorithm of text data to generate the automatic text summarization. In this approach, we extract text summaries from the text data carpus using natural language processing techniques. We propose a mining approach in semantic parsing and generate the automatic text summaries. We conduct the experiments on the real-world dataset and show the proposed approach is useful than the existing approaches. © Springer Nature Singapore Pte Ltd. 2020.",2020,Advances in Intelligent Systems and Computing,0,extraction of meaningful knowledge @ a unstructured data is a complex task @ in @ literature effort @ @ made @ text mining approach @ @ approach employ rich amount of resource in mining @ textual datasets @ in @ @ @ focus on optimization of mining algorithm of text data to generate @ automatic text summarization @ in @ approach @ extract text summary @ @ text data carpus @ natural language processing technique @ @ propose a mining approach in semantic parsing and generate @ automatic text summary @ @ conduct @ experiment on @ real-world dataset and @ @ proposed approach is useful @ @ existing approach @ @ nature singapore pte ltd @ @ 
287,Summarization of Coal Mine Accident Reports: A Natural-Language-Processing-Based Approach,"Coal mine production plays an important role in promoting economic development, but the frequent occurrence of coal mine accidents is always a major problem of safety production. Correspondingly, a large number of coal mine accident reports have been generated. These reports detail the causes of safety accidents and the corresponding treatment measures, which have important guidance and reference for the handling of subsequent safety accidents. Nowadays, the analysis efficiency of coal mine accident reports using traditional manual methods is relatively low, and a large amount of valuable information may be not fully exploited and utilized. In response to this deficiency, this paper implements the text mining analysis of coal mine accident reports with the help of currently efficient natural language processing (NLP) technologies, mainly to achieve the text summary of coal mine accident reports. Firstly, after analyzing the overall text framework of the coal mine accident report, the general text structure is presented accordingly. Then, on the basis of it, combining with the TextRank method and Word2vec technique in NLP, the report text structure is optimized, and the report summary library is also obtained autonomously, which helps the staff to deal with accident reports intelligently. The experimental results on the actual text report data verify the effectiveness of the developed method. © 2020, Springer Nature Singapore Pte Ltd.",2020,Communications in Computer and Information Science,0,coal mine production play @ important role in promoting economic development @ @ frequent occurrence of coal mine accident is always a major problem of safety production @ correspondingly a @ number of coal mine accident report @ @ generated @ @ report detail @ cause of safety accident and @ corresponding treatment measure @ @ important guidance and reference @ @ handling of subsequent safety accident @ nowadays @ analysis efficiency of coal mine accident report @ traditional manual method is relatively low and a @ amount of valuable information may @ not fully exploited and utilized @ in response to @ deficiency @ @ implement @ text mining analysis of coal mine accident report @ @ help of currently efficient natural language processing @ nlp @ technology mainly to achieve @ text summary of coal mine accident report @ firstly @ analyzing @ overall text framework of @ coal mine accident report @ general text structure is presented accordingly @ @ on @ basis of @ combining @ @ textrank method and word vec technique in nlp @ report text structure is optimized and @ report summary library is @ obtained autonomously @ help @ staff to deal @ accident report intelligently @ @ experimental @ on @ actual text report data verify @ effectiveness of @ developed method @ @ nature singapore pte ltd @ 
288,Simplified Framework of Natural Language Processing for Structure Management of Current-Age Data,"The adoption of natural language processing has become one of the essential part of artificial intelligence. Although, the conventional concept of natural language processing has been researched from more than a decade but still the better results are yet to arrive. Review of existing literatures shows the cases where case specific studies are carried out which still doesn’t address the problem associated with lightweight computational model. Therefore, the proposed study introduces a simplified modeling of natural language processing which is capable of handling the unstructured data unlike existing system without scoring any dependencies on extra resources or cost. The study also introduces an integrated syntactical-based and semantic-based which is quite novel and simplified in its form. The study outcome shows that it offers almost instantaneous response time for all the internal processes. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,@ adoption of natural language processing ha become @ of @ essential part of artificial intelligence @ although @ conventional concept of natural language processing ha @ researched @ more @ a decade @ still @ better @ @ yet to arrive @ review of existing literature @ @ case @ case specific study @ carried @ @ still @ t address @ problem associated @ lightweight computational model @ therefore @ proposed study introduces a simplified modeling of natural language processing @ is capable of handling @ unstructured data unlike existing system without scoring @ dependency on extra resource @ cost @ @ study @ introduces @ integrated syntactical-based and semantic-based @ is quite novel and simplified in @ form @ @ study outcome @ @ @ offer almost instantaneous response time @ @ @ internal process @ @ nature switzerland ag @ 
289,Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with a Mainstream Religious Group,"Online propaganda is central to the recruitment strategies of extremist groups and in recent years these efforts have increasingly extended to women. To investigate Islamic State’s approach to targeting women in their online propaganda and uncover implications for counterterrorism, we rely on text mining and natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (Islamic State’s online English language publications) to identify prominent topics. To identify similarities or differences between these texts and those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,online propaganda is central to @ recruitment strategy of extremist group and in recent year @ effort @ increasingly extended to woman @ to investigate islamic state s approach to targeting woman in @ online propaganda and uncover implication @ counterterrorism @ rely on text mining and natural language processing @ nlp @ @ specifically @ extract article published in dabiq and rumiyah @ islamic state s online english language publication @ to identify prominent topic @ to identify similarity @ difference @ @ text and @ produced by non-violent religious group @ extend @ analysis to article @ a catholic forum dedicated to woman @ @ @ perform @ emotional analysis of @ of @ resource to better understand @ emotional component of propaganda @ @ rely on depechemood @ a lexical-base emotion analysis method @ to detect emotion @ likely to @ evoked in reader of @ material @ @ finding indicate @ @ emotional appeal of isi and catholic material @ similar @ @ nature switzerland ag @ 
290,Case studies on using natural language processing techniques in customer relationship management software,"How can we use a text corpus stored in a customer relationship management (CRM) database for data mining and segmentation? To answer this question, we inherited the state of the art methods commonly used in natural language processing (NLP) literature, such as word embeddings, and deep learning literature, such as recurrent neural networks (RNN). We used the text notes from a CRM system taken by customer representatives of an internet ads consultancy agency between 2009 and 2020. We trained word embeddings by using the corresponding text corpus and showed that these word embeddings could be used directly for data mining and used in RNN architectures, which are deep learning frameworks built with long short-term memory (LSTM) units, for more comprehensive segmentation objectives. The obtained results prove that we can use structured text data populated in a CRM to mine valuable information. Hence, any CRM can be equipped with useful NLP features once we correctly built the problem definitions and conveniently implement the solution methods. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Journal of Intelligent Information Systems,0,@ @ @ use a text corpus stored in a customer relationship management @ crm @ database @ data mining and segmentation @ to answer @ question @ inherited @ state of @ art method commonly used in natural language processing @ nlp @ literature @ a word embeddings and deep learning literature @ a recurrent neural network @ rnn @ @ @ used @ text note @ a crm system taken by customer representative of @ internet ad consultancy agency @ and @ @ trained word embeddings by @ @ corresponding text corpus and showed @ @ word embeddings could @ used directly @ data mining and used in rnn architecture @ @ deep learning framework built @ long short-term memory @ lstm @ unit @ more comprehensive segmentation objective @ @ obtained @ prove @ @ @ use structured text data populated in a crm to mine valuable information @ hence @ crm @ @ equipped @ useful nlp feature @ @ correctly built @ problem definition and conveniently implement @ solution method @ @ science @ medium llc part of @ nature @ 
291,Extracting Spanish linguistic features for natural language processing tasks,"Spanish is one of the most popular languages on the Internet with approximately 344 million users; this fact, in conjunction with the rising of the Web 2.0. and the leading role of the users in the creation of content, has leaded Natural Language Processing (NLP) to become one of the outstanding technologies, with applications in machine translation, conversational systems or spam filters. However, some of the available resources are still at an early stage compared to other languages. In addition, some of the tools available are translations of their equivalent in English, so they may lose characteristics of Spanish. Therefore, the objective of this doctoral thesis is the development of a system of extraction of linguistic characteristics of texts in Spanish, which has applications in different fields of the NLP, such as opinion mining, plagiarism detection, or readability analysis. © 2020 CEUR-WS. All rights reserved.",2020,CEUR Workshop Proceedings,0,spanish is @ of @ @ popular language on @ internet @ approximately million user @ @ fact in conjunction @ @ rising of @ web @ @ and @ leading role of @ user in @ creation of content ha leaded natural language processing @ nlp @ to become @ of @ outstanding technology @ application in machine translation conversational system @ spam filter @ however some of @ available resource @ still at @ early stage compared to @ language @ in addition some of @ tool available @ translation of @ equivalent in english @ @ may lose characteristic of spanish @ therefore @ objective of @ doctoral thesis is @ development of a system of extraction of linguistic characteristic of text in spanish @ ha application in different field of @ nlp @ a opinion mining plagiarism detection @ readability analysis @ ceur-ws @ @ right reserved @ 
294,Mining Publication Papers via Text Mining: A Case Study,"The amount of data that produced is increased day after day especially data as a text, so with this massive production it would be difficult to analyze or extract information to discover the patterns from the unstructured text. Text mining is used for availing the massive amount of knowledge that is in the text and deriving high quality information from the text automatically. This Process would save effort and time. Text mining considered as a subset of data mining where data mining is more generic. This paper proposes a methodology of mining a text for a case study related to publication papers. Some of text mining approaches will be introduced for mining the publication papers using machine learning (ML) and natural language processing (NLP) techniques. Describing each phase as following: First phase is keywords extraction using natural language processing techniques, second phase named entity recognition and last phase is document classification. The last two phases are using the ML techniques. Then a case study is built to simulate the system phases, showing what is the input and the output in each phase. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,1,@ amount of data @ produced is increased day @ day especially data a a text @ @ @ massive production @ would @ difficult to analyze @ extract information to discover @ pattern @ @ unstructured text @ text mining is used @ availing @ massive amount of knowledge @ is in @ text and deriving high quality information @ @ text automatically @ @ process would save effort and time @ text mining considered a a subset of data mining @ data mining is more generic @ @ @ proposes a methodology of mining a text @ a case study related to publication @ @ some of text mining approach @ @ introduced @ mining @ publication @ @ machine learning @ ml @ and natural language processing @ nlp @ technique @ describing @ phase a following @ first phase is keywords extraction @ natural language processing technique second phase named entity recognition and last phase is document classification @ @ last @ phase @ @ @ ml technique @ @ a case study is built to simulate @ system phase showing @ is @ input and @ output in @ phase @ @ nature switzerland ag @ 
298,Wordnet – a Basic Resource for Natural Language Processing: The Case of plWordNet,"This paper presents a wide scope of wordnet applications on the example of applications of plWordNet – a wordnet of Polish. Wordnets are large lexical-semantic databases functioning as primary resources for language technology. They are machine-readable dictionaries. Thus, they are indispensible for tasks such as basic flow of text processing, text mining, word sense disambiguation, information extraction and retrieval. On a larger scale, wordnets are used in research, education and business. In this paper a few examples of specific plWordNet applications are described in detail. © 2020, Springer Nature Switzerland AG.",2020,Communications in Computer and Information Science,0,@ @ @ a wide scope of wordnet application on @ example of application of plwordnet a wordnet of polish @ wordnet @ @ lexical-semantic database functioning a primary resource @ language technology @ @ @ machine-readable dictionary @ thus @ @ indispensible @ task @ a basic flow of text processing text mining word sense disambiguation information extraction and retrieval @ on a larger scale wordnet @ used in research education and @ @ in @ @ a @ example of specific plwordnet application @ described in detail @ @ nature switzerland ag @ 
299,From CAQDAS to Text Mining. The Domain Ontology as a Model of Knowledge Representation About Qualitative Research Practices,"The nature of qualitative research practices is multiparadigmaticity which creates coexistence of different research and analytical approaches. This paper is a methodological reflection on how the process of qualitative data analysis is developing, moving from traditional CAQDAS coding procedures through Content Analysis dictionary-based approach towards the textual data exploration for knowledge discovery in corpora using Natural Language Processing and Text Mining procedures. This change is described on the example of the process of analyzing and discovering the ways through which qualitative research practices are conceptualized and represented in the vivid language of scholarly articles. Taking into account the problem of a “curse of abundance” in the present-day field of qualitative research I try to organize and articulate these practices in a legible system of knowledge representation employing the information concept of domain ontology. In the process of building the ontology of the contemporary field of qualitative research practices, I link know-how drawn from sociology, social science computing, NLP and text mining, digital humanities and corpus linguistics. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,@ nature of qualitative research practice is multiparadigmaticity @ creates coexistence of different research and analytical approach @ @ @ is a methodological reflection on @ @ process of qualitative data analysis is developing moving @ traditional caqdas coding procedure @ content analysis dictionary-based approach towards @ textual data exploration @ knowledge discovery in corpus @ natural language processing and text mining procedure @ @ change is described on @ example of @ process of analyzing and discovering @ way @ @ qualitative research practice @ conceptualized and represented in @ vivid language of scholarly article @ taking @ account @ problem of a curse of abundance in @ present-day field of qualitative research i try to organize and articulate @ practice in a legible system of knowledge representation employing @ information concept of domain ontology @ in @ process of building @ ontology of @ contemporary field of qualitative research practice i link know-how drawn @ sociology social science computing nlp and text mining digital humanity and corpus linguistics @ @ nature switzerland ag @ 
300,Text mining for evaluation of candidates based on their CVs,"The problem of CV (or resume) text mining becomes increasingly relevant nowadays as long as it could simplify the evaluation of future employees and their suitability for the post for which they apply. The paper proposes a procedure for automatic information extraction from text documents, namely from candidate’s CVs. The described algorithm is based on Natural Language Processing methods and allows to transform text information into categorical features or classes. These features may further be used as inputs for a machine learning model to predict the suitability of the candidate for the position. Besides the general method, the description of the experiments is given in which the algorithm was used for clusterization of future employees according to their previous position and job spheres they worked in. The obtained classes were used to predict the probability of the candidate’s turnover in the first six months. Their addition allowed to raise the model score. © Springer Nature Switzerland AG 2020.",2020,Communications in Computer and Information Science,0,@ problem of cv @ @ resume @ text mining becomes increasingly relevant nowadays a long a @ could simplify @ evaluation of future employee and @ suitability @ @ post @ @ @ apply @ @ @ proposes a procedure @ automatic information extraction @ text document namely @ candidate s cv @ @ described algorithm is based on natural language processing method and allows to transform text information @ categorical feature @ class @ @ feature may @ @ used a input @ a machine learning model to predict @ suitability of @ candidate @ @ position @ besides @ general method @ description of @ experiment is given in @ @ algorithm wa used @ clusterization of future employee according to @ previous position and job sphere @ worked in @ @ obtained class @ used to predict @ probability of @ candidate s turnover in @ first six month @ @ addition allowed to raise @ model score @ @ nature switzerland ag @ 
301,Improving strategic decision making by the detection of weak signals in heterogeneous documents by text mining techniques,"At present, one of the greatest threats to companies is not being able to cope with the constant changes that occur in the market because they do not predict them well in advance. Therefore, the development of new processes that facilitate the detection of significant phenomena and future changes is a key component for correct decision making that sets a correct course in the company. For this reason, a business intelligence architecture system is hereby proposed to allow the detection of discrete changes or weak signals in the present, indicative of more significant phenomena and transcendental changes in the future. In contrast to work currently available focusing on structured information sources, or at most with a single type of data source, the detection of these signals is here quantitatively based on heterogeneous and unstructured documents of various kinds (scientific journals, newspaper articles and social networks), to which text mining and natural language processing techniques (a multi-word expression analysis) are applied. The system has been tested to study the future of the artificial intelligence sector, obtaining promising results to help business experts in the recognition of new driving factors of their markets and the development of new opportunities. © 2019 - IOS Press and the authors. All rights reserved.",2020,AI Communications,1,at @ @ of @ greatest threat to company is not @ able to cope @ @ constant change @ occur in @ market @ @ @ not predict @ well in advance @ therefore @ development of @ process @ facilitate @ detection of significant phenomenon and future change is a key component @ correct decision making @ set a correct course in @ company @ @ @ reason a @ intelligence architecture system is hereby proposed to allow @ detection of discrete change @ weak signal in @ @ indicative of more significant phenomenon and transcendental change in @ future @ in contrast to work currently available focusing on structured information source @ at @ @ a single type of data source @ detection of @ signal is @ quantitatively based on heterogeneous and unstructured document of various kind @ scientific journal newspaper article and social network @ to @ text mining and natural language processing technique @ a multi-word expression analysis @ @ applied @ @ system ha @ tested to study @ future of @ artificial intelligence sector obtaining promising @ to help @ expert in @ recognition of @ driving factor of @ market and @ development of @ opportunity @ io @ and @ author @ @ right reserved @ 
302,Analysis of related documents using text mining,"Nowadays, Artificial Intelligence (AI) becomes the wide-spread technique in analyzing huge data. AI is used in analyzing the unstructured documents to extract the structured information with the help of Natural Language Processing. This paper gives the importance of text mining and the current status of the research in text mining. The objective of the paper is to extract the information from the collected documents that are related to each other. The Collected documents are related to the wireless technologies and mobile commerce. From the selected documents, the frequently occurred words are identified and analyzed. The result shows that the frequently occurred words in all the documents are related and it shows its importance in the document. © 2019 SERSC.",2020,International Journal of Advanced Science and Technology,0,nowadays artificial intelligence @ ai @ becomes @ wide-spread technique in analyzing huge data @ ai is used in analyzing @ unstructured document to extract @ structured information @ @ help of natural language processing @ @ @ give @ importance of text mining and @ current status of @ research in text mining @ @ objective of @ @ is to extract @ information @ @ collected document @ @ related to @ @ @ @ collected document @ related to @ wireless technology and mobile commerce @ @ @ selected document @ frequently occurred word @ identified and analyzed @ @ @ @ @ @ frequently occurred word in @ @ document @ related and @ @ @ importance in @ document @ sersc @ 
304,Google Play Content Scraping and Knowledge Engineering using Natural Language Processing Techniques with the Analysis of User Reviews,"To maintain the competitive edge and evaluating the needs of the quality app is in the mobile application market. The user's feedback on these applications plays an essential role in the mobile application development industry. The rapid growth of web technology gave people an opportunity to interact and express their review, rate and share their feedback about applications. In this paper we have scrapped 506259 of user reviews and applications rate from Google Play Store from 14 different categories. The statistical information was measured in the results using different of common machine learning algorithms such as the Logistic Regression, Random Forest Classifier, and Multinomial Naïve Bayes. Different parameters including the accuracy, precision, recall, and F1 score were used to evaluate Bigram, Trigram, and N-gram, and the statistical result of these algorithms was compared. The analysis of each algorithm, one by one, is performed, and the result has been evaluated. It is concluded that logistic regression is the best algorithm for review analysis of the Google Play Store applications. The results have been checked scientifically, and it is found that the accuracy of the logistic regression algorithm for analyzing different reviews based on three classes, i.e., positive, negative, and neutral. © 2020 H. Aldabbas et al., published by De Gruyter 2020.",2020,Journal of Intelligent Systems,0,to maintain @ competitive edge and evaluating @ need of @ quality app is in @ mobile application market @ @ user @ s feedback on @ application play @ essential role in @ mobile application development industry @ @ rapid growth of web technology gave people @ opportunity to interact and express @ review rate and share @ feedback @ application @ in @ @ @ @ scrapped of user review and application rate @ google play store @ different category @ @ statistical information wa measured in @ @ @ different of common machine learning algorithm @ a @ logistic regression random forest classifier and multinomial naïve bayes @ different parameter including @ accuracy precision recall and f score @ used to evaluate bigram trigram and n-gram and @ statistical @ of @ algorithm wa compared @ @ analysis of @ algorithm @ by @ is performed and @ @ ha @ evaluated @ @ is concluded @ logistic regression is @ best algorithm @ review analysis of @ google play store application @ @ @ @ @ checked scientifically and @ is found @ @ accuracy of @ logistic regression algorithm @ analyzing different review based on three class i @ e @ positive negative and neutral @ h @ aldabbas et al @ published by de gruyter @ 
306,Time expressions identification without human-labeled corpus for clinical text mining in russian,"To obtain accurate predictive models in medicine, it is necessary to use complete relevant information about the patient. We propose an approach for extracting temporary expressions from unlabeled natural language texts. This approach can be used for the first analysis of the corpus, for data labeling as the first stage, or for obtaining linguistic constructions that can be used for a rule-based approach to retrieve information. Our method includes the sequential use of several machine learning and natural language processing methods: classification of sentences, the transformation of word bag frequencies, clustering of sentences with time expressions, classification of new data into clusters and construction of sentence profiles using feature importances. With this method, we derive the list of the most frequent time expressions and extract events and/or time events for 9801 sentences of anamnesis in Russian. The proposed approach is independent of the corpus language and can be used for other tasks, for example, extracting an experiencer of a disease. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,to obtain accurate predictive model in medicine @ is necessary to use complete relevant information @ @ patient @ @ propose @ approach @ extracting temporary expression @ unlabeled natural language text @ @ approach @ @ used @ @ first analysis of @ corpus @ data labeling a @ first stage @ @ obtaining linguistic construction @ @ @ used @ a rule-based approach to retrieve information @ @ method includes @ sequential use of several machine learning and natural language processing method @ classification of sentence @ transformation of word bag frequency clustering of sentence @ time expression classification of @ data @ cluster and construction of sentence profile @ feature importance @ @ @ method @ derive @ list of @ @ frequent time expression and extract event and @ time event @ sentence of anamnesis in russian @ @ proposed approach is independent of @ corpus language and @ @ used @ @ task @ example extracting @ experiencer of a disease @ @ nature switzerland ag @ 
307,Emotional analysis with news using text mining for framing theory,"Framing theory posits that the media tend to present a constructed “reality” by selecting and highlighting particular aspects of reality while obscuring or omitting others. Hence, this process leads the audience or readers to a particular understanding of reality. Depending on how each event or issue is defined, the same event can be presented and understood in different ways or through different frames. “Traditional” or typical framing research has used content analysis to identify such frames by examining major news sources that each story adopted, words used, and the tone of the stories. However, this study aims to extend this framing research to a different level by using computer-assisted analysis. This new method allows us to analyze massive data and to visualize the representation through text-mining, natural language processing, and emotion lexicon. Within the broad framing approach, this study intends to show how major newspapers of several countries depict leaders of North Korea and South Korea. The following research questions are addressed in this study: How each leader of two Koreas is represented by different countries’ press? What image is dominant? How similar or different is the portrayed image? A total of eight newspapers written in English from six countries were selected for this case study: The Chosunilbo, The Korea Times, The Hankyoreh from South Korea, Uriminzokkiri from North Korea, The New York Times from USA, The Globe and Mail from Canada, People’s Daily from China and Thanh Nien Daily from Vietnam. Each of these countries was selected based on the history and geopolitical relations with two Koreas. Using a keyword search such as Jong-un Kim, Guen-Hye Park, national leader, or president, we identified relevant articles from these eight newspapers and analyzed them by using text mining and Natural Language Processing (NLP). Emotion analysis is a Lexicon-based method that can detect different emotions expressed in news stories. We assumed that various degrees of emotions associated with each leader would indicate the nature or orientation of reported frames. For this analysis, NRC Emotion Lexicon dictionary developed by Paul Ekman theory who identified a total of six emotions with 14,182 words was used. Then, the sentence score for each emotion was calculated. The significant contribution of this study is to present a new method of text mining and big data analysis for framing studies and to show how the overall media frames could be visualized for clear and better understanding of media representation. © Springer Nature Switzerland AG 2020.",2020,Studies in Computational Intelligence,0,framing theory posit @ @ medium tend to @ a constructed reality by selecting and highlighting particular aspect of reality @ obscuring @ omitting others @ hence @ process lead @ audience @ reader to a particular understanding of reality @ depending on @ @ event @ issue is defined @ @ event @ @ presented and understood in different way @ @ different frame @ traditional @ typical framing research ha used content analysis to identify @ frame by examining major news source @ @ story adopted word used and @ tone of @ story @ however @ study aim to extend @ framing research to a different level by @ computer-assisted analysis @ @ @ method allows u to analyze massive data and to visualize @ representation @ text-mining natural language processing and emotion lexicon @ within @ broad framing approach @ study intends to @ @ major newspaper of several country depict leader of north korea and south korea @ @ following research question @ addressed in @ study @ @ @ leader of @ korea is represented by different country @ @ @ image is dominant @ @ similar @ different is @ portrayed image @ a total of eight newspaper written in english @ six country @ selected @ @ case study @ @ chosunilbo @ korea time @ hankyoreh @ south korea uriminzokkiri @ north korea @ @ york time @ usa @ globe and mail @ canada people s daily @ china and thanh nien daily @ vietnam @ @ of @ country wa selected based on @ history and geopolitical relation @ @ korea @ @ a keyword search @ a jong-un kim guen-hye park national leader @ president @ identified relevant article @ @ eight newspaper and analyzed @ by @ text mining and natural language processing @ nlp @ @ emotion analysis is a lexicon-based method @ @ detect different emotion expressed in news story @ @ assumed @ various degree of emotion associated @ @ leader would indicate @ nature @ orientation of reported frame @ @ @ analysis nrc emotion lexicon dictionary developed by paul ekman theory @ identified a total of six emotion @ word wa used @ @ @ sentence score @ @ emotion wa calculated @ @ significant contribution of @ study is to @ a @ method of text mining and big data analysis @ framing study and to @ @ @ overall medium frame could @ visualized @ clear and better understanding of medium representation @ @ nature switzerland ag @ 
308,Survey of progressive era of text summarization for indian and foreign languages using natural language processing,"The last few years of Data Science definitely show the upward trend in growth of popularity, different industries which are effectively relating with data science & the transformation of world with e-commerce sites, social networking sites, travel aggregators, Google assistants. Here the need of text summarization comes in existence. This text summarization is a conception which actually deals with time saving and giving user the output with minimum text without changing its meaning. This approach is very impressive as the e-contents reading is very important aspect in many fields like academics, industries, hospitals, clinical notes, news papers, crime records, cyber security and many other. This paper shows the advancements which has initiated research for text summarization in many global and local languages. Text mining is coping up with many dimensions like text summarization, question generation, sentiment analysis, and translators. Natural Language Processing is one of the demanding areas now a day, which will definitely improve the human machine interaction. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes on Data Engineering and Communications Technologies,1,@ last @ year of data science definitely @ @ upward trend in growth of popularity different industry @ @ effectively relating @ data science @ transformation of world @ e-commerce site social networking site travel aggregator google assistant @ @ @ need of text summarization come in existence @ @ text summarization is a conception @ actually deal @ time saving and giving user @ output @ minimum text without changing @ meaning @ @ approach is @ impressive a @ e-contents reading is @ important aspect in many field like @ industry hospital clinical note news @ crime record cyber security and many @ @ @ @ @ @ advancement @ ha initiated research @ text summarization in many global and local language @ text mining is coping up @ many dimension like text summarization question generation sentiment analysis and translator @ natural language processing is @ of @ demanding area now a day @ @ definitely improve @ human machine interaction @ @ nature switzerland ag @ 
311,Deep-Learning Approach to Educational Text Mining and Application to the Analysis of Topics' Difficulty,"Learning analytics has emerged as a promising tool for optimizing the learning experience and results, especially in online educational environments. An important challenge in this area is identifying the most difficult topics for students in a subject, which is of great use to improve the quality of teaching by devoting more effort to those topics of greater difficulty, assigning them more time, resources and materials. We have approached the problem by means of natural language processing techniques. In particular, we propose a solution based on a deep learning model that automatically extracts the main topics that are covered in educational documents. This model is next applied to the problem of identifying the most difficult topics for students in a subject related to the study of algorithms and data structures in a Computer Science degree. Our results show that our topic identification model presents very high accuracy (around 90 percent) and may be efficiently used in learning analytics applications, such as the identification and understanding of what makes the learning of a subject difficult. An exhaustive analysis of the case study has also revealed that there are indeed topics that are consistently more difficult for most students, and also that the perception of difficulty in students and teachers does not always coincide with the actual difficulty indicated by the data, preventing to pay adequate attention to the most challenging topics. © 2013 IEEE.",2020,IEEE Access,0,learning analytics ha emerged a a promising tool @ optimizing @ learning experience and @ especially in online educational environment @ @ important challenge in @ area is identifying @ @ difficult topic @ student in a subject @ is of great use to improve @ quality of teaching by devoting more effort to @ topic of greater difficulty assigning @ more time resource and material @ @ @ approached @ problem by mean of natural language processing technique @ in particular @ propose a solution based on a deep learning model @ automatically extract @ main topic @ @ covered in educational document @ @ model is next applied to @ problem of identifying @ @ difficult topic @ student in a subject related to @ study of algorithm and data structure in a computer science degree @ @ @ @ @ @ topic identification model @ @ high accuracy @ around percent @ and may @ efficiently used in learning analytics application @ a @ identification and understanding of @ make @ learning of a subject difficult @ @ exhaustive analysis of @ case study ha @ revealed @ @ @ indeed topic @ @ consistently more difficult @ @ student and @ @ @ perception of difficulty in student and teacher doe not always coincide @ @ actual difficulty indicated by @ data preventing to pay adequate attention to @ @ challenging topic @ @ @ 
312,A systematic review of text mining approaches applied to various application areas in the biomedical domain,"Purpose: This work shows the results of a systematic literature review on biomedical text mining. The purpose of this study is to identify the different text mining approaches used in different application areas of the biomedical domain, the common tools used and the challenges of biomedical text mining as compared to generic text mining algorithms. This study will be of value to biomedical researchers by allowing them to correlate text mining approaches to specific biomedical application areas. Implications for future research are also discussed. Design/methodology/approach: The review was conducted following the principles of the Kitchenham method. A number of research questions were first formulated, followed by the definition of the search strategy. The papers were then selected based on a list of assessment criteria. Each of the papers were analyzed and information relevant to the research questions were extracted. Findings: It was found that researchers have mostly harnessed data sources such as electronic health records, biomedical literature, social media and health-related forums. The most common text mining technique was natural language processing using tools such as MetaMap and Unstructured Information Management Architecture, alongside the use of medical terminologies such as Unified Medical Language System. The main application area was the detection of adverse drug events. Challenges identified included the need to deal with huge amounts of text, the heterogeneity of the different data sources, the duality of meaning of words in biomedical text and the amount of noise introduced mainly from social media and health-related forums. Originality/value: To the best of the authors’ knowledge, other reviews in this area have focused on either specific techniques, specific application areas or specific data sources. The results of this review will help researchers to correlate most relevant and recent advances in text mining approaches to specific biomedical application areas by providing an up-to-date and holistic view of work done in this research area. The use of emerging text mining techniques has great potential to spur the development of innovative applications, thus considerably impacting on the advancement of biomedical research. © 2020, Emerald Publishing Limited.",2020,Journal of Knowledge Management,0,purpose @ @ work @ @ @ of a systematic literature review on biomedical text mining @ @ purpose of @ study is to identify @ different text mining approach used in different application area of @ biomedical domain @ common tool used and @ challenge of biomedical text mining a compared to generic text mining algorithm @ @ study @ @ of value to biomedical researcher by allowing @ to correlate text mining approach to specific biomedical application area @ implication @ future research @ @ discussed @ design methodology approach @ @ review wa conducted following @ principle of @ kitchenham method @ a number of research question @ first formulated followed by @ definition of @ search strategy @ @ @ @ @ selected based on a list of assessment criterion @ @ of @ @ @ analyzed and information relevant to @ research question @ extracted @ finding @ @ wa found @ researcher @ mostly harnessed data source @ a electronic health record biomedical literature social medium and health-related forum @ @ @ common text mining technique wa natural language processing @ tool @ a metamap and unstructured information management architecture alongside @ use of medical terminology @ a unified medical language system @ @ main application area wa @ detection of adverse drug event @ challenge identified included @ need to deal @ huge amount of text @ heterogeneity of @ different data source @ duality of meaning of word in biomedical text and @ amount of noise introduced mainly @ social medium and health-related forum @ originality value @ to @ best of @ author knowledge @ review in @ area @ focused on either specific technique specific application area @ specific data source @ @ @ of @ review @ help researcher to correlate @ relevant and recent advance in text mining approach to specific biomedical application area by providing @ up-to-date and holistic view of work done in @ research area @ @ use of emerging text mining technique ha great potential to spur @ development of innovative application thus considerably impacting on @ advancement of biomedical research @ emerald publishing limited @ 
314,Opinion Mining System for Twitter Sentiment Analysis,"After the paradigm shift produced by Web 2.0, the volume of opinion on the Internet has increased exponentially. The expansion of social media, whose textual content is somewhat subjective and comes loaded with opinions and assessments, can be very useful for recommending a product or brand. This information is an interesting challenge from the perspective of natural language processing, but is also an aspect of deep interest and great value not only as a marketing strategy for companies and political campaigns, but also as an indicator measuring consumer satisfaction with a product or service. In this paper, we present an opinion mining system that uses text mining techniques and natural language processing to automatically obtain useful knowledge about opinions, preferences and user trends. We studied improvements in the quality of opinion classification by using a voting system to choose the best classification of each tweet, base on of the absolute majority of the votes of the algorithms considered. In addition we developed a visualization tool that automatically combines these algorithms to assist end-user decision making. The opinion mining tool makes it possible to analyze and visualize data published on Twitter, to understand the sentiment analysis of users in relation to a product or service, by identifying the positive or negative sentiment expressed in Twitter messages. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ paradigm shift produced by web @ @ volume of opinion on @ internet ha increased exponentially @ @ expansion of social medium whose textual content is somewhat subjective and come loaded @ opinion and assessment @ @ @ useful @ recommending a product @ brand @ @ information is @ interesting challenge @ @ perspective of natural language processing @ is @ @ aspect of deep interest and great value not only a a marketing strategy @ company and political campaign @ @ a @ indicator measuring consumer satisfaction @ a product @ service @ in @ @ @ @ @ opinion mining system @ us text mining technique and natural language processing to automatically obtain useful knowledge @ opinion preference and user trend @ @ studied improvement in @ quality of opinion classification by @ a voting system to choose @ best classification of @ tweet base on of @ absolute majority of @ vote of @ algorithm considered @ in addition @ developed a visualization tool @ automatically combine @ algorithm to assist end-user decision making @ @ opinion mining tool make @ possible to analyze and visualize data published on twitter to understand @ sentiment analysis of user in relation to a product @ service by identifying @ positive @ negative sentiment expressed in twitter message @ @ nature switzerland ag @ 
315,Entity Extraction with Knowledge from Web Scale Corpora,"Entity extraction is an important task in text mining and natural language processing. A popular method for entity extraction is by comparing substrings from free text against a dictionary of entities. In this paper, we present several techniques as a post-processing step for improving the effectiveness of the existing entity extraction technique. These techniques utilise models trained with the web-scale corpora which makes our techniques robust and versatile. Experiments show that our techniques bring a notable improvement on efficiency and effectiveness. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,entity extraction is @ important task in text mining and natural language processing @ a popular method @ entity extraction is by comparing substring @ free text @ a dictionary of entity @ in @ @ @ @ several technique a a post-processing step @ improving @ effectiveness of @ existing entity extraction technique @ @ technique utilise model trained @ @ web-scale corpus @ make @ technique robust and versatile @ experiment @ @ @ technique bring a notable improvement on efficiency and effectiveness @ @ nature switzerland ag @ 
317,Network approach for visualizing the evolution of the research of cross-lingual semantic similarity,"The paper is devoted to the problem of the bibliometric study of publications on the topic “Cross-lingual Semantic Similarity”, available in the Dimensions database. Visualization of scientific networks showed fragmentation of research, limited interaction of organizations. Leading countries, leading organizations and authors are highlighted. Overlay visualization allowed us to assess the trends in citing authors. The expansion of the geography of research is shown. For international cooperation, the uniformity of semantic approaches to describing the concepts of critical infrastructure, incidents, resources and services related to their maintenance and protection is important. The stated approaches can be applied for visualization and modeling of technological development in the modern digital world. Semantic similarity is a longstanding problem in natural language processing (NLP). The semantic similarity between two words represents the semantic proximity (or semantic distance) between two words or concepts. This is an important problem in natural language processing, as it plays an important role in finding information, extracting information, text mining, web mining and many other applications. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)",2020,CEUR Workshop Proceedings,0,@ @ is devoted to @ problem of @ bibliometric study of publication on @ topic cross-lingual semantic similarity available in @ dimension database @ visualization of scientific network showed fragmentation of research limited interaction of organization @ leading country leading organization and author @ highlighted @ overlay visualization allowed u to ass @ trend in citing author @ @ expansion of @ geography of research is @ @ @ international cooperation @ uniformity of semantic approach to describing @ concept of critical infrastructure incident resource and service related to @ maintenance and protection is important @ @ stated approach @ @ applied @ visualization and modeling of technological development in @ modern digital world @ semantic similarity is a longstanding problem in natural language processing @ nlp @ @ @ semantic similarity @ @ word represents @ semantic proximity @ @ semantic distance @ @ @ word @ concept @ @ is @ important problem in natural language processing a @ play @ important role in finding information extracting information text mining web mining and many @ application @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ 
318,Movies emotional analysis using textual contents,"In this paper, we use movies and series subtitles and applied text mining and Natural Language Processing methods to evaluate emotions in videos. Three different word lexicons were used and one of the outcomes of this research is the generation of a secondary dataset with more than 3658 records which can be used for other data analysis and data mining research. We used our secondary dataset to find and display correlations between different emotions on the videos and the correlation between emotions on the movies and users’ scores on IMDb using the Pearson correlation method and found some statistically significant correlations. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ use movie and series subtitle and applied text mining and natural language processing method to evaluate emotion in video @ three different word lexicon @ used and @ of @ outcome of @ research is @ generation of a secondary dataset @ more @ record @ @ @ used @ @ data analysis and data mining research @ @ used @ secondary dataset to find and display correlation @ different emotion on @ video and @ correlation @ emotion on @ movie and user score on imdb @ @ pearson correlation method and found some statistically significant correlation @ @ nature switzerland ag @ 
321,Woman lingual cultural type analysis using cognitive modeling and graph theory,"This paper observes an analysis of lingual cultural type of woman by using graphical modeling approach. The study has been carried out in terms of applied linguistics and illustrates the integration of natural language processing technologies. The main contribution is an attempt to implement Text Mining tools to analyze poetic discourse of the late 16th – early 17th centuries and provide cognitive model of lingual cultural type. The topicality of cognitive modeling application in linguistic studies is the ability to structure and systematize the existing information, identify the scenarios for information system, and predict the relationship dynamics between the components of this system. The paper also examines the possibility of using graph theory to study lexical variables that reflect the objects of human life. Overall, the research findings indicate the most frequent connections between the lingual cultural type of woman and the lexical variables used to depict physical, social or spiritual objects of the reality in English culture. © Springer Nature Switzerland AG 2020.",2020,Advances in Intelligent Systems and Computing,1,@ @ observes @ analysis of lingual cultural type of woman by @ graphical modeling approach @ @ study ha @ carried @ in term of applied linguistics and illustrates @ integration of natural language processing technology @ @ main contribution is @ attempt to implement text mining tool to analyze poetic discourse of @ late th early th century and provide cognitive model of lingual cultural type @ @ topicality of cognitive modeling application in linguistic study is @ ability to structure and systematize @ existing information identify @ scenario @ information system and predict @ relationship dynamic @ @ component of @ system @ @ @ @ examines @ possibility of @ graph theory to study lexical variable @ reflect @ object of human life @ overall @ research finding indicate @ @ frequent connection @ @ lingual cultural type of woman and @ lexical variable used to depict physical social @ spiritual object of @ reality in english culture @ @ nature switzerland ag @ 
324,Causal Relation Extraction Based on Graph Attention Networks,"Causality represents a kind of correlation between cause and effect, where the happening of cause will leads to the happening of effect. As the most important type of relationship between entities, causality plays a vital role in many fields such as automatic reasoning and scenario generation. Therefore, extracting causal relation becomes a basic task in natural language processing and text mining. Different from traditional text classification methods or relation extraction methods, this paper proposes a sequence labeling method to extract causal entity in text and identify direction of causality, without relying on feature engineering or causal background knowledge. The main contributions of this paper can be summarized as follows: 1) we extend syntactic dependency tree to the syntactic dependency graph, adopt graph attention networks in natural language processing, and introduce the concept of S-GAT(graph attention network based on syntactic dependency graph); 2) Bi-LSTM+CRF+S-GAT model for causal extraction is proposed, which generates causal label of each word in sentence based on input word vectors; 3) SemEval data set is modified and extended, and rules are defined to relabel experimental data with an aim of overcoming defects of the original labeling method. Extensive experiments are conducted on the expanded SemEval dataset, which shows that our model achieves 0.064 improvement over state-of-the-art model Bi-LSTM+CRF+self-ATT in terms of prediction accuracy. © 2020, Science Press. All right reserved.",2020,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,2,causality represents a kind of correlation @ cause and effect @ @ happening of cause @ lead to @ happening of effect @ a @ @ important type of relationship @ entity causality play a vital role in many field @ a automatic reasoning and scenario generation @ therefore extracting causal relation becomes a basic task in natural language processing and text mining @ different @ traditional text classification method @ relation extraction method @ @ proposes a sequence labeling method to extract causal entity in text and identify direction of causality without relying on feature engineering @ causal background knowledge @ @ main contribution of @ @ @ @ summarized a follows @ @ @ extend syntactic dependency tree to @ syntactic dependency graph adopt graph attention network in natural language processing and introduce @ concept of s-gat @ graph attention network based on syntactic dependency graph @ @ @ bi-lstm crf s-gat model @ causal extraction is proposed @ generates causal label of @ word in sentence based on input word vector @ @ semeval data set is modified and extended and rule @ defined to relabel experimental data @ @ aim of overcoming defect of @ original labeling method @ extensive experiment @ conducted on @ expanded semeval dataset @ @ @ @ model achieves @ improvement @ state-of-the-art model bi-lstm crf self-att in term of prediction accuracy @ science @ @ @ right reserved @ 
325,Suicidal tendencies prediction in Greek poetry,"Natural language processing (NLP) has been successfully used to predict a writer’s tendency of committing suicide, using various text types: suicide notes, micro-blog posts, lyrics and poems. This paper is an extended version of earlier work. We extend our previous work on text mining in Greek Poetry by employing more sophisticated approaches. More specifically we have applied (i) Deep Neural Networks (DNN), (ii) additional morphosyntactic and semantic features based on writers' emotions and Big Five personality traits and (iii) feature selection, for suicide prediction in Greek poetry. We extend previous research to Greek, i.e. a language that has not been tackled before in this setting, using both language-dependent (but easily portable across languages) and language-independent linguistic features in order to represent the poems of 13 Greek poets of the twentieth century. Our results differ significantly from previous literature. In general, our proposed DNN model offers promising results for suicide prediction, despite the fact that this task poses multiple difficulties, especially for a language with limited related research support. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",2020,Evolving Systems,0,natural language processing @ nlp @ ha @ successfully used to predict a writer s tendency of committing suicide @ various text type @ suicide note micro-blog post lyric and poem @ @ @ is @ extended version of earlier work @ @ extend @ previous work on text mining in greek poetry by employing more sophisticated approach @ more specifically @ @ applied @ i @ deep neural network @ dnn @ @ ii @ additional morphosyntactic and semantic feature based on writer @ emotion and big five personality trait and @ iii @ feature selection @ suicide prediction in greek poetry @ @ extend previous research to greek i @ e @ a language @ ha not @ tackled @ in @ setting @ @ language-dependent @ @ easily portable across language @ and language-independent linguistic feature in order to represent @ poem of greek poet of @ twentieth century @ @ @ differ significantly @ previous literature @ in general @ proposed dnn model offer promising @ @ suicide prediction despite @ fact @ @ task pose multiple difficulty especially @ a language @ limited related research support @ springer-verlag gmbh germany part of @ nature @ 
326,A new information theory based clustering fusion method for multi-view representations of text documents,"Multi-view clustering is a complex problem that consists in extracting partitions from multiple representations of the same objects. In text mining and natural language processing, such views may come in the form of word frequencies, topic based representations and many other possible encoding forms coming from various vector space model algorithms. From there, in this paper we propose a clustering fusion algorithm that takes clustering results acquired from multiple vector space models of given documents, and merges them into a single partition. Our fusion method relies on an information theory model based on Kolmogorov complexity that was previously used for collaborative clustering applications. We apply our algorithm to different text corpuses frequently used in the literature with results that we find to be very satisfying. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,multi-view clustering is a complex problem @ consists in extracting partition @ multiple representation of @ @ object @ in text mining and natural language processing @ view may come in @ form of word frequency topic based representation and many @ possible encoding form coming @ various vector space model algorithm @ @ @ in @ @ @ propose a clustering fusion algorithm @ take clustering @ acquired @ multiple vector space model of given document and merges @ @ a single partition @ @ fusion method relies on @ information theory model based on kolmogorov complexity @ wa @ used @ collaborative clustering application @ @ apply @ algorithm to different text corpus frequently used in @ literature @ @ @ @ find to @ @ satisfying @ @ nature switzerland ag @ 
329,A research review and taxonomy development for decision support and business analytics using semantic text mining,"By 2018, business analytics (BA), believed by global CIOs to be of strategic importance, had for years been their top priority. It is also a focus of academic research, as shown by a large number of papers, books, and research reports. On the other hand, the BA domain suffers from several incorrect, imprecise, and incomplete notions. New areas and concepts emerge quickly; making it difficult to ascertain their structure. BA-related taxonomies play a crucial role in analyzing, classifying, and understanding related objects. However, according to the literature on taxonomy development in information systems (IS), in most cases the process is ad hoc. BA taxonomies and frameworks are available in the literature; however, some are excessively general frameworks with a high-level conceptual focus, while others are application or domain-specific. Our paper aims to present a novel semi-automatic method for taxonomy development and maintenance in the field of BA using content analysis and text mining. The contribution of our research is threefold: (1) the taxonomy development method, (2) the draft taxonomy for BA, and (3) identifying the latest research areas and trends in BA. © 2020 World Scientific Publishing Company.",2020,International Journal of Information Technology and Decision Making,2,by @ analytics @ ba @ believed by global cio to @ of strategic importance @ @ year @ @ top priority @ @ is @ a focus of @ research a @ by a @ number of @ book and research report @ on @ @ hand @ ba domain suffers @ several incorrect imprecise and incomplete notion @ @ area and concept emerge quickly @ making @ difficult to ascertain @ structure @ ba-related taxonomy play a crucial role in analyzing classifying and understanding related object @ however according to @ literature on taxonomy development in information system @ is @ in @ case @ process is ad hoc @ ba taxonomy and framework @ available in @ literature @ however some @ excessively general framework @ a high-level conceptual focus @ others @ application @ domain-specific @ @ @ aim to @ a novel semi-automatic method @ taxonomy development and maintenance in @ field of ba @ content analysis and text mining @ @ contribution of @ research is threefold @ @ @ @ taxonomy development method @ @ @ draft taxonomy @ ba and @ @ identifying @ latest research area and trend in ba @ world scientific publishing company @ 
330,Analysis of OWA operators for automatic keyphrase extraction in a semantic context,"Automatic keyphrase extraction from texts is useful for many computational systems in the fields of natural language processing and text mining. Although a number of solutions to this problem have been described, semantic analysis is one of the least exploited linguistic features in the most widely-known proposals, causing the results obtained to have low accuracy and performance rates. This paper presents an unsupervised method for keyphrase extraction, based on the use of lexico-syntactic patterns for extracting information from texts, and a fuzzy topic modeling. An OWA operator combining several semantic measures was applied to the topic modeling process. This new approach was evaluated with Inspec and 500N-KPCrowd datasets. Several approaches within our proposal were evaluated against each other. A statistical analysis was performed to substantiate the best approach of the proposal. This best approach was also compared with other reported systems, giving promising results. © 2020 - IOS Press and the authors. All rights reserved.",2020,Intelligent Data Analysis,0,automatic keyphrase extraction @ text is useful @ many computational system in @ field of natural language processing and text mining @ although a number of solution to @ problem @ @ described semantic analysis is @ of @ least exploited linguistic feature in @ @ widely-known proposal causing @ @ obtained to @ low accuracy and performance rate @ @ @ @ @ unsupervised method @ keyphrase extraction based on @ use of lexico-syntactic pattern @ extracting information @ text and a fuzzy topic modeling @ @ owa operator combining several semantic measure wa applied to @ topic modeling process @ @ @ approach wa evaluated @ inspec and n-kpcrowd datasets @ several approach within @ proposal @ evaluated @ @ @ @ a statistical analysis wa performed to substantiate @ best approach of @ proposal @ @ best approach wa @ compared @ @ reported system giving promising @ @ io @ and @ author @ @ right reserved @ 
331,Attention-based convolutional neural network for Bangla sentiment analysis,"With the accelerated evolution of the internet in the form of web-sites, social networks, microblogs, and online portals, a large number of reviews, opinions, recommendations, ratings, and feedback are generated by writers or users. This user-generated sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, and individuals. While this content is meant to be useful, a bulk of this writer-generated content requires using text mining techniques and sentiment analysis. However, there are several challenges facing the sentiment analysis and evaluation process. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting suitable sentiment polarity specifically in the Bangla language. Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text. This paper presents how the attention mechanism could be incorporated effectively and efficiently in analyzing the Bangla sentiment or opinion. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",2020,AI and Society,1,@ @ accelerated evolution of @ internet in @ form of web-sites social network microblogs and online portal a @ number of review opinion recommendation rating and feedback @ generated by writer @ user @ @ user-generated sentiment content @ @ @ book people hotel product research event etc @ @ sentiment become @ beneficial @ @ government and individual @ @ @ content is meant to @ useful a bulk of @ writer-generated content requires @ text mining technique and sentiment analysis @ however @ @ several challenge facing @ sentiment analysis and evaluation process @ @ challenge become obstacle in analyzing @ accurate meaning of sentiment and detecting suitable sentiment polarity specifically in @ bangla language @ sentiment analysis is @ practice of applying natural language processing and text analysis technique to identify and extract subjective information @ text @ @ @ @ @ @ attention mechanism could @ incorporated effectively and efficiently in analyzing @ bangla sentiment @ opinion @ springer-verlag london ltd @ part of @ nature @ 
332,"Named entity recognition, concept normalization and clinical coding: Overview of the cantemist track for cancer text mining in Spanish, corpus, guidelines, methods and results","Cancer still represents one of the leading causes of death worldwide, resulting in a considerable healthcare impact. Recent research efforts from the clinical and molecular oncology scientific communities were able to increase considerably life expectancy of patients for some cancer types. Most of the current cancer diagnoses are primarily determined by pathology laboratories, providing an essential source for information to guide the treatment of patients with cancer. Pathology observations essentially characterize the results of microscopic or macroscopic studies of cells or tissues following a biopsy or surgery. Clinicians and researchers alike, require systems that automatically detect, read and generate structured data representations from pathology examinations. The resulting structured or coded clinical information, normalized using controlled vocabularies like the ICD-O or SNOMED-CT is critical for large-scale analysis of specific tumor types or to determine response to specific treatments or prognosis. Text mining and NLP approaches are showing promising results to transform medical text into useful clinical information, bridging the gap between free-text and structured representation of clinical information. Nonetheless, in the case of cancer text mining approaches, most efforts were exclusively focused on medical records in English. Moreover, due to the lack of high quality manually labeled clinical texts annotated by oncology experts most previous efforts, even for English relied mainly on customized dictionaries of names or rules to recognize clinical concept mentions despite the promising results of advanced deep learning technologies. To address these issues we have organized the Cantemist (CANcer TExt Mining Shared Task) track at IberLEF 2020. It represents the first community effort to evaluate and promote the development of resources for named entity recognition, concept normalization and clinical coding specifically focusing on cancer data in Spanish. Evaluation of participating systems was done using the Cantemist corpus, a publicly accessible dataset (together with annotation consistency analysis and guidelines) of manually annotated mentions of tumor morphology entities and their mappings to the Spanish version of ICD-O.We received a total of 121 systems or runs from 25 teams for one of the three Cantemist sub-tasks, obtaining very competitive results. Most participants implemented sophisticated AI approaches; mainly deep learning algorithms based on Long-Short Term Memory Units and language models (BERT, BETO, RoBERTa, etc) with a classifier layer such as a Conditional Random Field. In addition to using pre-trained language models, word and character embeddings were also explored. Cantemist corpus: Https://doi.org/10.5281/zenodo.3773228. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,19,cancer still represents @ of @ leading cause of death worldwide resulting in a considerable healthcare impact @ recent research effort @ @ clinical and molecular oncology scientific community @ able to increase considerably life expectancy of patient @ some cancer type @ @ of @ current cancer diagnosis @ primarily determined by pathology laboratory providing @ essential source @ information to guide @ treatment of patient @ cancer @ pathology observation essentially characterize @ @ of microscopic @ macroscopic study of cell @ tissue following a biopsy @ surgery @ clinician and researcher alike require system @ automatically detect read and generate structured data representation @ pathology examination @ @ resulting structured @ coded clinical information normalized @ controlled vocabulary like @ icd-o @ snomed-ct is critical @ large-scale analysis of specific tumor type @ to determine response to specific treatment @ prognosis @ text mining and nlp approach @ showing promising @ to transform medical text @ useful clinical information bridging @ gap @ free-text and structured representation of clinical information @ nonetheless in @ case of cancer text mining approach @ effort @ exclusively focused on medical record in english @ moreover due to @ lack of high quality manually labeled clinical text annotated by oncology expert @ previous effort even @ english relied mainly on customized dictionary of name @ rule to recognize clinical concept mention despite @ promising @ of advanced deep learning technology @ to address @ issue @ @ organized @ cantemist @ cancer text mining shared task @ track at iberlef @ @ represents @ first community effort to evaluate and promote @ development of resource @ named entity recognition concept normalization and clinical coding specifically focusing on cancer data in spanish @ evaluation of participating system wa done @ @ cantemist corpus a publicly accessible dataset @ together @ annotation consistency analysis and guideline @ of manually annotated mention of tumor morphology entity and @ mapping to @ spanish version of icd-o @ @ received a total of system @ run @ team @ @ of @ three cantemist sub-tasks obtaining @ competitive @ @ @ participant implemented sophisticated ai approach @ mainly deep learning algorithm based on long-short term memory unit and language model @ bert beto roberta etc @ @ a classifier layer @ a a conditional random field @ in addition to @ pre-trained language model word and character embeddings @ @ explored @ cantemist corpus @ http @ doi @ org @ zenodo @ @ @ @ @ @ by @ author @ use permitted @ @ 
333,"IberLEF 2020 - Proceedings of the Iberian Languages Evaluation Forum, co-located with 36th Conference of the Spanish Society for Natural Language Processing, SEPLN 2020","The proceedings contain 55 papers. The topics discussed include: Vicomtech at ALexS 2020: unsupervised complex word identification based on domain frequency; general lexicon-based complex word identification extended with stem N-grams and morphological engines; Hulat - ALexS CWI Task - CWI for language and learning disabilities applied to university educational text; overview of ALexS 2020: first Workshop on Lexical Analysis at SEPLN; named entity recognition, concept normalization and clinical coding: overview of the cantemist track for cancer text mining in Spanish, corpus, guidelines, methods and results; extracting neoplasms morphology mentions in Spanish clinical cases through word embeddings; and NLNDE at CANTEMIST: neural sequence labeling and parsing approaches for clinical concept extraction.",2020,CEUR Workshop Proceedings,0,@ proceeding contain @ @ @ topic discussed include @ vicomtech at alexs @ unsupervised complex word identification based on domain frequency @ general lexicon-based complex word identification extended @ stem n-grams and morphological engine @ hulat alexs cwi task cwi @ language and learning disability applied to university educational text @ overview of alexs @ first workshop on lexical analysis at sepln @ named entity recognition concept normalization and clinical coding @ overview of @ cantemist track @ cancer text mining in spanish corpus guideline method and @ @ extracting neoplasm morphology mention in spanish clinical case @ word embeddings @ and nlnde at cantemist @ neural sequence labeling and parsing approach @ clinical concept extraction @ 
335,Graph-Based Text Representation and Matching: A Review of the State of the Art and Future Challenges,"Graph-based text representation is one of the important preprocessing steps in data and text mining, Natural Language Processing (NLP), and information retrieval approaches. The graph-based methods focus on how to represent text documents in the shape of a graph to exploit the best features of their characteristics. This study reviews and lists the advantages and disadvantages of such methods employed or developed in graph-based text representations. The literature shows that some of the proposed graph-based methods suffer from a lack of representing texts in certain situations. Currently, several techniques are commonly used in graph-based text representation. However, there are still some weaknesses and shortages in these techniques and tools that significantly affect the success of graph representation and graph matching. In this review, we conduct an inclusive survey of the state of the art in graph-based text representation and learning. We provide a formal description of the problem of graph-based text representation and introduce some basic concepts. More significantly, this study proposes a new taxonomy of graph-based text representation, categorizing the existing studies based on representation characteristics and scheme techniques. In terms of the representation scheme taxonomy, we introduce four main types of conceptual graph schemes and summarize the challenges faced in each scheme. The main issues of graph representation, such as research topics and the sub-taxonomy of graph models for web documents, are introduced and categorized. This research also covers some tasks of understanding natural language processing (NLP) that depend on different types of graph structures. In addition, the graph matching taxonomy implements three main categories based on the matching approach, including structural-, semantic-, and similarity-based approaches. Moreover, a deep comparison of these approaches is discussed and reported in terms of methods and tools, the concepts of matching and locality, and the application domains that use these tools. Finally, the paper recommends seven promising future study directions in the graph-based text representation field. These recommendation points are summarized and highlighted as open problems and challenges of graph-based text representation and learning to facilitate and fill the research gaps for scientific researchers in this field. © 2013 IEEE.",2020,IEEE Access,1,graph-based text representation is @ of @ important preprocessing step in data and text mining natural language processing @ nlp @ and information retrieval approach @ @ graph-based method focus on @ to represent text document in @ shape of a graph to exploit @ best feature of @ characteristic @ @ study review and list @ advantage and disadvantage of @ method employed @ developed in graph-based text representation @ @ literature @ @ some of @ proposed graph-based method suffer @ a lack of representing text in certain situation @ currently several technique @ commonly used in graph-based text representation @ however @ @ still some weakness and shortage in @ technique and tool @ significantly affect @ success of graph representation and graph matching @ in @ review @ conduct @ inclusive survey of @ state of @ art in graph-based text representation and learning @ @ provide a formal description of @ problem of graph-based text representation and introduce some basic concept @ more significantly @ study proposes a @ taxonomy of graph-based text representation categorizing @ existing study based on representation characteristic and scheme technique @ in term of @ representation scheme taxonomy @ introduce four main type of conceptual graph scheme and summarize @ challenge faced in @ scheme @ @ main issue of graph representation @ a research topic and @ sub-taxonomy of graph model @ web document @ introduced and categorized @ @ research @ cover some task of understanding natural language processing @ nlp @ @ depend on different type of graph structure @ in addition @ graph matching taxonomy implement three main category based on @ matching approach including structural semantic and similarity-based approach @ moreover a deep comparison of @ approach is discussed and reported in term of method and tool @ concept of matching and locality and @ application domain @ use @ tool @ finally @ @ recommends seven promising future study direction in @ graph-based text representation field @ @ recommendation point @ summarized and highlighted a open problem and challenge of graph-based text representation and learning to facilitate and fill @ research gap @ scientific researcher in @ field @ @ @ 
336,Learning Chinese word representation better by cascade morphological n-gram,"Word embedding refers to mapping words or phrases to vectors of real numbers. This is the precondition of text classification, sentiment analysis and text mining in natural language processing using deep neural networks. Taking English as an example, most of current word embedding algorithms obtain the vectors by learning the distribution of word’s prefix, suffix, etyma and the entire word itself. Unlike English, Chinese words are composed of components and strokes. Furthermore, those components and strokes usually hint the meaning of the word. Thus, components and strokes distribution must be fully considered and learnt when one’s doing Chinese word embedding. In this paper, we propose a component-based cascade n-gram (CBC n-gram) model and a stroke-based cascade n-gram (SBC n-gram) model. By overlaying component and stroke n-gram vectors on word vectors, we successfully improve Chinese word embedding so as to preserve as more morphological information as possible at different granularity levels. We evaluate our models on word similarity, word analogy and text classification tasks using wordsim-240, wordsim-296, Chinese word analogy dataset and Fudan Corpus, respectively. Experimental and comparison results show that our models outperform other state-of-the-art methods. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",2020,Neural Computing and Applications,0,word embedding refers to mapping word @ phrase to vector of real number @ @ is @ precondition of text classification sentiment analysis and text mining in natural language processing @ deep neural network @ taking english a @ example @ of current word embedding algorithm obtain @ vector by learning @ distribution of word s prefix suffix etymon and @ entire word @ @ unlike english chinese word @ composed of component and stroke @ furthermore @ component and stroke usually hint @ meaning of @ word @ thus component and stroke distribution must @ fully considered and learnt @ @ s @ chinese word embedding @ in @ @ @ propose a component-based cascade n-gram @ cbc n-gram @ model and a stroke-based cascade n-gram @ sbc n-gram @ model @ by overlaying component and stroke n-gram vector on word vector @ successfully improve chinese word embedding @ a to preserve a more morphological information a possible at different granularity level @ @ evaluate @ model on word similarity word analogy and text classification task @ wordsim wordsim chinese word analogy dataset and fudan corpus respectively @ experimental and comparison @ @ @ @ model outperform @ state-of-the-art method @ springer-verlag london ltd @ part of @ nature @ 
337,A Semantic Based Approach for Topic Evaluation in Information Filtering,"Topic Modelling has been successfully applied in many text mining applications such as natural language processing, information retrieval, information filtering, etc. In information filtering systems (IFs), user interest representation is the core part which determines the success of the system. Topics in a topic model generated from a user's documents can be used to represent the user's information interest. However, the quality of a topic model generated from a document collection is not always accurate because the topics of the topic model might contain meaningless or ambiguous words. This ambiguity problem can affect the performance of IFs which use a topic model to represent user information interest. Hence, a topic evaluation method to assess the quality of topics in a topic model is important for ensuring the effectiveness of utilizing the topic model in text mining applications. One method in measuring the quality of a topic model is to match the topical words of the model to concepts in an ontology. However, a limitation of this method is that some topical words in an examined topic cannot be found in the mapping ontology. In this study, we propose a new model to evaluate the quality of topics by matching concepts in an ontology. In particular, word embedding technique is applied to dealing with the ambiguity problem by finding similar concept words based on word embeddings. The assessed topics are then used in an information filtering system for filtering relevant documents for a user. The proposed model was evaluated against some state-of-the-art baseline models in terms of term-based, phrase-based, and topic-based user interest representations, and also some topic evaluation models. The result of the evaluation shows that the new proposed model outperforms the state-of-the-art baseline models. © 2013 IEEE.",2020,IEEE Access,0,topic modelling ha @ successfully applied in many text mining application @ a natural language processing information retrieval information filtering etc @ in information filtering system @ ifs @ user interest representation is @ core part @ determines @ success of @ system @ topic in a topic model generated @ a user @ s document @ @ used to represent @ user @ s information interest @ however @ quality of a topic model generated @ a document collection is not always accurate @ @ topic of @ topic model might contain meaningless @ ambiguous word @ @ ambiguity problem @ affect @ performance of ifs @ use a topic model to represent user information interest @ hence a topic evaluation method to ass @ quality of topic in a topic model is important @ ensuring @ effectiveness of utilizing @ topic model in text mining application @ @ method in measuring @ quality of a topic model is to match @ topical word of @ model to concept in @ ontology @ however a limitation of @ method is @ some topical word in @ examined topic cannot @ found in @ mapping ontology @ in @ study @ propose a @ model to evaluate @ quality of topic by matching concept in @ ontology @ in particular word embedding technique is applied to dealing @ @ ambiguity problem by finding similar concept word based on word embeddings @ @ assessed topic @ @ used in @ information filtering system @ filtering relevant document @ a user @ @ proposed model wa evaluated @ some state-of-the-art baseline model in term of term-based phrase-based and topic-based user interest representation and @ some topic evaluation model @ @ @ of @ evaluation @ @ @ @ proposed model outperforms @ state-of-the-art baseline model @ @ @ 
339,Hybrid attention based neural architecture for text semantics similarity measurement,"Text semantics similarity measurement is a crucial problem in many real world applications, such as text mining, information retrieval and natural language processing. It is a complicated task due to the ambiguity and variability of linguistic expression. Previous studies focus on modeling the representation of a sentence in multiple granularities and then measure the similarity based on the representations. However, above methods cannot make full use of the diverse importance of different parts in a sentence. To address this problem, in this paper we propose a neural architecture with hybrid attention mechanism to highlight the important signals in different granularities within a text. We first utilize a Bi-directional Long Short Term Memory (BiLSTM) network to encode each sentence. Then we apply the hybrid attention mechanism on top of BiLSTM network. To detect the important parts of a sentence, we adopt a self-attention component to generate sentence level representations and then measure their relevance with a neural tensor network. To better utilize the interaction information, we devise an inter-attention component to further consider the influence of one sentence on another when modeling finer granularity interactions. We evaluate our proposed method on the task of paraphrase identification using two real world datasets. Experimental results demonstrate the superiority of this framework. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,text semantics similarity measurement is a crucial problem in many real world application @ a text mining information retrieval and natural language processing @ @ is a complicated task due to @ ambiguity and variability of linguistic expression @ previous study focus on modeling @ representation of a sentence in multiple granularity and @ measure @ similarity based on @ representation @ however @ method cannot make full use of @ diverse importance of different part in a sentence @ to address @ problem in @ @ @ propose a neural architecture @ hybrid attention mechanism to highlight @ important signal in different granularity within a text @ @ first utilize a bi-directional long short term memory @ bilstm @ network to encode @ sentence @ @ @ apply @ hybrid attention mechanism on top of bilstm network @ to detect @ important part of a sentence @ adopt a self-attention component to generate sentence level representation and @ measure @ relevance @ a neural tensor network @ to better utilize @ interaction information @ devise @ inter-attention component to @ consider @ influence of @ sentence on another @ modeling finer granularity interaction @ @ evaluate @ proposed method on @ task of paraphrase identification @ @ real world datasets @ experimental @ demonstrate @ superiority of @ framework @ @ nature switzerland ag @ 
342,Text categorization using various advanced ml techniques in text mining,"Text mining is an important research field which challenges to different strategies from Machine Learning, Natural Language Processing, and Information Retrieval (IR), Data Mining as well as Knowledge Management contribute strategies to the resolve major issues in data overload through Text mining, an important research field. Text mining is now a widespread move to keep associated with the rapid expansion of scientific literature. Text Mining is to extract unstructured (textual) data, obtain relevant statistical correlations from text, and hence make the system stored in the data visible to the different Statistical and Machine Learning Techniques. Data to extract summaries for both the terms stored in the records or to estimate excerpts based on words reflected in the texts. Hence, clusters of reports are used in texts to identify words. Text mining can typically ""transform text into numbers"" (substantial tuples), which can be adopted into certain examines such as statistical data. Text mining involves Pre-processing of catalogues of texts like Information Extraction, Term Retrieval, Document Classification and Storage of Transition Representation. A process of document mining involves a sequence of activities to execute to mine the data. The following activities are Document Pre-Processing, Feature Selection, Text Transformation. In this paper mainly focus on major FS Techniques for Text Classification and Clustering. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.",2020,Journal of Advanced Research in Dynamical and Control Systems,0,text mining is @ important research field @ challenge to different strategy @ machine learning natural language processing and information retrieval @ ir @ data mining a well a knowledge management contribute strategy to @ resolve major issue in data overload @ text mining @ important research field @ text mining is now a widespread move to keep associated @ @ rapid expansion of scientific literature @ text mining is to extract unstructured @ textual @ data obtain relevant statistical correlation @ text and hence make @ system stored in @ data visible to @ different statistical and machine learning technique @ data to extract summary @ @ @ term stored in @ record @ to estimate excerpt based on word reflected in @ text @ hence cluster of report @ used in text to identify word @ text mining @ typically @ transform text @ number @ @ substantial tuples @ @ @ @ adopted @ certain examines @ a statistical data @ text mining involves pre-processing of catalogue of text like information extraction term retrieval document classification and storage of transition representation @ a process of document mining involves a sequence of activity to execute to mine @ data @ @ following activity @ document pre-processing feature selection text transformation @ in @ @ mainly focus on major f technique @ text classification and clustering @ institute of advanced scientific research inc @ @ @ right reserved @ 
343,Comparison of rule-based and neural network models for negation detection in radiology reports,"Using natural language processing, it is possible to extract structured information from raw text in the electronic health record (EHR) at reasonably high accuracy. However, the accurate distinction between negated and non-negated mentions of clinical terms remains a challenge. EHR text includes cases where diseases are stated not to be present or only hypothesised, meaning a disease can be mentioned in a report when it is not being reported as present. This makes tasks such as document classification and summarisation more difficult. We have developed the rule-based EdIE-R-Neg, part of an existing text mining pipeline called EdIE-R (Edinburgh Information Extraction for Radiology reports), developed to process brain imaging reports, (https://www.ltg.ed.ac.uk/software/edie-r/) and two machine learning approaches; one using a bidirectional long short-term memory network and another using a feedforward neural network. These were developed on data from the Edinburgh Stroke Study (ESS) and tested on data from routine reports from NHS Tayside (Tayside). Both datasets consist of written reports from medical scans. These models are compared with two existing rule-based models: pyConText (Harkema et al. 2009. Journal of Biomedical Informatics 42(5), 839-851), a python implementation of a generalisation of NegEx, and NegBio (Peng et al. 2017. NegBio: A high-performance tool for negation and uncertainty detection in radiology reports. arXiv e-prints, p. arXiv:1712.05898), which identifies negation scopes through patterns applied to a syntactic representation of the sentence. On both the test set of the dataset from which our models were developed, as well as the largely similar Tayside test set, the neural network models and our custom-built rule-based system outperformed the existing methods. EdIE-R-Neg scored highest on F1 score, particularly on the test set of the Tayside dataset, from which no development data were used in these experiments, showing the power of custom-built rule-based systems for negation detection on datasets of this size. The performance gap of the machine learning models to EdIE-R-Neg on the Tayside test set was reduced through adding development Tayside data into the ESS training set, demonstrating the adaptability of the neural network models. © The Author(s), 2020. Published by Cambridge University Press.",2020,Natural Language Engineering,1,@ natural language processing @ is possible to extract structured information @ raw text in @ electronic health record @ ehr @ at reasonably high accuracy @ however @ accurate distinction @ negated and non-negated mention of clinical term remains a challenge @ ehr text includes case @ disease @ stated not to @ @ @ only hypothesised meaning a disease @ @ mentioned in a report @ @ is not @ reported a @ @ @ make task @ a document classification and summarisation more difficult @ @ @ developed @ rule-based edie-r-neg part of @ existing text mining pipeline called edie-r @ edinburgh information extraction @ radiology report @ developed to process brain imaging report @ http @ www @ ltg @ ed @ ac @ uk software edie-r @ and @ machine learning approach @ @ @ a bidirectional long short-term memory network and another @ a feedforward neural network @ @ @ developed on data @ @ edinburgh stroke study @ es @ and tested on data @ routine report @ nh tayside @ tayside @ @ @ datasets consist of written report @ medical scan @ @ model @ compared @ @ existing rule-based model @ pycontext @ harkema et al @ @ journal of biomedical informatics @ @ @ a python implementation of a generalisation of negex and negbio @ peng et al @ @ negbio @ a high-performance tool @ negation and uncertainty detection in radiology report @ arxiv e-prints p @ arxiv @ @ @ @ identifies negation scope @ pattern applied to a syntactic representation of @ sentence @ on @ @ test set of @ dataset @ @ @ model @ developed a well a @ largely similar tayside test set @ neural network model and @ custom-built rule-based system outperformed @ existing method @ edie-r-neg scored highest on f score particularly on @ test set of @ tayside dataset @ @ no development data @ used in @ experiment showing @ power of custom-built rule-based system @ negation detection on datasets of @ size @ @ performance gap of @ machine learning model to edie-r-neg on @ tayside test set wa reduced @ adding development tayside data @ @ es training set demonstrating @ adaptability of @ neural network model @ @ author @ s @ @ published by cambridge university @ @ 
346,Interactive Text Graph Mining with a Prolog-based Dialog Engine,"On top of a neural network-based dependency parser and a graph-based natural language processing module we design a Prolog-based dialog engine that explores interactively a ranked fact database extracted from a text document. We reorganize dependency graphs to focus on the most relevant content elements of a sentence, integrate sentence identifiers as graph nodes and after ranking the graph we take advantage of the implicit semantic information that dependency links bring in the form of subject-verb-object, “is-a” and “part-of” relations. Working on the Prolog facts and their inferred consequences, the dialog engine specializes the text graph with respect to a query and reveals interactively the document’s most relevant content elements. The open-source code of the integrated system is available at https://github.com/ptarau/DeepRank. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,on top of a neural network-based dependency parser and a graph-based natural language processing module @ design a prolog-based dialog engine @ explores interactively a ranked fact database extracted @ a text document @ @ reorganize dependency graph to focus on @ @ relevant content element of a sentence integrate sentence identifier a graph node and @ ranking @ graph @ take advantage of @ implicit semantic information @ dependency link bring in @ form of subject-verb-object is-a and part-of relation @ working on @ prolog fact and @ inferred consequence @ dialog engine specializes @ text graph @ respect to a query and reveals interactively @ document s @ relevant content element @ @ open-source code of @ integrated system is available at http @ github @ com ptarau deeprank @ @ nature switzerland ag @ 
350,Financial markets sentiment analysis: developing a specialized Lexicon,"Natural language processing in specific domains such as financial markets requires the knowledge of domain ontology. Therefore, developing a domain-specific lexicon to improve financial context sentiment analysis is noteworthy. In this paper, by exploring a wide related corpus along with using lexical resources, a hybrid approach is proposed to build a lexicon specialized for financial markets sentiment analysis. The lexicon is applied on a large dataset gathered from Twitter during nine months. Experimental results demonstrate a significant correlation between extracted sentiments from the corpus and market trends which indicates lexicon’s superior efficiency in measuring market sentiment compared with general-purpose dictionaries. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",2020,Journal of Intelligent Information Systems,0,natural language processing in specific domain @ a financial market requires @ knowledge of domain ontology @ therefore developing a domain-specific lexicon to improve financial context sentiment analysis is noteworthy @ in @ @ by exploring a wide related corpus along @ @ lexical resource a hybrid approach is proposed to build a lexicon specialized @ financial market sentiment analysis @ @ lexicon is applied on a @ dataset gathered @ twitter @ nine month @ experimental @ demonstrate a significant correlation @ extracted sentiment @ @ corpus and market trend @ indicates lexicon s superior efficiency in measuring market sentiment compared @ general-purpose dictionary @ @ science @ medium llc part of @ nature @ 
351,Rule extraction from scientific texts: Evaluation in the specialty of gynecology,"Due to the considerable increase in freely available data (especially on the Web), extracting relevant information from textual content is a critical challenge. Most of the available data is embedded in unstructured texts and is not linked to formalized knowledge structures such as ontologies or rules. A potential solution to this problem is to acquire such knowledge through natural language processing (NLP) tools and text mining techniques. Prior work has focused on the automatic extraction of ontologies from texts, but the acquired knowledge is generally limited to simple hierarchies of terms. This paper presents a polyvalent framework for acquiring complex relationships from texts and coding these in the form of rules. Our approach begins with existing domain knowledge represented as an OWL ontology, and applies NLP tools and text matching techniques to deduce different atoms, such as classes, properties and literals, to capture deductive knowledge in the form of new rules. For the reason, to enrich the existing domain ontology by these rules, in order to obtain higher relational expressiveness, make reasoning and produce new facts. The approach was tested using medical reports, specifically, in the specialty of gynecology. It reports an F-measure of 95.83% on test our corpus. © 2020 The Authors",2020,Journal of King Saud University - Computer and Information Sciences,0,due to @ considerable increase in freely available data @ especially on @ web @ extracting relevant information @ textual content is a critical challenge @ @ of @ available data is embedded in unstructured text and is not linked to formalized knowledge structure @ a ontology @ rule @ a potential solution to @ problem is to acquire @ knowledge @ natural language processing @ nlp @ tool and text mining technique @ prior work ha focused on @ automatic extraction of ontology @ text @ @ acquired knowledge is generally limited to simple hierarchy of term @ @ @ @ a polyvalent framework @ acquiring complex relationship @ text and coding @ in @ form of rule @ @ approach begin @ existing domain knowledge represented a @ owl ontology and applies nlp tool and text matching technique to deduce different atom @ a class property and literal to capture deductive knowledge in @ form of @ rule @ @ @ reason to enrich @ existing domain ontology by @ rule in order to obtain higher relational expressiveness make reasoning and produce @ fact @ @ approach wa tested @ medical report specifically in @ specialty of gynecology @ @ report @ f-measure of @ on test @ corpus @ @ author
352,Syntactic edge-enhanced graph convolutional networks for aspect-level sentiment classification with interactive attention,"Aspect-level sentiment classification is a hot research topic in natural language processing (NLP). One of the key challenges is that how to develop effective algorithms to model the relationships between aspects and opinion words appeared in a sentence. Among the various methods proposed in the literature, the graph convolutional networks (GCNs) achieve the promising results due to their good ability to capture the long distance between the aspects and the opinion words. However, the existing methods cannot effectively leverage the edge information of dependency parsing tree, resulting in the sub-optimal results. In this article, we propose a syntactic edge-enhanced graph convolutional network (ASEGCN) for aspect-level sentiment classification with interactive attention. Our proposed method can effectively learn better representations of aspects and the opinion words by considering the different types of neighborhoods with the edge constraint. To evaluate the effectiveness of our proposed method, we conduct the experiments on five standard sentiment classification results. Our results demonstrate that our proposed method obtains the better performance than the state-of-the-art models on four datasets, and achieves a comparative performance on Rest16. © 2013 IEEE.",2020,IEEE Access,2,aspect-level sentiment classification is a hot research topic in natural language processing @ nlp @ @ @ of @ key challenge is @ @ to develop effective algorithm to model @ relationship @ aspect and opinion word appeared in a sentence @ among @ various method proposed in @ literature @ graph convolutional network @ gcns @ achieve @ promising @ due to @ good ability to capture @ long distance @ @ aspect and @ opinion word @ however @ existing method cannot effectively leverage @ edge information of dependency parsing tree resulting in @ sub-optimal @ @ in @ article @ propose a syntactic edge-enhanced graph convolutional network @ asegcn @ @ aspect-level sentiment classification @ interactive attention @ @ proposed method @ effectively learn better representation of aspect and @ opinion word by considering @ different type of neighborhood @ @ edge constraint @ to evaluate @ effectiveness of @ proposed method @ conduct @ experiment on five standard sentiment classification @ @ @ @ demonstrate @ @ proposed method obtains @ better performance @ @ state-of-the-art model on four datasets and achieves a comparative performance on rest @ @ @ 
353,Evaluating German transformer language models with syntactic agreement tests,"Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most state-of-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,pre-trained transformer language model @ tlms @ @ recently refashioned natural language processing @ nlp @ @ @ state-of-the-art nlp model now operate on top of tlms to benefit @ contextualization and knowledge induction @ to explain @ success @ scientific community conducted numerous analysis @ besides @ method syntactic agreement test @ utilized to analyse tlms @ @ of @ study @ conducted @ @ english language however @ in @ work @ analyse german tlms @ to @ end @ design numerous agreement task some of @ consider peculiarity of @ german language @ @ experimental @ @ @ state-of-the-art german tlms generally perform well on agreement task @ @ @ identify and discus syntactic structure @ push @ to @ limit @ @ @ @ @ by @ author @ 
354,Named Entity Extraction for Knowledge Graphs: A Literature Overview,"An enormous amount of digital information is expressed as natural-language (NL) text that is not easily processable by computers. Knowledge Graphs (KG) offer a widely used format for representing information in computer-processable form. Natural Language Processing (NLP) is therefore needed for mining (or lifting) knowledge graphs from NL texts. A central part of the problem is to extract the named entities in the text. The paper presents an overview of recent advances in this area, covering: Named Entity Recognition (NER), Named Entity Disambiguation (NED), and Named Entity Linking (NEL). We comment that many approaches to NED and NEL are based on older approaches to NER and need to leverage the outputs of state-of-the-art NER systems. There is also a need for standard methods to evaluate and compare named-entity extraction approaches. We observe that NEL has recently moved from being stepwise and isolated into an integrated process along two dimensions: the first is that previously sequential steps are now being integrated into end-to-end processes, and the second is that entities that were previously analysed in isolation are now being lifted in each other's context. The current culmination of these trends are the deep-learning approaches that have recently reported promising results. © 2013 IEEE.",2020,IEEE Access,6,@ enormous amount of digital information is expressed a natural-language @ nl @ text @ is not easily processable by computer @ knowledge graph @ kg @ offer a widely used format @ representing information in computer-processable form @ natural language processing @ nlp @ is therefore needed @ mining @ @ lifting @ knowledge graph @ nl text @ a central part of @ problem is to extract @ named entity in @ text @ @ @ @ @ overview of recent advance in @ area covering @ named entity recognition @ ner @ named entity disambiguation @ ned @ and named entity linking @ nel @ @ @ comment @ many approach to ned and nel @ based on older approach to ner and need to leverage @ output of state-of-the-art ner system @ @ is @ a need @ standard method to evaluate and compare named-entity extraction approach @ @ observe @ nel ha recently moved @ @ stepwise and isolated @ @ integrated process along @ dimension @ @ first is @ @ sequential step @ now @ integrated @ end-to-end process and @ second is @ entity @ @ @ analysed in isolation @ now @ lifted in @ @ @ s context @ @ current culmination of @ trend @ @ deep-learning approach @ @ recently reported promising @ @ @ @ 
355,Economic causal-chain search using text mining technology,"In this research, we extract causal information from textual data and construct a causality database in the economic field. We develop a method to produce causal chains starting from phrases representing specific events. The proposed method can offer possible ripple effects and factors of particular events or situations. Using our approach to Japanese textual data, we have implemented a prototype system that can display causal chains for user-entered words. A user can interactively edit the causal chains by selecting appropriate causalities and deleting inappropriate causalities. The economic causal-chain search algorithm can be applied to various financial information services. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ research @ extract causal information @ textual data and construct a causality database in @ economic field @ @ develop a method to produce causal chain starting @ phrase representing specific event @ @ proposed method @ offer possible ripple effect and factor of particular event @ situation @ @ @ approach to japanese textual data @ @ implemented a prototype system @ @ display causal chain @ user-entered word @ a user @ interactively edit @ causal chain by selecting appropriate causality and deleting inappropriate causality @ @ economic causal-chain search algorithm @ @ applied to various financial information service @ @ nature switzerland ag @ 
356,A Comparative Evaluation of Preprocessing Techniques for Short Texts in Spanish,"Natural Language Processing (NLP) is used to identify key information, generating predictive models, and explaining global events or trends. Also, NLP is supported during the process to create knowledge. Therefore, it is important to apply refinement techniques in major stages such as preprocessing, when data is frequently produced and processed with poor results. This document analyzes and measures the impact of combinations of preprocessing techniques and libraries for short texts that have been written in Spanish. These techniques were applied in tweets for analysis of sentiments considering evaluation parameters in its analysis, the processing time and characteristics of the techniques for each library. The performed experimentation provides readers insights for choosing the appropriate combination of techniques during preprocessing. The results show improvement of up to 5% to 9% in the performance of the classification. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,natural language processing @ nlp @ is used to identify key information generating predictive model and explaining global event @ trend @ @ nlp is supported @ @ process to create knowledge @ therefore @ is important to apply refinement technique in major stage @ a preprocessing @ data is frequently produced and processed @ poor @ @ @ document analyzes and measure @ impact of combination of preprocessing technique and library @ short text @ @ @ written in spanish @ @ technique @ applied in tweet @ analysis of sentiment considering evaluation parameter in @ analysis @ processing time and characteristic of @ technique @ @ library @ @ performed experimentation provides reader insight @ choosing @ appropriate combination of technique @ preprocessing @ @ @ @ improvement of up to to in @ performance of @ classification @ @ nature switzerland ag @ 
357,Forecasting Crypto-Asset Price Using Influencer Tweets,"Nowadays, crypto-asset is gaining immense interest in the field of finance. Bitcoin is a one such crypto-asset with a trading volume of more than 5 billion a day. On social networking services, there are people who have a great influence on social media users; these people are called influencers. In this study, we focus on crypto-asset influencers. We consider that influencer tweets may affect crypto-asset prices. In this research, we propose a method to predict whether bitcoin price will increase or decrease using influencer tweets. For this, we collect influencer tweets to extract features using natural language processing techniques; these features are used as input for machine learning methods, which also use bitcoin price data. The results of our experiment show that the influencer tweets affect crypto-asset prices. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,2,nowadays crypto-asset is gaining immense interest in @ field of finance @ bitcoin is a @ @ crypto-asset @ a trading volume of more @ billion a day @ on social networking service @ @ people @ @ a great influence on social medium user @ @ people @ called influencers @ in @ study @ focus on crypto-asset influencers @ @ consider @ influencer tweet may affect crypto-asset price @ in @ research @ propose a method to predict whether bitcoin price @ increase @ decrease @ influencer tweet @ @ @ @ collect influencer tweet to extract feature @ natural language processing technique @ @ feature @ used a input @ machine learning method @ @ use bitcoin price data @ @ @ of @ experiment @ @ @ influencer tweet affect crypto-asset price @ @ nature switzerland ag @ 
358,An NLP-SEM approach to examine the gratifications affecting user’s choice of different e-learning providers from user tweets,"In this digital era, it is important for service providers to gain insights from the customer-generated data and act accordingly for gaining a competitive advantage over competitors. However, there are few studies that have attempted at utilising the online user-reviews in structural-models for examining the factors affecting user-behaviour. Additionally, there is a paucity of studies that have utilised the sentiment and emotional aspects to understand the motives affecting usage intentions. This study attempts to address this gap by exploring various uses and gratifications valued by users for different e-learning providers in India, namely, Coursera, Lynda, Udemy, Udacity and Byjus, by analysing the tweets posted by users using various official handles. Utilising a Natural-Language-Processing (NLP)-based approach (sentiment and opinion-mining) and 5868 tweets, the customer motives were analysed and mapped to the various gratifications. Results of the natural language processing-based structural-equation-modelling (NLP-SEM) technique show that consumers of different companies value gratifications differently. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2020,Journal of Decision Systems,0,in @ digital era @ is important @ service provider to gain insight @ @ customer-generated data and act accordingly @ gaining a competitive advantage @ competitor @ however @ @ @ study @ @ attempted at utilising @ online user-reviews in structural-models @ examining @ factor affecting user-behaviour @ additionally @ is a paucity of study @ @ utilised @ sentiment and emotional aspect to understand @ motif affecting usage intention @ @ study attempt to address @ gap by exploring various us and gratification valued by user @ different e-learning provider in india namely coursera lynda udemy udacity and byjus by analysing @ tweet posted by user @ various official handle @ utilising a natural-language-processing @ nlp @ based approach @ sentiment and opinion-mining @ and tweet @ customer motif @ analysed and mapped to @ various gratification @ @ of @ natural language processing-based structural-equation-modelling @ nlp-sem @ technique @ @ consumer of different company value gratification differently @ informa uk limited trading a taylor francis group @ 
360,Authorship Attribution of Brazilian Literary Texts Through Machine Learning Techniques,"Authorship attribution is the process of identifying the author of a particular document. This task has been performed by experts in the field. However, with the advancement of natural language processing tools and machine learning techniques, this activity has also been performed by computer systems. Authorship attribution has applicability from the detection of plagiarism and copyright to the resolution of forensic problems. There are several works on this subject in the English idiom, however those that consider texts in Portuguese are few. Therefore, this paper aims to study authorship attribution of texts of Brazilian literature. We carried out our experiments using Naïve Bayes and Random Forests methods, and for the feature extraction we considered Term Frequency - Inverse Document Frequency and Part of Speech techniques. The results showed that the Random Forests using as input the textual features extracted by Part of Speech presented the best cross-validation accuracy, although not the best runtime. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,authorship attribution is @ process of identifying @ author of a particular document @ @ task ha @ performed by expert in @ field @ however @ @ advancement of natural language processing tool and machine learning technique @ activity ha @ @ performed by computer system @ authorship attribution ha applicability @ @ detection of plagiarism and @ to @ resolution of forensic problem @ @ @ several work on @ subject in @ english idiom however @ @ consider text in portuguese @ @ @ therefore @ @ aim to study authorship attribution of text of brazilian literature @ @ carried @ @ experiment @ naïve bayes and random forest method and @ @ feature extraction @ considered term frequency inverse document frequency and part of speech technique @ @ @ showed @ @ random forest @ a input @ textual feature extracted by part of speech presented @ best cross-validation accuracy although not @ best runtime @ @ nature switzerland ag @ 
361,Deep Learning-Based Document Modeling for Personality Detection from Turkish Texts,"The usage of social media is increasing exponentially since it has been the easiest and fastest way to share information between people or organizations. As a result of this broad usage and activity of people on social networks, considerable amount of data is generated continuously. The availability of user generated data makes it possible to analyze personality of people. Personality is the most distinctive feature for an individual. The results of these analyses can be utilized in several ways. They provide support for human resources recruitment units to consider suitable candidates. Similar products and services can be offered to people who share the similar personality characteristics. Personality traits help in diagnosis of certain mental illnesses. It is also helpful in forensics to use personality traits on suspects to clarify the forensic case. With the rapid dissemination of online documents in many different languages, the classification of these documents has become an important requirement. Machine Learning (ML) and Natural Language Processing (NLP) methods have been used to classify these digitized data. In this study, current ML techniques and methodologies have been used to classify text documents and analyze person characteristics from these datasets. As a result of classification, detailed information about the personality traits of the writer could be obtained. It was seen that the frequency-based analysis and the use of the emotional words at the word level are crucial in the textual personality analysis. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,1,@ usage of social medium is increasing exponentially since @ ha @ @ easiest and fastest way to share information @ people @ organization @ a a @ of @ broad usage and activity of people on social network considerable amount of data is generated continuously @ @ availability of user generated data make @ possible to analyze personality of people @ personality is @ @ distinctive feature @ @ individual @ @ @ of @ analysis @ @ utilized in several way @ @ provide support @ human resource recruitment unit to consider suitable candidate @ similar product and service @ @ offered to people @ share @ similar personality characteristic @ personality trait help in diagnosis of certain mental illness @ @ is @ helpful in forensics to use personality trait on suspect to clarify @ forensic case @ @ @ rapid dissemination of online document in many different language @ classification of @ document ha become @ important requirement @ machine learning @ ml @ and natural language processing @ nlp @ method @ @ used to classify @ digitized data @ in @ study current ml technique and methodology @ @ used to classify text document and analyze person characteristic @ @ datasets @ a a @ of classification detailed information @ @ personality trait of @ writer could @ obtained @ @ wa seen @ @ frequency-based analysis and @ use of @ emotional word at @ word level @ crucial in @ textual personality analysis @ @ nature switzerland ag @ 
365,Psychological Human Traits Detection based on Universal Language Modeling,"Personality Traits Detection is one of the important problems as a text analytics task in Natural Language Processing (NLP). Text analytics is the process of finding out insight knowledge over written text. Although most deep learning models give high performance, they often lack interpretability. Computer Vision (CV) has been affected significantly with inductive transfer learning, however training from scratch and task-specific modifications are still wanted in many NLP techniques. This paper addresses the problem of personality traits classification. We adopted the use of the Universal Language Model Fine-Tuning (ULMFiT) in personality traits detection. The model makes use of transfer learning rather than the classical shallow methods of word embedding and proved to be the most powerful model in many NLP problems. The basic advantage of using this model is that there is no need to do feature engineering before classification. When applied to benchmark dataset, the proposed method shows a statistical accuracy improvement of about 1% compared to the state-of-the-art results for the big five personality traits. © 2020",2020,Egyptian Informatics Journal,0,personality trait detection is @ of @ important problem a a text analytics task in natural language processing @ nlp @ @ text analytics is @ process of finding @ insight knowledge @ written text @ although @ deep learning model give high performance @ often lack interpretability @ computer vision @ cv @ ha @ affected significantly @ inductive transfer learning however training @ scratch and task-specific modification @ still wanted in many nlp technique @ @ @ address @ problem of personality trait classification @ @ adopted @ use of @ universal language model fine-tuning @ ulmfit @ in personality trait detection @ @ model make use of transfer learning rather @ @ classical shallow method of word embedding and proved to @ @ @ powerful model in many nlp problem @ @ basic advantage of @ @ model is @ @ is no need to @ feature engineering @ classification @ @ applied to benchmark dataset @ proposed method @ a statistical accuracy improvement of @ compared to @ state-of-the-art @ @ @ big five personality trait @ 
368,A Hybrid Stemmer for the Affix Stacking Language: Marathi,"Stemming is the process of term conflation that reduces the morphological variations of the terms to their common stem. It plays a significant role during preprocessing in most of the natural language processing, text mining, and information retrieval applications. The use of stemmers has proven highly effective for the task of information retrieval for many languages like English and Arabic. This paper focuses on the development of automated stemmer for Marathi language. We have adopted a hybrid technique for the development of proposed stemmer. The goal of this work is to overcome the limitations of the existing stemmers available for Marathi and to enhance the accuracy of Marathi stemming. The proposed stemmer is tested on Marathi news articles and the evaluation of the work shows that significant improvement is obtained in the accuracy, due to the proposed hybrid stemmer over the existing rule-based stemmer. We have achieved an average accuracy of 84.82% with the proposed hybrid stemmer for Marathi. © 2020, Springer Nature Singapore Pte Ltd.",2020,Advances in Intelligent Systems and Computing,0,stemming is @ process of term conflation @ reduces @ morphological variation of @ term to @ common stem @ @ play a significant role @ preprocessing in @ of @ natural language processing text mining and information retrieval application @ @ use of stemmer ha proven highly effective @ @ task of information retrieval @ many language like english and arabic @ @ @ focus on @ development of automated stemmer @ marathi language @ @ @ adopted a hybrid technique @ @ development of proposed stemmer @ @ goal of @ work is to overcome @ limitation of @ existing stemmer available @ marathi and to enhance @ accuracy of marathi stemming @ @ proposed stemmer is tested on marathi news article and @ evaluation of @ work @ @ significant improvement is obtained in @ accuracy due to @ proposed hybrid stemmer @ @ existing rule-based stemmer @ @ @ achieved @ average accuracy of @ @ @ proposed hybrid stemmer @ marathi @ @ nature singapore pte ltd @ 
369,Lifting news into a journalistic knowledge platform,"A massive amount of news is being shared online by individuals and news agencies, making it difficult to take advantage of these news and analyse them in traditional ways. In view of this, there is an urgent need to use recent technologies to analyse all news relevant information that is being shared in natural language and convert it into forms that can be more easily and precisely processed by computers. Knowledge Graphs (KGs) offer offer a good solution for such processing. Natural Language Processing (NLP) offers the possibility for mining and lifting natural language texts to knowledge graphs allowing to exploit its semantic capabilities, facilitating new possibilities for news analysis and understanding. However, the current available techniques are still away from perfect. Many approaches and frameworks have been proposed to track and analyse news in the last few years. The shortcomings of those systems are that they are static and not updateable, are not designed for large-scale data volumes, did not support real-time processing, dealt with limited data resources, used traditional lifting pipelines and supported limited tasks, or have neglected the use of knowledge graphs to represent news into a computer-processable form. Therefore, there is a need to better support lifting natural language into a KG. With the continuous development of NLP techniques, the design of new dynamic NLP lifters that can cope with all the previous shortcomings is required. This paper introduces a general NLP lifting architecture for automatically lifting and processing news reports in real-time based on the recent development of the NLP methods. © 2020 CEUR-WS. All rights reserved.",2020,CEUR Workshop Proceedings,0,a massive amount of news is @ shared online by individual and news agency making @ difficult to take advantage of @ news and analyse @ in traditional way @ in view of @ @ is @ urgent need to use recent technology to analyse @ news relevant information @ is @ shared in natural language and convert @ @ form @ @ @ more easily and precisely processed by computer @ knowledge graph @ kg @ offer offer a good solution @ @ processing @ natural language processing @ nlp @ offer @ possibility @ mining and lifting natural language text to knowledge graph allowing to exploit @ semantic capability facilitating @ possibility @ news analysis and understanding @ however @ current available technique @ still away @ perfect @ many approach and framework @ @ proposed to track and analyse news in @ last @ year @ @ shortcoming of @ system @ @ @ @ static and not updateable @ not designed @ large-scale data volume @ not support real-time processing dealt @ limited data resource used traditional lifting pipeline and supported limited task @ @ neglected @ use of knowledge graph to represent news @ a computer-processable form @ therefore @ is a need to better support lifting natural language @ a kg @ @ @ continuous development of nlp technique @ design of @ dynamic nlp lifter @ @ cope @ @ @ previous shortcoming is required @ @ @ introduces a general nlp lifting architecture @ automatically lifting and processing news report in real-time based on @ recent development of @ nlp method @ ceur-ws @ @ right reserved @ 
372,Creating Classification Models from Textual Descriptions of Companies Using Crunchbase,"This paper compares different models for multilabel text classification, using information collected from Crunchbase, a large database that holds information about more than 600000 companies. Each company is labeled with one or more categories, from a subset of 46 possible categories, and the proposed models predict the categories based solely on the company textual description. A number of natural language processing strategies have been tested for feature extraction, including stemming, lemmatization, and part-of-speech tags. This is a highly unbalanced dataset, where the frequency of each category ranges from 0.7% to 28%. Our findings reveal that the description text of each company contain features that allow to predict its area of activity, expressed by its corresponding categories, with about 70% precision, and 42% recall. In a second set of experiments, a multiclass problem that attempts to find the most probable category, we obtained about 67% accuracy using SVM and Fuzzy Fingerprints. The resulting models may constitute an important asset for automatic classification of texts, not only consisting of company descriptions, but also other texts, such as web pages, text blogs, news pages, etc. © 2020, Springer Nature Switzerland AG.",2020,Communications in Computer and Information Science,0,@ @ compare different model @ multilabel text classification @ information collected @ crunchbase a @ database @ hold information @ more @ company @ @ company is labeled @ @ @ more category @ a subset of possible category and @ proposed model predict @ category based solely on @ company textual description @ a number of natural language processing strategy @ @ tested @ feature extraction including stemming lemmatization and part-of-speech tag @ @ is a highly unbalanced dataset @ @ frequency of @ category range @ @ to @ @ finding reveal @ @ description text of @ company contain feature @ allow to predict @ area of activity expressed by @ corresponding category @ @ precision and recall @ in a second set of experiment a multiclass problem @ attempt to find @ @ probable category @ obtained @ accuracy @ svm and fuzzy fingerprint @ @ resulting model may constitute @ important asset @ automatic classification of text not only consisting of company description @ @ @ text @ a web page text blog news page etc @ @ nature switzerland ag @ 
376,BERT-Based Chinese Relation Extraction for Public Security,"The past few years have witnessed some public safety incidents occurring around the world. With the advent of the big data era, effectively extracting public security information from the internet has become of great significance. Up to hundreds of TBs of data are injected into the network every second, and thus it is impossible to process them manually. Natural Language Processing (NLP) is dedicated to the development of an intelligent system for effective text information mining. By analysing the text and quickly extracting the relationships between the relevant entities, NLP can establish the knowledge graph (KG) of public security, which lays the foundation for safety case analysis, information monitoring, and activity tracking and locating. One of the current pre-training relation extraction models is the Word2Vec model. The Word2vec model is single mapped, and it produces a static, single representation of the words in sentences. Then, the BERT model considers contextual information and provides more dynamic, richer vector representations of generated words. Therefore, in this paper, we propose a Bidirectional Encoder Representation from Transformers (BERT) based on the Chinese relation extraction algorithm for public security, which can effectively mine security information. The BERT model is obtained by training the Masked Language Model and predicting the next sentence task, which is based on the Transformer Encoder and the main model structure is the stacked Transformers. Extensive simulations are conducted to evaluate our proposed algorithm in comparison to some state-of-the-art schemes. © 2013 IEEE.",2020,IEEE Access,1,@ past @ year @ witnessed some public safety incident occurring around @ world @ @ @ advent of @ big data era effectively extracting public security information @ @ internet ha become of great significance @ up to hundred of tb of data @ injected @ @ network every second and thus @ is impossible to process @ manually @ natural language processing @ nlp @ is dedicated to @ development of @ intelligent system @ effective text information mining @ by analysing @ text and quickly extracting @ relationship @ @ relevant entity nlp @ establish @ knowledge graph @ kg @ of public security @ lay @ foundation @ safety case analysis information monitoring and activity tracking and locating @ @ of @ current pre-training relation extraction model is @ word vec model @ @ word vec model is single mapped and @ produce a static single representation of @ word in sentence @ @ @ bert model considers contextual information and provides more dynamic richer vector representation of generated word @ therefore in @ @ @ propose a bidirectional encoder representation @ transformer @ bert @ based on @ chinese relation extraction algorithm @ public security @ @ effectively mine security information @ @ bert model is obtained by training @ masked language model and predicting @ next sentence task @ is based on @ transformer encoder and @ main model structure is @ stacked transformer @ extensive simulation @ conducted to evaluate @ proposed algorithm in comparison to some state-of-the-art scheme @ @ @ 
377,Personalized Video Recommendation Strategy Based on User's Playback Behavior Sequence,"Due to the spread of online web services about videos such as YouTube and Tencent, the number of videos posted to them is increasing. We can now watch a large amount of videos with ease, however, explosive videos available frequently overwhelmed users, leading them to make poor decisions. Recommendation systems are designed to predict the future preferences of users' based on their previous interactions with the items. For example, a user's previous viewing information can be used to make recommendations on his/her future viewing video. The vast amount of information produced by the users can be used by different methods for recommendations, with either neighborhood based methods, or machine-learning based methods or matrix-factorization based methods. These all use low-rank approximation of input data, however, may suffer from data sparsity or noise in data. Recently, word embedding methods in deep learning are successfully used to learn linguistic regularities and semantic information from large text datasets. It has made a big innovation in natural language processing and text mining fields. They can learn low-dimensional vector space representation of input elements with effect, which lays the foundation of the recommendation research in online video services. Actually, user's viewing history is one of the important factors reflecting the user's attention and also it has good scalability, high accuracy and flexibility comparing to rating data and reviews written by users. In this work, we aim to recommend next videos by adopting word embedding techniques proposed in Word2Vec framework via analyzing user's playback behavior. Unlike the previous works that use Word2Vec for recommendation, a non-textual feature namely the past viewing videos of the users, is used to make recommendations. By representing each user (according to her/his history viewing behavior) with a high dimensional vector space, we can filter the target user potential interest in candidate video, thus a personalized recommendation strategy based on the user's playback behavior sequence for online video service websites is proposed. This strategy maps the video into feature vectors and extracts the semantic features of the video via user's video playing behavior. Then, the user's interest distribution matrix is modeled by clustering the feature vectors of the user's video playing history. A recommendation list is generated in combination with user interest preferences and user viewing history aging. Offline experiments were conducted in a large-scale video service system. Compared with random algorithms, item-based collaborative filtering recommendations, and user-based collaborative filtering recommendations, it improves the average accuracy of Top-N recommendation for users watching videos 22.3%, 30.7% and 934% respectively. The relative increase of 52.8%, 41% and 1065% respectively in the recall rate indicator is achieved. Moreover, compared with the matrix decomposition algorithm SVD++, the model based on the bidirectional LSTM+Attention and the deep interest network DIN, the proposed method achieves different levels of improvement in both the Top-N recommendation accuracy rate and the recall rate. The recommendation strategy not only obtains better performance as a whole, but also attempts to solve the problems of data requirements, data sparsity and data noise faced by traditional recommendation algorithms. © 2020, Science Press. All right reserved.",2020,Jisuanji Xuebao/Chinese Journal of Computers,0,due to @ spread of online web service @ video @ a youtube and tencent @ number of video posted to @ is increasing @ @ @ now watch a @ amount of video @ ease however explosive video available frequently overwhelmed user leading @ to make poor decision @ recommendation system @ designed to predict @ future preference of user @ based on @ previous interaction @ @ item @ @ example a user @ s previous viewing information @ @ used to make recommendation on @ @ future viewing video @ @ vast amount of information produced by @ user @ @ used by different method @ recommendation @ either neighborhood based method @ machine-learning based method @ matrix-factorization based method @ @ @ use low-rank approximation of input data however may suffer @ data sparsity @ noise in data @ recently word embedding method in deep learning @ successfully used to learn linguistic regularity and semantic information @ @ text datasets @ @ ha made a big innovation in natural language processing and text mining field @ @ @ learn low-dimensional vector space representation of input element @ effect @ lay @ foundation of @ recommendation research in online video service @ actually user @ s viewing history is @ of @ important factor reflecting @ user @ s attention and @ @ ha good scalability high accuracy and flexibility comparing to rating data and review written by user @ in @ work @ aim to recommend next video by adopting word embedding technique proposed in word vec framework via analyzing user @ s playback behavior @ unlike @ previous work @ use word vec @ recommendation a non-textual feature namely @ past viewing video of @ user is used to make recommendation @ by representing @ user @ according to @ @ history viewing behavior @ @ a high dimensional vector space @ @ filter @ target user potential interest in candidate video thus a personalized recommendation strategy based on @ user @ s playback behavior sequence @ online video service website is proposed @ @ strategy map @ video @ feature vector and extract @ semantic feature of @ video via user @ s video playing behavior @ @ @ user @ s interest distribution matrix is modeled by clustering @ feature vector of @ user @ s video playing history @ a recommendation list is generated in combination @ user interest preference and user viewing history aging @ offline experiment @ conducted in a large-scale video service system @ compared @ random algorithm item-based collaborative filtering recommendation and user-based collaborative filtering recommendation @ improves @ average accuracy of top-n recommendation @ user watching video @ @ and respectively @ @ relative increase of @ and respectively in @ recall rate indicator is achieved @ moreover compared @ @ matrix decomposition algorithm svd @ model based on @ bidirectional lstm attention and @ deep interest network din @ proposed method achieves different level of improvement in @ @ top-n recommendation accuracy rate and @ recall rate @ @ recommendation strategy not only obtains better performance a a whole @ @ attempt to solve @ problem of data requirement data sparsity and data noise faced by traditional recommendation algorithm @ science @ @ @ right reserved @ 
379,Detecting potential subscribers on twitch: A text mining approach with XGBoost - discovery challenge ChAT: CoolStoryBob,"In this paper we describe our approach to solve the text classification problem of the Chat Analytics for Twitch (ChAT) discovery challenge of ECML-PKDD 2020. The task was to predict the subscription status of Twitch users for a given channel based on their comments posted within the Twitch chat. Users have the opportunity to support channels in the form of monthly subscriptions, giving them exclusive subscriber-only features in return. Half of the earnings from subscriptions are received by the streamers themselves, with the other half going to Twitch. Thus, there is a monetary motivation for Twitch and the streamers to acquire more subscriptions. The motivation of this research is to detect potential subscribers by predicting a user's subscription status using a trained ML model. These users can then be targeted with marketing campaigns. For our solution we use BOW and TF-IDF vectors as text features as well as additional extracted numerical features. We applied downsampling to the majority class and used XGBoost as the binary classifier. On the organizers' evaluation set our submission achieved an F1-score of 0.2647 on the class of subscribers (random baseline: 0.0741) and reached second place among all submissions. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2020,CEUR Workshop Proceedings,0,in @ @ @ describe @ approach to solve @ text classification problem of @ chat analytics @ twitch @ chat @ discovery challenge of ecml-pkdd @ @ task wa to predict @ subscription status of twitch user @ a given channel based on @ comment posted within @ twitch chat @ user @ @ opportunity to support channel in @ form of monthly subscription giving @ exclusive subscriber-only feature in return @ half of @ earnings @ subscription @ received by @ streamer @ @ @ @ half going to twitch @ thus @ is a monetary motivation @ twitch and @ streamer to acquire more subscription @ @ motivation of @ research is to detect potential subscriber by predicting a user @ s subscription status @ a trained ml model @ @ user @ @ @ targeted @ marketing campaign @ @ @ solution @ use bow and tf-idf vector a text feature a well a additional extracted numerical feature @ @ applied downsampling to @ majority class and used xgboost a @ binary classifier @ on @ organizer @ evaluation set @ submission achieved @ f score of @ on @ class of subscriber @ random baseline @ @ @ and reached second place among @ submission @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
382,Generating Biomedical Question Answering Corpora from QA Forums,"Question Answering (QA) is a natural language processing task that aims at obtaining relevant answers to user questions. While some progress has been made in this area, biomedical questions are still a challenge to most QA approaches, due to the complexity of the domain and limited availability of training sets. We present a method to automatically extract question-article pairs from QA web forums, which can be used for document retrieval, a crucial step of most QA systems. The proposed framework extracts from selected forums the questions and the respective answers that contain citations. This way, QA systems based on document retrieval can be developed and evaluated using the question-article pairs annotated by users of these forums. We generated the BiQA corpus by applying our framework to three forums, obtaining 7,453 questions and 14,239 question-article pairs. We evaluated how the number of articles associated with each question and the number of votes on each answer affects the performance of baseline document retrieval approaches. Also, we demonstrated that the articles given as answers are significantly similar to the questions and trained a state-of-the-art deep learning model that obtained similar performance to using a dataset manually annotated by experts. The proposed framework can be used to update the BiQA corpus from the same forums as new posts are made, and from other forums that support their answers with documents. The BiQA corpus and the framework used to generate it are available at https://github.com/lasigeBioTM/BiQA. © 2013 IEEE.",2020,IEEE Access,0,question answering @ qa @ is a natural language processing task @ aim at obtaining relevant answer to user question @ @ some progress ha @ made in @ area biomedical question @ still a challenge to @ qa approach due to @ complexity of @ domain and limited availability of training set @ @ @ a method to automatically extract question-article pair @ qa web forum @ @ @ used @ document retrieval a crucial step of @ qa system @ @ proposed framework extract @ selected forum @ question and @ respective answer @ contain citation @ @ way qa system based on document retrieval @ @ developed and evaluated @ @ question-article pair annotated by user of @ forum @ @ generated @ biqa corpus by applying @ framework to three forum obtaining question and question-article pair @ @ evaluated @ @ number of article associated @ @ question and @ number of vote on @ answer affect @ performance of baseline document retrieval approach @ @ @ demonstrated @ @ article given a answer @ significantly similar to @ question and trained a state-of-the-art deep learning model @ obtained similar performance to @ a dataset manually annotated by expert @ @ proposed framework @ @ used to update @ biqa corpus @ @ @ forum a @ post @ made and @ @ forum @ support @ answer @ document @ @ biqa corpus and @ framework used to generate @ @ available at http @ github @ com lasigebiotm biqa @ @ @ 
384,A neural approach for detecting inline mathematical expressions from scientific documents,"Scientific documents generally contain multiple mathematical expressions in them. Detecting inline mathematical expressions are one of the most important and challenging tasks in scientific text mining. Recent works that detect inline mathematical expressions in scientific documents have looked at the problem from an image processing perspective. There is little work that has targeted the problem from NLP perspective. Towards this, we define a few features and applied Conditional Random Fields (CRF) to detect inline mathematical expressions in scientific documents. Apart from this feature based approach, we also propose a hybrid algorithm that combines Bidirectional Long Short Term Memory networks (Bi-LSTM) and feature-based approach for this task. Experimental results suggest that this proposed hybrid method outperforms several baselines in the literature and also individual methods in the hybrid approach. © 2020 John Wiley & Sons, Ltd",2020,Expert Systems,0,scientific document generally contain multiple mathematical expression in @ @ detecting inline mathematical expression @ @ of @ @ important and challenging task in scientific text mining @ recent work @ detect inline mathematical expression in scientific document @ looked at @ problem @ @ image processing perspective @ @ is little work @ ha targeted @ problem @ nlp perspective @ towards @ @ define a @ feature and applied conditional random field @ crf @ to detect inline mathematical expression in scientific document @ apart @ @ feature based approach @ @ propose a hybrid algorithm @ combine bidirectional long short term memory network @ bi-lstm @ and feature-based approach @ @ task @ experimental @ suggest @ @ proposed hybrid method outperforms several baseline in @ literature and @ individual method in @ hybrid approach @ john wiley son ltd
385,Harrymotions - Classifying relationships in harry potter based on emotion analysis,"Sentiment Analysis has long been a topic of interest in natural language processing and computational literary studies, where it can be used to infer the relationships between fictional characters. Building on the dataset and results of Kim and Klinger (2019), we propose a classifier based on BERT that improves the results reported therein and show that we can use this classifier to determine the relation between characters in Harry Potter novels. Our proposed sentiment classifier yields an F1-score of up to 75 % for binary classification of emotions. Aggregating these emotions over novels, we reach an F1-score of up to 68 % for the classification of a pair of characters as friendly or unfriendly. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,sentiment analysis ha long @ a topic of interest in natural language processing and computational literary study @ @ @ @ used to infer @ relationship @ fictional character @ building on @ dataset and @ of kim and klinger @ @ @ propose a classifier based on bert @ improves @ @ reported therein and @ @ @ @ use @ classifier to determine @ relation @ character in harry potter novel @ @ proposed sentiment classifier yield @ f score of up to @ binary classification of emotion @ aggregating @ emotion @ novel @ reach @ f score of up to @ @ classification of a pair of character a friendly @ unfriendly @ @ @ @ @ by @ author @ 
388,User Review Classification and Star Rating Prediction by Sentimental Analysis and Machine Learning Classifiers,"With the digital media explosion, in today’s increasing Internet usage, the data generated is wide and varied. Huge volumes of new data are injected daily into the Web for various prospects. Procedures like text mining and analysis are required to make the best use of this potential. User review analysis benefits us with the exact understanding of the user’s feedback toward the product. In this paper, we have proposed a unique approach by performing abstract-level sentimental analysis of user review by n-gram classification and POS tagging. This classification is then used as entropy for machine learning algorithm. This paper leverages upon the proposed methodology with promising outcomes and improved accuracy by evaluating the data with the help of two algorithms, MaxEnt model and Naïve Bayes classifier, after analyzing few algorithms including SVM and random forest. © Springer Nature Singapore Pte Ltd. 2020.",2020,Advances in Intelligent Systems and Computing,1,@ @ digital medium explosion in today s increasing internet usage @ data generated is wide and varied @ huge volume of @ data @ injected daily @ @ web @ various prospect @ procedure like text mining and analysis @ required to make @ best use of @ potential @ user review analysis benefit u @ @ exact understanding of @ user s feedback toward @ product @ in @ @ @ @ proposed a unique approach by performing abstract-level sentimental analysis of user review by n-gram classification and po tagging @ @ classification is @ used a entropy @ machine learning algorithm @ @ @ leverage upon @ proposed methodology @ promising outcome and improved accuracy by evaluating @ data @ @ help of @ algorithm maxent model and naïve bayes classifier @ analyzing @ algorithm including svm and random forest @ @ nature singapore pte ltd @ @ 
389,Analysis and Prediction of Customers’ Reviews with Amazon Dataset on Products,"The main objective of this paper is to get a deeper knowledge of the text classification methods used in text mining. This paper describes different methods and algorithms used in text mining. Various text preprocessing steps have been performed like tokenization, case folding, stemming, stopword removal, etc. The customer reviews posted in the amazon website have been used as the training set and used with various classifiers like Naive Bayes, KNN, random forest and decision tree. The performance parameter of each method is determined with standard evaluation parameters such as precision, recall, and kappa measures. The results show that K-nearest neighbor method gives the optimal performance with the same dataset. © 2020, Springer Nature Singapore Pte Ltd.",2020,Advances in Intelligent Systems and Computing,0,@ main objective of @ @ is to get a deeper knowledge of @ text classification method used in text mining @ @ @ describes different method and algorithm used in text mining @ various text preprocessing step @ @ performed like tokenization case folding stemming stopword removal etc @ @ customer review posted in @ amazon website @ @ used a @ training set and used @ various classifier like naive bayes knn random forest and decision tree @ @ performance parameter of @ method is determined @ standard evaluation parameter @ a precision recall and kappa measure @ @ @ @ @ k-nearest neighbor method give @ optimal performance @ @ @ dataset @ @ nature singapore pte ltd @ 
391,Citizen-centric driven approach on disaster resilience priority needs through text mining,"The Philippines is a country prone to natural disasters such as floods, earthquake and volcanic eruption. At present, Legazpi City is investing in disaster resilience as the top most priority program of action among others. The study observed Participatory Action Research (PAR) by employing a stratified random sampling, Key Informant Interview (KII), and Topic Modelling using Latent Dirichlet Algorithm (LDA), thus, from 662 data sets, after filtering and cleaning, this resulted to 649 unique instances that were used for thematic analysis. Through text mining, results showed that majority of the vulnerable groups including the youth regardless of the hazard type suggested that in times of disaster, an emergency items should be in their get go bags such as canned goods, water, cell phone, portable radio, first aid kit, flashlight, medicines, hygiene kit, important documents, slippers, extra clothing, match and lighter, and money. Other emergency items can be optional depending on their personal preferences. With similar parameters, the topic models generated were sufficient relief goods and disaster facility, availability of emergency supplies and transportation, information dissemination on evacuation sites, adequate evacuation center, and disaster resiliency campaigns. Cascading of necessary information to community members is being suggested. A symbol of resilience among community members can be achieved if the community members are well-informed, aware of the evacuation plan, always ready with emergency bags, and routinely conduct inspection of their home infrastructure. The results were presented to community experts for inclusion in their contingency disaster management plan and development of mobile app pre-crisis mapping. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.",2020,Journal of Advanced Research in Dynamical and Control Systems,0,@ philippine is a country prone to natural disaster @ a flood earthquake and volcanic eruption @ at @ legazpi city is investing in disaster resilience a @ top @ priority program of action among others @ @ study observed participatory action research @ par @ by employing a stratified random sampling key informant interview @ kii @ and topic modelling @ latent dirichlet algorithm @ lda @ thus @ data set @ filtering and cleaning @ resulted to unique instance @ @ used @ thematic analysis @ @ text mining @ showed @ majority of @ vulnerable group including @ youth regardless of @ hazard type suggested @ in time of disaster @ emergency item @ @ in @ get go bag @ a canned good water cell phone portable radio first aid kit flashlight medicine hygiene kit important document slipper extra clothing match and lighter and money @ @ emergency item @ @ optional depending on @ personal preference @ @ similar parameter @ topic model generated @ sufficient relief good and disaster facility availability of emergency supply and transportation information dissemination on evacuation site adequate evacuation center and disaster resiliency campaign @ cascading of necessary information to community member is @ suggested @ a symbol of resilience among community member @ @ achieved if @ community member @ well-informed aware of @ evacuation plan always ready @ emergency bag and routinely conduct inspection of @ home infrastructure @ @ @ @ presented to community expert @ inclusion in @ contingency disaster management plan and development of mobile app pre-crisis mapping @ institute of advanced scientific research inc @ @ @ right reserved @ 
392,An Integrated Knowledge Graph for Microbe-Disease Associations,"Following the rapid advances of the human microbiome, the importance of micro-organisms especially bacteria is gradually recognized. The interactions among bacteria and their host are particulary important for understanding the mechanism of microbe-relate diseases. This article mainly introduces an explorative study to extract the relations between bacteria and diseases based on biomedical text mining. We have constructed a Microbe-Disease Knowledge Graph (MDKG) through integrating multi-source heterogeneous data from Wikipedia text and other related databases. Specifically, we introduce the word embedding obtained from biomedical literature into traditional method. Results show that the pre-trained relation vectors can better represent the real associations between entities. Therefore, the construction of MDKG can also provide a new way to predict and analyse the associations between microbes and diseases based on text mining. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,following @ rapid advance of @ human microbiome @ importance of micro-organism especially bacteria is gradually recognized @ @ interaction among bacteria and @ host @ particulary important @ understanding @ mechanism of microbe-relate disease @ @ article mainly introduces @ explorative study to extract @ relation @ bacteria and disease based on biomedical text mining @ @ @ constructed a microbe-disease knowledge graph @ mdkg @ @ integrating multi-source heterogeneous data @ wikipedia text and @ related database @ specifically @ introduce @ word embedding obtained @ biomedical literature @ traditional method @ @ @ @ @ pre-trained relation vector @ better represent @ real association @ entity @ therefore @ construction of mdkg @ @ provide a @ way to predict and analyse @ association @ microbe and disease based on text mining @ @ nature switzerland ag @ 
394,Improvement of misleading and fake news classification for flective languages by morphological group analysis,"Due to the constantly evolving social media and different types of sources of information, we are facing different fake news and different types of misinformation. Currently, we are working on a project to identify applicable methods for identifying fake news for floating language types. We explored different approaches to detect fake news in the presented research, which are based on morphological analysis. This is one of the basic components of natural language processing. The aim of the article is to find out whether it is possible to improve the methods of dataset preparation based on morphological analysis. We collected our own and unique dataset, which consisted of articles from verified publishers and articles from news portals that are known as the publishers of fake and misleading news. Articles were in the Slovak language, which belongs to the floating types of languages. We explored different approaches in this article to the dataset preparation based on morphological analysis. The prepared datasets were the input data for creating the classifier of fake and real news. We selected decision trees for classification. The evaluation of the success of two different methods of preparation was carried out because of the success of the created classifier. We found a suitable dataset pre-processing technique by morphological group analysis. This technique could be used for improving fake news classification. © 2020 by the authors.",2020,Informatics,2,due to @ constantly evolving social medium and different type of source of information @ @ facing different fake news and different type of misinformation @ currently @ @ working on a project to identify applicable method @ identifying fake news @ floating language type @ @ explored different approach to detect fake news in @ presented research @ @ based on morphological analysis @ @ is @ of @ basic component of natural language processing @ @ aim of @ article is to find @ whether @ is possible to improve @ method of dataset preparation based on morphological analysis @ @ collected @ @ and unique dataset @ consisted of article @ verified publisher and article @ news portal @ @ known a @ publisher of fake and misleading news @ article @ in @ slovak language @ belongs to @ floating type of language @ @ explored different approach in @ article to @ dataset preparation based on morphological analysis @ @ prepared datasets @ @ input data @ creating @ classifier of fake and real news @ @ selected decision tree @ classification @ @ evaluation of @ success of @ different method of preparation wa carried @ @ of @ success of @ created classifier @ @ found a suitable dataset pre-processing technique by morphological group analysis @ @ technique could @ used @ improving fake news classification @ by @ author @ 
395,Automatic icd code classification with label description attention mechanism,"We present our submission for the CANTEMIST (CANcer TExt Mining Shared Task-tumor named entity recognition) 2020 task [1]. We participated in track 3, which focuses on automatic eCIE-O-3.1 codes assignment (English version: ICD-O-3), an extreme multi-label classification (XMLC) problem. We developed a model which utilizes a BERT-like encoder and word-level attention mechanism between input clinical cases and textual descriptions of the labels. We found that our model predicted a wider variety of codes across the test set than our baseline, thereby capturing more low-resource labels. Our final submission achieved a mean average precision (MAP) of 39.4% and F1-micro of 26.8%. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,0,@ @ @ submission @ @ cantemist @ cancer text mining shared task-tumor named entity recognition @ task @ @ participated in track @ focus on automatic ecie-o @ code assignment @ english version @ icd-o @ @ extreme multi-label classification @ xmlc @ problem @ @ developed a model @ utilizes a bert-like encoder and word-level attention mechanism @ input clinical case and textual description of @ label @ @ found @ @ model predicted a wider variety of code across @ test set @ @ baseline thereby capturing more low-resource label @ @ final submission achieved a mean average precision @ map @ of @ and f micro of @ @ @ @ @ @ by @ author @ use permitted @ @ 
396,Semantic enriched deep learning for document classification,Textual data are available in large unstructured volumes. Processing this data is becoming crucial and document classification is a way of structuring and processing this information based on its content. This paper introduces an effective semantic text mining approach for document classification. The proposed approach Semantic Enriched Deep Learning Architecture (SE-DLA) allows the model to learn simultaneously from the generated semantic vector representations and the original document vectors. We evaluated the proposed method on topic categorizations and multi-label classification. The experiments demonstrate that the proposed hybrid architecture with the additional semantic knowledge improves the results. This approach was compared to some state-of-the-art text classification approaches not including semantic knowledge. The proposed SE-DLA achieved higher accuracy and maintained great results during the experimental process. Copyright © 2020 for this paper by its authors.,2020,CEUR Workshop Proceedings,0,textual data @ available in @ unstructured volume @ processing @ data is becoming crucial and document classification is a way of structuring and processing @ information based on @ content @ @ @ introduces @ effective semantic text mining approach @ document classification @ @ proposed approach semantic enriched deep learning architecture @ se-dla @ allows @ model to learn simultaneously @ @ generated semantic vector representation and @ original document vector @ @ evaluated @ proposed method on topic categorization and multi-label classification @ @ experiment demonstrate @ @ proposed hybrid architecture @ @ additional semantic knowledge improves @ @ @ @ approach wa compared to some state-of-the-art text classification approach not including semantic knowledge @ @ proposed se-dla achieved higher accuracy and maintained great @ @ @ experimental process @ @ @ @ @ by @ author @ 
397,Biomedical Text Recognition Using Convolutional Neural Networks: Content Based Deep Learning,"Named Entity Recognition (NER) targets to automatically detect the drug and disease mentions from biomedical texts and is fundamental step in the biomedical text mining. Although deep learning has been successfully implemented, the accuracy and processing time are still major issues preventing it from achieving NMR. This research aims to upgrade the accuracy of classification while decreasing the processing time, by paying more attention to significant areas of NMR. The novel proposed system consists of a Bi-Directional Long Short-Term Memory with Conditional Random Field (BiLSTM-CRF) using dropout strategy to effectively prevent overfitting and enhancing the generalization abilities. The system built includes the attention mechanism and attention fusion for redistributing the weight of samples belonging to each class in order to compensate the problem occurring from data imbalance and to focus only on the critical areas of the observed things and ignoring non-critical areas. © 2020, Springer Nature Switzerland AG.",2020,Communications in Computer and Information Science,0,named entity recognition @ ner @ target to automatically detect @ drug and disease mention @ biomedical text and is fundamental step in @ biomedical text mining @ although deep learning ha @ successfully implemented @ accuracy and processing time @ still major issue preventing @ @ achieving nmr @ @ research aim to upgrade @ accuracy of classification @ decreasing @ processing time by paying more attention to significant area of nmr @ @ novel proposed system consists of a bi-directional long short-term memory @ conditional random field @ bilstm-crf @ @ dropout strategy to effectively prevent overfitting and enhancing @ generalization ability @ @ system built includes @ attention mechanism and attention fusion @ redistributing @ weight of sample belonging to @ class in order to compensate @ problem occurring @ data imbalance and to focus only on @ critical area of @ observed thing and ignoring non-critical area @ @ nature switzerland ag @ 
398,A framework for sentiment analysis of online news articles,"The current innovation era has changed the conventional way of life in many areas. Information Technology has been overflowed with an enormous quantity of data, which is being distributed each moment of consistently, by a large number of users, in the form of online journals, feedbacks, reviews, news blogs, comments, micro-blogging websites, social media and some more. Due to this absolute amount of assessment of precious web assets, a significant part of the research is concentrating on the sentiment analysis. Opinion analysis is the act of implementing text analysis and natural language processing techniques to recognize and extricate abstract data from content. News analysis can be utilized to plot the company's conduct after some time and in this manner, yield significant vital bits of knowledge about companies. Sentiment analysis is additionally helpful in web-based life checking to consequently describe the general inclination or state of mind of purchasers as reflected in web-based life toward a particular brand or organization and decide if they see positively or negatively. Today, it is an everyday exercise for some users to read the news on the internet. Persons' viewpoints, in general, will encounter a variation as per the news they go through. News reports occasions that involve feelings – good, bad, neutral. The human feelings available in the textual information can be studies by using sentiment analysis. Finding the sentiments news articles involves many challenges as it requires going through each and every word of the news text articles to find the particular sentiments. This paper presents a brief study of sentiment analysis and opinion mining and an experimental approach of sentiment analysis of news articles obtained from various news websites. © 2020, Research Trend. All rights reserved.",2020,International Journal on Emerging Technologies,0,@ current innovation era ha changed @ conventional way of life in many area @ information technology ha @ overflowed @ @ enormous quantity of data @ is @ distributed @ moment of consistently by a @ number of user in @ form of online journal feedback review news blog comment micro-blogging website social medium and some more @ due to @ absolute amount of assessment of precious web asset a significant part of @ research is concentrating on @ sentiment analysis @ opinion analysis is @ act of implementing text analysis and natural language processing technique to recognize and extricate abstract data @ content @ news analysis @ @ utilized to plot @ company @ s conduct @ some time and in @ manner yield significant vital bit of knowledge @ company @ sentiment analysis is additionally helpful in web-based life checking to consequently describe @ general inclination @ state of mind of purchaser a reflected in web-based life toward a particular brand @ organization and decide if @ see positively @ negatively @ today @ is @ everyday exercise @ some user to read @ news on @ internet @ person @ viewpoint in general @ encounter a variation a per @ news @ go @ @ news report occasion @ involve feeling good bad neutral @ @ human feeling available in @ textual information @ @ study by @ sentiment analysis @ finding @ sentiment news article involves many challenge a @ requires going @ @ and every word of @ news text article to find @ particular sentiment @ @ @ @ a brief study of sentiment analysis and opinion mining and @ experimental approach of sentiment analysis of news article obtained @ various news website @ research trend @ @ right reserved @ 
399,A Grey Wolf Optimizer for Text Document Clustering,"Text clustering problem (TCP) is a leading process in many key areas such as information retrieval, text mining, and natural language processing. This presents the need for a potent document clustering algorithm that can be used effectively to navigate, summarize, and arrange information to congregate large data sets. This paper encompasses an adaptation of the grey wolf optimizer (GWO) for TCP, referred to as TCP-GWO. The TCP demands a degree of accuracy beyond that which is possible with metaheuristic swarm-based algorithms. The main issue to be addressed is how to split text documents on the basis of GWO into homogeneous clusters that are sufficiently precise and functional. Specifically, TCP-GWO, or referred to as the document clustering algorithm, used the average distance of documents to the cluster centroid (ADDC) as an objective function to repeatedly optimize the distance between the clusters of the documents. The accuracy and efficiency of the proposed TCP-GWO was demonstrated on a sufficiently large number of documents of variable sizes, documents that were randomly selected from a set of six publicly available data sets. Documents of high complexity were also included in the evaluation process to assess the recall detection rate of the document clustering algorithm. The experimental results for a test set of over a part of 1300 documents showed that failure to correctly cluster a document occurred in less than 20% of cases with a recall rate of more than 65% for a highly complex data set. The high F-measure rate and ability to cluster documents in an effective manner are important advances resulting from this research. The proposed TCP-GWO method was compared to the other well-established text clustering methods using randomly selected data sets. Interestingly, TCP-GWO outperforms the comparative methods in terms of precision, recall, and F-measure rates. In a nutshell, the results illustrate that the proposed TCP-GWO is able to excel compared to the other comparative clustering methods in terms of measurement criteria, whereby more than 55% of the documents were correctly clustered with a high level of accuracy. © 2020 Walter de Gruyter GmbH, Berlin/Boston.",2020,Journal of Intelligent Systems,7,text clustering problem @ tcp @ is a leading process in many key area @ a information retrieval text mining and natural language processing @ @ @ @ need @ a potent document clustering algorithm @ @ @ used effectively to navigate summarize and arrange information to congregate @ data set @ @ @ encompasses @ adaptation of @ grey wolf optimizer @ gwo @ @ tcp referred to a tcp-gwo @ @ tcp demand a degree of accuracy beyond @ @ is possible @ metaheuristic swarm-based algorithm @ @ main issue to @ addressed is @ to split text document on @ basis of gwo @ homogeneous cluster @ @ sufficiently precise and functional @ specifically tcp-gwo @ referred to a @ document clustering algorithm used @ average distance of document to @ cluster centroid @ addc @ a @ objective function to repeatedly optimize @ distance @ @ cluster of @ document @ @ accuracy and efficiency of @ proposed tcp-gwo wa demonstrated on a sufficiently @ number of document of variable size document @ @ randomly selected @ a set of six publicly available data set @ document of high complexity @ @ included in @ evaluation process to ass @ recall detection rate of @ document clustering algorithm @ @ experimental @ @ a test set of @ a part of document showed @ failure to correctly cluster a document occurred in le @ of case @ a recall rate of more @ @ a highly complex data set @ @ high f-measure rate and ability to cluster document in @ effective manner @ important advance resulting @ @ research @ @ proposed tcp-gwo method wa compared to @ @ well-established text clustering method @ randomly selected data set @ interestingly tcp-gwo outperforms @ comparative method in term of precision recall and f-measure rate @ in a nutshell @ @ illustrate @ @ proposed tcp-gwo is able to excel compared to @ @ comparative clustering method in term of measurement criterion whereby more @ of @ document @ correctly clustered @ a high level of accuracy @ walter de gruyter gmbh @ boston @ 
400,A semantic taxonomy for weighting assumptions to reduce feature selection from social media and forum posts,"Numerous researchers have worked on the knowledge-based semantics of words to clarify the ambiguity of (https://github.com/alimuttaleb/ Ali-Muttaleb/blob/master/Synonym.txt) synonyms in various natural-language processing fields, such as Wikipedia, websites, and social networks. This paper attempts to clarify ambiguities in the lexical semantics of taxonomy in social media. It proposes a new knowledge-based semantic representation approach that can handle ambiguity and high dimensionality issues in text mining. The proposed approach consists of two main components, namely, a feature-based method for incorporating the relationships between lexical sources and a topic-based reduction method to overcome high dimensionality issues. These components help weight and reduce the relevant features of a concept. The proposed approach captures further lexical semantic similarity between words. It also evaluates the use of (https://wordnet.princeton.edu) WordNet 3.1 in text clustering and constant weighting assumption in the feature-based method used to select concepts/words from social media. To address ambiguity, the semantics of concepts with small feature subset size reduction are represented, and the performance of the semantic similarity measurement is improved. The proposed method evaluates word semantic similarity using the (https://github.com/ alimuttaleb/semantictaxonomy/blob/master/mc30.txt) MC30 dataset in Word-Net and obtains the following results for semantic representation: r = 0.82, p = 0.81, m = 0.81, and nz = 0.96. © Springer Nature Switzerland AG 2020.",2020,Advances in Intelligent Systems and Computing,0,numerous researcher @ worked on @ knowledge-based semantics of word to clarify @ ambiguity of @ http @ github @ com alimuttaleb ali-muttaleb blob master synonym @ txt @ synonym in various natural-language processing field @ a wikipedia website and social network @ @ @ attempt to clarify ambiguity in @ lexical semantics of taxonomy in social medium @ @ proposes a @ knowledge-based semantic representation approach @ @ handle ambiguity and high dimensionality issue in text mining @ @ proposed approach consists of @ main component namely a feature-based method @ incorporating @ relationship @ lexical source and a topic-based reduction method to overcome high dimensionality issue @ @ component help weight and reduce @ relevant feature of a concept @ @ proposed approach capture @ lexical semantic similarity @ word @ @ @ evaluates @ use of @ http @ wordnet @ princeton @ edu @ wordnet @ in text clustering and constant weighting assumption in @ feature-based method used to select concept word @ social medium @ to address ambiguity @ semantics of concept @ small feature subset size reduction @ represented and @ performance of @ semantic similarity measurement is improved @ @ proposed method evaluates word semantic similarity @ @ @ http @ github @ com alimuttaleb semantictaxonomy blob master mc @ txt @ mc dataset in word-net and obtains @ following @ @ semantic representation @ r @ p @ @ @ and nz @ @ @ nature switzerland ag @ 
403,Proposal of the first international workshop on semantic indexing and information retrieval for health from heterogeneous content types and languages (SIIRH),"The application of Information Retrieval (IR) and deep learning strategies to explore the vast amount of rapidly growing health-related content is of utmost importance, but is also particularly challenging, due to the very specialized domain language, and implicit differences in language characteristics depending on the content type. This workshop aims at presenting and discussing current and future directions for IR and machine learning approaches devoted to the retrieval and classification of different types of health-related documents ranging from layman or patient generated texts to highly specialized medical literature or clinical records. It includes a session on the MESINESP shared task, supported by the Spanish National Language Technology plan (Plan TL), in order to address the importance and impact of community evaluation efforts, in particular BioASQ, BioCreative, eHealth CLEF, MEDIQA and TREC, as scenarios for exploring evaluation settings and generate data collections of key importance for promoting the development and comparison of IR resources. Additionally, an open session will address IR technologies for heterogeneous health-related content open to multiple languages with a particular interest in the exploitation of structured controlled vocabularies and entity linking, covering the following topics: multilingual and non-English health-related IR, concept indexing, text categorization, generation of evaluation resources biomedical document IR strategies; scalability, robustness and reproducibility of health IR and text mining resources; use of specialized machine translation and advanced deep learning approaches for improving health related search results; medical Question Answering search tools; retrieval of multilingual health related web-content; and other related topics. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ application of information retrieval @ ir @ and deep learning strategy to explore @ vast amount of rapidly growing health-related content is of utmost importance @ is @ particularly challenging due to @ @ specialized domain language and implicit difference in language characteristic depending on @ content type @ @ workshop aim at presenting and discussing current and future direction @ ir and machine learning approach devoted to @ retrieval and classification of different type of health-related document ranging @ layman @ patient generated text to highly specialized medical literature @ clinical record @ @ includes a session on @ mesinesp shared task supported by @ spanish national language technology plan @ plan tl @ in order to address @ importance and impact of community evaluation effort in particular bioasq biocreative ehealth clef mediqa and trec a scenario @ exploring evaluation setting and generate data collection of key importance @ promoting @ development and comparison of ir resource @ additionally @ open session @ address ir technology @ heterogeneous health-related content open to multiple language @ a particular interest in @ exploitation of structured controlled vocabulary and entity linking covering @ following topic @ multilingual and non-english health-related ir concept indexing text categorization generation of evaluation resource biomedical document ir strategy @ scalability robustness and reproducibility of health ir and text mining resource @ use of specialized machine translation and advanced deep learning approach @ improving health related search @ @ medical question answering search tool @ retrieval of multilingual health related web-content @ and @ related topic @ @ nature switzerland ag @ 
404,An Enhanced Feature Selection for Text Documents,"In the current digital world, a vast amount of data is recorded in a variety of forms like pictures, data, video, and audio. Generally, such type of information which is available in large voluminous form is actually not available in an organized manner which is appropriate for text processing. Text mining is a subfield of data mining which aims at exploring the useful information from the recorded resources. Document clustering helps the users to effectively navigate, review, and classify text documents into significant clusters, the knowledge that helps to handle the enormous amount of text mining. Preprocessing and feature selection are of tremendous importance in document clustering. In document clustering, preprocessing techniques applied to the documents are Bag of Words (BOW), Stop word removal, and Porter stemming. In this paper, we proposed an easy to use framework for preprocessing and Enhanced Term Frequency—Inverse Document Frequency (Enhanced TF–IDF) method for feature selection. © 2020, Springer Nature Singapore Pte Ltd.",2020,"Smart Innovation, Systems and Technologies",1,in @ current digital world a vast amount of data is recorded in a variety of form like picture data video and audio @ generally @ type of information @ is available in @ voluminous form is actually not available in @ organized manner @ is appropriate @ text processing @ text mining is a subfield of data mining @ aim at exploring @ useful information @ @ recorded resource @ document clustering help @ user to effectively navigate review and classify text document @ significant cluster @ knowledge @ help to handle @ enormous amount of text mining @ preprocessing and feature selection @ of tremendous importance in document clustering @ in document clustering preprocessing technique applied to @ document @ bag of word @ bow @ stop word removal and porter stemming @ in @ @ @ proposed @ easy to use framework @ preprocessing and enhanced term frequency inverse document frequency @ enhanced tf idf @ method @ feature selection @ @ nature singapore pte ltd @ 
405,A human-machine language dictionary,"In this paper, we propose a framework for building a human-machine language dictionary. Given a concept/word, an application can extract the definition of the concept from the dictionary, and consequently “understand” its meaning. In the dictionary, a concept is defined through its relations with other concepts. Relations are specified in the machine language. To a certain degree, the proposed dictionary has a resemblance to WordNet, which consists of a set of concepts/words with synonyms being linked to form the net. WordNet plays an important role in text mining, such as sentiment analysis, document classification, text summarization and question answering systems, etc. However, merely providing synonyms is not sufficient. The proposed dictionary provides a definition for each concept. Based on the definition, the application can accurately estimate the distance and similarity between concepts. As a monotonic mapping, the algorithm for estimating distances and similarities is proved to be always convergent. We envisage that the dictionary will become an important tool in all Text Mining disciplines. © 2020 The Authors.",2020,International Journal of Computational Intelligence Systems,0,in @ @ @ propose a framework @ building a human-machine language dictionary @ given a concept word @ application @ extract @ definition of @ concept @ @ dictionary and consequently understand @ meaning @ in @ dictionary a concept is defined @ @ relation @ @ concept @ relation @ specified in @ machine language @ to a certain degree @ proposed dictionary ha a resemblance to wordnet @ consists of a set of concept word @ synonym @ linked to form @ net @ wordnet play @ important role in text mining @ a sentiment analysis document classification text summarization and question answering system etc @ however merely providing synonym is not sufficient @ @ proposed dictionary provides a definition @ @ concept @ based on @ definition @ application @ accurately estimate @ distance and similarity @ concept @ a a monotonic mapping @ algorithm @ estimating distance and similarity is proved to @ always convergent @ @ envisage @ @ dictionary @ become @ important tool in @ text mining discipline @ @ author @ 
406,Recognai'sworking notes for cantemist-ner track,"These working notes describe the two Named Entity Recognition (NER) systems designed by Team Recognai for the CANTEMIST (CANcer TExt Mining Shared Task-tumor named entity recognition) NER track. While the first system tries to maximise the performance with respect to the F1-score, the second system tries to maximise its efficiency with respect to model size and speed while maintaining acceptable performance.,. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,@ working note describe @ @ named entity recognition @ ner @ system designed by team recognai @ @ cantemist @ cancer text mining shared task-tumor named entity recognition @ ner track @ @ @ first system try to maximise @ performance @ respect to @ f score @ second system try to maximise @ efficiency @ respect to model size and speed @ maintaining acceptable performance @ @ @ @ @ @ by @ author @ use permitted @ @ 
407,An Efficient Semantic Document Similarity Calculation Method Based on Double-Relations in Gene Ontology,"Semantic text mining is a challenging research topic in recent years. Many types of research focus on measuring the similarity of two documents with ontologies such as Medical Subject Headings (Mesh) and Gene Ontology (GO). However, most of the researches considered the single relationship in an ontology. To represent the document comprehensively, a semantic document similarity calculation method is proposed, based on utilizing Average Maximum Match algorithm with double-relations in GO. In the experiment, the results show that the double-relations based similarity calculation method is better than traditional semantic similarity measurements. © Springer Nature Singapore Pte Ltd. 2020.",2020,"Smart Innovation, Systems and Technologies",0,semantic text mining is a challenging research topic in recent year @ many type of research focus on measuring @ similarity of @ document @ ontology @ a medical subject heading @ mesh @ and gene ontology @ go @ @ however @ of @ research considered @ single relationship in @ ontology @ to represent @ document comprehensively a semantic document similarity calculation method is proposed based on utilizing average maximum match algorithm @ double-relations in go @ in @ experiment @ @ @ @ @ double-relations based similarity calculation method is better @ traditional semantic similarity measurement @ @ nature singapore pte ltd @ @ 
408,Clinical ner using Spanish bert embeddings,"This paper presents an overview of transfer learning-based approach to the Named Entity Recognition (NER) sub-task from Cancer Text Mining Shared Task (CANTEMIST) conducted as a part of Iberian Languages Evaluation Forum (IberLEF) 2020. We explore the use of Bidirectional Encoder Representations from Transformers (BERT) based contextual embeddings trained on general domain Spanish text to extract tumor morphology from clinical reports written in Spanish. We achieve an F1 score of 73.4% on NER without using any feature engineered or rule-based approaches, and present our work as inspiration for further research on this task. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,@ @ @ @ overview of transfer learning-based approach to @ named entity recognition @ ner @ sub-task @ cancer text mining shared task @ cantemist @ conducted a a part of iberian language evaluation forum @ iberlef @ @ @ explore @ use of bidirectional encoder representation @ transformer @ bert @ based contextual embeddings trained on general domain spanish text to extract tumor morphology @ clinical report written in spanish @ @ achieve @ f score of @ on ner without @ @ feature engineered @ rule-based approach and @ @ work a inspiration @ @ research on @ task @ @ @ @ @ by @ author @ use permitted @ @ 
409,AutoOverview: A framework for generating structured overviews over many documents,"This article is an exposition of a recent study on automatic generation of a structured overview (SOV) over a very large corpus of documents, where an SOV is organized as sections and subsections according to the latent hierarchy of topics contained in the documents. We present a new framework called AutoOverview that includes and extends our previous scheme called NDORGS (best paper runner-up in ACM DocEng’2019) [47]. Different from the standard NLP task of generating a coherent summary typically over a handful of documents, AutoOverview needs to balance between two competitive objectives of accuracy and efficiency over thousands of documents. It incorporates hierarchical topic clustering, single-document summarization, multiple-document summarization, title generation, and other text mining techniques into a single platform. To assess the quality of an SOV generated over many documents, while it is possible to rely on human annotators to judge its readability, the sheer size of the inputs would make it formidable for human judges to determine if an SOV has covered all major points contained in the original texts. To overcome this obstacle, we present a text mining mechanism to evaluate topic coverage of the SOV against the topics contained in the original documents. We use multi-attribute decision making to help determine a suitable suite of algorithms to implement AutoOverview and the values of parameters for achieving a satisfactory SOV with respect to both accuracy and efficiency. We use NDORGS as an implementation example to address these issues and present evaluation results over a corpus of over 2,000 classified news articles and a corpus of over 5,000 unclassified news articles in a span of 10 years obtained from a search of the same keyword. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ article is @ exposition of a recent study on automatic generation of a structured overview @ sov @ @ a @ @ corpus of document @ @ sov is organized a section and subsection according to @ latent hierarchy of topic contained in @ document @ @ @ a @ framework called autooverview @ includes and extends @ previous scheme called ndorgs @ best @ runner-up in acm doceng @ @ different @ @ standard nlp task of generating a coherent summary typically @ a handful of document autooverview need to balance @ @ competitive objective of accuracy and efficiency @ thousand of document @ @ incorporates hierarchical topic clustering single-document summarization multiple-document summarization title generation and @ text mining technique @ a single platform @ to ass @ quality of @ sov generated @ many document @ @ is possible to rely on human annotator to judge @ readability @ sheer size of @ input would make @ formidable @ human judge to determine if @ sov ha covered @ major point contained in @ original text @ to overcome @ obstacle @ @ a text mining mechanism to evaluate topic coverage of @ sov @ @ topic contained in @ original document @ @ use multi-attribute decision making to help determine a suitable suite of algorithm to implement autooverview and @ value of parameter @ achieving a satisfactory sov @ respect to @ accuracy and efficiency @ @ use ndorgs a @ implementation example to address @ issue and @ evaluation @ @ a corpus of @ classified news article and a corpus of @ unclassified news article in a span of year obtained @ a search of @ @ keyword @ @ nature switzerland ag @ 
410,Assessing the impact of OCR errors in information retrieval,"A significant amount of the textual content available on the Web is stored in PDF files. These files are typically converted into plain text before they can be processed by information retrieval or text mining systems. Automatic conversion typically introduces various errors, especially if OCR is needed. In this empirical study, we simulate OCR errors and investigate the impact that misspelled words have on retrieval accuracy. In order to quantify such impact, errors were systematically inserted at varying rates in an initially clean IR collection. Our results showed that significant impacts are noticed starting at a 5% error rate. Furthermore, stemming has proven to make systems more robust to errors. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,a significant amount of @ textual content available on @ web is stored in pdf file @ @ file @ typically converted @ plain text @ @ @ @ processed by information retrieval @ text mining system @ automatic conversion typically introduces various error especially if ocr is needed @ in @ empirical study @ simulate ocr error and investigate @ impact @ misspelled word @ on retrieval accuracy @ in order to quantify @ impact error @ systematically inserted at varying rate in @ initially clean ir collection @ @ @ showed @ significant impact @ noticed starting at a error rate @ furthermore stemming ha proven to make system more robust to error @ @ nature switzerland ag @ 
411,Tumor entity recognition and coding for Spanish electronic health records,"This paper describes a two-stage system to solve tumor entity detection and coding in Spanish health records. This system is submitted to the CANcer TExt Mining Shared Task (CANTEMIST), a challenge in the IberLEF 2020 Workshop. We include a comparison between two kinds of systems to tackle this problem. The first kind employ feature-based Conditional Random Fields (CRF), and the second kind is based on deep learning models. The reported experiments show that our proposals and their combination achieve a micro-F1 of 83.1% and 78.6% on the test data set for the first and second sub-tasks, respectively, and a MAP of 79.7% on the third sub-task. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,@ @ describes a two-stage system to solve tumor entity detection and coding in spanish health record @ @ system is submitted to @ cancer text mining shared task @ cantemist @ a challenge in @ iberlef workshop @ @ include a comparison @ @ kind of system to tackle @ problem @ @ first kind employ feature-based conditional random field @ crf @ and @ second kind is based on deep learning model @ @ reported experiment @ @ @ proposal and @ combination achieve a micro-f of @ and @ on @ test data set @ @ first and second sub-tasks respectively and a map of @ on @ third sub-task @ @ @ @ @ by @ author @ use permitted @ @ 
412,Protein/Gene Entity Recognition and Normalization with Domain Knowledge and Local Context,"Biomedical named entity recognition and normalization aim at recognizing biomedical entity mentions from text and mapping them to their unique database entity identifiers (IDs), which are the primary task of biomedical text mining. However, name variation and entity ambiguity problems make this task challenging. In this paper, we leverage domain knowledge by a novel knowledge feature representation method to recognize more entity variants, and model important local context through a dual attention mechanism and a gating mechanism to perform entity normalization. Experimental results on the BioCreative VI Bio-ID corpus show that our proposed system achieves the new state-of-the-art performance (0.844 F1-score for protein/gene entity recognition and 0.408 F1-score for normalization). © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,biomedical named entity recognition and normalization aim at recognizing biomedical entity mention @ text and mapping @ to @ unique database entity identifier @ id @ @ @ @ primary task of biomedical text mining @ however name variation and entity ambiguity problem make @ task challenging @ in @ @ @ leverage domain knowledge by a novel knowledge feature representation method to recognize more entity variant and model important local context @ a dual attention mechanism and a gating mechanism to perform entity normalization @ experimental @ on @ biocreative vi bio-id corpus @ @ @ proposed system achieves @ @ state-of-the-art performance @ @ f score @ protein gene entity recognition and @ f score @ normalization @ @ @ nature switzerland ag @ 
413,End-to-end neural coder for tumor named entity recognition,"This paper describes E2ENC, the system that we have developed to participate in CANTEMIST (CANcer TExt Mining Shared Task-tumor named entity recognition). E2ENC is a data-driven and end-to-end neural network-based system. It does not rely on external resources such as part-of-speech tagger. It proposes to solve two problems jointly; the first problem is to automatically extract the tumor morphology mentions that can be found in medical documents written in Spanish. The second task is to find the corresponding eCIE-O-3.1 codes for each extracted entity. E2ENC shows promising results, comparing to the baseline system. The reported results show that the proposed system achieve a micro-F1 of 84.9% and 77.7% on the test set for the first and second sub-tasks, respectively, and a MAP of 73.7% on the third sub-task. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,@ @ describes e enc @ system @ @ @ developed to participate in cantemist @ cancer text mining shared task-tumor named entity recognition @ @ e enc is a data-driven and end-to-end neural network-based system @ @ doe not rely on external resource @ a part-of-speech tagger @ @ proposes to solve @ problem jointly @ @ first problem is to automatically extract @ tumor morphology mention @ @ @ found in medical document written in spanish @ @ second task is to find @ corresponding ecie-o @ code @ @ extracted entity @ e enc @ promising @ comparing to @ baseline system @ @ reported @ @ @ @ proposed system achieve a micro-f of @ and @ on @ test set @ @ first and second sub-tasks respectively and a map of @ on @ third sub-task @ @ @ @ @ by @ author @ use permitted @ @ 
416,Identification of cancer entities in clinical text combining transformers with dictionary features,"Clinical NLP tools that automatically extract cancer concepts from unstructured Electronic Health Record (EHR) text can benefit cancer treatment matching, clinical trials cohort identification, and reportable cancer abstraction. We used a combination of two BERT-based [1] language models, BETO [2] and MBERT [1]; with regular expressions constructed from training data; and ICD-O dictionary based features to participate in the tumor named-entity recognition subtask of the 2020 CANTEMIST (CANcer TExt Mining Shared Task) [3]. Our goal is to explore the incorporation of dictionary-based features into these models to provide better integration between machine learning models and external knowledge resources. Results on the test data set were highest with a regular expression based system (F-Score 0.73) and development set results showed a 5 point drop in F-Score (0.76 to 0.71) when integrating dictionary features into our BETO based system. We suggest that dictionary-based features will need careful integration to improve the performance of masked language models. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,clinical nlp tool @ automatically extract cancer concept @ unstructured electronic health record @ ehr @ text @ benefit cancer treatment matching clinical trial cohort identification and reportable cancer abstraction @ @ used a combination of @ bert-based language model beto and mbert @ @ regular expression constructed @ training data @ and icd-o dictionary based feature to participate in @ tumor named-entity recognition subtask of @ cantemist @ cancer text mining shared task @ @ @ goal is to explore @ incorporation of dictionary-based feature @ @ model to provide better integration @ machine learning model and external knowledge resource @ @ on @ test data set @ highest @ a regular expression based system @ f-score @ @ and development set @ showed a point drop in f-score @ @ to @ @ @ integrating dictionary feature @ @ beto based system @ @ suggest @ dictionary-based feature @ need careful integration to improve @ performance of masked language model @ @ @ @ @ by @ author @ use permitted @ @ 
417,Overview of CLEF HIPE 2020: Named Entity Recognition and Linking on Historical Newspapers,"This paper presents an overview of the first edition of HIPE (Identifying Historical People, Places and other Entities), a pioneering shared task dedicated to the evaluation of named entity processing on historical newspapers in French, German and English. Since its introduction some twenty years ago, named entity (NE) processing has become an essential component of virtually any text mining application and has undergone major changes. Recently, two main trends characterise its developments: the adoption of deep learning architectures and the consideration of textual material originating from historical and cultural heritage collections. While the former opens up new opportunities, the latter introduces new challenges with heterogeneous, historical and noisy inputs. In this context, the objective of HIPE, run as part of the CLEF 2020 conference, is threefold: strengthening the robustness of existing approaches on non-standard inputs, enabling performance comparison of NE processing on historical texts, and, in the long run, fostering efficient semantic indexing of historical documents. Tasks, corpora, and results of 13 participating teams are presented. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ @ @ overview of @ first edition of hipe @ identifying historical people place and @ entity @ a pioneering shared task dedicated to @ evaluation of named entity processing on historical newspaper in french german and english @ since @ introduction some twenty year ago named entity @ ne @ processing ha become @ essential component of virtually @ text mining application and ha undergone major change @ recently @ main trend characterise @ development @ @ adoption of deep learning architecture and @ consideration of textual material originating @ historical and cultural heritage collection @ @ @ former open up @ opportunity @ latter introduces @ challenge @ heterogeneous historical and noisy input @ in @ context @ objective of hipe run a part of @ clef conference is threefold @ strengthening @ robustness of existing approach on non-standard input enabling performance comparison of ne processing on historical text and in @ long run fostering efficient semantic indexing of historical document @ task corpus and @ of participating team @ presented @ @ nature switzerland ag @ 
419,Introducing the CLEF 2020 HIPE shared task: Named entity recognition and linking on historical newspapers,"Since its introduction some twenty years ago, named entity (NE) processing has become an essential component of virtually any text mining application and has undergone major changes. Recently, two main trends characterise its developments: the adoption of deep learning architectures and the consideration of textual material originating from historical and cultural heritage collections. While the former opens up new opportunities, the latter introduces new challenges with heterogeneous, historical and noisy inputs. If NE processing tools are increasingly being used in the context of historical documents, performance values are below the ones on contemporary data and are hardly comparable. In this context, this paper introduces the CLEF 2020 Evaluation Lab HIPE (Identifying Historical People, Places and other Entities) on named entity recognition and linking on diachronic historical newspaper material in French, German and English. Our objective is threefold: strengthening the robustness of existing approaches on non-standard inputs, enabling performance comparison of NE processing on historical texts, and, in the long run, fostering efficient semantic indexing of historical documents in order to support scholarship on digital cultural heritage collections. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,since @ introduction some twenty year ago named entity @ ne @ processing ha become @ essential component of virtually @ text mining application and ha undergone major change @ recently @ main trend characterise @ development @ @ adoption of deep learning architecture and @ consideration of textual material originating @ historical and cultural heritage collection @ @ @ former open up @ opportunity @ latter introduces @ challenge @ heterogeneous historical and noisy input @ if ne processing tool @ increasingly @ used in @ context of historical document performance value @ @ @ @ on contemporary data and @ hardly comparable @ in @ context @ @ introduces @ clef evaluation lab hipe @ identifying historical people place and @ entity @ on named entity recognition and linking on diachronic historical newspaper material in french german and english @ @ objective is threefold @ strengthening @ robustness of existing approach on non-standard input enabling performance comparison of ne processing on historical text and in @ long run fostering efficient semantic indexing of historical document in order to support scholarship on digital cultural heritage collection @ @ nature switzerland ag @ 
421,Systematic study on deep learning techniques for prediction of movies,"Deep learning has achieved great success in various fields, such as computer vision and natural language processing. It is effective as it has a strong learning ability and it can extract higher-level features from raw input. In this study, the characteristics of the movies that generate the highest revenue from given datasets have been explored. With the global presence of data in social media, it‟s a challenge to classify the conveyed mindset or feelings of viewers such as good, bad, positive, negative, thumbs up, thumbs down, etc. To overcome the issue of expressed views, sentiment analysis and deep learning techniques can be merged together. Deep learning techniques such as Natural Language Processing, Text Mining, and Learning Vector Quantization etc. allow better feature extraction which also helps the film industry to predict and choose those characteristics that lead to the highest popularity of the movie. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.",2020,Journal of Advanced Research in Dynamical and Control Systems,0,deep learning ha achieved great success in various field @ a computer vision and natural language processing @ @ is effective a @ ha a strong learning ability and @ @ extract higher-level feature @ raw input @ in @ study @ characteristic of @ movie @ generate @ highest revenue @ given datasets @ @ explored @ @ @ global presence of data in social medium @ s a challenge to classify @ conveyed mindset @ feeling of viewer @ a good bad positive negative thumb up thumb down etc @ to overcome @ issue of expressed view sentiment analysis and deep learning technique @ @ merged together @ deep learning technique @ a natural language processing text mining and learning vector quantization etc @ allow better feature extraction @ @ help @ film industry to predict and choose @ characteristic @ lead to @ highest popularity of @ movie @ institute of advanced scientific research inc @ @ @ right reserved @ 
422,Exploring Convolutional Neural Networks and Recurrent Neural Networks for Arabic Question Classification,"Questions classification which consists of assigning a category to each question is a crucial process in question answering systems. Deep neural networks have appeared to be effective in several text mining applications such as text categorization, information retrieval, etc. However, these models have not gained much attention in the field of Arabic question classification. In this paper, we propose an efficient Arabic question classification method based on continuous distributed word representation and deep neural networks. First, we opt for continuous distributed representation to capture syntactic and semantic relations between words. Then, we apply Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) to classify Arabic questions into seven categories (or classes) according to Li and Roth [3] taxonomy. We investigate several variations of CNN and RNN where the output layer depends on two activation functions, the softmax and the sigmoid functions. We carry out several experiments and compare different architectures of CNN and RNN trained on our dataset containing 3173 Arabic labeled questions. The obtained results demonstrate that both deep neural network models are promising as they achieve up to 92% in terms of micro average F1 measure. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,question classification @ consists of assigning a category to @ question is a crucial process in question answering system @ deep neural network @ appeared to @ effective in several text mining application @ a text categorization information retrieval etc @ however @ model @ not gained much attention in @ field of arabic question classification @ in @ @ @ propose @ efficient arabic question classification method based on continuous distributed word representation and deep neural network @ first @ opt @ continuous distributed representation to capture syntactic and semantic relation @ word @ @ @ apply convolutional neural network @ cnn @ and recurrent neural network @ rnn @ to classify arabic question @ seven category @ @ class @ according to li and roth taxonomy @ @ investigate several variation of cnn and rnn @ @ output layer depends on @ activation function @ softmax and @ sigmoid function @ @ carry @ several experiment and compare different architecture of cnn and rnn trained on @ dataset containing arabic labeled question @ @ obtained @ demonstrate @ @ deep neural network model @ promising a @ achieve up to in term of micro average f measure @ @ nature switzerland ag @ 
423,Soulmate: Short-text author linking through Multi-aspect temporal-textual embedding,"Linking authors of short-text contents has important usages in many applications, including Named Entity Recognition (NER) and human community detection. However, certain challenges lie ahead. Firstly, the input short-text contents are noisy, ambiguous, and do not follow the grammatical rules. Secondly, traditional text mining methods fail to effectively extract concepts through words and phrases. Thirdly, the textual contents are temporally skewed, which can affect the semantic understanding by multiple time facets. Finally, using knowledge-bases can make the results biased to the content of the external database and deviate the meaning from the input short text corpus. To overcome these challenges, we devise a neural network-based temporal-textual framework that generates the subgraphs with highly correlated authors from short-text contents. Our approach, on the one hand, computes the relevance score (edge weight) between the authors through considering a portmanteau of contents and concepts, and on the other hand, employs a stack-wise graph cutting algorithm to extract the communities of the related authors. Experimental results show that compared to other knowledge-centered competitors, our multi-aspect vector space model can achieve a higher performance in linking short-text authors. In addition, given the author linking task, the more comprehensive the dataset is, the higher the significance of the extracted concepts will be. © 2020 IEEE Computer Society. All rights reserved.",2020,IEEE Transactions on Knowledge and Data Engineering,0,linking author of short-text content ha important usage in many application including named entity recognition @ ner @ and human community detection @ however certain challenge lie ahead @ firstly @ input short-text content @ noisy ambiguous and @ not follow @ grammatical rule @ secondly traditional text mining method fail to effectively extract concept @ word and phrase @ thirdly @ textual content @ temporally skewed @ @ affect @ semantic understanding by multiple time facet @ finally @ knowledge-bases @ make @ @ biased to @ content of @ external database and deviate @ meaning @ @ input short text corpus @ to overcome @ challenge @ devise a neural network-based temporal-textual framework @ generates @ subgraphs @ highly correlated author @ short-text content @ @ approach on @ @ hand computes @ relevance score @ edge weight @ @ @ author @ considering a portmanteau of content and concept and on @ @ hand employ a stack-wise graph cutting algorithm to extract @ community of @ related author @ experimental @ @ @ compared to @ knowledge-centered competitor @ multi-aspect vector space model @ achieve a higher performance in linking short-text author @ in addition given @ author linking task @ more comprehensive @ dataset is @ higher @ significance of @ extracted concept @ @ @ @ computer society @ @ right reserved @ 
424,How to Detect Novelty in Textual Data Streams? A Comparative Study of Existing Methods,"Since datasets with annotation for novelty at the document and/or word level are not easily available, we present a simulation framework that allows us to create different textual datasets in which we control the way novelty occurs. We also present a benchmark of existing methods for novelty detection in textual data streams. We define a few tasks to solve and compare several state-of-the-art methods. The simulation framework allows us to evaluate their performances according to a set of limited scenarios and test their sensitivity to some parameters. Finally, we experiment with the same methods on different kinds of novelty in the New York Times Annotated Dataset. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,since datasets @ annotation @ novelty at @ document and @ word level @ not easily available @ @ a simulation framework @ allows u to create different textual datasets in @ @ control @ way novelty occurs @ @ @ @ a benchmark of existing method @ novelty detection in textual data stream @ @ define a @ task to solve and compare several state-of-the-art method @ @ simulation framework allows u to evaluate @ performance according to a set of limited scenario and test @ sensitivity to some parameter @ finally @ experiment @ @ @ method on different kind of novelty in @ @ york time annotated dataset @ @ nature switzerland ag @ 
425,"Internet data analysis methodology for cyberterrorism vocabulary detection, combining techniques of big data analytics, NLP and semantic web","This article presents a methodology for the analysis of data on the Internet, combining techniques of Big Data analytics, NLP and semantic web in order to find knowledge about large amounts of information on the web. To test the effectiveness of the proposed method, webpages about cyberterrorism were analyzed as a case study. The procedure implemented a genetic strategy in parallel, which integrates (Crawler to locate and download information from the web; to retrieve the vocabulary, using techniques of NLP (tokenization, stop word, TF, TFIDF), methods of stemming and synonyms). For the pursuit of knowledge was built a dataset through the description of a linguistic corpus with semantic ontologies, considering the characteristics of cyber-terrorism, which was analyzed with the algorithms, Random Forests (parallel), Boosting, SVM, neural network, K-nn and Bayes. The results reveal a percentage of the 95.62% accuracy in the detection of the vocabulary of cyber-terrorism, which were approved through cross validation, reaching 576% time savings with parallel processing. Copyright © 2020, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",2020,International Journal on Semantic Web and Information Systems,0,@ article @ a methodology @ @ analysis of data on @ internet combining technique of big data analytics nlp and semantic web in order to find knowledge @ @ amount of information on @ web @ to test @ effectiveness of @ proposed method webpage @ cyberterrorism @ analyzed a a case study @ @ procedure implemented a genetic strategy in parallel @ integrates @ crawler to locate and download information @ @ web @ to retrieve @ vocabulary @ technique of nlp @ tokenization stop word tf tfidf @ method of stemming and synonym @ @ @ @ pursuit of knowledge wa built a dataset @ @ description of a linguistic corpus @ semantic ontology considering @ characteristic of cyber-terrorism @ wa analyzed @ @ algorithm random forest @ parallel @ boosting svm neural network k-nn and bayes @ @ @ reveal a percentage of @ @ accuracy in @ detection of @ vocabulary of cyber-terrorism @ @ approved @ cross validation reaching time saving @ parallel processing @ @ igi global @ copying @ distributing in print @ electronic form without written permission of igi global is prohibited @ 
426,Identifying journalistically relevant social media texts using human and automatic methodologies,"Social networks have provided the means for constant connectivity and fast information dissemination. In addition, real-time posting allows a new form of citizen journalism, where users can report events from a witness perspective. Therefore, information propagates through the network at a faster pace than traditional media reports it. However, relevant information is a small percentage of all the content shared. Our goal is to develop and evaluate models that can automatically detect journalistic relevance. To do it, we need solid and reliable ground truth data with a significantly large quantity of annotated posts, so that the models can learn to detect relevance over all the spectrum. In this article, we present and confront two different methodologies: an automatic and a human approach. Results on a test data set labelled by experts’ show that the models trained with automatic methodology tend to perform better in contrast to the ones trained using human annotated data. Copyright © 2020 Inderscience Enterprises Ltd.",2020,International Journal of Grid and Utility Computing,0,social network @ provided @ mean @ constant connectivity and fast information dissemination @ in addition real-time posting allows a @ form of citizen journalism @ user @ report event @ a witness perspective @ therefore information propagates @ @ network at a faster pace @ traditional medium report @ @ however relevant information is a small percentage of @ @ content shared @ @ goal is to develop and evaluate model @ @ automatically detect journalistic relevance @ to @ @ @ need solid and reliable ground truth data @ a significantly @ quantity of annotated post @ @ @ model @ learn to detect relevance @ @ @ spectrum @ in @ article @ @ and confront @ different methodology @ @ automatic and a human approach @ @ on a test data set labelled by expert @ @ @ model trained @ automatic methodology tend to perform better in contrast to @ @ trained @ human annotated data @ @ inderscience enterprise ltd @ 
427,Development of Text Data Processing Pipeline for Scientific Systems,"The aim of this work was to develop pipeline processing of scientific texts, including articles and abstracts, for their further categorization, identify patterns and build recommendations to users of scientific systems. The authors proposed a number of methods of pre-processing of texts, the method of cluster and classification analysis of texts, developed a software system of recommendations to users of scientific publications. To solve the problem of data preprocessing it is proposed to use parametrical approach to retrieve new – semantic – feature from textual publications – the type of scientific result. Scientific result type extraction is built just based on user’s need for content having specific property. To solve the problem of users’ profile clustering it is proposed to use ensemble method with distance metric change. For classification, ensemble method based on entropy is used. Evaluation of proposed methods and algorithms employment efficiency was carried out as applied to operation of search module of “Technologies in Education” International Congress of Conferences information system. Author acknowledges support from the MEPhI Academic Excellence Project (Contract No. 02.a03.21.0005). © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,@ aim of @ work wa to develop pipeline processing of scientific text including article and abstract @ @ @ categorization identify pattern and build recommendation to user of scientific system @ @ author proposed a number of method of pre-processing of text @ method of cluster and classification analysis of text developed a software system of recommendation to user of scientific publication @ to solve @ problem of data preprocessing @ is proposed to use parametrical approach to retrieve @ semantic feature @ textual publication @ type of scientific @ @ scientific @ type extraction is built @ based on user s need @ content @ specific property @ to solve @ problem of user profile clustering @ is proposed to use ensemble method @ distance metric change @ @ classification ensemble method based on entropy is used @ evaluation of proposed method and algorithm employment efficiency wa carried @ a applied to operation of search module of technology in education international congress of conference information system @ author acknowledges support @ @ mephi @ excellence project @ contract no @ @ a @ @ @ @ @ nature switzerland ag @ 
432,Lasigebiotm at cantemist: Named entity recognition and normalization of tumour morphology entities and clinical coding of Spanish health-related documents,"The CANTEMIST track included three subtasks for the automatic assignment of codes related with tumour morphology entities to Spanish health-related documents: CANTEMIST-NER, CANTEMISTNORM and CANTEMIST-CODING. For CANTEMIST-NER, we trained Spanish biomedical Flair embeddings on PubMed abstracts and then trained a BiLSTM+CRF Named Entity Recognition tagger on the CANTEMIST corpus using the trained embeddings. For CANTEMIST-NORM,we adapted a graph-based model that uses the Personalized PageRank algorithm to rank the eCIE-O-3.1 candidates for each entity mention. As for CANTEMIST-CODING, we adapted X-Transformer, a state-of-the-art deep learning Extreme Multi-Label Classification algorithm, to classify the clinical cases with a ranked list of eCIEO-3.1 terms in a multilingual and biomedical panorama. The results obtained were a F1-score of 0.749 and 0.069 for the CANTEMIST-NER and the CANTEMIST-NORM subtasks, respectively, and our best scoring submission achieved a MAP score of 0.506 in the CANTEMIST-CODING subtask. © 2020 Copyright for this paper by its authors. Use permitted under.",2020,CEUR Workshop Proceedings,1,@ cantemist track included three subtasks @ @ automatic assignment of code related @ tumour morphology entity to spanish health-related document @ cantemist-ner cantemistnorm and cantemist-coding @ @ cantemist-ner @ trained spanish biomedical flair embeddings on pubmed abstract and @ trained a bilstm crf named entity recognition tagger on @ cantemist corpus @ @ trained embeddings @ @ cantemist-norm @ adapted a graph-based model @ us @ personalized pagerank algorithm to rank @ ecie-o @ candidate @ @ entity mention @ a @ cantemist-coding @ adapted x-transformer a state-of-the-art deep learning extreme multi-label classification algorithm to classify @ clinical case @ a ranked list of ecieo @ term in a multilingual and biomedical panorama @ @ @ obtained @ a f score of @ and @ @ @ cantemist-ner and @ cantemist-norm subtasks respectively and @ best scoring submission achieved a map score of @ in @ cantemist-coding subtask @ @ @ @ @ by @ author @ use permitted @ @ 
433,Using emotion mining to detect real-life problems,"Emotions play an important role in human intelligence and behaviour and are a major vehicle for communication. Emotion mining is a relatively recent task that attempts to identify different emotional categories in text. However, due to its complexity and the limited availability of annotated lexical resources, it is still in the early stages of research. In addition, most of the work and resources have been focus on English texts, but the presence of other languages, such as Spanish, is growing on the Web. Therefore, in this work, we describe a thesis project that will focus on the development of emotion recognition systems in Spanish texts. In addition, we aim to use these systems to solve other relevant tasks, such as, hate speech identification on social media or mental disorders detection. © 2020 CEUR-WS. All rights reserved.",2020,CEUR Workshop Proceedings,0,emotion play @ important role in human intelligence and behaviour and @ a major vehicle @ communication @ emotion mining is a relatively recent task @ attempt to identify different emotional category in text @ however due to @ complexity and @ limited availability of annotated lexical resource @ is still in @ early stage of research @ in addition @ of @ work and resource @ @ focus on english text @ @ presence of @ language @ a spanish is growing on @ web @ therefore in @ work @ describe a thesis project @ @ focus on @ development of emotion recognition system in spanish text @ in addition @ aim to use @ system to solve @ relevant task @ a hate speech identification on social medium @ mental disorder detection @ ceur-ws @ @ right reserved @ 
434,Analysis and multilabel classification of quebec court decisions in the domain of housing law,"The Régie du Logement du Québec (RDL) is a tribunal with exclusive jurisdiction in matters regarding rental leases. Within the framework of the ACT (Autonomy Through Cyberjustice Technologies) project, we processed an original collection of court decisions in French and performed a thorough analysis to reveal biases that may influence prediction experiments. We studied a multilabel classification task that consists in predicting the types of verdict in order to illustrate the importance of prior data analysis. Our best model, based on the FlauBERT language model, achieves F1 score micro averages of 93.7% and 84.9% in Landlord v. Tenant and Tenant v. Landlord cases respectively. However, with the support of our in-depth analysis, we emphasize that these results should be kept in perspective and that some metrics may not be suitable for evaluating systems in sensitive domains such as housing law. © Springer Nature Switzerland AG 2020.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ régie du logement du québec @ rdl @ is a tribunal @ exclusive jurisdiction in matter regarding rental lease @ within @ framework of @ act @ autonomy @ cyberjustice technology @ project @ processed @ original collection of court decision in french and performed a thorough analysis to reveal bias @ may influence prediction experiment @ @ studied a multilabel classification task @ consists in predicting @ type of verdict in order to illustrate @ importance of prior data analysis @ @ best model based on @ flaubert language model achieves f score micro average of @ and @ in landlord v @ tenant and tenant v @ landlord case respectively @ however @ @ support of @ in-depth analysis @ emphasize @ @ @ @ @ kept in perspective and @ some metric may not @ suitable @ evaluating system in sensitive domain @ a housing law @ @ nature switzerland ag @ 
435,A Semi-Supervised Paraphrase Identification Model Based on Multi-Granularity Interaction Reasoning,"Conventional paraphrase identification (PI) models based on deep learning usually focus on text representation and ignore the mining and matching of multi-granular interaction features. In addition, supervised learning relies on a large labeled data. However, labeled training set for PI is small in comparison with the high complexity of the task. To solve the problems, we propose a semi-supervised deep learning framework for PI. We use a neural encoder with word-by-word attention mechanism to reason equivalence or contradiction over pairs of words, phrases and sentences. We employ a two-stage training procedure. First, we use a language modeling objective to learn the initial parameters on the unlabeled corpora of more than one million pairs of sentences. This is followed by a supervised training, where we adapt these parameters to a specific classification task with labeled data. Experimental results on MRPC (Microsoft Research Paraphrase Corpus) and SICK (Sentences Involving Compositional Knowledge) datasets demonstrate the effectiveness of our approach. Compared with the previous neural network models, we achieve absolute improvements in accuracy of 7.6% and F1 of 5.4% on MRPC, Pearson's r of 4.5% and Spearman's\rho of 5.1% on SICK. © 2013 IEEE.",2020,IEEE Access,0,conventional paraphrase identification @ pi @ model based on deep learning usually focus on text representation and ignore @ mining and matching of multi-granular interaction feature @ in addition supervised learning relies on a @ labeled data @ however labeled training set @ pi is small in comparison @ @ high complexity of @ task @ to solve @ problem @ propose a semi-supervised deep learning framework @ pi @ @ use a neural encoder @ word-by-word attention mechanism to reason equivalence @ contradiction @ pair of word phrase and sentence @ @ employ a two-stage training procedure @ first @ use a language modeling objective to learn @ initial parameter on @ unlabeled corpus of more @ @ million pair of sentence @ @ is followed by a supervised training @ @ adapt @ parameter to a specific classification task @ labeled data @ experimental @ on mrpc @ microsoft research paraphrase corpus @ and sick @ sentence involving compositional knowledge @ datasets demonstrate @ effectiveness of @ approach @ compared @ @ previous neural network model @ achieve absolute improvement in accuracy of @ and f of @ on mrpc pearson @ s r of @ and spearman @ s rho of @ on sick @ @ @ 
436,Using Twitter Streams for Opinion Mining: A Case Study on Airport Noise,"This paper proposes a classification model for opinion mining around airport noise based on techniques such as event detection and sentiment analysis applied on Twitter posts. Tweets are retrieved using the Twitter API either because of location or content. A dataset of preprocessed, with NLP techniques, tweets is manually annotated and then used to train an SVM (Support Vector Machine) classifier in order to extract the relevant ones from the obtained collections. The extracted tweets from the SVM classifier are fed to a lexicon-based classifier to filter out the false relevant and to increase precision. A lexicon-based sentiment classifier is then applied in order to separate positive, negative and neutral tweets. The sentiment classifier uses emoticons, polarity of words with subjective intensity, intensifiers, negation effect with dynamic scope, contrast effect and SWN to detect the sentiment of tweets in a hierarchical manner. The information present in the classified tweets is used for a statistical survey-like study. © 2020, Springer Nature Switzerland AG.",2020,Communications in Computer and Information Science,0,@ @ proposes a classification model @ opinion mining around airport noise based on technique @ a event detection and sentiment analysis applied on twitter post @ tweet @ retrieved @ @ twitter api either @ of location @ content @ a dataset of preprocessed @ nlp technique tweet is manually annotated and @ used to train @ svm @ support vector machine @ classifier in order to extract @ relevant @ @ @ obtained collection @ @ extracted tweet @ @ svm classifier @ fed to a lexicon-based classifier to filter @ @ false relevant and to increase precision @ a lexicon-based sentiment classifier is @ applied in order to separate positive negative and neutral tweet @ @ sentiment classifier us emoticon polarity of word @ subjective intensity intensifier negation effect @ dynamic scope contrast effect and swn to detect @ sentiment of tweet in a hierarchical manner @ @ information @ in @ classified tweet is used @ a statistical survey-like study @ @ nature switzerland ag @ 
437,Opinion Mining to Detect Irony in Twitter Messages in Spanish,"Companies, among other sectors, require that the opinions generated on the web be extracted automatically, obtaining their polarity on products or services, to achieve their objectives. Since the opinions are subjective and unstructured, there are still many problems within this field that must be solved. To mention a few, the problem of ambiguity and the support of languages, directly affect in the time to make the right classification of opinions, because most of the tools used in the processing of texts, they only work well with data in English. With the aim of contributing to the solution of both problems and evaluating the real behavior of sentiment analysis for the Spanish language, a system is proposed that allows determining the positive or negative polarity, trying to detect the irony as a problem of ambiguity. For the classification, a supervised learning method was implemented, with the Naive Bayes algorithm. The evaluation of the results of the classification shows that the problem of detecting ironies in Spanish, using the classical techniques of opinion mining, is not completely resolved. However, we believe that these results can be improved by applying some strategies. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,company among @ sector require @ @ opinion generated on @ web @ extracted automatically obtaining @ polarity on product @ service to achieve @ objective @ since @ opinion @ subjective and unstructured @ @ still many problem within @ field @ must @ solved @ to mention a @ @ problem of ambiguity and @ support of language directly affect in @ time to make @ right classification of opinion @ @ of @ tool used in @ processing of text @ only work well @ data in english @ @ @ aim of contributing to @ solution of @ problem and evaluating @ real behavior of sentiment analysis @ @ spanish language a system is proposed @ allows determining @ positive @ negative polarity trying to detect @ irony a a problem of ambiguity @ @ @ classification a supervised learning method wa implemented @ @ naive bayes algorithm @ @ evaluation of @ @ of @ classification @ @ @ problem of detecting irony in spanish @ @ classical technique of opinion mining is not completely resolved @ however @ believe @ @ @ @ @ improved by applying some strategy @ @ nature switzerland ag @ 
441,Statistical localization of bibliographic descriptions in unstructured full-texts documents,"The article describes the results of experiments in the field of automatic localization of bibliographic descriptions (single and group as part of lists) drown up according to GOST 7.0.100-2018 (or close standards). The experiments were performed on the set of unstructured full-text Russian-language documents of various styles. The proposed solution is based on several parameters of bibliographic descriptions: lengths distribution (in characters), the frequency of prescribed punctuation characters and autocorrelation factors. The use of these features in an explicit form during simple classifiers creation made it possible to obtain criteria of Recall and F1-scores, comparable to previously obtained one using structural recognition methods. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2020,CEUR Workshop Proceedings,0,@ article describes @ @ of experiment in @ field of automatic localization of bibliographic description @ single and group a part of list @ drown up according to gost @ @ @ @ close standard @ @ @ experiment @ performed on @ set of unstructured full-text russian-language document of various style @ @ proposed solution is based on several parameter of bibliographic description @ length distribution @ in character @ @ frequency of prescribed punctuation character and autocorrelation factor @ @ use of @ feature in @ explicit form @ simple classifier creation made @ possible to obtain criterion of recall and f score comparable to @ obtained @ @ structural recognition method @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
443,Case Study: Predicting Students Objectivity in Self-evaluation Responses Using Bert Single-Label and Multi-Label Fine-Tuned Deep-Learning Models,"Students’ feedback data regarding teachers, courses, teaching tools, and methods represent valuable information for the education system. The obtained data can contribute in enhancing and improving the education system. Feedback from students is of great essence in the process of extracting hidden knowledge using various techniques for data mining and knowledge discovery. This paper presents various tools and methods for analyzing students’ feedback using Sentiment and Semantic analyses. The essential task in Sentiment analysis is to extract the particular sentiment from textual student responses in terms of negative and positive reactions, while the Semantic analysis contributes to combine textual response in the specific group based on questions. The output produced by the Sentiment and Semantic analyses provides a direct relationship between qualitative and quantitative parts of the evaluation in the form of student comments and grades. © 2020, Springer Nature Switzerland AG.",2020,Communications in Computer and Information Science,0,student feedback data regarding teacher course teaching tool and method represent valuable information @ @ education system @ @ obtained data @ contribute in enhancing and improving @ education system @ feedback @ student is of great essence in @ process of extracting hidden knowledge @ various technique @ data mining and knowledge discovery @ @ @ @ various tool and method @ analyzing student feedback @ sentiment and semantic analysis @ @ essential task in sentiment analysis is to extract @ particular sentiment @ textual student response in term of negative and positive reaction @ @ semantic analysis contributes to combine textual response in @ specific group based on question @ @ output produced by @ sentiment and semantic analysis provides a direct relationship @ qualitative and quantitative part of @ evaluation in @ form of student comment and grade @ @ nature switzerland ag @ 
444,Understanding the language of ISIS: An empirical approach to detect radical content on twitter using machine learning,"The internet, particularly online social networking platforms have revolutionized the way extremist groups are influencing and radicalizing individuals. Recent research reveals that the process initiates by exposing vast audiences to extremist content and then migrating potential victims to confined platforms for intensive radicalization. Consequently, social networks have evolved as a persuasive tool for extremism aiding as recruitment platform and psychological warfare. Thus, recognizing potential radical text or material is vital to restrict the circulation of the extremist chronicle. The aim of this research work is to identify radical text in social media. Our contributions are as follows: (i) A new dataset to be employed in radicalization detection; (ii) In depth analysis of new and previous datasets so that the variation in extremist group narrative could be identified; (iii) An approach to train classifier employing religious features along with radical features to detect radicalization; (iv) Observing the use of violent and bad words in radical, neutral and random groups by employing violent, terrorism and bad words dictionaries. Our research results clearly indicate that incorporating religious text in model training improves the accuracy, precision, recall, and F1-score of the classifiers. Secondly a variation in extremist narrative has been observed implying that usage of new dataset can have substantial effect on classifier performance. In addition to this, violence and bad words are creating a differentiating factor between radical and random users but for neutral (anti-ISIS) group it needs further investigation. © 2021 Tech Science Press. All rights reserved.",2020,"Computers, Materials and Continua",0,@ internet particularly online social networking platform @ revolutionized @ way extremist group @ influencing and radicalizing individual @ recent research reveals @ @ process initiate by exposing vast audience to extremist content and @ migrating potential victim to confined platform @ intensive radicalization @ consequently social network @ evolved a a persuasive tool @ extremism aiding a recruitment platform and psychological warfare @ thus recognizing potential radical text @ material is vital to restrict @ circulation of @ extremist chronicle @ @ aim of @ research work is to identify radical text in social medium @ @ contribution @ a follows @ @ i @ a @ dataset to @ employed in radicalization detection @ @ ii @ in depth analysis of @ and previous datasets @ @ @ variation in extremist group narrative could @ identified @ @ iii @ @ approach to train classifier employing religious feature along @ radical feature to detect radicalization @ @ @ @ observing @ use of violent and bad word in radical neutral and random group by employing violent terrorism and bad word dictionary @ @ research @ clearly indicate @ incorporating religious text in model training improves @ accuracy precision recall and f score of @ classifier @ secondly a variation in extremist narrative ha @ observed implying @ usage of @ dataset @ @ substantial effect on classifier performance @ in addition to @ violence and bad word @ creating a differentiating factor @ radical and random user @ @ neutral @ anti-isis @ group @ need @ investigation @ tech science @ @ @ right reserved @ 
445,Harnessing Artificial Intelligence to Improve the Quality of Answers in Online Question-answering Health Forums,"Quality of answers in health-related community-based question answering (HCQA) forums has been a concern for both users and forum administrators. We conducted a two-phase study to better understand the quality of answers in HCQA forums. First, we employed machine learning to examine the quality of health content. We validated our algorithmic quality ratings by comparing them with those of two physicians. Second, using data from Yahoo! Answers Health section, we examined the effect of the quality of the first answer on the quality of the subsequent answers. Our results suggest that the quality of the subsequent answers is impacted by the quality of the first displayed answer. We further show that the impact of the first displayed answer is larger when the answerers are more familiar with the forum but smaller when the forum provides tips for answering questions. Our study helps HCQA forums to improve the overall quality of answers by 1- creating an algorithmic solution that reliably measures the quality of answers, and 2- adjusting the order of existing answers to encourage higher quality subsequent answers. Our findings also extend the applicability of the order effect to online forums and provide evidence that experienced users would be more influenced by the order effect in such forums. © 2020 Taylor & Francis Group, LLC.",2020,Journal of Management Information Systems,0,quality of answer in health-related community-based question answering @ hcqa @ forum ha @ a concern @ @ user and forum administrator @ @ conducted a two-phase study to better understand @ quality of answer in hcqa forum @ first @ employed machine learning to examine @ quality of health content @ @ validated @ algorithmic quality rating by comparing @ @ @ of @ physician @ second @ data @ yahoo @ answer health section @ examined @ effect of @ quality of @ first answer on @ quality of @ subsequent answer @ @ @ suggest @ @ quality of @ subsequent answer is impacted by @ quality of @ first displayed answer @ @ @ @ @ @ impact of @ first displayed answer is larger @ @ answerer @ more familiar @ @ forum @ smaller @ @ forum provides tip @ answering question @ @ study help hcqa forum to improve @ overall quality of answer by creating @ algorithmic solution @ reliably measure @ quality of answer and adjusting @ order of existing answer to encourage higher quality subsequent answer @ @ finding @ extend @ applicability of @ order effect to online forum and provide evidence @ experienced user would @ more influenced by @ order effect in @ forum @ taylor francis group llc @ 
446,Language model CNN-driven similarity matching and classification for HTML-embedded product data,"The Semantic Web Challenge Mining the Web of HTML- embedded Product Data aims to benchmark current technologies on the data integration tasks (1) product matching and (2) product classification, as recent years have seen significant use of semantic annotations in the e-commerce domain, but often with inconsistencies, no complete coverage or con icting information. We introduce a transformer-based approach for textual product matching and extend it with an CNN for product classification.We compare the in uence of different input feature combinations against prediction performance and introduce a technique to augment the classification task with additional information. We are able to outperform baseline results using text-only approaches. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ semantic web challenge mining @ web of html embedded product data aim to benchmark current technology on @ data integration task @ @ product matching and @ @ product classification a recent year @ seen significant use of semantic annotation in @ e-commerce domain @ often @ inconsistency no complete coverage @ con icting information @ @ introduce a transformer-based approach @ textual product matching and extend @ @ @ cnn @ product classification @ @ compare @ in uence of different input feature combination @ prediction performance and introduce a technique to augment @ classification task @ additional information @ @ @ able to outperform baseline @ @ text-only approach @ @ @ @ @ by @ author @ 
448,Pairwise Causality Structure: Towards Nested Causality Mining on Financial Statements,"Causality mining, which aims to find cause-effect relations in text, is an important yet challenging problem in natural language understanding. The extraction of causal relations is beneficial to practitioners in document-intensive industries. For instance, it enables investors and regulators in financial industries to quickly understand the correlation between events in financial statements. However, this problem is difficult since the expression of causality is diverse, and more importantly, nested. Specifically, causality often has a nested structure, where a pair of cause-effect can be the cause of another higher-level causality. Recent works deal with this problem by a bottom-up relation extraction solution, but it performs worse for relations on higher levels. In this study, we find that the nested causality structure can be transformed into a graph of pairwise causality between sentence segments. Then we propose a two-step solution: first, a segmenter disassembles a sentence into segments by detecting causality connectives; second, a relation classifier predicts whether a pair of segments has cause-effect relation or not. Two modules above are trained jointly in our proposed Causality Detection Network (CDNet). On a large dataset we collect, the precision of our model reaches 92.11% and the recall reaches 93.07% for this task. Compared with the existing state-of-the-art solution, the precision of our model is improved by 3.28% and 3.03% for recall. We also observe that the percentage of exactly correct sentences from prediction is 74.26% without post-processing, indicating the hardness of our problem and space for improvement. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,causality mining @ aim to find cause-effect relation in text is @ important yet challenging problem in natural language understanding @ @ extraction of causal relation is beneficial to practitioner in document-intensive industry @ @ instance @ enables investor and regulator in financial industry to quickly understand @ correlation @ event in financial statement @ however @ problem is difficult since @ expression of causality is diverse and more importantly nested @ specifically causality often ha a nested structure @ a pair of cause-effect @ @ @ cause of another higher-level causality @ recent work deal @ @ problem by a bottom-up relation extraction solution @ @ performs worse @ relation on higher level @ in @ study @ find @ @ nested causality structure @ @ transformed @ a graph of pairwise causality @ sentence segment @ @ @ propose a two-step solution @ first a segmenter disassembles a sentence @ segment by detecting causality connective @ second a relation classifier predicts whether a pair of segment ha cause-effect relation @ not @ @ module @ @ trained jointly in @ proposed causality detection network @ cdnet @ @ on a @ dataset @ collect @ precision of @ model reach @ and @ recall reach @ @ @ task @ compared @ @ existing state-of-the-art solution @ precision of @ model is improved by @ and @ @ recall @ @ @ observe @ @ percentage of exactly correct sentence @ prediction is @ without post-processing indicating @ hardness of @ problem and space @ improvement @ @ nature switzerland ag @ 
449,Overview of ChEMU 2020: Named Entity Recognition and Event Extraction of Chemical Reactions from Patents,"In this paper, we provide an overview of the Cheminformatics Elsevier Melbourne University (ChEMU) evaluation lab 2020, part of the Conference and Labs of the Evaluation Forum 2020 (CLEF2020). The ChEMU evaluation lab focuses on information extraction over chemical reactions from patent texts. Using the ChEMU corpus of 1500 “snippets” (text segments) sampled from 170 patent documents and annotated by chemical experts, we defined two key information extraction tasks. Task 1 addresses chemical named entity recognition, the identification of chemical compounds and their specific roles in chemical reactions. Task 2 focuses on event extraction, the identification of reaction steps, relating the chemical compounds involved in a chemical reaction. Herein, we describe the resources created for these tasks and the evaluation methodology adopted. We also provide a brief summary of the participants of this lab and the results obtained across 46 runs from 11 teams, finding that several submissions achieve substantially better results than our baseline methods. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ provide @ overview of @ cheminformatics @ melbourne university @ chemu @ evaluation lab part of @ conference and lab of @ evaluation forum @ clef @ @ @ chemu evaluation lab focus on information extraction @ chemical reaction @ patent text @ @ @ chemu corpus of snippet @ text segment @ sampled @ patent document and annotated by chemical expert @ defined @ key information extraction task @ task address chemical named entity recognition @ identification of chemical compound and @ specific role in chemical reaction @ task focus on event extraction @ identification of reaction step relating @ chemical compound involved in a chemical reaction @ herein @ describe @ resource created @ @ task and @ evaluation methodology adopted @ @ @ provide a brief summary of @ participant of @ lab and @ @ obtained across run @ team finding @ several submission achieve substantially better @ @ @ baseline method @ @ nature switzerland ag @ 
451,High-Precision Biomedical Relation Extraction for Reducing Human Curation Efforts in Industrial Applications,"The body of biomedical literature is growing at an unprecedented rate, exceeding the ability of researchers to make effective use of this knowledge-rich amount of information. This growth has created interest in biomedical relation extraction approaches to extract domain-specific knowledge for diverse applications. Despite the great progress in the techniques, the retrieved evidence still needs to undergo a time-consuming manual curation process to be truly useful. Most relation extraction systems have been conceived in the context of Shared Tasks, with the goal of maximizing the F1 score on restricted, domain-specific test sets. However, in industrial applications relations typically serve as input to a pipeline of biologically driven analyses; as a result, highly precise extractions are central for cutting down the manual curation effort, thus to translate the research evidence into practice smoothly and reliably. In this paper, we present a highly precise relation extraction system designed to reduce human curation efforts. The engine is made up of sophisticated rules that leverage linguistic aspects of the texts rather than sticking on application-specific training data. As a result, the system could be applied to diverse needs. Experiments on gold-standard corpora show that the system achieves the highest precision compared with previous rule-based, kernel-based, and neural approaches, while maintaining a F1 score comparable or superior to other methods. To show the usefulness of our approach in industrial scenarios, we finally present a case study on the mTOR pathway, showing how it could be applied on a large-scale. © 2013 IEEE.",2020,IEEE Access,0,@ body of biomedical literature is growing at @ unprecedented rate exceeding @ ability of researcher to make effective use of @ knowledge-rich amount of information @ @ growth ha created interest in biomedical relation extraction approach to extract domain-specific knowledge @ diverse application @ despite @ great progress in @ technique @ retrieved evidence still need to undergo a time-consuming manual curation process to @ truly useful @ @ relation extraction system @ @ conceived in @ context of shared task @ @ goal of maximizing @ f score on restricted domain-specific test set @ however in industrial application relation typically serve a input to a pipeline of biologically driven analysis @ a a @ highly precise extraction @ central @ cutting down @ manual curation effort thus to translate @ research evidence @ practice smoothly and reliably @ in @ @ @ @ a highly precise relation extraction system designed to reduce human curation effort @ @ engine is made up of sophisticated rule @ leverage linguistic aspect of @ text rather @ sticking on application-specific training data @ a a @ @ system could @ applied to diverse need @ experiment on gold-standard corpus @ @ @ system achieves @ highest precision compared @ previous rule-based kernel-based and neural approach @ maintaining a f score comparable @ superior to @ method @ to @ @ usefulness of @ approach in industrial scenario @ finally @ a case study on @ mtor pathway showing @ @ could @ applied on a large-scale @ @ @ 
452,Labelling companies referred to in newspaper articles,"There are several domains where establishing links between newspaper articles and companies is useful. In this paper, we will present the first elements of our solution to predict links between a newspaper article written in French and a list of companies identified by their name and activity domain. We base our study on a semi-automatically annotated article corpus and the almost complete list of official French company names. We combine statistical linguistic methods with acronym generation and filtering techniques to propose a global score that predicts a distance between a text and a company. The main objective of the study presented in this paper is the creation of a usual name list for each company in order to improve the labelling of newspaper articles. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2020,CEUR Workshop Proceedings,0,@ @ several domain @ establishing link @ newspaper article and company is useful @ in @ @ @ @ @ @ first element of @ solution to predict link @ a newspaper article written in french and a list of company identified by @ name and activity domain @ @ base @ study on a semi-automatically annotated article corpus and @ almost complete list of official french company name @ @ combine statistical linguistic method @ acronym generation and filtering technique to propose a global score @ predicts a distance @ a text and a company @ @ main objective of @ study presented in @ @ is @ creation of a usual name list @ @ company in order to improve @ labelling of newspaper article @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
454,Multi-Element Hierarchical Attention Capsule Network for Stock Prediction,"Stock prediction is a challenging task concerned by researchers due to its considerable returns. It is difficult because of the high randomness in the stock market. Stock price movement is mainly related to the capital situation and hot events. In recent years, researchers improved prediction accuracy with news and social media. However, the existing methods do not take into account the different influences of events. To solve this problem, we propose a multi-element hierarchical attention capsule network, which consists of two components. The former component, multi-element hierarchical attention, quantifies the importance of valuable information contained in multiple news and social media through its weights assignment process. And the latter component, capsule network, learns more context information from the events through its vector representation in the hidden layer. Moreover, we construct a combined data set to maintain the complementarity between social media and news. Finally, we achieve better results than baselines, and experiments show that our model improves prediction accuracy by quantifying the different influences of events. © 2013 IEEE.",2020,IEEE Access,2,stock prediction is a challenging task concerned by researcher due to @ considerable return @ @ is difficult @ of @ high randomness in @ stock market @ stock price movement is mainly related to @ capital situation and hot event @ in recent year researcher improved prediction accuracy @ news and social medium @ however @ existing method @ not take @ account @ different influence of event @ to solve @ problem @ propose a multi-element hierarchical attention capsule network @ consists of @ component @ @ former component multi-element hierarchical attention quantifies @ importance of valuable information contained in multiple news and social medium @ @ weight assignment process @ and @ latter component capsule network learns more context information @ @ event @ @ vector representation in @ hidden layer @ moreover @ construct a combined data set to maintain @ complementarity @ social medium and news @ finally @ achieve better @ @ baseline and experiment @ @ @ model improves prediction accuracy by quantifying @ different influence of event @ @ @ 
455,Improving sentiment analysis using hybrid deep learning model,"Background: Sentiment analysis is a contextual mining of text which determines viewpoint of users with respect to some sentimental topics commonly present at social networking websites. Twitter is one of the social sites where people express their opinion about any topic in the form of tweets. These tweets can be examined using various sentiment classification methods to find the opinion of users. Traditional sentiment analysis methods use manually extracted features for opinion classification. The manual feature extraction process is a complicated task since it requires predefined sentiment lexicons. On the other hand, deep learning methods automatically extract relevant features from data hence; they provide better performance and richer representation competency than the traditional methods. Objective: The main aim of this paper is to enhance the sentiment classification accuracy and to reduce the computational cost. Method: To achieve the objective, a hybrid deep learning model, based on convolution neural network and bi-directional long-short term memory neural network has been introduced. Results: The proposed sentiment classification method achieves the highest accuracy for the most of the datasets. Further, from the statistical analysis efficacy of the proposed method has been validated. Conclusion: Sentiment classification accuracy can be improved by creating veracious hybrid models. Moreover, performance can also be enhanced by tuning the hyper parameters of deep leaning models. © 2020 Bentham Science Publishers.",2020,Recent Advances in Computer Science and Communications,0,background @ sentiment analysis is a contextual mining of text @ determines viewpoint of user @ respect to some sentimental topic commonly @ at social networking website @ twitter is @ of @ social site @ people express @ opinion @ @ topic in @ form of tweet @ @ tweet @ @ examined @ various sentiment classification method to find @ opinion of user @ traditional sentiment analysis method use manually extracted feature @ opinion classification @ @ manual feature extraction process is a complicated task since @ requires predefined sentiment lexicon @ on @ @ hand deep learning method automatically extract relevant feature @ data hence @ @ provide better performance and richer representation competency @ @ traditional method @ objective @ @ main aim of @ @ is to enhance @ sentiment classification accuracy and to reduce @ computational cost @ method @ to achieve @ objective a hybrid deep learning model based on convolution neural network and bi-directional long-short term memory neural network ha @ introduced @ @ @ @ proposed sentiment classification method achieves @ highest accuracy @ @ @ of @ datasets @ @ @ @ statistical analysis efficacy of @ proposed method ha @ validated @ conclusion @ sentiment classification accuracy @ @ improved by creating veracious hybrid model @ moreover performance @ @ @ enhanced by tuning @ hyper parameter of deep leaning model @ bentham science publisher @ 
456,An Infoveillance System for Detecting and Tracking Relevant Topics from Italian Tweets during the COVID-19 Event,"The year 2020 opened with a dramatic epidemic caused by a new species of coronavirus that soon has been declared a pandemic by the WHO due to the high number of deaths and the critical mass of worldwide hospitalized patients, of order of millions. The COVID-19 pandemic has forced the governments of hundreds of countries to apply several heavy restrictions in the citizens' socio-economic life. Italy was one of the most affected countries with long-term restrictions, impacting the socio-economic tissue. During this lockdown period, people got informed mostly on Online Social Media, where a heated debate followed all main ongoing events. In this scenario, the following study presents an in-depth analysis of the main emergent topics discussed during the lockdown phase within the Italian Twitter community. The analysis has been conducted through a general purpose methodological framework, grounded on a biological metaphor and on a chain of NLP and graph analysis techniques, in charge of detecting and tracking emerging topics in Online Social Media, e.g. streams of Twitter data. A term-frequency analysis in subsequent time slots is pipelined with nutrition and energy metrics for computing hot terms by also exploiting the tweets quality information, such as the social influence of the users. Finally, a co-occurrence analysis is adopted for building a topic graph where emerging topics are suitably selected. We demonstrate via a careful parameter setting the effectiveness of the topic tracking system, tailored to the current Twitter standard API restrictions, in capturing the main sociopolitical events that occurred during this dramatic phase. © 2013 IEEE.",2020,IEEE Access,3,@ year opened @ a dramatic epidemic caused by a @ specie of coronavirus @ soon ha @ declared a pandemic by @ @ due to @ high number of death and @ critical mass of worldwide hospitalized patient of order of million @ @ covid pandemic ha forced @ government of hundred of country to apply several heavy restriction in @ citizen @ socio-economic life @ italy wa @ of @ @ affected country @ long-term restriction impacting @ socio-economic tissue @ @ @ lockdown period people got informed mostly on online social medium @ a heated debate followed @ main ongoing event @ in @ scenario @ following study @ @ in-depth analysis of @ main emergent topic discussed @ @ lockdown phase within @ italian twitter community @ @ analysis ha @ conducted @ a general purpose methodological framework grounded on a biological metaphor and on a chain of nlp and graph analysis technique in charge of detecting and tracking emerging topic in online social medium e @ g @ stream of twitter data @ a term-frequency analysis in subsequent time slot is pipelined @ nutrition and energy metric @ computing hot term by @ exploiting @ tweet quality information @ a @ social influence of @ user @ finally a co-occurrence analysis is adopted @ building a topic graph @ emerging topic @ suitably selected @ @ demonstrate via a careful parameter setting @ effectiveness of @ topic tracking system tailored to @ current twitter standard api restriction in capturing @ main sociopolitical event @ occurred @ @ dramatic phase @ @ @ 
457,Improvement of Automatic Extraction of Inventive Information with Patent Claims Structure Recognition,"Our recent research finding produces methods for automatic extraction of inventive information out of patents thanks to the use NLP; notably the automatic text processing. However, these methods have drawbacks due to a high amount of noise (duplicates, errors) in the output result that prevent the further use of TRIZ methodology. In the mean-time, we observed that patent claims are the most important source for inventive information. These text paragraphs have nevertheless a dual nature (combining legal and technical vocabulary) and this nature engender part of the observed noise. We postulate that taking into consideration claims hierarchical structure and its structural information can reduce the time for extraction and refine the final output quality, which is the principal aim of the paper. In this paper, we report on the methodology we have employed based on the patent claim structure recognition as a way to address our objectives. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,@ recent research finding produce method @ automatic extraction of inventive information @ of patent thanks to @ use nlp @ notably @ automatic text processing @ however @ method @ drawback due to a high amount of noise @ duplicate error @ in @ output @ @ prevent @ @ use of triz methodology @ in @ mean-time @ observed @ patent claim @ @ @ important source @ inventive information @ @ text paragraph @ nevertheless a dual nature @ combining legal and technical vocabulary @ and @ nature engender part of @ observed noise @ @ postulate @ taking @ consideration claim hierarchical structure and @ structural information @ reduce @ time @ extraction and refine @ final output quality @ is @ principal aim of @ @ @ in @ @ @ report on @ methodology @ @ employed based on @ patent claim structure recognition a a way to address @ objective @ @ nature switzerland ag @ 
458,Ranking georeferences for efficient crowdsourcing of toponym annotations in a historical corpus of alpine texts,"This paper presents a simple method to rank georeference candidates to optimally support the workflow of a citizen science web application for toponym annotation in historical texts. We implement the general idea of efficient crowdsourcing based on human and artificial intelligence working hand in hand. For named entity recognition, we apply recent neural pretraining-based NER tagger methods. For named entity linking to geographical knowledge bases, we report on georeference ranking experiments testing the hypothesis that textual proximity indicates geographic proximity. Simulation results with online reranking that immediately integrates user verification show further improvements. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ @ a simple method to rank georeference candidate to optimally support @ workflow of a citizen science web application @ toponym annotation in historical text @ @ implement @ general idea of efficient crowdsourcing based on human and artificial intelligence working hand in hand @ @ named entity recognition @ apply recent neural pretraining-based ner tagger method @ @ named entity linking to geographical knowledge base @ report on georeference ranking experiment testing @ hypothesis @ textual proximity indicates geographic proximity @ simulation @ @ online reranking @ immediately integrates user verification @ @ improvement @ @ @ @ @ by @ author @ 
459,UPB at germeval-2020 task 3: Assessing summaries for German texts using BERTScore and Sentence-BERT,"The overwhelming amount of online text information available today has increased the need for more research on its automatic summarization. In this work, we describe our participation in GermEval-2020, Task 3: German Text Summarization. We compare two BERT-based metrics, Sentence-BERT and BERTScore, to automatically evaluate the quality of summaries in the German language. Our lowest error rate achieved was 31.9925, ranking us in 4th place out of 6 participating teams. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ overwhelming amount of online text information available today ha increased @ need @ more research on @ automatic summarization @ in @ work @ describe @ participation in germeval task @ german text summarization @ @ compare @ bert-based metric sentence-bert and bertscore to automatically evaluate @ quality of summary in @ german language @ @ lowest error rate achieved wa @ ranking u in th place @ of participating team @ @ @ @ @ by @ author @ 
460,Cultural differences in bias? Origin and gender bias in pre-trained German and French word embeddings,"Smart applications often rely on training data in form of text. If there is a bias in that training data, the decision of the applications might not be fair. Common training data has been shown to be biased towards different groups of minorities. However, there is no generic algorithm to determine the fairness of training data. One existing approach is to measure gender bias using word embeddings. Most research in this field has been dedicated to the English language. In this work, we identified that there is a bias towards gender and origin in both German and French word embeddings. In particular, we found that real-world bias and stereotypes from the 18th century are still included in today's word embeddings. Furthermore, we show that the gender bias in German has a different form from English and there is indication that bias has cultural differences that need to be considered when analyzing texts and word embeddings in different languages. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,smart application often rely on training data in form of text @ if @ is a bias in @ training data @ decision of @ application might not @ fair @ common training data ha @ @ to @ biased towards different group of minority @ however @ is no generic algorithm to determine @ fairness of training data @ @ existing approach is to measure gender bias @ word embeddings @ @ research in @ field ha @ dedicated to @ english language @ in @ work @ identified @ @ is a bias towards gender and origin in @ german and french word embeddings @ in particular @ found @ real-world bias and stereotype @ @ th century @ still included in today @ s word embeddings @ furthermore @ @ @ @ gender bias in german ha a different form @ english and @ is indication @ bias ha cultural difference @ need to @ considered @ analyzing text and word embeddings in different language @ @ @ @ @ by @ author @ 
461,ZHAW-InIT at germeval 2020 task 4: Low-resource speech-to-text,"This paper presents the contribution of ZHAW-InIT to Task 4”Low-Resource STT” at GermEval 2020. The goal of the task is to develop a system for translating Swiss German dialect speech into Standard German text in the domain of parliamentary debates. Our approach is based on Jasper, a CNN Acoustic Model, which we fine-tune on the task data. We enhance the base system with an extended Language Model containing in-domain data and speed perturbation and run further experiments with post-processing. Our submission achieved first place with a final Word Error Rate of 40.29%. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ @ @ contribution of zhaw-init to task low-resource stt at germeval @ @ goal of @ task is to develop a system @ translating swiss german dialect speech @ standard german text in @ domain of parliamentary debate @ @ approach is based on jasper a cnn acoustic model @ @ fine-tune on @ task data @ @ enhance @ base system @ @ extended language model containing in-domain data and speed perturbation and run @ experiment @ post-processing @ @ submission achieved first place @ a final word error rate of @ @ @ @ @ @ by @ author @ 
462,Germeval 2020 task 4: Low-resource speech-to-text,"We present the results and findings of GermEval 2020 Task 4 on Low-Resource Speech-to-Text. Participants were asked to build a system translating Swiss German speech to Standard German text and minimize its word error rate. The task was based on a new dataset for Swiss German to Standard German speech translation, which contains 74 hours of sentence-level speech-text-pairs. 3 teams participated, with the winning contribution reaching a word error rate of 40.29 %. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ @ @ and finding of germeval task on low-resource speech-to-text @ participant @ asked to build a system translating swiss german speech to standard german text and minimize @ word error rate @ @ task wa based on a @ dataset @ swiss german to standard german speech translation @ contains hour of sentence-level speech-text-pairs @ team participated @ @ winning contribution reaching a word error rate of @ @ @ @ @ @ by @ author @ 
463,Detecting noisy swiss German web text using RNN- And rule-based techniques,"This paper presents the system we submitted to the Swiss German language detection shared task, part of the GermEval 2020 Campaign, held at the SwissText & KONVENS 2020 conference. The goal of the task is to identify if a given text snippet is written in Swiss German. Our approach includes a reformulation of a binary to a multi-way classification problem, a character filter, a neural RNN-based classifier, and the addition of synthetic noise to the training set. The official evaluation of our submitted system results in an F1 score of 96.8%, achieving the second place in this shared task. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ @ @ system @ submitted to @ swiss german language detection shared task part of @ germeval campaign held at @ swisstext konvens conference @ @ goal of @ task is to identify if a given text snippet is written in swiss german @ @ approach includes a reformulation of a binary to a multi-way classification problem a character filter a neural rnn-based classifier and @ addition of synthetic noise to @ training set @ @ official evaluation of @ submitted system @ in @ f score of @ achieving @ second place in @ shared task @ @ @ @ @ by @ author @ 
464,Compiling a large Swiss German dialect corpus,"The Swiss German Dialect Corpus (Schweizer Mundartkorpus CHMK) is an initiative launched by the Swiss German dictionary Schweizerisches Idiotikon. It is an unbalanced, opportunistic corpus and the largest dialect corpus for Swiss German to date. The corpus will be accessible through a query engine and, in part, as an open-source XML corpus. In this paper we provide an overview of the concept, workflow, and challenges of compiling a corpus for a non-standard linguistic variety. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ swiss german dialect corpus @ schweizer mundartkorpus chmk @ is @ initiative launched by @ swiss german dictionary schweizerisches idiotikon @ @ is @ unbalanced opportunistic corpus and @ largest dialect corpus @ swiss german to date @ @ corpus @ @ accessible @ a query engine and in part a @ open-source xml corpus @ in @ @ @ provide @ overview of @ concept workflow and challenge of compiling a corpus @ a non-standard linguistic variety @ @ @ @ @ by @ author @ 
465,X-stance: A multilingual multi-target dataset for stance detection,"We extract a large-scale stance detection dataset from comments written by candidates of elections in Switzerland. The dataset consists of German, French and Italian text, allowing for a cross-lingual evaluation of stance detection. It contains 67 000 comments on more than 150 political issues (targets). Unlike stance detection models that have specific target issues, we use the dataset to train a single model on all the issues. To make learning across targets possible, we prepend to each instance a natural question that represents the target (e.g. “Do you support X?”). Baseline results from multilingual BERT show that zero-shot cross-lingual and cross-target transfer of stance detection is moderately successful with this approach. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ extract a large-scale stance detection dataset @ comment written by candidate of election in switzerland @ @ dataset consists of german french and italian text allowing @ a cross-lingual evaluation of stance detection @ @ contains comment on more @ political issue @ target @ @ unlike stance detection model @ @ specific target issue @ use @ dataset to train a single model on @ @ issue @ to make learning across target possible @ prepend to @ instance a natural question @ represents @ target @ e @ g @ @ @ support x @ @ @ baseline @ @ multilingual bert @ @ zero-shot cross-lingual and cross-target transfer of stance detection is moderately successful @ @ approach @ @ @ @ @ by @ author @ 
466,Harmonization sometimes harms,"In this paper we argue that harmonization is not the preferred way to produce a gold standard in all cases. Neither does a majority vote based harmonization produce an appropriate gold standard centroid, nor would a mere centroid be a good basis for training a system that reproduces prototypical user reactions given some understanding task. We discuss these claims in the context of sentiment inference. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,1,in @ @ @ argue @ harmonization is not @ preferred way to produce a gold standard in @ case @ neither doe a majority vote based harmonization produce @ appropriate gold standard centroid @ would a mere centroid @ a good basis @ training a system @ reproduces prototypical user reaction given some understanding task @ @ discus @ claim in @ context of sentiment inference @ @ @ @ @ by @ author @ 
467,Overview of the germeval 2020 shared task on Swiss German language identification,"In this paper, we present the findings of the Shared Task on Swiss German Language Identification organised as part of the 7th edition of GermEval, co-located with SwissText and KONVENS 2020. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,in @ @ @ @ @ finding of @ shared task on swiss german language identification organised a part of @ th edition of germeval co-located @ swisstext and konvens @ @ @ @ @ by @ author @ 
468,Spoken dialect identification in twitter using a multi-filter architecture,"This paper presents our approach for SwissText & KONVENS 2020 shared task 2, which is a multi-stage neural model for Swiss German (GSW) identification on Twitter. Our model outputs either GSW or non-GSW and is not meant to be used as a generic language identifier. Our architecture consists of two independent filters where the first one favors recall, and the second one filter favors precision (both towards GSW). Moreover, we do not use binary models (GSW vs. not-GSW) in our filters but rather a multi-class classifier with GSW being one of the possible labels. Our model reaches F1-score of 0.982 on the test set of the shared task. Copyright© 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ @ @ approach @ swisstext konvens shared task @ is a multi-stage neural model @ swiss german @ gsw @ identification on twitter @ @ model output either gsw @ non-gsw and is not meant to @ used a a generic language identifier @ @ architecture consists of @ independent filter @ @ first @ favor recall and @ second @ filter favor precision @ @ towards gsw @ @ moreover @ @ not use binary model @ gsw v @ not-gsw @ in @ filter @ rather a multi-class classifier @ gsw @ @ of @ possible label @ @ model reach f score of @ on @ test set of @ shared task @ @ @ @ @ by @ author @ 
469,Idiap submission to Swiss-German language detection shared task,"Language detection is a key part of the NLP pipeline for text processing. The task of automatically detecting languages belonging to disjoint groups is relatively easy. It is considerably challenging to detect languages that have similar origins or dialects. This paper describes Idiap's submission to the 2020 Germeval evaluation campaign1 on Swiss-German language detection. In this work, we have given high dimensional features generated from the text data as input to a supervised autoencoder for detecting languages with dialect variances. Bayesian optimizer was used to fine-tune the hyper-parameters of the supervised autoencoder. To the best of our knowledge, we are first to apply supervised autoencoder for the language detection task. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,language detection is a key part of @ nlp pipeline @ text processing @ @ task of automatically detecting language belonging to disjoint group is relatively easy @ @ is considerably challenging to detect language @ @ similar origin @ dialect @ @ @ describes idiap @ s submission to @ germeval evaluation campaign on swiss-german language detection @ in @ work @ @ given high dimensional feature generated @ @ text data a input to a supervised autoencoder @ detecting language @ dialect variance @ bayesian optimizer wa used to fine-tune @ hyper-parameters of @ supervised autoencoder @ to @ best of @ knowledge @ @ first to apply supervised autoencoder @ @ language detection task @ @ @ @ @ by @ author @ 
470,Psychological distance in German and english brand language of eight international brands,"Language offers additional insights to sentiment and content. The same content can be described with psychologically close or distant language. According to the Construal-Level Theory (Trope & Liberman, 2010), psychological distance influences decision-making. Seven of the eight examined brands psychologically approach customers with their English brand language but psychologically distance themselves from customers with their German brand language on Twitter. Only one brand shows no psychological distance difference between their English and German brand language on Twitter. Implications on decision-making and brand positioning are discussed. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,language offer additional insight to sentiment and content @ @ @ content @ @ described @ psychologically close @ distant language @ according to @ construal-level theory @ trope liberman @ psychological distance influence decision-making @ seven of @ eight examined brand psychologically approach customer @ @ english brand language @ psychologically distance @ @ customer @ @ german brand language on twitter @ only @ brand @ no psychological distance difference @ @ english and german brand language on twitter @ implication on decision-making and brand positioning @ discussed @ @ @ @ @ by @ author @ 
471,Hybrid ensemble predictor as quality metric for German text summarization: Fraunhofer IAIS at germeval 2020 task 3,"We propose an alternative quality metric to evaluate automatically generated texts based on an ensemble of different scores, combining simple rule-based metrics with more complex models of very different nature, including ROUGE, tf-idf, neural sentence embeddings, and a matrix factorization method. Our approach achieved one of the top scores on the second German Text Summarization Challenge. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ propose @ alternative quality metric to evaluate automatically generated text based on @ ensemble of different score combining simple rule-based metric @ more complex model of @ different nature including rouge tf-idf neural sentence embeddings and a matrix factorization method @ @ approach achieved @ of @ top score on @ second german text summarization challenge @ @ @ @ @ by @ author @ 
472,Predicting the concreteness of German words,"Concreteness of words has been measured and used in psycholinguistics already for decades. Recently, it is also used in retrieval and NLP tasks. For English a number of well known datasets has been established with average values for perceived concreteness. We give an overview of available datasets for German, their correlation and evaluate prediction algorithms for concreteness of German words. We show that these algorithms achieve similar results as for English datasets. Moreover, we show for all datasets there are no significant differences between a prediction model based on a regression model using word embeddings as features and a prediction algorithm based on word similarity according to the same embeddings. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,concreteness of word ha @ measured and used in psycholinguistics already @ decade @ recently @ is @ used in retrieval and nlp task @ @ english a number of well known datasets ha @ established @ average value @ perceived concreteness @ @ give @ overview of available datasets @ german @ correlation and evaluate prediction algorithm @ concreteness of german word @ @ @ @ @ algorithm achieve similar @ a @ english datasets @ moreover @ @ @ @ datasets @ @ no significant difference @ a prediction model based on a regression model @ word embeddings a feature and a prediction algorithm based on word similarity according to @ @ embeddings @ @ @ @ @ by @ author @ 
473,Cross-lingual transfer-learning approach to negation scope resolution,"Detecting instances of negation in text is crucially important for several applications, yet it is often neglected. Several decades of research in automated negation detection have not yet provided a reliable solution, especially in a multilingual context. Negation scope resolution poses particular challenges since identifying the scope of influence of a negation cue in a sentence requires a deeper level of natural language understanding. Little work has been done on negation scope resolution in languages other than English. Meanwhile, transfer learning is in wide use and large multilingual models are available to the public. This paper explores the feasibility of a cross-lingual transfer-learning approach to negation scope resolution. Preliminary experiments with the Multilingual BERT model and data in English, French, and Spanish show solid results with the highest F1-score 84.73 on zero-shot transfer between English and French. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,detecting instance of negation in text is crucially important @ several application yet @ is often neglected @ several decade of research in automated negation detection @ not yet provided a reliable solution especially in a multilingual context @ negation scope resolution pose particular challenge since identifying @ scope of influence of a negation cue in a sentence requires a deeper level of natural language understanding @ little work ha @ done on negation scope resolution in language @ @ english @ meanwhile transfer learning is in wide use and @ multilingual model @ available to @ public @ @ @ explores @ feasibility of a cross-lingual transfer-learning approach to negation scope resolution @ preliminary experiment @ @ multilingual bert model and data in english french and spanish @ solid @ @ @ highest f score @ on zero-shot transfer @ english and french @ @ @ @ @ by @ author @ 
474,"To BERT or not to BERT - Comparing contextual embeddings in a deep learning architecture for the automatic recognition of four types of speech, thought and writing representation","We present recognizers for four very different types of speech, thought and writing representation (STWR) for German texts. The implementation is based on deep learning with two different customized contextual embeddings, namely FLAIR embeddings and BERT embeddings. This paper gives an evaluation of our recognizers with a particular focus on the differences in performance we observed between those two embeddings. FLAIR performed best for direct STWR (F1=0.85), BERT for indirect (F1=0.76) and free indirect (F1=0.59) STWR. For reported STWR, the comparison was inconclusive, but BERT gave the best average results and best individual model (F1=0.60). Our best recognizers, our customized language embeddings and most of our test and training data are freely available and can be found via www.redewiedergabe.de or at github.com/redewiedergabe. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ @ recognizers @ four @ different type of speech thought and writing representation @ stwr @ @ german text @ @ implementation is based on deep learning @ @ different customized contextual embeddings namely flair embeddings and bert embeddings @ @ @ give @ evaluation of @ recognizers @ a particular focus on @ difference in performance @ observed @ @ @ embeddings @ flair performed best @ direct stwr @ f @ @ bert @ indirect @ f @ @ and free indirect @ f @ @ stwr @ @ reported stwr @ comparison wa inconclusive @ bert gave @ best average @ and best individual model @ f @ @ @ @ best recognizers @ customized language embeddings and @ of @ test and training data @ freely available and @ @ found via www @ redewiedergabe @ de @ at github @ com redewiedergabe @ @ @ @ @ by @ author @ 
475,On the comparability of pre-trained language models,"Recent developments in unsupervised representation learning have successfully established the concept of transfer learning in NLP. Instead of simply plugging in static pre-trained representations, end-to-end trainable model architectures are making better use of contextual information through more intelligently designed language modelling objectives. Along with this, larger corpora are used for self-supervised pre-training of models which are afterwards fine-tuned on supervised tasks. Advances in parallel computing made it possible to train these models with growing capacities in the same or even in shorter time than previously established models. These developments agglomerate in new state-of-the-art results being revealed in an increasing frequency. Nevertheless, we show that it is not possible to completely disentangle the contributions of the three driving forces to these improvements. We provide a concise overview on several large pre-trained language models, which achieved state-of-the-art results on different leaderboards in the last two years, and compare them with respect to their use of new architectures and resources. We clarify where the differences between the models are and attempt to gain some insight into the single contributions of lexical and computational improvements as well as those of architectural changes. We do not intend to quantify these contributions, but rather see our work as an overview in order to identify potential starting points for benchmark comparisons. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,recent development in unsupervised representation learning @ successfully established @ concept of transfer learning in nlp @ instead of simply plugging in static pre-trained representation end-to-end trainable model architecture @ making better use of contextual information @ more intelligently designed language modelling objective @ along @ @ larger corpus @ used @ self-supervised pre-training of model @ @ afterwards fine-tuned on supervised task @ advance in parallel computing made @ possible to train @ model @ growing capacity in @ @ @ even in shorter time @ @ established model @ @ development agglomerate in @ state-of-the-art @ @ revealed in @ increasing frequency @ nevertheless @ @ @ @ is not possible to completely disentangle @ contribution of @ three driving force to @ improvement @ @ provide a concise overview on several @ pre-trained language model @ achieved state-of-the-art @ on different leaderboards in @ last @ year and compare @ @ respect to @ use of @ architecture and resource @ @ clarify @ @ difference @ @ model @ and attempt to gain some insight @ @ single contribution of lexical and computational improvement a well a @ of architectural change @ @ @ not intend to quantify @ contribution @ rather see @ work a @ overview in order to identify potential starting point @ benchmark comparison @ @ @ @ @ by @ author @ 
476,UZH TILT: A kaldi recipe for Swiss German speech to standard German text,"Swiss German Speech-to-Text (STT) is a challenging task due to the fact that no single-dominant pronunciation or standardised orthography exists. This is compounded by a severe lack of appropriate training data. One potential avenue, and that which is investigated as part of the GermEval 2020 Task 4 on Low-Resource Speech-to-Text, is to translate spoken Swiss German into standard German text implicitly through STT. In this paper, we describe our proposed system that makes use of the Kaldi Speech Recognition Toolkit to implement a time delay neural network (TDNN) Acoustic Model (AM) with an extended pronunciation lexicon and language model. Using this approach, we achieve a word error rate of 45.45% on the held-out test set. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,swiss german speech-to-text @ stt @ is a challenging task due to @ fact @ no single-dominant pronunciation @ standardised orthography exists @ @ is compounded by a severe lack of appropriate training data @ @ potential avenue and @ @ is investigated a part of @ germeval task on low-resource speech-to-text is to translate spoken swiss german @ standard german text implicitly @ stt @ in @ @ @ describe @ proposed system @ make use of @ kaldi speech recognition toolkit to implement a time delay neural network @ tdnn @ acoustic model @ @ @ @ @ extended pronunciation lexicon and language model @ @ @ approach @ achieve a word error rate of @ on @ held-out test set @ @ @ @ @ by @ author @ 
477,LTL-UDE at low-resource speech-to-text shared task: Investigating mozilla deepspeech in a low-resource setting,"We describe our system participating in the SwissText/KONVENS shared task on low-resource speech-to-text (Plüss et al., 2020). We train an end-to-end neural model based on Mozilla DeepSpeech. We examine various methods to improve over the baseline results: transfer learning from standard German and English, data augmentation, and post-processing. Our best system achieves a somewhat disappointing WER of 58.9% on the held-out test set, indicating that it is currently challenging to obtain good results with this approach in a low-resource setting. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,@ describe @ system participating in @ swisstext konvens shared task on low-resource speech-to-text @ plüss et al @ @ @ @ train @ end-to-end neural model based on mozilla deepspeech @ @ examine various method to improve @ @ baseline @ @ transfer learning @ standard german and english data augmentation and post-processing @ @ best system achieves a somewhat disappointing wer of @ on @ held-out test set indicating @ @ is currently challenging to obtain good @ @ @ approach in a low-resource setting @ @ @ @ @ by @ author @ 
478,Supervised pun detection and location with feature engineering and logistic regression,"Puns, by exploiting ambiguities, are commonly used in literature to achieve a humorous or rhetorical effect. Previous approaches mainly focus on machine learning models or rule-based methods, however, they have not addressed how and why a pun is detected or located. Focusing on this, we propose a system for recognizing and locating English puns. Regarding the fact of limited training data and the aim of measuring how relevant a predictor and its direction of the association is, we compile a dataset and explore different feature sets as input for logistic regression, and measure their influence in terms of the assigned weights. To our best knowledge, our system achieves better results than state-of-the-art systems on three subtasks for different types of puns respectively. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,pun by exploiting ambiguity @ commonly used in literature to achieve a humorous @ rhetorical effect @ previous approach mainly focus on machine learning model @ rule-based method however @ @ not addressed @ and @ a pun is detected @ located @ focusing on @ @ propose a system @ recognizing and locating english pun @ regarding @ fact of limited training data and @ aim of measuring @ relevant a predictor and @ direction of @ association is @ compile a dataset and explore different feature set a input @ logistic regression and measure @ influence in term of @ assigned weight @ to @ best knowledge @ system achieves better @ @ state-of-the-art system on three subtasks @ different type of pun respectively @ @ @ @ @ by @ author @ 
479,Investigating the influence of selected linguistic features on authorship attribution using German news articles,"In this work, we perform authorship attribution on a new dataset of German news articles. We seek to classify over 3,700 articles to their five corresponding authors, using four conventional machine learning approaches (naïve Bayes, logistic regression, SVM and kNN) and a convolutional neural network. We analyze the effect of character and word n-grams on the prediction accuracy, as well as the influence of stop words, punctuation, numbers, and lowercasing when preprocessing raw text. The experiments show that higher order character n-grams (n = 5,6) perform better than lower orders and word n-grams slightly outperform those with characters. Combining both in fusion models further improves results up to 92% for SVM. A multilayer convolutional structure allows the CNN to achieve 90.5% accuracy. We found stop words and punctuation to be important features for author identification; removing them leads to a measurable decrease in performance. Finally, we evaluate the topic dependency of the algorithms by gradually replacing named entities, nouns, verbs and eventually all tokens in the dataset according to their POS-tags. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,0,in @ work @ perform authorship attribution on a @ dataset of german news article @ @ seek to classify @ article to @ five corresponding author @ four conventional machine learning approach @ naïve bayes logistic regression svm and knn @ and a convolutional neural network @ @ analyze @ effect of character and word n-grams on @ prediction accuracy a well a @ influence of stop word punctuation number and lowercasing @ preprocessing raw text @ @ experiment @ @ higher order character n-grams @ n @ perform better @ lower order and word n-grams slightly outperform @ @ character @ combining @ in fusion model @ improves @ up to @ svm @ a multilayer convolutional structure allows @ cnn to achieve @ accuracy @ @ found stop word and punctuation to @ important feature @ author identification @ removing @ lead to a measurable decrease in performance @ finally @ evaluate @ topic dependency of @ algorithm by gradually replacing named entity noun verb and eventually @ token in @ dataset according to @ pos-tags @ @ @ @ @ by @ author @ 
481,Report of the first international workshop on semantic indexing and information retrieval for health from heterogeneous content types and languages (SIIRH),"This article briefly summarizes the talks and discussions that occurred during the first edition of the International Workshop on Semantic Indexing and Information Retrieval for Health from heterogeneous content types and languages (SIIRH). The workshop was a virtual event held on April 14, 2020 in conjunction with the 42nd European Conference on Information Retrieval (ECIR2020). The article also presents the main conclusions and future perspectives of the field taking into account the discussions that occurred during the event. All the documents and videos related to the workshop are available at the workshop site: https://sites.google.com/view/siirh2020/. Copyright © 2020 for this paper by its authors.",2020,CEUR Workshop Proceedings,1,@ article briefly summarizes @ talk and discussion @ occurred @ @ first edition of @ international workshop on semantic indexing and information retrieval @ health @ heterogeneous content type and language @ siirh @ @ @ workshop wa a virtual event held on april in conjunction @ @ nd european conference on information retrieval @ ecir @ @ @ article @ @ @ main conclusion and future perspective of @ field taking @ account @ discussion @ occurred @ @ event @ @ @ document and video related to @ workshop @ available at @ workshop site @ http @ site @ google @ com view siirh @ @ @ @ @ by @ author @ 
482,Sentiment analysis of social and topic context using machine learning techniques,"Sentiment Analysis could be a new area in research and is beneficial in many other fields. In the present time, a large amount of textual data is collected using surveys, comments, and reviews online. All the collected data are employed to enhance the items and services provided by both public and private organizations around the world. This Paper introduces a sentiment analysis of social and context reviews using feature-based opinion mining and supervised machine learning. The Sentiment Analysis techniques are to function on a series of expressions for a given item that supported the product quality, and item features. Sentiment analysis is additionally called Opinion mining because of the significant volume of opinion. Analyzing customer opinion is extremely important to rate the items. To automate rate the opinions within the type of unstructured data is been a challenging problem today. Social context and topic context are combined by the Laplacian matrix of the graph built by these contexts and Laplacian regularization is added into the microblog sentiment analysis model. Experimental results on two real Twitter data sets demonstrate that our proposed model can outperform baseline methods consistently and significantly. Various topics beyond item reviews like online shopping, stock markets, elections, disasters, medicine, software engineering, and Cyberbullying extend the utilization of sentiment analysis. © 2020 Innovare Academics Sciences Pvt. Ltd. All rights reserved.",2020,Journal of Critical Reviews,0,sentiment analysis could @ a @ area in research and is beneficial in many @ field @ in @ @ time a @ amount of textual data is collected @ survey comment and review online @ @ @ collected data @ employed to enhance @ item and service provided by @ public and private organization around @ world @ @ @ introduces a sentiment analysis of social and context review @ feature-based opinion mining and supervised machine learning @ @ sentiment analysis technique @ to function on a series of expression @ a given item @ supported @ product quality and item feature @ sentiment analysis is additionally called opinion mining @ of @ significant volume of opinion @ analyzing customer opinion is extremely important to rate @ item @ to automate rate @ opinion within @ type of unstructured data is @ a challenging problem today @ social context and topic context @ combined by @ laplacian matrix of @ graph built by @ context and laplacian regularization is added @ @ microblog sentiment analysis model @ experimental @ on @ real twitter data set demonstrate @ @ proposed model @ outperform baseline method consistently and significantly @ various topic beyond item review like online shopping stock market election disaster medicine software engineering and cyberbullying extend @ utilization of sentiment analysis @ innovare @ science pvt @ ltd @ @ right reserved @ 
486,Aggregating customer review attributes for online reputation generation,"In this paper, we face the problem of generating reputation for movies, products, hotels, restaurants and services by mining customer reviews expressed in natural language. To the best of our knowledge, previous studies on reputation generation for online entities have primarily examined semantic and sentiment orientation of customer reviews, disregarding other useful information that could be extracted from reviews, such as review helpfulness and review time. Therefore, we propose a new approach that combines review helpfulness, review time, review attached rating and review sentiment orientation for the purpose of generating a single reputation value toward various entities. The contribution of the paper is threefold. First, we design two equations to compute review helpfulness and review time scores, and we fine-tune Bidirectional Encoder Representations from Transformers (BERT) model to predict the review sentiment orientation probability. Second, we design a formula to assign a numerical score to each review. Then, we propose a new formula to compute reputation value toward the target entity (movie, product, hotel, restaurant, service, etc). Finally, we propose a new form to visualize reputation that depicts numerical reputation value, opinion categories, top positive review and top negative review. Experimental results coming from several real-world data sets of miscellaneous domains collected from IMDb, TripAdvisor and Amazon websites show the effectiveness of the proposed method in generating and visualizing reputation compared to three state-of-the-art reputation systems. © 2013 IEEE.",2020,IEEE Access,3,in @ @ @ face @ problem of generating reputation @ movie product hotel restaurant and service by mining customer review expressed in natural language @ to @ best of @ knowledge previous study on reputation generation @ online entity @ primarily examined semantic and sentiment orientation of customer review disregarding @ useful information @ could @ extracted @ review @ a review helpfulness and review time @ therefore @ propose a @ approach @ combine review helpfulness review time review attached rating and review sentiment orientation @ @ purpose of generating a single reputation value toward various entity @ @ contribution of @ @ is threefold @ first @ design @ equation to compute review helpfulness and review time score and @ fine-tune bidirectional encoder representation @ transformer @ bert @ model to predict @ review sentiment orientation probability @ second @ design a formula to assign a numerical score to @ review @ @ @ propose a @ formula to compute reputation value toward @ target entity @ movie product hotel restaurant service etc @ @ finally @ propose a @ form to visualize reputation @ depicts numerical reputation value opinion category top positive review and top negative review @ experimental @ coming @ several real-world data set of miscellaneous domain collected @ imdb tripadvisor and amazon website @ @ effectiveness of @ proposed method in generating and visualizing reputation compared to three state-of-the-art reputation system @ @ @ 
487,Investigating of Disease Name Normalization Using Neural Network and Pre-Training,"Normalizing disease names is a crucial task for biomedical and healthcare domains. Previous work explored various approaches, including rules, machine learning and deep learning, which focused on only one approach or one model. In this study, we systematically investigated the performances of various neural models and the effects of different features. Our investigation was performed on two benchmark datasets, namely the NCBI disease corpus and the BioCreative V Chemical Disease Relation (BC5CDR) corpus. The convolutional neural network (CNN) performed the best (F1 90.11%) in the NCBI disease corpus and the attention neural network (Attention) performed the best (F1 90.78%) in the BC5CDR corpus. Compared with the state-of-the-art system, DNorm, our models improved the F1s by 1.74% and 0.86% respectively. In terms of features, character information could improve the F1 by about 0.5-1.0% while sentence information worsened the F1 by about 3-4%. Moreover, we proposed a novel approach for pre-training models, which improved the F1 by up to 9%. The CNN and Attention models are comparable in the task of disease name normalization while the recurrent neural network performs much worse. In addition, character information and pre-training techniques are helpful for this task while sentence information hurts the performance. Our proposed models and pre-training approach can be easily adapted to the normalization task for any other type of entities. Our source code is available at: https://github.com/yx100/EntityNorm. © 2013 IEEE.",2020,IEEE Access,0,normalizing disease name is a crucial task @ biomedical and healthcare domain @ previous work explored various approach including rule machine learning and deep learning @ focused on only @ approach @ @ model @ in @ study @ systematically investigated @ performance of various neural model and @ effect of different feature @ @ investigation wa performed on @ benchmark datasets namely @ ncbi disease corpus and @ biocreative v chemical disease relation @ bc cdr @ corpus @ @ convolutional neural network @ cnn @ performed @ best @ f @ @ in @ ncbi disease corpus and @ attention neural network @ attention @ performed @ best @ f @ @ in @ bc cdr corpus @ compared @ @ state-of-the-art system dnorm @ model improved @ f s by @ and @ respectively @ in term of feature character information could improve @ f by @ @ @ @ sentence information worsened @ f by @ @ moreover @ proposed a novel approach @ pre-training model @ improved @ f by up to @ @ cnn and attention model @ comparable in @ task of disease name normalization @ @ recurrent neural network performs much worse @ in addition character information and pre-training technique @ helpful @ @ task @ sentence information hurt @ performance @ @ proposed model and pre-training approach @ @ easily adapted to @ normalization task @ @ @ type of entity @ @ source code is available at @ http @ github @ com yx entitynorm @ @ @ 
488,A review on multi-lingual sentiment analysis by machine learning methods,"The arrival of e-commerce and the multitude of information presented by the web have established the internet as a principal destination for consumers looking for truthful opinions and multiple viewpoints for some product, news, topic, or trend in the markets. Thus, it is desirable to make this search easier by using systems which sift through the mass of data and summarize the available opinions for easy understanding of the seeker. This task, known as sentiment analysis, is currently a prominent area of research. Sentiment analysis can be useful for businesses, data analysts and data scientists, as well as customers. Even though many methods are designed to perform this task on English data, there is a lack of systems that can analyze data in other languages. This paper attempts to provide a detailed study on the sentiment analysis methods applied on languages other than English. The tools used, pros and cons, and efficiency of all methods is covered. The associated challenges are also discussed. The paper covers methods that analyze translated data as well as methods that analyze available data in the target language. © 2020 School of Science, IHU.",2020,Journal of Engineering Science and Technology Review,1,@ arrival of e-commerce and @ multitude of information presented by @ web @ established @ internet a a principal destination @ consumer looking @ truthful opinion and multiple viewpoint @ some product news topic @ trend in @ market @ thus @ is desirable to make @ search easier by @ system @ sift @ @ mass of data and summarize @ available opinion @ easy understanding of @ seeker @ @ task known a sentiment analysis is currently a prominent area of research @ sentiment analysis @ @ useful @ @ data analyst and data scientist a well a customer @ even though many method @ designed to perform @ task on english data @ is a lack of system @ @ analyze data in @ language @ @ @ attempt to provide a detailed study on @ sentiment analysis method applied on language @ @ english @ @ tool used pro and con and efficiency of @ method is covered @ @ associated challenge @ @ discussed @ @ @ cover method @ analyze translated data a well a method @ analyze available data in @ target language @ school of science ihu @ 
490,A Graph Attention Model for Dictionary-Guided Named Entity Recognition,"The lack of human annotations has been one of the main obstacles for neural named entity recognition in low-resource domains. To address this problem, there have been many efforts on automatically generating silver annotations according to domain-specific dictionaries. However, the information of domain dictionaries is usually limited, and the generated annotations may be noisy which poses significant challenges on learning effective models. In this work, we try to alleviate these issues by introducing a dictionary-guided graph attention model. First, domain-specific dictionaries are utilized to extract entity mention candidates by a graph matching algorithm, which can capture word patterns of domain entities. Furthermore, a word-mention interactive graph is leveraged to integrate the semantic and boundary information of entities into their context. We evaluated our model on the biomedical-domain datasets of recognizing chemical and disease entities, namely BC5CDR and NCBI disease corpora. The results show that our model outperforms several state-of-the-art models with different methodologies, such as feature-based models (e.g., BANNER), ensemble models (e.g., CollaboNet), multi-task learning models (e.g., MTM-CW), dictionary-based models (e.g., AutoNER). Moreover, the performance of our model is also comparable with BioBERT that owns huge parameters and needs large-scale pre-training. © 2013 IEEE.",2020,IEEE Access,2,@ lack of human annotation ha @ @ of @ main obstacle @ neural named entity recognition in low-resource domain @ to address @ problem @ @ @ many effort on automatically generating silver annotation according to domain-specific dictionary @ however @ information of domain dictionary is usually limited and @ generated annotation may @ noisy @ pose significant challenge on learning effective model @ in @ work @ try to alleviate @ issue by introducing a dictionary-guided graph attention model @ first domain-specific dictionary @ utilized to extract entity mention candidate by a graph matching algorithm @ @ capture word pattern of domain entity @ furthermore a word-mention interactive graph is leveraged to integrate @ semantic and boundary information of entity @ @ context @ @ evaluated @ model on @ biomedical-domain datasets of recognizing chemical and disease entity namely bc cdr and ncbi disease corpus @ @ @ @ @ @ model outperforms several state-of-the-art model @ different methodology @ a feature-based model @ e @ g @ banner @ ensemble model @ e @ g @ collabonet @ multi-task learning model @ e @ g @ mtm-cw @ dictionary-based model @ e @ g @ autoner @ @ moreover @ performance of @ model is @ comparable @ biobert @ owns huge parameter and need large-scale pre-training @ @ @ 
493,An ensemble clustering approach for topic discovery using implicit text segmentation,"Text segmentation (TS) is the process of dividing multi-topic text collections into cohesive segments using topic boundaries. Similarly, text clustering has been renowned as a major concern when it comes to multi-topic text collections, as they are distinguished by sub-topic structure and their contents are not associated with each other. Existing clustering approaches follow the TS method which relies on word frequencies and may not be suitable to cluster multi-topic text collections. In this work, we propose a new ensemble clustering approach (ECA) is a novel topic-modelling-based clustering approach, which induces the combination of TS and text clustering. We improvised a LDA-onto (LDA-ontology) is a TS-based model, which presents a deterioration of a document into segments (i.e. sub-documents), wherein each sub-document is associated with exactly one sub-topic. We deal with the problem of clustering when it comes to a document that is intrinsically related to various topics and its topical structure is missing. ECA is tested through well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithms using LDA-onto. ECA exhibits the semantic relations of keywords in sub-documents and resultant clusters belong to original documents that they contain. Moreover, present research sheds the light on clustering performances and it indicates that there is no difference over performances (in terms of F-measure) when the number of topics changes. Our findings give above par results in order to analyse the problem of text clustering in a broader spectrum without applying dimension reduction techniques over high sparse data. Specifically, ECA provides an efficient and significant framework than the traditional and segment-based approach, such that achieved results are statistically significant with an average improvement of over 10.2%. For the most part, proposed framework can be evaluated in applications where meaningful data retrieval is useful, such as document summarization, text retrieval, novelty and topic detection. © The Author(s) 2020.",2020,Journal of Information Science,0,text segmentation @ t @ is @ process of dividing multi-topic text collection @ cohesive segment @ topic boundary @ similarly text clustering ha @ renowned a a major concern @ @ come to multi-topic text collection a @ @ distinguished by sub-topic structure and @ content @ not associated @ @ @ @ existing clustering approach follow @ t method @ relies on word frequency and may not @ suitable to cluster multi-topic text collection @ in @ work @ propose a @ ensemble clustering approach @ eca @ is a novel topic-modelling-based clustering approach @ induces @ combination of t and text clustering @ @ improvised a lda-onto @ lda-ontology @ is a ts-based model @ @ a deterioration of a document @ segment @ i @ e @ sub-documents @ wherein @ sub-document is associated @ exactly @ sub-topic @ @ deal @ @ problem of clustering @ @ come to a document @ is intrinsically related to various topic and @ topical structure is missing @ eca is tested @ well-known datasets in order to provide a comprehensive presentation and validation of clustering algorithm @ lda-onto @ eca exhibit @ semantic relation of keywords in sub-documents and resultant cluster belong to original document @ @ contain @ moreover @ research shed @ light on clustering performance and @ indicates @ @ is no difference @ performance @ in term of f-measure @ @ @ number of topic change @ @ finding give @ par @ in order to analyse @ problem of text clustering in a broader spectrum without applying dimension reduction technique @ high sparse data @ specifically eca provides @ efficient and significant framework @ @ traditional and segment-based approach @ @ achieved @ @ statistically significant @ @ average improvement of @ @ @ @ @ @ part proposed framework @ @ evaluated in application @ meaningful data retrieval is useful @ a document summarization text retrieval novelty and topic detection @ @ author @ s @ @ 
494,What is the Message About? Automatic Multi-label Classification of Open Source Repository Messages into Content Types,"Users of Open Source Software (OSS) projects discuss a diverse range of topics online. The content of a post often corresponds to one or more context-sensitive content types, e.g. a suggestion for a solution, a request for further clarification or indication that a proposed solution did not work. The detection of content types can provide several benefits for software developers. For instance, content types can be used as indicators that summarise the content of the messages. These indicators can be exploited as part of a developer-centric knowledge mining platform allowing developers and project managers to create action alerts concerning new bugs found outside of a bug tracker or they can be combined with other metrics to assess the quality of an OSS project. We present a multi-label classifier, able to classify messages exchanged on communication means about OSS, and detailed evaluation results. We experimented with two state-of-the-art multi-label classification approaches HOMER (Hierarchy Of Multilabel classifiER) and RAkEL (RAndom k-labELsets) as these met the technical requirements of the CROSSMINER project. A manually-annotated threaded corpus of posts form newsgroups discussions, bug tracking systems and forums related to Eclipse projects was also used. The results are promising and indicate the potential to attract novel and deeper research for this task. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,user of open source software @ os @ project discus a diverse range of topic online @ @ content of a post often corresponds to @ @ more context-sensitive content type e @ g @ a suggestion @ a solution a request @ @ clarification @ indication @ a proposed solution @ not work @ @ detection of content type @ provide several benefit @ software developer @ @ instance content type @ @ used a indicator @ summarise @ content of @ message @ @ indicator @ @ exploited a part of a developer-centric knowledge mining platform allowing developer and project manager to create action alert concerning @ bug found outside of a bug tracker @ @ @ @ combined @ @ metric to ass @ quality of @ os project @ @ @ a multi-label classifier able to classify message exchanged on communication mean @ os and detailed evaluation @ @ @ experimented @ @ state-of-the-art multi-label classification approach homer @ hierarchy of multilabel classifier @ and rakel @ random k-labelsets @ a @ met @ technical requirement of @ crossminer project @ a manually-annotated threaded corpus of post form newsgroups discussion bug tracking system and forum related to eclipse project wa @ used @ @ @ @ promising and indicate @ potential to attract novel and deeper research @ @ task @ @ nature switzerland ag @ 
496,A Contextual Semantic-Based Approach for Domain-Centric Lexicon Expansion,"This paper presents a contextual semantic-based approach for expansion of an initial lexicon containing domain-centric seed words. Starting with a small lexicon containing some domain-centric seed words, the proposed approach models text corpus as a weighted word-graph, where the initial weight of a node (word) represents the contextual semantic-based association between the node and the target domain, and the weight of an edge represents the co-occurrence frequency of the respective nodes. The semantic-based association between a node and the target domain is calculated as a function of three contextual semantic-based association metrics. Thereafter, a random walk-based modified PageRank algorithm is applied on the weighted graph to rank and select the most relevant terms for domain-centric lexicon expansion. The proposed approach is evaluated over five datasets, and found to perform significantly better than three baselines and three state-of-the-art approaches. © 2020, Springer Nature Switzerland AG.",2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ a contextual semantic-based approach @ expansion of @ initial lexicon containing domain-centric seed word @ starting @ a small lexicon containing some domain-centric seed word @ proposed approach model text corpus a a weighted word-graph @ @ initial weight of a node @ word @ represents @ contextual semantic-based association @ @ node and @ target domain and @ weight of @ edge represents @ co-occurrence frequency of @ respective node @ @ semantic-based association @ a node and @ target domain is calculated a a function of three contextual semantic-based association metric @ thereafter a random walk-based modified pagerank algorithm is applied on @ weighted graph to rank and select @ @ relevant term @ domain-centric lexicon expansion @ @ proposed approach is evaluated @ five datasets and found to perform significantly better @ three baseline and three state-of-the-art approach @ @ nature switzerland ag @ 
498,Segment Information Extraction from Financial Annual Reports Using Neural Network,"This is an extension from a selected paper from JSAI2019. To extract business contents automatically from financial reports is an important problem in the financial area. Especially, segment names and their explanations are important contents that should be extracted. However, the methods for extracting these types of information from financial reports have not been established. In this study, we aim to develop a practical solution for extracting these types of information. To solve this problem, we developed a manually annotated dataset for the task of extracting the segment names and their explanations of each company from financial reports and then developed a recurrent neural network model to solve this task. Our method using the manually annotated dataset outperformed the baseline methods in the task of extracting segment names and their explanations of each company from annual financial reports. In addition, we experimentally demonstrated that our method can be available for this task even when we have a small training dataset. This work is the first work for applying a machine learning method to the task of extracting segment names and their explanations. The insights from this work should be valuable in the industrial area. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,0,@ is @ extension @ a selected @ @ jsai @ to extract @ content automatically @ financial report is @ important problem in @ financial area @ especially segment name and @ explanation @ important content @ @ @ extracted @ however @ method @ extracting @ type of information @ financial report @ not @ established @ in @ study @ aim to develop a practical solution @ extracting @ type of information @ to solve @ problem @ developed a manually annotated dataset @ @ task of extracting @ segment name and @ explanation of @ company @ financial report and @ developed a recurrent neural network model to solve @ task @ @ method @ @ manually annotated dataset outperformed @ baseline method in @ task of extracting segment name and @ explanation of @ company @ annual financial report @ in addition @ experimentally demonstrated @ @ method @ @ available @ @ task even @ @ @ a small training dataset @ @ work is @ first work @ applying a machine learning method to @ task of extracting segment name and @ explanation @ @ insight @ @ work @ @ valuable in @ industrial area @ @ nature switzerland ag @ 
499,Coreference resolution using neural MCDM and fuzzy weighting technique,"Coreference resolution has been an active field of research in the past several decades and plays a vital role in many areas such as information extraction, document summarization, machine translation, and question answering systems. This paper presents a new coreference resolution approach by incorporating RoBERTa embedding with a neural multi-criteria decision making (MCDM) method. The proposed model does not use any syntactic and dependency parser. Mentions were extracted from the text with an unhand engineered mention detector and features were extracted from a deep neural network. Next, the problem is modeled in the form of effective parameters of the performance such as error rate reduction and enhances the F1 by Kohonen MCDM neural network. The weights assigned to the features represent their importance and suggests the best reference for a mention where such weights are computed using a fuzzy weighting method. Comparing to state-of-the-art coreference resolution models, the simulation results show significant improvements for the proposed approach on different datasets in terms of precision and recall and achieving marginal improvements on the following datasets: English CoNLL-2012 shared task (+3.1 F1), Yahoo’s news site (+6.6 F1), and English Gigaword (+7.04). © 2020 The Authors. Published by Atlantis Press SARL.",2020,International Journal of Computational Intelligence Systems,1,coreference resolution ha @ @ active field of research in @ past several decade and play a vital role in many area @ a information extraction document summarization machine translation and question answering system @ @ @ @ a @ coreference resolution approach by incorporating roberta embedding @ a neural multi-criteria decision making @ mcdm @ method @ @ proposed model doe not use @ syntactic and dependency parser @ mention @ extracted @ @ text @ @ unhand engineered mention detector and feature @ extracted @ a deep neural network @ next @ problem is modeled in @ form of effective parameter of @ performance @ a error rate reduction and enhances @ f by kohonen mcdm neural network @ @ weight assigned to @ feature represent @ importance and suggests @ best reference @ a mention @ @ weight @ computed @ a fuzzy weighting method @ comparing to state-of-the-art coreference resolution model @ simulation @ @ significant improvement @ @ proposed approach on different datasets in term of precision and recall and achieving marginal improvement on @ following datasets @ english conll shared task @ @ f @ yahoo s news site @ @ f @ and english gigaword @ @ @ @ @ author @ published by atlantis @ sarl @ 
500,Extracting Phrases as Software Features from Overlapping Sentence Clusters in Product Descriptions,"Extracting software features from the public product descriptions in the natural language is beneficial for developing new products. Because software features are often expressed in phrases, many approaches currently propose to define phrase patterns and extract phrases as features from product descriptions accordingly. However, there are often lots of noisy phrases extracted because public product descriptions are described freely by different designers and it is difficult to obtain accurate phrase patterns in practice. It is also not suitable to filter those noisy phrases according to frequencies because some important features may be infrequent. To address such issues, this paper proposes a feature extraction approach by extracting phrases as features from the sentence clusters among product descriptions rather than directly from the product descriptions. Considering that more than one feature can be described in one sentence, a new algorithm is designed to detect the overlapping sentence clusters from public product descriptions. It can detect all potential sentence clusters and reduce the affection of noisy descriptions. By taking bigram collocations as the phrase pattern, the bigram collocations containing cluster keywords are elicited as features from each detected sentence cluster. The evaluations conducted on the public software product descriptions from the application market of Softpedia.com, have shown that the proposed approach has better performance than the competitive approaches in terms of precision and time consumption. © 2013 IEEE.",2020,IEEE Access,1,extracting software feature @ @ public product description in @ natural language is beneficial @ developing @ product @ @ software feature @ often expressed in phrase many approach currently propose to define phrase pattern and extract phrase a feature @ product description accordingly @ however @ @ often lot of noisy phrase extracted @ public product description @ described freely by different designer and @ is difficult to obtain accurate phrase pattern in practice @ @ is @ not suitable to filter @ noisy phrase according to frequency @ some important feature may @ infrequent @ to address @ issue @ @ proposes a feature extraction approach by extracting phrase a feature @ @ sentence cluster among product description rather @ directly @ @ product description @ considering @ more @ @ feature @ @ described in @ sentence a @ algorithm is designed to detect @ overlapping sentence cluster @ public product description @ @ @ detect @ potential sentence cluster and reduce @ affection of noisy description @ by taking bigram collocation a @ phrase pattern @ bigram collocation containing cluster keywords @ elicited a feature @ @ detected sentence cluster @ @ evaluation conducted on @ public software product description @ @ application market of softpedia @ com @ @ @ @ proposed approach ha better performance @ @ competitive approach in term of precision and time consumption @ @ @ 
501,Weighted inverse document frequency and vector space model for hadith search engine,"Hadith is the second source of Islamic law after Qur’an which make many types and references of hadith need to be studied. However, there are not many Muslims know about it and many even have difficulties in studying hadiths. This study aims to build a hadith search engine from reliable source by utilizing Information Retrieval techniques. The structured representation of the text that used is Bag of Word (1-term) with the Weighted Inverse Document Frequency (WIDF) method to calculate the frequency of occurrence of each term before being converted in vector form with the Vector Space Model (VSM). Based on the experiment results using 380 texts of hadith, the recall value of WIDF and VSM is 96%, while precision value is just around 35.46%. This is because the structured representation for text that used is bag of words (1-gram) that can not maintain the meaning of text well). Copyright © 2020 Institute of Advanced Engineering and Science. All rights reserved.",2020,Indonesian Journal of Electrical Engineering and Computer Science,3,hadith is @ second source of islamic law @ qur @ @ make many type and reference of hadith need to @ studied @ however @ @ not many muslim know @ @ and many even @ difficulty in studying hadith @ @ study aim to build a hadith search engine @ reliable source by utilizing information retrieval technique @ @ structured representation of @ text @ used is bag of word @ term @ @ @ weighted inverse document frequency @ widf @ method to calculate @ frequency of occurrence of @ term @ @ converted in vector form @ @ vector space model @ vsm @ @ based on @ experiment @ @ text of hadith @ recall value of widf and vsm is @ precision value is @ around @ @ @ is @ @ structured representation @ text @ used is bag of word @ gram @ @ @ not maintain @ meaning of text well @ @ @ institute of advanced engineering and science @ @ right reserved @ 
504,Smart and Incremental Model to Build Clustered Trending Topics of Web Documents,"The abstract Social media trends, which have become more popular nowadays, introduce a rich hub of a broad spectrum of topics. It is of great importance to track emerging related topics when major events occur. The source of such information would be available not only through social portals but also through news, articles and web portals. All this information is aggregated together, by the proposed news aggregator model, to be useful for retrieving the recent popular trends of a certain category or country. The proposed model addresses the identification of semantically related topics from user preferences and favorites that are added manually by the user. Their textual contexts are acquired from the news search and then a clustering technique is applied followed by tracking of trending topics in term space. By quantitative experiments on manually annotated trends, we compared the model with two other well-known algorithms, using three different online datasets. The presented results demonstrate that the model reliably achieves a better entropy and F-measure, and so outperforms the two other mentioned algorithms. © 2020, Springer Nature Switzerland AG.",2020,Advances in Intelligent Systems and Computing,1,@ abstract social medium trend @ @ become more popular nowadays introduce a rich hub of a broad spectrum of topic @ @ is of great importance to track emerging related topic @ major event occur @ @ source of @ information would @ available not only @ social portal @ @ @ news article and web portal @ @ @ information is aggregated together by @ proposed news aggregator model to @ useful @ retrieving @ recent popular trend of a certain category @ country @ @ proposed model address @ identification of semantically related topic @ user preference and favorite @ @ added manually by @ user @ @ textual context @ acquired @ @ news search and @ a clustering technique is applied followed by tracking of trending topic in term space @ by quantitative experiment on manually annotated trend @ compared @ model @ @ @ well-known algorithm @ three different online datasets @ @ presented @ demonstrate @ @ model reliably achieves a better entropy and f-measure and @ outperforms @ @ @ mentioned algorithm @ @ nature switzerland ag @ 
505,Design Consideration of Malay Text Stemmer Using Structured Approach,"Word stemmer (or text stemmer) is used to remove bound morphemes from derived words so that various morphological variants are mapped into common base forms. It is usually used as one of the preprocessing tools in text classification, text mining, and information retrieval tasks. Therefore, the design of an effective text stemmer is crucial for ensuring text stemming process maps morphological variants into correct base forms. This paper investigates the design consideration of an effective text stemmer from the perspective of the Malay language. These design considerations are based on current challenges faced by previous researchers in performing text stemming against Malay texts. By adopting these considerations, an effective text stemmer is expected to address common stemming errors and also, expected to produce promising stemming accuracy. © 2020, Springer Nature Singapore Pte Ltd.",2020,"Smart Innovation, Systems and Technologies",0,word stemmer @ @ text stemmer @ is used to remove bound morpheme @ derived word @ @ various morphological variant @ mapped @ common base form @ @ is usually used a @ of @ preprocessing tool in text classification text mining and information retrieval task @ therefore @ design of @ effective text stemmer is crucial @ ensuring text stemming process map morphological variant @ correct base form @ @ @ investigates @ design consideration of @ effective text stemmer @ @ perspective of @ malay language @ @ design consideration @ based on current challenge faced by previous researcher in performing text stemming @ malay text @ by adopting @ consideration @ effective text stemmer is expected to address common stemming error and @ expected to produce promising stemming accuracy @ @ nature singapore pte ltd @ 
506,AI supported Topic Modeling using KNIME-Workflows,Topic modeling algorithms traditionally model topics as list of weighted terms. These topic models can be used effectively to classify texts or to support text mining tasks such as text summarization or fact extraction. The general procedure relies on statistical analysis of term frequencies. The focus of this work is on the implementation of the knowledge-based topic modelling services in a KNIME2 workflow. A brief description and evaluation of the DBPedia3based enrichment approach and the comparative evaluation of enriched topic models will be outlined based on our previous work. DBpedia-Spotlight4 is used to identify entities in the input text and information from DBpedia is used to extend these entities. We provide a workflow developed in KNIME implementing this approach and perform a result comparison of topic modeling supported by knowledge base information to traditional LDA. This topic modeling approach allows semantic interpretation both by algorithms and by humans. Copyright © 2020 for this paper by its authors.,2020,CEUR Workshop Proceedings,0,topic modeling algorithm traditionally model topic a list of weighted term @ @ topic model @ @ used effectively to classify text @ to support text mining task @ a text summarization @ fact extraction @ @ general procedure relies on statistical analysis of term frequency @ @ focus of @ work is on @ implementation of @ knowledge-based topic modelling service in a knime workflow @ a brief description and evaluation of @ dbpedia based enrichment approach and @ comparative evaluation of enriched topic model @ @ outlined based on @ previous work @ dbpedia-spotlight is used to identify entity in @ input text and information @ dbpedia is used to extend @ entity @ @ provide a workflow developed in knime implementing @ approach and perform a @ comparison of topic modeling supported by knowledge base information to traditional lda @ @ topic modeling approach allows semantic interpretation @ by algorithm and by human @ @ @ @ @ by @ author @ 
508,Knowledge-based semantic relatedness measure using semantic features,"Measuring semantic relatedness has received much attention for uses in many fields such as information retrieval and natural language processing. For handling synonymous problem in distributional-based measures, many researchers are investigating how to exploit semantic features in lexical sources to form knowledge-based measures. In the knowledge-based measures, a hierarchy model is used to measure the relatedness between words based on only the taxonomical features extracted from a provided lexical source. In this paper, a new knowledge feature-based measure is proposed to build the semantic vector of a word construct on taxonomical and non-taxonomical feature of relation words. The proposed measure utilised the topological parameters that weight the importance of each element in the semantic vector. One of the gold dataset used to assess the proposed model and compare the findings with other related works. The results demonstrated the effectiveness of the proposed model on measuring semantic relatedness between words. In this paper, the research framework is identified based on the observations made on the previous related works that have been conducted for semantic representation and semantic relatedness measures. The required data in this research includes the semantic knowledge-based approach and the evaluation datasets. The semantic knowledge that will be used throughout of this research is extracted from English WordNet 3.1. On the other hand, the evaluation datasets covers the gold standard benchmarks which have been used for evaluating the semantic relatedness measurements and text mining tasks. Finally, the evaluation is preform to evaluate the proposed method (PM) based on approach in this research, in which obtained the result have been analyzed, to discuss and compare based on different performance measure and finding the strength and weakness in this paper, to alternative the semantic representation correlated to this research, to designing and develop the topical-based on the semantic representation method for text mining from Social media. © 2020, World Academy of Research in Science and Engineering. All rights reserved.",2020,International Journal of Advanced Trends in Computer Science and Engineering,4,measuring semantic relatedness ha received much attention @ us in many field @ a information retrieval and natural language processing @ @ handling synonymous problem in distributional-based measure many researcher @ investigating @ to exploit semantic feature in lexical source to form knowledge-based measure @ in @ knowledge-based measure a hierarchy model is used to measure @ relatedness @ word based on only @ taxonomical feature extracted @ a provided lexical source @ in @ @ a @ knowledge feature-based measure is proposed to build @ semantic vector of a word construct on taxonomical and non-taxonomical feature of relation word @ @ proposed measure utilised @ topological parameter @ weight @ importance of @ element in @ semantic vector @ @ of @ gold dataset used to ass @ proposed model and compare @ finding @ @ related work @ @ @ demonstrated @ effectiveness of @ proposed model on measuring semantic relatedness @ word @ in @ @ @ research framework is identified based on @ observation made on @ previous related work @ @ @ conducted @ semantic representation and semantic relatedness measure @ @ required data in @ research includes @ semantic knowledge-based approach and @ evaluation datasets @ @ semantic knowledge @ @ @ used throughout of @ research is extracted @ english wordnet @ @ on @ @ hand @ evaluation datasets cover @ gold standard benchmark @ @ @ used @ evaluating @ semantic relatedness measurement and text mining task @ finally @ evaluation is preform to evaluate @ proposed method @ pm @ based on approach in @ research in @ obtained @ @ @ @ analyzed to discus and compare based on different performance measure and finding @ strength and weakness in @ @ to alternative @ semantic representation correlated to @ research to designing and develop @ topical-based on @ semantic representation method @ text mining @ social medium @ world academy of research in science and engineering @ @ right reserved @ 
520,Lexical and semantic analysis of sacred texts using machine learning and natural language processing,"Text mining is the process of exploring and analyzing large amounts of text data and extracting high-quality inf ormation based on patterns and trends in data. The patterns and trends include the analysis of similarity measures and opinion mining or sentimental analysi s expressed in the texts. Text data mining reveals relationships that lie in single or multiple text data. Applying text mining to religious texts can yield various insights on the cultural and religious basis. Reliable automatic knowledge extraction from sacred texts is a challenging task but it will be beneficial to mankind. This research started with the hypothesis that there is intersection between the Bible, Tanakh, and Quran. All these three books have origins in the Middle East. Bible is the holy book of Christians contains a collection of scriptures that were written by many authors, at different time and locations. Tanakh is the sacred text of Jews with 24 books with three parts-Torah, Nevi‘im and Ketuvim and Quran is the central religious text of Islam with 114 chapters as Surah. These three sacred texts contain semi-structured information due to its organized structure of scriptures and numbered chapters, so the comparative studies of the theodicy of three religious texts should reveal interesti ng insights. The objectives of the research are to implement text analytics on sacred texts and reveal the similarity insights of these sacred texts using Natural Language Processing, ontology modeling, and Machine Learning techniques. © IJSTR 2019.",2019,International Journal of Scientific and Technology Research,0,text mining is @ process of exploring and analyzing @ amount of text data and extracting high-quality inf ormation based on pattern and trend in data @ @ pattern and trend include @ analysis of similarity measure and opinion mining @ sentimental analysi s expressed in @ text @ text data mining reveals relationship @ lie in single @ multiple text data @ applying text mining to religious text @ yield various insight on @ cultural and religious basis @ reliable automatic knowledge extraction @ sacred text is a challenging task @ @ @ @ beneficial to mankind @ @ research started @ @ hypothesis @ @ is intersection @ @ bible tanakh and quran @ @ @ three book @ origin in @ middle east @ bible is @ holy book of christian contains a collection of scripture @ @ written by many author at different time and location @ tanakh is @ sacred text of jew @ book @ three parts-torah nevus im and ketuvim and quran is @ central religious text of islam @ chapter a surah @ @ three sacred text contain semi-structured information due to @ organized structure of scripture and numbered chapter @ @ comparative study of @ theodicy of three religious text @ reveal interesti ng insight @ @ objective of @ research @ to implement text analytics on sacred text and reveal @ similarity insight of @ sacred text @ natural language processing ontology modeling and machine learning technique @ ijstr @ 
531,Attention-based Sentiment Reasoner for aspect-based sentiment analysis,"Aspect-based sentiment analysis (ABSA) is a powerful way of predicting the sentiment polarity of text in natural language processing. However, understanding human emotions and reasoning from text like a human continues to be a challenge. In this paper, we propose a model, named Attention-based Sentiment Reasoner (AS-Reasoner), to alleviate the problem of how to capture precise sentiment expressions in ABSA for reasoning. AS-Reasoner assigns importance degrees to different words in a sentence to capture key sentiment expressions towards a specific aspect, and transfers them into a sentiment sentence representation for reasoning in the next layer. To obtain appropriate importance degree values for different words in a sentence, two attention mechanisms we designed: intra attention and global attention. Specifically, intra attention captures the sentiment similarity between any two words in a sentence to compute weights and global attention computes weights by a global perspective. Experiments on all four English and four Chinese datasets show that the proposed model achieves state-of-the-art accuracy and macro-F1 results for aspect term level sentiment analysis and obtains the best accuracy for aspect category level sentiment analysis. The experimental results also indicate that AS-Reasoner is language-independent. © 2019, The Author(s).",2019,Human-centric Computing and Information Sciences,8,aspect-based sentiment analysis @ absa @ is a powerful way of predicting @ sentiment polarity of text in natural language processing @ however understanding human emotion and reasoning @ text like a human continues to @ a challenge @ in @ @ @ propose a model named attention-based sentiment reasoner @ as-reasoner @ to alleviate @ problem of @ to capture precise sentiment expression in absa @ reasoning @ as-reasoner assigns importance degree to different word in a sentence to capture key sentiment expression towards a specific aspect and transfer @ @ a sentiment sentence representation @ reasoning in @ next layer @ to obtain appropriate importance degree value @ different word in a sentence @ attention mechanism @ designed @ intra attention and global attention @ specifically intra attention capture @ sentiment similarity @ @ @ word in a sentence to compute weight and global attention computes weight by a global perspective @ experiment on @ four english and four chinese datasets @ @ @ proposed model achieves state-of-the-art accuracy and macro-f @ @ aspect term level sentiment analysis and obtains @ best accuracy @ aspect category level sentiment analysis @ @ experimental @ @ indicate @ as-reasoner is language-independent @ @ author @ s @ @ 
536,Text-mined dataset of inorganic materials synthesis recipes,"Materials discovery has become significantly facilitated and accelerated by high-throughput ab-initio computations. This ability to rapidly design interesting novel compounds has displaced the materials innovation bottleneck to the development of synthesis routes for the desired material. As there is no a fundamental theory for materials synthesis, one might attempt a data-driven approach for predicting inorganic materials synthesis, but this is impeded by the lack of a comprehensive database containing synthesis processes. To overcome this limitation, we have generated a dataset of “codified recipes” for solid-state synthesis automatically extracted from scientific publications. The dataset consists of 19,488 synthesis entries retrieved from 53,538 solid-state synthesis paragraphs by using text mining and natural language processing approaches. Every entry contains information about target material, starting compounds, operations used and their conditions, as well as the balanced chemical equation of the synthesis reaction. The dataset is publicly available and can be used for data mining of various aspects of inorganic materials synthesis. © 2019, The Author(s).",2019,Scientific Data,20,material discovery ha become significantly facilitated and accelerated by high-throughput ab-initio computation @ @ ability to rapidly design interesting novel compound ha displaced @ material innovation bottleneck to @ development of synthesis route @ @ desired material @ a @ is no a fundamental theory @ material synthesis @ might attempt a data-driven approach @ predicting inorganic material synthesis @ @ is impeded by @ lack of a comprehensive database containing synthesis process @ to overcome @ limitation @ @ generated a dataset of codified recipe @ solid-state synthesis automatically extracted @ scientific publication @ @ dataset consists of synthesis entry retrieved @ solid-state synthesis paragraph by @ text mining and natural language processing approach @ every entry contains information @ target material starting compound operation used and @ condition a well a @ balanced chemical equation of @ synthesis reaction @ @ dataset is publicly available and @ @ used @ data mining of various aspect of inorganic material synthesis @ @ author @ s @ @ 
537,"BioWordVec, improving biomedical word embeddings with subword information and MeSH","Distributed word representations have become an essential foundation for biomedical natural language processing (BioNLP), text mining and information retrieval. Word embeddings are traditionally computed at the word level from a large corpus of unlabeled text, ignoring the information present in the internal structure of words or any information available in domain specific structured resources such as ontologies. However, such information holds potentials for greatly improving the quality of the word representation, as suggested in some recent studies in the general domain. Here we present BioWordVec: an open set of biomedical word vectors/embeddings that combines subword information from unlabeled biomedical text with a widely-used biomedical controlled vocabulary called Medical Subject Headings (MeSH). We assess both the validity and utility of our generated word embeddings over multiple NLP tasks in the biomedical domain. Our benchmarking results demonstrate that our word embeddings can result in significantly improved performance over the previous state of the art in those challenging tasks. © 2019, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.",2019,Scientific Data,39,distributed word representation @ become @ essential foundation @ biomedical natural language processing @ bionlp @ text mining and information retrieval @ word embeddings @ traditionally computed at @ word level @ a @ corpus of unlabeled text ignoring @ information @ in @ internal structure of word @ @ information available in domain specific structured resource @ a ontology @ however @ information hold potential @ greatly improving @ quality of @ word representation a suggested in some recent study in @ general domain @ @ @ @ biowordvec @ @ open set of biomedical word vector embeddings @ combine subword information @ unlabeled biomedical text @ a widely-used biomedical controlled vocabulary called medical subject heading @ mesh @ @ @ ass @ @ validity and utility of @ generated word embeddings @ multiple nlp task in @ biomedical domain @ @ benchmarking @ demonstrate @ @ word embeddings @ @ in significantly improved performance @ @ previous state of @ art in @ challenging task @ @ is a u @ s @ government work and not @ @ protection in @ u @ s @ @ foreign @ protection may apply @ 
549,Buzzwords build momentum: Global financial Twitter sentiment and the aggregate stock market,"We examine the long-term relationship between signals derived from nine years of unstructured social media microblog text data and financial market developments in five major economic regions. Employing statistical language modeling techniques we construct directional sentiment metrics and link these to aggregate stock index returns. To address the noise in finance-related Twitter messages we identify expert users whose tweets predominantly focus on finance topics. We document that expert users are the main drivers behind an interdependence between Twitter sentiment and financial markets. The direct prediction value of expert sentiment metrics for stock index returns, however, is found to be elusive and short-lived. Yet, we detect significant predictive gains over benchmark models in times of negative market returns. In consequence, the relation between expert sentiment metrics and stock indices is sufficient to devise hypothetically profitable cross-sectional as well as time series momentum investment strategies for futures based on Twitter signals that survive basic transaction cost assumptions. In this context, our results show that expert sentiment signals can yield higher risk-adjusted returns than classical price-based signals. © 2019 The Authors",2019,Expert Systems with Applications,11,@ examine @ long-term relationship @ signal derived @ nine year of unstructured social medium microblog text data and financial market development in five major economic region @ employing statistical language modeling technique @ construct directional sentiment metric and link @ to aggregate stock index return @ to address @ noise in finance-related twitter message @ identify expert user whose tweet predominantly focus on finance topic @ @ document @ expert user @ @ main driver behind @ interdependence @ twitter sentiment and financial market @ @ direct prediction value of expert sentiment metric @ stock index return however is found to @ elusive and short-lived @ yet @ detect significant predictive gain @ benchmark model in time of negative market return @ in consequence @ relation @ expert sentiment metric and stock index is sufficient to devise hypothetically profitable cross-sectional a well a time series momentum investment strategy @ future based on twitter signal @ survive basic transaction cost assumption @ in @ context @ @ @ @ expert sentiment signal @ yield higher risk-adjusted return @ classical price-based signal @ @ author
550,Beyond lexical frequencies: using R for text analysis in the digital humanities,"This paper presents a combination of R packages—user contributed toolkits written in a common core programming language—to facilitate the humanistic investigation of digitised, text-based corpora.Our survey of text analysis packages includes those of our own creation (cleanNLP and fasttextM) as well as packages built by other research groups (stringi, readtext, hyphenatr, quanteda, and hunspell). By operating on generic object types, these packages unite research innovations in corpus linguistics, natural language processing, machine learning, statistics, and digital humanities. We begin by extrapolating on the theoretical benefits of R as an elaborate gluing language for bringing together several areas of expertise and compare it to linguistic concordancers and other tool-based approaches to text analysis in the digital humanities. We then showcase the practical benefits of an ecosystem by illustrating how R packages have been integrated into a digital humanities project. Throughout, the focus is on moving beyond the bag-of-words, lexical frequency model by incorporating linguistically-driven analyses in research. © 2019, Springer Nature B.V.",2019,Language Resources and Evaluation,0,@ @ @ a combination of r package user contributed toolkits written in a common core programming language to facilitate @ humanistic investigation of digitised text-based corpus @ @ survey of text analysis package includes @ of @ @ creation @ cleannlp and fasttextm @ a well a package built by @ research group @ stringi readtext hyphenatr quanteda and hunspell @ @ by operating on generic object type @ package unite research innovation in corpus linguistics natural language processing machine learning statistic and digital humanity @ @ begin by extrapolating on @ theoretical benefit of r a @ elaborate gluing language @ bringing together several area of expertise and compare @ to linguistic concordancers and @ tool-based approach to text analysis in @ digital humanity @ @ @ showcase @ practical benefit of @ ecosystem by illustrating @ r package @ @ integrated @ a digital humanity project @ throughout @ focus is on moving beyond @ bag-of-words lexical frequency model by incorporating linguistically-driven analysis in research @ @ nature b @ v @ 
554,Developing insights from social media using semantic lexical chains to mine short text structures,"Social media is increasingly being used for communication by individuals and organizations. Social media stores vast amounts of publicly available data that provides a rich source of information and insights. Often, social media users can easily infer meaning from short text such as microblogs and Facebook posts because they understand the context and terminology used. Although automated data-mining can be effective for gaining insights from text data, a significant challenge is to accurately infer meaning from social media text derived from a single social media account. This is difficult because social media communication uses very short, or sparse, text, which yields a relatively small sample of usable words for analysis. Furthermore, interpreting the contextual meaning from a relatively small set of words is challenging. This research proposes a methodology for extracting semantic lexical chains from frequently occurring words in a single social media account and using these chains to mine short text structures to infer the overall themes of the user. The methodology is based on a proposed clustering algorithm and illustrated with examples from Facebook posts. The algorithm is tested and illustrated by comparing it to existing work and further applying it to a variety of news posts. This methodology could be useful for gaining decision-making insights from social media, or other online forms with short or sparse text. © 2019 Elsevier B.V.",2019,Decision Support Systems,2,social medium is increasingly @ used @ communication by individual and organization @ social medium store vast amount of publicly available data @ provides a rich source of information and insight @ often social medium user @ easily infer meaning @ short text @ a microblogs and facebook post @ @ understand @ context and terminology used @ although automated data-mining @ @ effective @ gaining insight @ text data a significant challenge is to accurately infer meaning @ social medium text derived @ a single social medium account @ @ is difficult @ social medium communication us @ short @ sparse text @ yield a relatively small sample of usable word @ analysis @ furthermore interpreting @ contextual meaning @ a relatively small set of word is challenging @ @ research proposes a methodology @ extracting semantic lexical chain @ frequently occurring word in a single social medium account and @ @ chain to mine short text structure to infer @ overall theme of @ user @ @ methodology is based on a proposed clustering algorithm and illustrated @ example @ facebook post @ @ algorithm is tested and illustrated by comparing @ to existing work and @ applying @ to a variety of news post @ @ methodology could @ useful @ gaining decision-making insight @ social medium @ @ online form @ short @ sparse text @ @ b @ v @ 
555,An effective short text conceptualization based on new short text similarity,"Recently short text messages, tweets, comments and so on, have become a large portion of the online text data. They are limited in length and different from traditional documents in their shortness and sparseness. As a result, short text tends to be ambiguous and its degree is not the same for all languages; and as Arabic is a very high flexional language, where a single word can have multiple meanings, the short text representation plays a vital role in any Text Mining task. To address these issues, we propose an efficient representation for short text based on concepts instead of terms using BabelNet as an external knowledge. However, in the conceptualization process, while searching polysemic term-corresponding concepts, multiple matches are detected. Therefore, assigning a term to a concept is a crucial step and we believe that short text similarity can be useful to overcome the problem of mapping term to the corresponding concept. In this paper, we reintroduce Web-based Kernel function for measuring the semantic relatedness between concepts to disambiguate an expression versus multiple concepts. The proposed method has been evaluated using an Arabic short text categorization system and the obtained results illustrate the interest of our contribution. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature.",2019,Social Network Analysis and Mining,6,recently short text message tweet comment and @ on @ become a @ portion of @ online text data @ @ @ limited in length and different @ traditional document in @ shortness and sparseness @ a a @ short text tends to @ ambiguous and @ degree is not @ @ @ @ language @ and a arabic is a @ high flexional language @ a single word @ @ multiple meaning @ short text representation play a vital role in @ text mining task @ to address @ issue @ propose @ efficient representation @ short text based on concept instead of term @ babelnet a @ external knowledge @ however in @ conceptualization process @ searching polysemic term-corresponding concept multiple match @ detected @ therefore assigning a term to a concept is a crucial step and @ believe @ short text similarity @ @ useful to overcome @ problem of mapping term to @ corresponding concept @ in @ @ @ reintroduce web-based kernel function @ measuring @ semantic relatedness @ concept to disambiguate @ expression versus multiple concept @ @ proposed method ha @ evaluated @ @ arabic short text categorization system and @ obtained @ illustrate @ interest of @ contribution @ springer-verlag gmbh austria part of @ nature @ 
556,Speech-acts based analysis for requirements discovery from online discussions,"Online discussions about software applications and services that take place on web-based communication platforms represent an invaluable knowledge source for diverse software engineering tasks, including requirements elicitation. The amount of research work on developing effective tool-supported analysis methods is rapidly increasing, as part of the so called software analytics. Textual messages in App store reviews, tweets, online discussions taking place in mailing lists and user forums, are processed by combining natural language techniques to filter out irrelevant data; text mining and machine learning algorithms to classify messages into different categories, such as bug report and feature request. Our research objective is to exploit a linguistic technique based on speech-acts for the analysis of online discussions with the ultimate goal of discovering requirements-relevant information. In this paper, we present a revised and extended version of the speech-acts based analysis technique, which we previously presented at CAiSE 2017, together with a detailed experimental characterisation of its properties. Datasets used in the experimental evaluation are taken from a widely used open source software project (161120 textual comments), as well as from an industrial project in the home energy management domain. We make them available for experiment replication purposes. On these datasets, our approach is able to successfully classify messages into Feature/Enhancement and Other, with F-measure of 0.81 and 0.84 respectively. We also found evidence that there is an association between types of speech-acts and categories of issues, and that there is correlation between some of the speech-acts and issue priority, thus motivating further research on the exploitation of our speech-acts based analysis technique in semi-automated multi-criteria requirements prioritisation. © 2018 Elsevier Ltd",2019,Information Systems,9,online discussion @ software application and service @ take place on web-based communication platform represent @ invaluable knowledge source @ diverse software engineering task including requirement elicitation @ @ amount of research work on developing effective tool-supported analysis method is rapidly increasing a part of @ @ called software analytics @ textual message in app store review tweet online discussion taking place in mailing list and user forum @ processed by combining natural language technique to filter @ irrelevant data @ text mining and machine learning algorithm to classify message @ different category @ a bug report and feature request @ @ research objective is to exploit a linguistic technique based on speech-acts @ @ analysis of online discussion @ @ ultimate goal of discovering requirements-relevant information @ in @ @ @ @ a revised and extended version of @ speech-acts based analysis technique @ @ @ presented at caise together @ a detailed experimental characterisation of @ property @ datasets used in @ experimental evaluation @ taken @ a widely used open source software project @ textual comment @ a well a @ @ industrial project in @ home energy management domain @ @ make @ available @ experiment replication purpose @ on @ datasets @ approach is able to successfully classify message @ feature enhancement and @ @ f-measure of @ and @ respectively @ @ @ found evidence @ @ is @ association @ type of speech-acts and category of issue and @ @ is correlation @ some of @ speech-acts and issue priority thus motivating @ research on @ exploitation of @ speech-acts based analysis technique in semi-automated multi-criteria requirement prioritisation @ @ ltd
565,Text mining in education,"The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends. This article is categorized under: Application Areas ' Education and Learning Ensemble Methods ' Text Mining. © 2019 Wiley Periodicals, Inc.",2019,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,12,@ explosive growth of online education environment is generating a massive volume of data specially in text format @ forum chat social network assessment essay among others @ @ produce exciting challenge on @ to mine text data in order to find useful knowledge @ educational stakeholder @ despite @ increasing number of educational application of text mining published recently @ @ not found @ @ surveying @ @ in @ line @ work @ a systematic overview of @ current status of @ educational text mining field @ @ final goal is to answer three main research question @ @ @ @ text mining technique @ used in educational environment @ @ @ @ @ used educational resource @ and @ @ @ main application @ educational goal @ finally @ outline @ conclusion and @ more interesting future trend @ @ article is categorized @ @ application area @ education and learning ensemble method @ text mining @ wiley periodical inc @ 
570,Discovering the influence of sarcasm in social media responses,"Sarcasm in verbal and nonverbal communication is known to attract higher attention and create deeper influence than other negative responses. Many people are adept at including sarcasm in written communication thus sarcastic comments have the potential to stimulate the virality of social media content. Although diverse computational approaches have been used to detect sarcasm in social media, the use of text mining to explore the influential role of sarcasm in spreading negative content is limited. Using tweets during a service disruption of a leading Australian organization as a case study, we explore this phenomenon using a text mining framework with a combination of statistical modeling and natural language processing (NLP) techniques. Our work targets two main outcomes: the quantification of the influence of sarcasm and the exploration of the change in topical relationships in the conversations over time. We found that sarcastic expressions during the service disruption are higher than on regular days and negative sarcastic tweets attract significantly higher social media responses when compared to literal negative expressions. The content analysis showed that consumers initially complaining sarcastically about the outage tended to eventually widen the negative sarcasm in a cascading effect towards the organization's internal issues and strategies. Organizations could utilize such insights to enable proactive decision-making during crisis situations. Moreover, detailed exploration of these impacts would elevate the current text mining applications, to better understand the impact of sarcasm by stakeholders expressed in a social media environment, which can significantly affect the reputation and goodwill of an organization. This article is categorized under: Technologies > Data Preprocessing Ensemble Methods > Text Mining Application Areas > Industry Specific Applications Fundamental Concepts of Data and Knowledge > Big Data Mining. © 2019 Wiley Periodicals, Inc.",2019,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,3,sarcasm in verbal and nonverbal communication is known to attract higher attention and create deeper influence @ @ negative response @ many people @ adept at including sarcasm in written communication thus sarcastic comment @ @ potential to stimulate @ virality of social medium content @ although diverse computational approach @ @ used to detect sarcasm in social medium @ use of text mining to explore @ influential role of sarcasm in spreading negative content is limited @ @ tweet @ a service disruption of a leading australian organization a a case study @ explore @ phenomenon @ a text mining framework @ a combination of statistical modeling and natural language processing @ nlp @ technique @ @ work target @ main outcome @ @ quantification of @ influence of sarcasm and @ exploration of @ change in topical relationship in @ conversation @ time @ @ found @ sarcastic expression @ @ service disruption @ higher @ on regular day and negative sarcastic tweet attract significantly higher social medium response @ compared to literal negative expression @ @ content analysis showed @ consumer initially complaining sarcastically @ @ outage tended to eventually widen @ negative sarcasm in a cascading effect towards @ organization @ s internal issue and strategy @ organization could utilize @ insight to enable proactive decision-making @ crisis situation @ moreover detailed exploration of @ impact would elevate @ current text mining application to better understand @ impact of sarcasm by stakeholder expressed in a social medium environment @ @ significantly affect @ reputation and goodwill of @ organization @ @ article is categorized @ @ technology data preprocessing ensemble method text mining application area industry specific application fundamental concept of data and knowledge big data mining @ wiley periodical inc @ 
572,Question classification using a rule based model,"Question Answering is one of the most common applications for data acquisition. Although the majority of text-mining applications strive to improve the user experience and the tools used to find appropriate answers, the problems still exist because the web content is constantly increasing. The Questions Classification (QC) task is one of the main tasks in improving the classification system is to classify types of questions in the text mining application. A large number of QC methods are introduced to help resolve classification problems, most of which are bag of words approaches. In this project, we propose a QC system that uses Parts of Speech (POS) Tagger and Named Entity Recognition (NER) Tagger from the Stanford core Natural Language Processing (NLP) to classify the questions correctly. We started by cleaning the data by removing the available labels in the questions then we proceed by tagging the questions by splitting words and tagging each and every words in the input question with the POS Tagger. After this step, we will convert them into a pattern without changing the structure of the question. Then we proceed by tagging the question with NER Tagger. Finally, we will do confirmation process for certain question types which is performed by confirming question type module to make the system work efficiently. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,0,question answering is @ of @ @ common application @ data acquisition @ although @ majority of text-mining application strive to improve @ user experience and @ tool used to find appropriate answer @ problem still exist @ @ web content is constantly increasing @ @ question classification @ qc @ task is @ of @ main task in improving @ classification system is to classify type of question in @ text mining application @ a @ number of qc method @ introduced to help resolve classification problem @ of @ @ bag of word approach @ in @ project @ propose a qc system @ us part of speech @ po @ tagger and named entity recognition @ ner @ tagger @ @ stanford core natural language processing @ nlp @ to classify @ question correctly @ @ started by cleaning @ data by removing @ available label in @ question @ @ proceed by tagging @ question by splitting word and tagging @ and every word in @ input question @ @ po tagger @ @ @ step @ @ convert @ @ a pattern without changing @ structure of @ question @ @ @ proceed by tagging @ question @ ner tagger @ finally @ @ @ confirmation process @ certain question type @ is performed by confirming question type module to make @ system work efficiently @ beiesp @ 
579,A multi-centrality index for graph-based keyword extraction,"Keyword extraction aims to capture the main topics of a document and is an important step in natural language processing (NLP) applications. The use of different graph centrality measures has been proposed to extract automatic keywords. However, there is no consensus yet on how these measures compare in this task. Here, we present the multi-centrality index (MCI) approach, which aims to find the optimal combination of word rankings according to the selection of centrality measures. We analyze nine centrality measures (Betweenness, Clustering Coefficient, Closeness, Degree, Eccentricity, Eigenvector, K-Core, PageRank, Structural Holes) for identifying keywords in co-occurrence word-graphs representation of documents. We perform experiments on three datasets of documents and demonstrate that all individual centrality methods achieve similar statistical results, while the proposed MCI approach significantly outperforms the individual centralities, three clustering algorithms, and previously reported results in the literature. © 2019 Elsevier Ltd",2019,Information Processing and Management,17,keyword extraction aim to capture @ main topic of a document and is @ important step in natural language processing @ nlp @ application @ @ use of different graph centrality measure ha @ proposed to extract automatic keywords @ however @ is no consensus yet on @ @ measure compare in @ task @ @ @ @ @ multi-centrality index @ mci @ approach @ aim to find @ optimal combination of word ranking according to @ selection of centrality measure @ @ analyze nine centrality measure @ betweenness clustering coefficient closeness degree eccentricity eigenvector k-core pagerank structural hole @ @ identifying keywords in co-occurrence word-graphs representation of document @ @ perform experiment on three datasets of document and demonstrate @ @ individual centrality method achieve similar statistical @ @ @ proposed mci approach significantly outperforms @ individual centrality three clustering algorithm and @ reported @ in @ literature @ @ ltd
582,New labeled dataset of interconnected lexical typos for automatic correction in the bug reports,"Large-scale and especially open-source projects use software triage systems like Bugzilla to manage their user’s requests like bugs, suggestions, and requirements. The software triage systems have many tasks like prioritizing, finding duplicate and assigning bug reports to developers automatically, which needs text mining, information retrieval, and natural language processing techniques. We already showed there are many typos in the bug reports which reduce the performance of artificial intelligence techniques. The connected terms were one of the most types of typos in the context of bug reports. Also, we introduce some algorithms to correct the connected terms earlier, but there was not any labeled dataset that can be used to evaluate the accuracy of process of typo correction. Now we made a new labeled dataset including 42,970 typos between 182,096 to can be used for the typo correction evaluation process. There are 52% connected typos in the labeled dataset, which show the previous results about the number of connected typos were correct. Then we used the typo correction algorithms which were introduced in prior studies to evaluate their accuracy. The experimental results show 81.6% and 83.3% accuracy in top-5 and top-10 suggestions of the list of typo corrections, respectively. © 2019, Springer Nature Switzerland AG.",2019,SN Applied Sciences,2,large-scale and especially open-source project use software triage system like bugzilla to manage @ user s request like bug suggestion and requirement @ @ software triage system @ many task like prioritizing finding duplicate and assigning bug report to developer automatically @ need text mining information retrieval and natural language processing technique @ @ already showed @ @ many typo in @ bug report @ reduce @ performance of artificial intelligence technique @ @ connected term @ @ of @ @ type of typo in @ context of bug report @ @ @ introduce some algorithm to correct @ connected term earlier @ @ wa not @ labeled dataset @ @ @ used to evaluate @ accuracy of process of typo correction @ now @ made a @ labeled dataset including typo @ to @ @ used @ @ typo correction evaluation process @ @ @ connected typo in @ labeled dataset @ @ @ previous @ @ @ number of connected typo @ correct @ @ @ used @ typo correction algorithm @ @ introduced in prior study to evaluate @ accuracy @ @ experimental @ @ @ and @ accuracy in top and top suggestion of @ list of typo correction respectively @ @ nature switzerland ag @ 
597,Dense semantic matching network for multi-turn conversation,"Mining the semantic information in the text to model the multi-turn conversation has attracted great interests. Previous models ignore the capturing and utilizing of the matching patterns at different levels, which causes the loss of the valuable information for the calculation of the matching degree. To address the problem, we propose a dense semantic matching network (DMN). Given a context-response pair, DMN first constructs semantic representations for the response candidate and each utterance in the context. Then, DMN models the interaction between the response candidate and each utterance in the context to generate interactive matrices. By processing the interactive matrices with dense convolutional blocks, the hierarchical matching patterns are generated for each utterance-response pair. The matching patterns of all the utterance-response pairs are finally accumulated in chronological order with bidirectional long short-term memory network. The final matching score of the response candidate and the multi-turn context is finally calculated. We evaluate the performance of our network on the benchmark dataset. The results show that our network yields a significant performance gain compared with other methods. © 2019 IEEE.",2019,"Proceedings - IEEE International Conference on Data Mining, ICDM",0,mining @ semantic information in @ text to model @ multi-turn conversation ha attracted great interest @ previous model ignore @ capturing and utilizing of @ matching pattern at different level @ cause @ loss of @ valuable information @ @ calculation of @ matching degree @ to address @ problem @ propose a dense semantic matching network @ dmn @ @ given a context-response pair dmn first construct semantic representation @ @ response candidate and @ utterance in @ context @ @ dmn model @ interaction @ @ response candidate and @ utterance in @ context to generate interactive matrix @ by processing @ interactive matrix @ dense convolutional block @ hierarchical matching pattern @ generated @ @ utterance-response pair @ @ matching pattern of @ @ utterance-response pair @ finally accumulated in chronological order @ bidirectional long short-term memory network @ @ final matching score of @ response candidate and @ multi-turn context is finally calculated @ @ evaluate @ performance of @ network on @ benchmark dataset @ @ @ @ @ @ network yield a significant performance gain compared @ @ method @ @ @ 
598,Word embeddings for negation detection in health records written in Spanish,"This work focuses on the creation of a system to detect negated medical entities in electronic health records (EHRs) written in Spanish. The importance of this task rests on the influence that the negation can have in the automatic understanding of information given that it inverts the truth value of a clause. We explore a novel continuous characterization as an alternative to previous negation extraction approaches based on discrete characterizations. The aim is to increase the ability of the characterization to generalize over discrete features. We also included other features that could be useful for the negation detection task. In addition, the negation detection is approached as a named entity recognition task where we want to find only the negated entities. EHRs are represented by the corresponding embeddings. In addition, this approach is compared with a traditional discrete characterization based on words. These representations are employed by a supervised classifier such as conditional random fields to infer the predictive model. The approach is assessed on health records from different hospitals, namely IxaMed-GS and IULA. The best performance is achieved by virtue of the embedding-based characterization, leading to an f-measure of 75.3 and 81.6 for the IxaMed-GS and IULA corpus, respectively. With this work, we prove that the use of embedding-based representations can also be useful for the detection of negated medical entities. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",2019,Soft Computing,7,@ work focus on @ creation of a system to detect negated medical entity in electronic health record @ ehrs @ written in spanish @ @ importance of @ task rest on @ influence @ @ negation @ @ in @ automatic understanding of information given @ @ inverts @ truth value of a clause @ @ explore a novel continuous characterization a @ alternative to previous negation extraction approach based on discrete characterization @ @ aim is to increase @ ability of @ characterization to generalize @ discrete feature @ @ @ included @ feature @ could @ useful @ @ negation detection task @ in addition @ negation detection is approached a a named entity recognition task @ @ want to find only @ negated entity @ ehrs @ represented by @ corresponding embeddings @ in addition @ approach is compared @ a traditional discrete characterization based on word @ @ representation @ employed by a supervised classifier @ a conditional random field to infer @ predictive model @ @ approach is assessed on health record @ different hospital namely ixamed-gs and iula @ @ best performance is achieved by virtue of @ embedding-based characterization leading to @ f-measure of @ and @ @ @ ixamed-gs and iula corpus respectively @ @ @ work @ prove @ @ use of embedding-based representation @ @ @ useful @ @ detection of negated medical entity @ springer-verlag gmbh germany part of @ nature @ 
599,Learning dynamic author representations with temporal language models,"Language models are at the heart of numerous works, notably in the text mining and information retrieval communities. These statistical models aim at extracting word distributions, from simple unigram models to recurrent approaches with latent variables that capture subtle dependencies in texts. However, those models are learned from word sequences only, and authors' identities, as well as publication dates, are seldom considered. We propose a neural model, based on recurrent language modeling, which aims at capturing language diffusion tendencies in author communities through time. By conditioning language models with author and temporal vector states, we are able to leverage the latent dependencies between the text contexts. This allows us to beat several temporal and non-temporal language baselines on two real-world corpora, and to learn meaningful author representations that vary through time. © 2019 IEEE.",2019,"Proceedings - IEEE International Conference on Data Mining, ICDM",0,language model @ at @ heart of numerous work notably in @ text mining and information retrieval community @ @ statistical model aim at extracting word distribution @ simple unigram model to recurrent approach @ latent variable @ capture subtle dependency in text @ however @ model @ learned @ word sequence only and author @ identity a well a publication date @ seldom considered @ @ propose a neural model based on recurrent language modeling @ aim at capturing language diffusion tendency in author community @ time @ by conditioning language model @ author and temporal vector state @ @ able to leverage @ latent dependency @ @ text context @ @ allows u to beat several temporal and non-temporal language baseline on @ real-world corpus and to learn meaningful author representation @ vary @ time @ @ @ 
600,Manifold regularized discriminative feature selection for multi-label learning,"In multi-label learning, objects are essentially related to multiple semantic meanings, and the type of data is confronted with the impact of high feature dimensionality simultaneously, such as the bioinformatics and text mining applications. To tackle the learning problem, the key technology, i.e., feature selection, is developed to reduce dimensionality, whereas most of the previous methods for multi-label feature selection are either directly transformed from traditional single-label feature selection methods or half-baked in the label information exploitation, and thus causing the redundant or irrelevant features involved in the selected feature subset. Aimed to seek discriminative features across multiple class labels, we propose an embedded multi-label feature selection method with manifold regularization. To be specific, a low-dimensional embedding is constructed based on the original feature space to fit the label distribution for capturing the label correlations locally, which is also constrained using the label information in consideration of the co-occurrence relationships of label pairs. Following this principle, we design an optimization objective function involving l2,1-norm regularization to achieve multi-label feature selection, and the convergence is guaranteed. Empirical studies on various multi-label data sets reveal that the proposed method can obtain highly competitive performance against some state-of-the-art multi-label feature selection methods. © 2019 Elsevier Ltd",2019,Pattern Recognition,36,in multi-label learning object @ essentially related to multiple semantic meaning and @ type of data is confronted @ @ impact of high feature dimensionality simultaneously @ a @ bioinformatics and text mining application @ to tackle @ learning problem @ key technology i @ e @ feature selection is developed to reduce dimensionality whereas @ of @ previous method @ multi-label feature selection @ either directly transformed @ traditional single-label feature selection method @ half-baked in @ label information exploitation and thus causing @ redundant @ irrelevant feature involved in @ selected feature subset @ aimed to seek discriminative feature across multiple class label @ propose @ embedded multi-label feature selection method @ manifold regularization @ to @ specific a low-dimensional embedding is constructed based on @ original feature space to fit @ label distribution @ capturing @ label correlation locally @ is @ constrained @ @ label information in consideration of @ co-occurrence relationship of label pair @ following @ principle @ design @ optimization objective function involving l norm regularization to achieve multi-label feature selection and @ convergence is guaranteed @ empirical study on various multi-label data set reveal @ @ proposed method @ obtain highly competitive performance @ some state-of-the-art multi-label feature selection method @ @ ltd
602,Analysis using natural language processing of feedback data from two mathematics support centres,"This paper explores analysis of feedback data collected from student consultations at two mathematics support centres at universities in Australia and Ireland. Unstructured text data was collected over six years and includes qualitative data on student queries collected during the consultations from mathematics and statistics related subjects. Topic modelling and clustering algorithms are used to uncover key themes in the data across stages. Common areas of difficulty experienced by undergraduate students at both universities are investigated and a comparison between them is shown. The results suggest that, despite institutional differences, there is considerable overlap in the types of mathematical and statistical difficulties experienced by students in their first and second year of university at these institutions. We discuss how the ability to uncover such common mathematical and statistical themes with the aid of text mining techniques can be used to improve the support provided by mathematics support centres in terms of providing an efficient and effective service. The code for analyses at both institutions is provided in a GitHub repository so other academic support centres may use it. Outcomes of this analysis have implications for mainstream mathematics and statistics instructors who wish to gain further insights into their students' learning. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",2019,International Journal of Mathematical Education in Science and Technology,1,@ @ explores analysis of feedback data collected @ student consultation at @ mathematics support centre at university in australia and ireland @ unstructured text data wa collected @ six year and includes qualitative data on student query collected @ @ consultation @ mathematics and statistic related subject @ topic modelling and clustering algorithm @ used to uncover key theme in @ data across stage @ common area of difficulty experienced by undergraduate student at @ university @ investigated and a comparison @ @ is @ @ @ @ suggest @ despite institutional difference @ is considerable overlap in @ type of mathematical and statistical difficulty experienced by student in @ first and second year of university at @ institution @ @ discus @ @ ability to uncover @ common mathematical and statistical theme @ @ aid of text mining technique @ @ used to improve @ support provided by mathematics support centre in term of providing @ efficient and effective service @ @ code @ analysis at @ institution is provided in a github repository @ @ @ support centre may use @ @ outcome of @ analysis @ implication @ mainstream mathematics and statistic instructor @ wish to gain @ insight @ @ student @ learning @ informa uk limited trading a taylor francis group @ 
605,Automated essay scoring using ontology generator and natural language processing with question generator based on blooms taxonomy's cognitive level,"Essay writing examination is commonly used learning activity in all levels of education and disciplines. It is advantageous in evaluating the student’s learning outcomes because it gives them the chance to exhibit their knowledge and skills freely. For these reasons, a lot of researchers turned their interest in Automated essay scoring (AES) is one of the most remarkable innovations in text mining using Natural Language Processing and Machine learning algorithms. The purpose of this study is to develop an automated essay scoring that uses ontology and Natural Language Processing. Different learning algorithms showed agreeing prediction outcomes but still regression algorithm with the proper features incorporated with it may produce more accurate essay score. This study aims to increase the accuracy, reliability and validity of the AES by implementing the Gradient ridge regression with the domain ontology and other features. Linear regression, linear lasso regression and ridge regression were also used in conjunction with the different features that was extracted. The different features extracted are the domain concepts, average word length, orthography (spelling mistakes), grammar and sentiment score. The first dataset used is the ASAP dataset from Kaggle website is used to train and test different machine learning algorithms that is consist of linear regression, linear lasso regression, ridge regression and gradient boosting regression together with the different features identified. The second dataset used is the one extracted from the student’s essay exam in Human Computer Interaction course. The results show that the Gradient Boosting Regression has the highest variance and kappa scores. However, we can tell that there are similarities when it comes to performances for Linear, Ridge and Lasso regressions due to the dataset used which is ASAP. Furthermore, the results were evaluated using Cohen Weighted Kappa (CWA) score and compared the agreement between the human raters. The CWA result is 0.659 that can be interpreted as Strong level of agreement between the Human Grader and the automated essay score. Therefore, the proposed AES has 64-81% reliability level. © BEIESP.",2019,International Journal of Engineering and Advanced Technology,0,essay writing examination is commonly used learning activity in @ level of education and discipline @ @ is advantageous in evaluating @ student s learning outcome @ @ give @ @ chance to exhibit @ knowledge and skill freely @ @ @ reason a lot of researcher turned @ interest in automated essay scoring @ aes @ is @ of @ @ remarkable innovation in text mining @ natural language processing and machine learning algorithm @ @ purpose of @ study is to develop @ automated essay scoring @ us ontology and natural language processing @ different learning algorithm showed agreeing prediction outcome @ still regression algorithm @ @ proper feature incorporated @ @ may produce more accurate essay score @ @ study aim to increase @ accuracy reliability and validity of @ aes by implementing @ gradient ridge regression @ @ domain ontology and @ feature @ linear regression linear lasso regression and ridge regression @ @ used in conjunction @ @ different feature @ wa extracted @ @ different feature extracted @ @ domain concept average word length orthography @ spelling mistake @ grammar and sentiment score @ @ first dataset used is @ asap dataset @ kaggle website is used to train and test different machine learning algorithm @ is consist of linear regression linear lasso regression ridge regression and gradient boosting regression together @ @ different feature identified @ @ second dataset used is @ @ extracted @ @ student s essay exam in human computer interaction course @ @ @ @ @ @ gradient boosting regression ha @ highest variance and kappa score @ however @ @ tell @ @ @ similarity @ @ come to performance @ linear ridge and lasso regression due to @ dataset used @ is asap @ furthermore @ @ @ evaluated @ cohen weighted kappa @ cwa @ score and compared @ agreement @ @ human raters @ @ cwa @ is @ @ @ @ interpreted a strong level of agreement @ @ human grader and @ automated essay score @ therefore @ proposed aes ha reliability level @ beiesp @ 
609,Knowledge discovery in medical records through text mining,"The clinical institutions generate a large amount of unstructured data both in the registration of procedures in free text by medical staff, and by the images and videos generated by diagnostic aids. This paper proposes a process of knowledge discovery in the unstructured text of the medical records of the trauma area of the San Vicente Foundation Hospital through text mining. Text preparation techniques were applied such as elimination of non-relevant words, substitution of terms, elimination of accents and derivation of words. Regarding mining processes, supervised and unsupervised learning techniques were applied such as decision trees, logistic regression, nearest k-neighbors, hierarchical clustering and association rules. The result obtained is the conformation of a model of the most relevant words in the clinical records of the Hospital in the area of traumatology. © 2019, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.",2019,RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao,1,@ clinical institution generate a @ amount of unstructured data @ in @ registration of procedure in free text by medical staff and by @ image and video generated by diagnostic aid @ @ @ proposes a process of knowledge discovery in @ unstructured text of @ medical record of @ trauma area of @ san vicente foundation hospital @ text mining @ text preparation technique @ applied @ a elimination of non-relevant word substitution of term elimination of accent and derivation of word @ regarding mining process supervised and unsupervised learning technique @ applied @ a decision tree logistic regression nearest k-neighbors hierarchical clustering and association rule @ @ @ obtained is @ conformation of a model of @ @ relevant word in @ clinical record of @ hospital in @ area of traumatology @ associacao iberica de sistemas e tecnologias de informacao @ @ right reserved @ 
625,Evaluation of sentiment data using classifier model in rapid miner tool,"Evaluation of internet and the usage of internet as websites which is for penetrating to gain a specific requirements, like group communication as social networks (such as face book, twitter,etc.,),blogs for opinions, online portals (such as iGoogle, MSN) for communication, experience as reviews, suggestions as opinions, combination of reviews and opinions as recommendations, ratings and feedbacks which is identified and elevating in almost all the field now-a-days. The writers of online portal, review, opinion and recommendation in any social media take measures as beneficial factor for the improvement of businesses, organization, governments and mostly individuals. When this content boost up the study of content and the need of data mining, text mining techniques and sentiment analysis is inescapable. Natural language processing and text analysis techniques are used in sentiment analysis to recognize and extract information from the text [1]. This paper provides a result of sentiment analysis with the intellectual tool named Rapid Miner to show the sentiment comments about the contents in the online traders. © BEIESP.",2019,International Journal of Engineering and Advanced Technology,2,evaluation of internet and @ usage of internet a website @ is @ penetrating to gain a specific requirement like group communication a social network @ @ a face book twitter etc @ @ blog @ opinion online portal @ @ a igoogle msn @ @ communication experience a review suggestion a opinion combination of review and opinion a recommendation rating and feedback @ is identified and elevating in almost @ @ field now-a-days @ @ writer of online portal review opinion and recommendation in @ social medium take measure a beneficial factor @ @ improvement of @ organization government and mostly individual @ @ @ content boost up @ study of content and @ need of data mining text mining technique and sentiment analysis is inescapable @ natural language processing and text analysis technique @ used in sentiment analysis to recognize and extract information @ @ text @ @ @ provides a @ of sentiment analysis @ @ intellectual tool named rapid miner to @ @ sentiment comment @ @ content in @ online trader @ beiesp @ 
628,Sentiment analysis for customer opinion on hotel using machine learning techniques,"Opinions from others play a significant part to take our own decision, The people’s opinions, attitudes and emotions are a computational study toward an entity is called as Sentiment Analysis (SA) or Opinion Mining (OM). In today's world, everything like business, organization and even individuals wants to know opinion from public or customers about their presentation, products and about their services which will give clear idea about their product, portfolio in the market and if these services is not up to the mark how their services they improve, so that their business will perform better. To give output as positive, negative or neutral and find the difference of a specified user text or data from the dataset is the main task of the sentiment or opinion analysis. The opinions, sentiments and subjectivity of text are computational treatment in text mining with Sentiment Analysis (SA). With the help of sentiment analysis this paper describe the machine learning classification techniques for hotel reviews for which dataset obtained from Trip advisor hotel reviews website. System got 99.07 % accuracy for MAXENT Classifier with Train size and Test size 80% and 20% respectively. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,0,opinion @ others play a significant part to take @ @ decision @ people s opinion attitude and emotion @ a computational study toward @ entity is called a sentiment analysis @ sa @ @ opinion mining @ om @ @ in today @ s world everything like @ organization and even individual want to know opinion @ public @ customer @ @ presentation product and @ @ service @ @ give clear idea @ @ product portfolio in @ market and if @ service is not up to @ mark @ @ service @ improve @ @ @ @ @ perform better @ to give output a positive negative @ neutral and find @ difference of a specified user text @ data @ @ dataset is @ main task of @ sentiment @ opinion analysis @ @ opinion sentiment and subjectivity of text @ computational treatment in text mining @ sentiment analysis @ sa @ @ @ @ help of sentiment analysis @ @ describe @ machine learning classification technique @ hotel review @ @ dataset obtained @ trip advisor hotel review website @ system got @ accuracy @ maxent classifier @ train size and test size and respectively @ beiesp @ 
635,"Editorial for the special issue on ""natural language processing and text mining""",,2019,Information (Switzerland),0,
646,Graph-based clustering of extracted paraphrases for labelling crime reports,"Paraphrases are well-known as synonyms that express the same context in different articulations. Extracting paraphrases from a large text corpus is a challenging task in Natural Language Processing applications. The present work proposes a graph based clustering technique for discovering labels of crime reports based on extracted paraphrases from large untagged crime corpora. Initially, the entity pairs are represented as shallow parse trees where the headword in each tree reflects the actual meaning of the phrase between the entities. Though the phrases having similar headwords have been collected together, there exist many phrases between the entities that express similar context without sharing the same headword. Therefore, clustering is done to create a group of similar meaning phrases termed as paraphrases. A complete weighted graph is constructed with the phrases as nodes and cosine similarity between pair of phrases as the weight of an edge with the phrases as terminal nodes. The graph is made sparse by removing edges with weights less than a threshold value and clustering coefficient has been calculated for each node. The subgraph(s) comprising node(s) with the highest clustering coefficient has been extracted with their adjacent edges. The remaining nodes with their adjacent edges in the graph are added one at a time to an extracted subgraph, if and only if the average clustering coefficient of the resultant subgraph increases and an agglomerative merging technique is applied to merge the extracted subgraphs until no merging takes place. Finally, each subgraph represents a cluster of phrases, yields one aspect of crime. Based on the extracted paraphrases, the reports can be easily labelled. The proposed work deals with crime reports for United States of America (USA), United Arab Emirates (UAE) and India and the evaluation is performed in terms of various supervised and unsupervised techniques. © 2019 Elsevier B.V.",2019,Knowledge-Based Systems,9,paraphrase @ well-known a synonym @ express @ @ context in different articulation @ extracting paraphrase @ a @ text corpus is a challenging task in natural language processing application @ @ @ work proposes a graph based clustering technique @ discovering label of crime report based on extracted paraphrase @ @ untagged crime corpus @ initially @ entity pair @ represented a shallow parse tree @ @ headword in @ tree reflects @ actual meaning of @ phrase @ @ entity @ though @ phrase @ similar headword @ @ collected together @ exist many phrase @ @ entity @ express similar context without sharing @ @ headword @ therefore clustering is done to create a group of similar meaning phrase termed a paraphrase @ a complete weighted graph is constructed @ @ phrase a node and cosine similarity @ pair of phrase a @ weight of @ edge @ @ phrase a terminal node @ @ graph is made sparse by removing edge @ weight le @ a threshold value and clustering coefficient ha @ calculated @ @ node @ @ subgraph @ s @ comprising node @ s @ @ @ highest clustering coefficient ha @ extracted @ @ adjacent edge @ @ remaining node @ @ adjacent edge in @ graph @ added @ at a time to @ extracted subgraph if and only if @ average clustering coefficient of @ resultant subgraph increase and @ agglomerative merging technique is applied to merge @ extracted subgraphs @ no merging take place @ finally @ subgraph represents a cluster of phrase yield @ aspect of crime @ based on @ extracted paraphrase @ report @ @ easily labelled @ @ proposed work deal @ crime report @ united state of america @ usa @ united arab emirate @ uae @ and india and @ evaluation is performed in term of various supervised and unsupervised technique @ @ b @ v @ 
647,Constructing a paraphrase database for agglutinative languages,"Paraphrase databases (PPDBs) are valuable resources for applications that use natural language processing (NLP) technology. In order to construct a high-quality PPDB for agglutinative languages, we propose a phrasal paraphrase extraction method; namely, affix modification-based bilingual pivoting method (AMBPM). AMBPM is suitable for agglutinative languages because it addresses the problems of lexical data sparsity and of not considering morphological word structure. In addition, we propose “improved AMBPM,” which is an improvement on AMBPM by addressing the problem of extracting incorrect stem paraphrase pairs caused by low semantic content stems (LSCSs) by using a rule-based filtering approach. In our experiments on AMBPM, we evaluate AMBPM and compare two state-of-the-art paraphrase extraction methods: the syntactic constraints-based bilingual pivoting method (SCBPM) and word embedding method. In the experiments on improved AMPBM, we evaluate our method and compare the resulting PPDB with four types of databases; PPDB constructed by using the original AMBPM, two PPDBs constructed by using two types of word-embedding-based methods (stem embedding and phrase embedding), and an existing thesaurus. The comparison is performed by using two NLP applications: sentential paraphrase generation and a question answering (QA) system. The experimental results demonstrate that, AMBPM outperforms the state-of-the-art paraphrase extraction methods. In addition, the improved AMBPM, which uses a rule-based filtering method, significantly improves AMBPM. Moreover, although a small amount of training data was used with no aid from linguistic resources, the PPDB constructed with the improved AMBPM is more useful than the four databases for the agglutinative language used in our study. We also publicized the Korean PPDB that was constructed using the improved AMBPM. © 2017 Elsevier B.V.",2019,Data and Knowledge Engineering,1,paraphrase database @ ppdbs @ @ valuable resource @ application @ use natural language processing @ nlp @ technology @ in order to construct a high-quality ppdb @ agglutinative language @ propose a phrasal paraphrase extraction method @ namely affix modification-based bilingual pivoting method @ ambpm @ @ ambpm is suitable @ agglutinative language @ @ address @ problem of lexical data sparsity and of not considering morphological word structure @ in addition @ propose improved ambpm @ is @ improvement on ambpm by addressing @ problem of extracting incorrect stem paraphrase pair caused by low semantic content stem @ lscss @ by @ a rule-based filtering approach @ in @ experiment on ambpm @ evaluate ambpm and compare @ state-of-the-art paraphrase extraction method @ @ syntactic constraints-based bilingual pivoting method @ scbpm @ and word embedding method @ in @ experiment on improved ampbm @ evaluate @ method and compare @ resulting ppdb @ four type of database @ ppdb constructed by @ @ original ambpm @ ppdbs constructed by @ @ type of word-embedding-based method @ stem embedding and phrase embedding @ and @ existing thesaurus @ @ comparison is performed by @ @ nlp application @ sentential paraphrase generation and a question answering @ qa @ system @ @ experimental @ demonstrate @ ambpm outperforms @ state-of-the-art paraphrase extraction method @ in addition @ improved ambpm @ us a rule-based filtering method significantly improves ambpm @ moreover although a small amount of training data wa used @ no aid @ linguistic resource @ ppdb constructed @ @ improved ambpm is more useful @ @ four database @ @ agglutinative language used in @ study @ @ @ publicized @ korean ppdb @ wa constructed @ @ improved ambpm @ @ b @ v @ 
649,A survey on fake news and rumour detection techniques,"False or unverified information spreads just like accurate information on the web, thus possibly going viral and influencing the public opinion and its decisions. Fake news and rumours represent the most popular forms of false and unverified information, respectively, and should be detected as soon as possible for avoiding their dramatic effects. The interest in effective detection techniques has been therefore growing very fast in the last years. In this paper we survey the different approaches to automatic detection of fake news and rumours proposed in the recent literature. In particular, we focus on five main aspects. First, we report and discuss the various definitions of fake news and rumours that have been considered in the literature. Second, we highlight how the collection of relevant data for performing fake news and rumours detection is problematic and we present the various approaches, which have been adopted to gather these data, as well as the publicly available datasets. Third, we describe the features that have been considered in fake news and rumour detection approaches. Fourth, we provide a comprehensive analysis on the various techniques used to perform rumour and fake news detection. Finally, we identify and discuss future directions. © 2019 Elsevier Inc.",2019,Information Sciences,56,false @ unverified information spread @ like accurate information on @ web thus possibly going viral and influencing @ public opinion and @ decision @ fake news and rumour represent @ @ popular form of false and unverified information respectively and @ @ detected a soon a possible @ avoiding @ dramatic effect @ @ interest in effective detection technique ha @ therefore growing @ fast in @ last year @ in @ @ @ survey @ different approach to automatic detection of fake news and rumour proposed in @ recent literature @ in particular @ focus on five main aspect @ first @ report and discus @ various definition of fake news and rumour @ @ @ considered in @ literature @ second @ highlight @ @ collection of relevant data @ performing fake news and rumour detection is problematic and @ @ @ various approach @ @ @ adopted to gather @ data a well a @ publicly available datasets @ third @ describe @ feature @ @ @ considered in fake news and rumour detection approach @ fourth @ provide a comprehensive analysis on @ various technique used to perform rumour and fake news detection @ finally @ identify and discus future direction @ @ inc @ 
650,Potential Technologies Review: A hybrid information retrieval framework to accelerate demand-pull innovation in biomedical engineering,"Launching biomedical innovations based on clinical demands instead of translating basic research findings to practice reduces the risk that the results will not fit the clinical routine. To realize this type of innovation, a meta‐analysis of the body of research is necessary to reveal demand‐matching concepts. However, both the data deluge and the narrow time constraints for innovation make it impossible to perform such reviews manually. Thus, this paper proposes a specifically adapted “Potential Technologies Review” approach focusing on automated text mining and information retrieval techniques. The novel framework combines features from both systematic and scoping reviews. It aims at high coverage and reproducibility while mapping technologies—even with a fuzzy initial scope. To achieve these goals for search and triage, a set of closely interrelated methods has been developed: (a) automated query optimization, (b) screening prioritization, and (c) recall estimation. To determine appropriate parameters, a variety of published literature corpora were used and compared with an evaluation on a real‐world dataset. Our results show that it is feasible to automate the identification of relevant works using this newly introduced framework. It achieved a workload reduction of up to 91% “Work‐saved‐over Sampling (WSS)” with a 76% overall recall compared with manually screening search results. Reducing the workload is a prerequisite for a rapid Potential Technologies Review when conducting demand‐pull innovations. Moreover, it facilitates the updating and closer monitoring of latest findings. Studying the robustness of the framework and expanding it to patent documents are future tasks. © 2019 John Wiley & Sons Ltd.",2019,Research Synthesis Methods,0,launching biomedical innovation based on clinical demand instead of translating basic research finding to practice reduces @ risk @ @ @ @ not fit @ clinical routine @ to realize @ type of innovation a meta analysis of @ body of research is necessary to reveal demand matching concept @ however @ @ data deluge and @ narrow time constraint @ innovation make @ impossible to perform @ review manually @ thus @ @ proposes a specifically adapted potential technology review approach focusing on automated text mining and information retrieval technique @ @ novel framework combine feature @ @ systematic and scoping review @ @ aim at high coverage and reproducibility @ mapping technology even @ a fuzzy initial scope @ to achieve @ goal @ search and triage a set of closely interrelated method ha @ developed @ @ a @ automated query optimization @ b @ screening prioritization and @ c @ recall estimation @ to determine appropriate parameter a variety of published literature corpus @ used and compared @ @ evaluation on a real world dataset @ @ @ @ @ @ is feasible to automate @ identification of relevant work @ @ newly introduced framework @ @ achieved a workload reduction of up to work saved @ sampling @ w @ @ a overall recall compared @ manually screening search @ @ reducing @ workload is a prerequisite @ a rapid potential technology review @ conducting demand pull innovation @ moreover @ facilitates @ updating and closer monitoring of latest finding @ studying @ robustness of @ framework and expanding @ to patent document @ future task @ john wiley son ltd @ 
657,Data mining for smart legal systems,"Smart legal systems carry immense potential to provide legal community and public with valuable insights using legal data. These systems can consequently help in analyzing and mitigating various social issues. In Pakistan, since last couple of years, courts have been reporting judgments online for public consumption. This public data, once processed, can be utilized for betterment of society and policy making in Pakistan. This study takes the first step to realize smart legal system by extracting various entities such as dates, case numbers, reference cases, person names, etc. from legal judgments. To automatically extract these entities, the primary requirement is to construct dataset using legal judgments. Hence, firstly annotation guidelines are prepared followed by preparation of annotated dataset for extraction of various legal entities. Experiments conducted using variety of datasets, multiple algorithms and annotation schemes, resulted into maximum F1-score of 91.51% using Conditional Random Fields. © 2019",2019,Computers and Electrical Engineering,1,smart legal system carry immense potential to provide legal community and public @ valuable insight @ legal data @ @ system @ consequently help in analyzing and mitigating various social issue @ in pakistan since last couple of year court @ @ reporting judgment online @ public consumption @ @ public data @ processed @ @ utilized @ betterment of society and policy making in pakistan @ @ study take @ first step to realize smart legal system by extracting various entity @ a date case number reference case person name etc @ @ legal judgment @ to automatically extract @ entity @ primary requirement is to construct dataset @ legal judgment @ hence firstly annotation guideline @ prepared followed by preparation of annotated dataset @ extraction of various legal entity @ experiment conducted @ variety of datasets multiple algorithm and annotation scheme resulted @ maximum f score of @ @ conditional random field @ 
658,Enhancing portfolio return based on sentiment-of-topic,"While time-series analysis is commonly used in financial forecasting, a key source of market-sentiments is often omitted. Financial news is known to be making persuasive impact on the markets. Without considering this additional source of signals, only sub-optimal predictions can be made. This paper proposes a notion of sentiment-of-topic (SoT) to address the problem. It is achieved by considering sentiment-linked topics, which are retrieved from time-series with heterogeneous dimensions (i.e., numbers and texts). Using this approach, we successfully improve the prediction accuracy of a proprietary trade recommendation platform. Different from traditional sentiment analysis and unsupervised topic modeling methods, topics associated with different sentiment levels are used to quantify market conditions. In particular, sentiment levels are learned from historical market performances and commentaries instead of using subjective interpretations of human expressions. By capturing the domain knowledge of respective industries and markets, an impressive double-digit improvement in portfolio return is obtained as shown in our experiments. © 2017",2019,Data and Knowledge Engineering,1,@ time-series analysis is commonly used in financial forecasting a key source of market-sentiments is often omitted @ financial news is known to @ making persuasive impact on @ market @ without considering @ additional source of signal only sub-optimal prediction @ @ made @ @ @ proposes a notion of sentiment-of-topic @ sot @ to address @ problem @ @ is achieved by considering sentiment-linked topic @ @ retrieved @ time-series @ heterogeneous dimension @ i @ e @ number and text @ @ @ @ approach @ successfully improve @ prediction accuracy of a proprietary trade recommendation platform @ different @ traditional sentiment analysis and unsupervised topic modeling method topic associated @ different sentiment level @ used to quantify market condition @ in particular sentiment level @ learned @ historical market performance and commentary instead of @ subjective interpretation of human expression @ by capturing @ domain knowledge of respective industry and market @ impressive double-digit improvement in portfolio return is obtained a @ in @ experiment @ 
659,A Knowledge-Based Semisupervised Hierarchical Online Topic Detection Framework,"Topic models have achieved big success in recent years. To detect topics in a text stream, various online topic models have been proposed in the literature. The limitations of these works include that: 1) most of them run with fixed topic numbers and 2) the overlaps between the topics may enlarge in the evolving process. Hierarchical topic model is a candidate solution to these problems since it can reveal many useful relationships between the topics. These relationships can help to find high quality topics and reduce topic overlaps. In this paper, a knowledge-based semisupervised hierarchical online topic detection framework is proposed. The proposed framework can detect topics in an online hierarchical way. In addition, it has been proven that introducing external knowledge can improve the performance of text mining. Therefore, the knowledge from external knowledge sources and human experts are also integrated in the proposed framework. Experiments are conducted to evaluate the proposed framework with different metrics. The results show that compared with the baseline methods, our framework can achieve better performance with competitive time efficiency. © 2018 IEEE.",2019,IEEE Transactions on Cybernetics,1,topic model @ achieved big success in recent year @ to detect topic in a text stream various online topic model @ @ proposed in @ literature @ @ limitation of @ work include @ @ @ @ of @ run @ fixed topic number and @ @ overlap @ @ topic may enlarge in @ evolving process @ hierarchical topic model is a candidate solution to @ problem since @ @ reveal many useful relationship @ @ topic @ @ relationship @ help to find high quality topic and reduce topic overlap @ in @ @ a knowledge-based semisupervised hierarchical online topic detection framework is proposed @ @ proposed framework @ detect topic in @ online hierarchical way @ in addition @ ha @ proven @ introducing external knowledge @ improve @ performance of text mining @ therefore @ knowledge @ external knowledge source and human expert @ @ integrated in @ proposed framework @ experiment @ conducted to evaluate @ proposed framework @ different metric @ @ @ @ @ compared @ @ baseline method @ framework @ achieve better performance @ competitive time efficiency @ @ @ 
660,An efficient framework of utilizing the latent semantic analysis in text extraction,"The use of the latent semantic analysis (LSA) in text mining demands large space and time requirements. This paper proposes a new text extraction method that sets a framework on how to employ the statistical semantic analysis in the text extraction in an efficient way. The method uses the centrality feature and omits the segments of the text that have a high verbatim, statistical, or semantic similarity with previously processed segments. The identification of similarity is based on a new multi-layer similarity method that computes the similarity in three statistical layers, it uses the Jaccard similarity and the vector space model in the first and second layers respectively, and uses the LSA in the third layer. The multi-layer similarity restricts the use of the third layer for the segments that the first and second layers failed to estimate their similarities. Rouge tool is used in the evaluation, but because Rouge does not consider the extract’s size, we supplemented it with a new evaluation strategy based on the compression rate and the ratio of the sentences intersections between the automatic and the reference extracts. Our comparisons with classical LSA and traditional statistical extractions showed that we reduced the use of the LSA procedure by 52%, and we obtained 65% reduction on the original matrix dimensions, also, we obtained remarkable accuracy results. It is concluded that the employment of the centrality feature with the proposed multi-layer framework yields a significant solution in terms of efficiency and accuracy in the field of text extraction. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2019,International Journal of Speech Technology,0,@ use of @ latent semantic analysis @ lsa @ in text mining demand @ space and time requirement @ @ @ proposes a @ text extraction method @ set a framework on @ to employ @ statistical semantic analysis in @ text extraction in @ efficient way @ @ method us @ centrality feature and omits @ segment of @ text @ @ a high verbatim statistical @ semantic similarity @ @ processed segment @ @ identification of similarity is based on a @ multi-layer similarity method @ computes @ similarity in three statistical layer @ us @ jaccard similarity and @ vector space model in @ first and second layer respectively and us @ lsa in @ third layer @ @ multi-layer similarity restricts @ use of @ third layer @ @ segment @ @ first and second layer failed to estimate @ similarity @ rouge tool is used in @ evaluation @ @ rouge doe not consider @ extract s size @ supplemented @ @ a @ evaluation strategy based on @ compression rate and @ ratio of @ sentence intersection @ @ automatic and @ reference extract @ @ comparison @ classical lsa and traditional statistical extraction showed @ @ reduced @ use of @ lsa procedure by and @ obtained reduction on @ original matrix dimension @ @ obtained remarkable accuracy @ @ @ is concluded @ @ employment of @ centrality feature @ @ proposed multi-layer framework yield a significant solution in term of efficiency and accuracy in @ field of text extraction @ @ science @ medium llc part of @ nature @ 
667,Joint sentiment/topic modeling on text data using a boosted restricted Boltzmann Machine,"Recently by the development of the Internet and the Web, different types of social media such as web blogs become an immense source of text data. Through the processing of these data, it is possible to discover practical information about different topics, individual’s opinions and a thorough understanding of the society. Therefore, applying models which can automatically extract the subjective information from documents would be efficient and helpful. Topic modeling methods and sentiment analysis are the raised topics in natural language processing and text mining fields. In this paper a new structure for joint sentiment-topic modeling based on a Restricted Boltzmann Machine (RBM) which is a type of neural networks is proposed. By modifying the structure of RBM as well as appending a layer which is analogous to sentiment of text data to it, we propose a generative structure for joint sentiment topic modeling based on neural networks. The proposed method is supervised and trained by the Contrastive Divergence algorithm. The new attached layer in the proposed model is a layer with the multinomial probability distribution which can be used in text data sentiment classification or any other supervised application. The proposed model is compared with existing models in the experiments such as evaluating as a generative model, sentiment classification, information retrieval and the corresponding results demonstrate the efficiency of the method. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Multimedia Tools and Applications,1,recently by @ development of @ internet and @ web different type of social medium @ a web blog become @ immense source of text data @ @ @ processing of @ data @ is possible to discover practical information @ different topic individual s opinion and a thorough understanding of @ society @ therefore applying model @ @ automatically extract @ subjective information @ document would @ efficient and helpful @ topic modeling method and sentiment analysis @ @ raised topic in natural language processing and text mining field @ in @ @ a @ structure @ joint sentiment-topic modeling based on a restricted boltzmann machine @ rbm @ @ is a type of neural network is proposed @ by modifying @ structure of rbm a well a appending a layer @ is analogous to sentiment of text data to @ @ propose a generative structure @ joint sentiment topic modeling based on neural network @ @ proposed method is supervised and trained by @ contrastive divergence algorithm @ @ @ attached layer in @ proposed model is a layer @ @ multinomial probability distribution @ @ @ used in text data sentiment classification @ @ @ supervised application @ @ proposed model is compared @ existing model in @ experiment @ a evaluating a a generative model sentiment classification information retrieval and @ corresponding @ demonstrate @ efficiency of @ method @ @ science @ medium llc part of @ nature @ 
673,Research on cache transition techniques for semantic graph parsing for optimizing search process using text mining,"This paper elaborates the transition system that gives the standard transition-based dependency parsing techniques for generating the graph. It is essential to know the standard transition techniques for all graphical problems. Cache transition technique plays a vital role in optimizing the search process in various text mining applications. This paper provides an overview on cache transition technique for parsing semantic graphs for several Natural Language Processing (NLP) applications. According to this paper, the cache is having the fixed size m, by tree decomposition theory according to which there is a relationship between the parameter m and class of graphs produced by the theory. © BEIESP.",2019,International Journal of Recent Technology and Engineering,0,@ @ elaborates @ transition system @ give @ standard transition-based dependency parsing technique @ generating @ graph @ @ is essential to know @ standard transition technique @ @ graphical problem @ cache transition technique play a vital role in optimizing @ search process in various text mining application @ @ @ provides @ overview on cache transition technique @ parsing semantic graph @ several natural language processing @ nlp @ application @ according to @ @ @ cache is @ @ fixed size @ by tree decomposition theory according to @ @ is a relationship @ @ parameter @ and class of graph produced by @ theory @ beiesp @ 
683,Similarity matching of pairs of text using CACT algorithm,"In data mining, shorter text analysis is performed more widely for many applications. Based on the syntax of the language, it is very difficult to analyze the short text with several traditional tools of natural language processing and this is not applied correctly either. In short text, it is known that there are rare and insufficient data available and further it is difficult to identify semantic knowledge with the great noise and ambiguity of short texts. In this paper, the authors proposed to replace the coefficient of similarity of Cosine with the measure of similarity of Jaro-Winkler to obtain the coincidence of similarity between pairs of text (source text and target text). Jaro-Winkler does a better job of determining the similarity of the strings because it takes an order into account when using the positional indices to estimate relevance. It is presumed that the performance of CACT driven by Jaro-Wrinkler with respect to one-to-many data links offers optimized performance when compared to the operation of CACT driven by cosine. In this paper, the ensemble algorithm CACTS and SAE is adopted with Jaro-Winkler similarity approach. The new algorithm is employed for short text analysis and better results. An evaluation of our proposed concept is sufficient as validation. © BEIESP.",2019,International Journal of Engineering and Advanced Technology,1,in data mining shorter text analysis is performed more widely @ many application @ based on @ syntax of @ language @ is @ difficult to analyze @ short text @ several traditional tool of natural language processing and @ is not applied correctly either @ in short text @ is known @ @ @ rare and insufficient data available and @ @ is difficult to identify semantic knowledge @ @ great noise and ambiguity of short text @ in @ @ @ author proposed to replace @ coefficient of similarity of cosine @ @ measure of similarity of jaro-winkler to obtain @ coincidence of similarity @ pair of text @ source text and target text @ @ jaro-winkler doe a better job of determining @ similarity of @ string @ @ take @ order @ account @ @ @ positional index to estimate relevance @ @ is presumed @ @ performance of cact driven by jaro-wrinkler @ respect to one-to-many data link offer optimized performance @ compared to @ operation of cact driven by cosine @ in @ @ @ ensemble algorithm cacts and sae is adopted @ jaro-winkler similarity approach @ @ @ algorithm is employed @ short text analysis and better @ @ @ evaluation of @ proposed concept is sufficient a validation @ beiesp @ 
684,Sentiment trend analysis of big data,"Various fields like Text Mining, Linguistics, Decision Making and Natural Language Processing together form the basis for Opinion Mining or Sentiment Analysis. People share their feelings, observations and thoughts on social media, which has emerged as a powerful tool for rapidly growing enormous repository of real time discussions and thoughts shared by people. In this paper, we aim to decipher the current popular opinions or emotions from various sources, hence, contributing to sentiment analysis domain. Text from social media, blogs and product reviews are classified according to the sentiment they project. We re-examine the traditional processes of sentiment extraction, to incorporate the increase in complexity and number of the data sources and relevant topics, while re-populating the meaning of sentiment. Working across and within numerous streams of social media, expression of sentiment and classification of polarity is re-examined, thereby redefining and enhancing the realm of sentiment. Numerous social media streams are analyzed to build datasets that are topical for each stream and are later polarized according to their sentiment expression. In conclusion, defining a sentiment and developing tools for its analysis in real time of human idea exchange is the motive. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,0,various field like text mining linguistics decision making and natural language processing together form @ basis @ opinion mining @ sentiment analysis @ people share @ feeling observation and thought on social medium @ ha emerged a a powerful tool @ rapidly growing enormous repository of real time discussion and thought shared by people @ in @ @ @ aim to decipher @ current popular opinion @ emotion @ various source hence contributing to sentiment analysis domain @ text @ social medium blog and product review @ classified according to @ sentiment @ project @ @ re-examine @ traditional process of sentiment extraction to incorporate @ increase in complexity and number of @ data source and relevant topic @ re-populating @ meaning of sentiment @ working across and within numerous stream of social medium expression of sentiment and classification of polarity is re-examined thereby redefining and enhancing @ realm of sentiment @ numerous social medium stream @ analyzed to build datasets @ @ topical @ @ stream and @ later polarized according to @ sentiment expression @ in conclusion defining a sentiment and developing tool @ @ analysis in real time of human idea exchange is @ motive @ beiesp @ 
688,Assessing manufacturing strategy definitions utilising text-mining,"The variations in Manufacturing Strategy (MS) definitions create confusion and lead to lack of shared understanding between academic researchers and practitioners on its scope. The purpose of this study is to provide an empirical analysis of the paradox in the difference between academic and industry definitions of MS. Natural Language Processing (NLP) based text mining is used to extract primary elements from the various academic, and industry definitions of MS. Co-word and Principal Component Analysis (PCA) provide empirical support for the grouping into nine primary elements. We posit from the terms evolution analysis that there is a stasis currently faced in academic literature towards MS definition while the industry with its emphasis on ‘context’ has been dynamic. We believe that the proposed approach and results of the present empirical analysis can contribute to overcoming the current challenges to MS design and deployment–imprecise definition leading to its inadequate operationalisation. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2019,International Journal of Production Research,4,@ variation in manufacturing strategy @ @ @ definition create confusion and lead to lack of shared understanding @ @ researcher and practitioner on @ scope @ @ purpose of @ study is to provide @ empirical analysis of @ paradox in @ difference @ @ and industry definition of @ @ natural language processing @ nlp @ based text mining is used to extract primary element @ @ various @ and industry definition of @ @ co-word and principal component analysis @ pca @ provide empirical support @ @ grouping @ nine primary element @ @ posit @ @ term evolution analysis @ @ is a stasis currently faced in @ literature towards @ definition @ @ industry @ @ emphasis on context ha @ dynamic @ @ believe @ @ proposed approach and @ of @ @ empirical analysis @ contribute to overcoming @ current challenge to @ design and deployment imprecise definition leading to @ inadequate operationalisation @ informa uk limited trading a taylor francis group @ 
690,Automatic online subjective text evaluation using text mining,"Semantic based text mining is essential in high dimensional data processing in today’s environment. In educational sector Question Answer (QA) evaluation has done using semantic as well as semantic analysis in many approaches. Numerous existing mechanisms have implemented using different machine learning algorithms. Semantic and semantic both works for evaluate the text data, but semantic approach should done same work with low time complexity. In this work system carried out automated text evaluation for online examination system with semi structured dataset. The system has categorized into two phases, NLP and Features base evaluation. Natural Language Processing (NLP) has used for preprocessing of data using tokenization, stop word removal, porter stemmer etc. Similarity technique has used for generate similarity score between test answer and train answer data. Artificial Neural Network (ANN) has used to generate the similarity score between two features vectors. Experimental analysis shows the how proposed system is better than some traditional approaches for semantic text evaluation. © BEIESP.",2019,International Journal of Recent Technology and Engineering,0,semantic based text mining is essential in high dimensional data processing in today s environment @ in educational sector question answer @ qa @ evaluation ha done @ semantic a well a semantic analysis in many approach @ numerous existing mechanism @ implemented @ different machine learning algorithm @ semantic and semantic @ work @ evaluate @ text data @ semantic approach @ done @ work @ low time complexity @ in @ work system carried @ automated text evaluation @ online examination system @ semi structured dataset @ @ system ha categorized @ @ phase nlp and feature base evaluation @ natural language processing @ nlp @ ha used @ preprocessing of data @ tokenization stop word removal porter stemmer etc @ similarity technique ha used @ generate similarity score @ test answer and train answer data @ artificial neural network @ ann @ ha used to generate @ similarity score @ @ feature vector @ experimental analysis @ @ @ proposed system is better @ some traditional approach @ semantic text evaluation @ beiesp @ 
692,Critique on cache transition techniques for semantic graph parsing for optimizing search process using text mining,"This paper elaborates the transition system that gives the standard transition-based dependency parsing techniques for generating the graph. It is essential to know the standard transition techniques for all graphical problems. Cache transition technique plays a vital role in optimizing the search process in various text mining applications. This paper provides an overview on cache transition technique for parsing semantic graphs for several Natural Language Processing (NLP) applications. According to this paper, the cache is having the fixed size m, by tree decomposition theory according to which there is a relationship between the parameter m and class of graphs produced by the theory. © BEIESP.",2019,International Journal of Recent Technology and Engineering,0,@ @ elaborates @ transition system @ give @ standard transition-based dependency parsing technique @ generating @ graph @ @ is essential to know @ standard transition technique @ @ graphical problem @ cache transition technique play a vital role in optimizing @ search process in various text mining application @ @ @ provides @ overview on cache transition technique @ parsing semantic graph @ several natural language processing @ nlp @ application @ according to @ @ @ cache is @ @ fixed size @ by tree decomposition theory according to @ @ is a relationship @ @ parameter @ and class of graph produced by @ theory @ beiesp @ 
695,Methods and trends in information retrieval in big data genomic research,"This paper described information retrieval (IR) and the common methods of finding, extracting, and mining information in genomic research through text mining, and natural language processing (NLP). There was a surge of genomic information from the different literature and the production of genome datasets that catapulted the development of several tools for analyzing and presenting new found knowledge in the biomedical and genome research. This paper presented the recent research trends, survey, reviews, experiments, and concepts in information retrieval applied to text, images and object features in big data genomic research. The method used is exploratory survey research in IR uses in genomic research that presents the concepts, methods, evaluation results and next steps described by the key researchers. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,0,@ @ described information retrieval @ ir @ and @ common method of finding extracting and mining information in genomic research @ text mining and natural language processing @ nlp @ @ @ wa a surge of genomic information @ @ different literature and @ production of genome datasets @ catapulted @ development of several tool @ analyzing and presenting @ found knowledge in @ biomedical and genome research @ @ @ presented @ recent research trend survey review experiment and concept in information retrieval applied to text image and object feature in big data genomic research @ @ method used is exploratory survey research in ir us in genomic research @ @ @ concept method evaluation @ and next step described by @ key researcher @ beiesp @ 
698,A conceptual dependency graph based keyword extraction model for source code to API documentation mapping,"Natural language processing on software systems usually contain high dimensional noisy and irrelevant features which lead to inaccurate and poor contextual similarity between the project source code and its API documentation. Most of the traditional source code analysis models are independent of finding and extracting the relevant features for contextual similarity. As the size of the project source code and its related API documentation increases, these models incorporate the contextual similarity between the source code and API documentation for code analysis. One of the best solutions for this problem is finding the essential features using the source code dependency graph. In this paper, the dependency graph is used to compute the contextual similarity computation between the source code metrics and its API documents. A novel contextual similarity measure is used to find the relationship between the project source code metrics to the API documents. Proposed model is evaluated on different project source codes and API documents in terms of pre-processing, context similarity and runtime. Experimental results show that the proposed model has high computational efficiency compared to the existing models on the large size datasets. © BEIESP.",2019,International Journal of Recent Technology and Engineering,1,natural language processing on software system usually contain high dimensional noisy and irrelevant feature @ lead to inaccurate and poor contextual similarity @ @ project source code and @ api documentation @ @ of @ traditional source code analysis model @ independent of finding and extracting @ relevant feature @ contextual similarity @ a @ size of @ project source code and @ related api documentation increase @ model incorporate @ contextual similarity @ @ source code and api documentation @ code analysis @ @ of @ best solution @ @ problem is finding @ essential feature @ @ source code dependency graph @ in @ @ @ dependency graph is used to compute @ contextual similarity computation @ @ source code metric and @ api document @ a novel contextual similarity measure is used to find @ relationship @ @ project source code metric to @ api document @ proposed model is evaluated on different project source code and api document in term of pre-processing context similarity and runtime @ experimental @ @ @ @ proposed model ha high computational efficiency compared to @ existing model on @ @ size datasets @ beiesp @ 
701,Enhanced cross-domain sentiment classification utilizing a multi-source transfer learning approach,"Online social networks have become extremely popular with the ever-increasing reachability of internet to the common person. There are millions of tweets, Facebook messages, and product reviews posted every day. Such huge amount of data presents an opportunity to analyze the sentiment of masses in order to facilitate the decision making for the betterment of society. Sentiment analysis is the research area that quantitates the opinions expressed in natural language. It is a combination of various research fields such as text mining, natural language processing, artificial intelligence, statistics. The application of supervised machine learning algorithms is limited due to the unavailability of labeled data whereas the unsupervised or lexicon-based methodologies show weak performance. This scenario sets the stage for transfer learning or cross-domain learning approaches where the knowledge is learned from the source domain which is then applied to the target domain. The proposed approach computes the feature weights by the application of cosine similarity measure to SentiWordNet and generates revised sentiment scores. Model learning is performed by support vector machine using two experimental settings, i.e., single source and multiple target domains and multiple source and single target domains (MSST). Nine benchmark datasets have been employed for performance evaluation. Best performance was obtained using the MSST settings with 85.05% accuracy, 85.01% precision, 85.10% recall, and 85.05% F-measure. State-of-the-art performance comparison proved that the cosine similarity-based transfer learning approach outperforms other approaches. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",2019,Soft Computing,4,online social network @ become extremely popular @ @ ever-increasing reachability of internet to @ common person @ @ @ million of tweet facebook message and product review posted every day @ @ huge amount of data @ @ opportunity to analyze @ sentiment of mass in order to facilitate @ decision making @ @ betterment of society @ sentiment analysis is @ research area @ quantitates @ opinion expressed in natural language @ @ is a combination of various research field @ a text mining natural language processing artificial intelligence statistic @ @ application of supervised machine learning algorithm is limited due to @ unavailability of labeled data whereas @ unsupervised @ lexicon-based methodology @ weak performance @ @ scenario set @ stage @ transfer learning @ cross-domain learning approach @ @ knowledge is learned @ @ source domain @ is @ applied to @ target domain @ @ proposed approach computes @ feature weight by @ application of cosine similarity measure to sentiwordnet and generates revised sentiment score @ model learning is performed by support vector machine @ @ experimental setting i @ e @ single source and multiple target domain and multiple source and single target domain @ msst @ @ nine benchmark datasets @ @ employed @ performance evaluation @ best performance wa obtained @ @ msst setting @ @ accuracy @ precision @ recall and @ f-measure @ state-of-the-art performance comparison proved @ @ cosine similarity-based transfer learning approach outperforms @ approach @ springer-verlag gmbh germany part of @ nature @ 
706,Sentiment analysis using rapid miner,"Now a day the data grows day by day so data mining replaced by big data. Under data mining, Text mining is one of the processes of deriving structured or quality information or data from text document. It helps to business for finding valuable knowledge. Sentiment analysis is one of the applications in text mining. In sentiment analysis, determine the emotional tone under the text. It is the major task of natural language processing. The objective of this paper to categorize the document in sentence level and review level, and classification techniques applied on the dataset (electronic product data). There is an ensemble number of classification techniques applied on the dataset. Then compare each techniques, based on various parameters and find out which one is best. According to that give better suggestions to the company for improving the product. © Blue Eyes Intelligence Engineering & Sciences Publication.",2019,International Journal of Innovative Technology and Exploring Engineering,0,now a day @ data grows day by day @ data mining replaced by big data @ @ data mining text mining is @ of @ process of deriving structured @ quality information @ data @ text document @ @ help to @ @ finding valuable knowledge @ sentiment analysis is @ of @ application in text mining @ in sentiment analysis determine @ emotional tone @ @ text @ @ is @ major task of natural language processing @ @ objective of @ @ to categorize @ document in sentence level and review level and classification technique applied on @ dataset @ electronic product data @ @ @ is @ ensemble number of classification technique applied on @ dataset @ @ compare @ technique based on various parameter and find @ @ @ is best @ according to @ give better suggestion to @ company @ improving @ product @ blue eye intelligence engineering science publication @ 
714,Understanding Big Data Through a Systematic Literature Review: The ITMI Model,"The concept of Big Data in academic and professional literature has developed in a euphoric, chaotic, and unstructured manner. Decision-making is increasingly relying on Big Data, resorting to novel analytic methodologies that are applied in many different industries. This study aims to provide clarity over the Big Data phenomenon by means of a comprehensive and systematic literature review, able to produce a clear description of what Big Data is today, a structured classification of the various streams of current research, and a list of promising emerging trends. This study analyses a corpus of 4,327 articles through a novel combination of unsupervised algorithms that produces a hierarchical topic structure which empirically validates and enhances the ""Information,"" ""Technology,"" ""Methods,"" and ""Impact"" conceptual model of Big Data, identifying 17 fundamental topics and providing researchers and practitioners with a meaningful overview of the body of knowledge and a proposed research agenda. © 2019 World Scientific Publishing Company.",2019,International Journal of Information Technology and Decision Making,3,@ concept of big data in @ and professional literature ha developed in a euphoric chaotic and unstructured manner @ decision-making is increasingly relying on big data resorting to novel analytic methodology @ @ applied in many different industry @ @ study aim to provide clarity @ @ big data phenomenon by mean of a comprehensive and systematic literature review able to produce a clear description of @ big data is today a structured classification of @ various stream of current research and a list of promising emerging trend @ @ study analysis a corpus of article @ a novel combination of unsupervised algorithm @ produce a hierarchical topic structure @ empirically validates and enhances @ @ information @ @ technology @ @ method @ and @ impact @ conceptual model of big data identifying fundamental topic and providing researcher and practitioner @ a meaningful overview of @ body of knowledge and a proposed research agenda @ world scientific publishing company @ 
715,Modeling public mood and emotion: Blog and news sentiment and socio-economic phenomena,"The development of online virtual communities has raised the importance in analyzing massive volume of text from websites and social networks. This research analyzed financial blogs and online news articles to develop a public mood dynamic prediction model for stock markets, referencing the perspectives of behavioral finance and the characteristics of online financial communities. This research applies big data and opinion mining approaches to the investors’ sentiment analysis in Taiwan. The proposed model was verified using experimental datasets from ChinaTimes.com, cnYES.com, Yahoo stock market news, and Google stock market news over an 18 month period. Empirical results indicate the big data analysis techniques to assess emotional content of commentary on current stock or financial issues can effectively forecast stock price movement. © 2017 Elsevier B.V.",2019,Future Generation Computer Systems,14,@ development of online virtual community ha raised @ importance in analyzing massive volume of text @ website and social network @ @ research analyzed financial blog and online news article to develop a public mood dynamic prediction model @ stock market referencing @ perspective of behavioral finance and @ characteristic of online financial community @ @ research applies big data and opinion mining approach to @ investor sentiment analysis in taiwan @ @ proposed model wa verified @ experimental datasets @ chinatimes @ com cnyes @ com yahoo stock market news and google stock market news @ @ month period @ empirical @ indicate @ big data analysis technique to ass emotional content of commentary on current stock @ financial issue @ effectively forecast stock price movement @ @ b @ v @ 
716,Searching Activity Trajectories with Semantics,"With the widespread use of smart phones and mobile Internet, social network users have generated massive geo-tagged tweets, photos and videos to form lots of informative trajectories which reveal not only their spatio-temporal dynamics, but also their activities in the physical world. Existing spatial trajectory query studies mainly focus on analyzing the spatio-temporal properties of the users’ trajectories, while leaving the understanding of their activities largely untouched. In this paper, we incorporate the semantics of the activity information embedded in trajectories into query modelling and processing, with the aim of providing end users more informative and meaningful results. To this end, we propose a novel trajectory query that not only considers the spatio-temporal closeness but also, more importantly, leverages a proven technique in text mining field, probabilistic topic modelling, to capture the semantic relatedness of the activities between the data and query. To support efficient query processing, we design a hierarchical grid-based index by integrating the probabilistic topic distribution on the substructures of trajectories and their spatio-temporal extent at the corresponding level of the index hierarchy. This specialized structure enables a top-down search algorithm to traverse the index while pruning unqualified trajectories in spatial and topical dimensions simultaneously. The experimental results on real-world datasets demonstrate the good efficiency and scalability performance of the proposed indices and trajectory search methods. © 2019, Springer Science+Business Media, LLC & Science Press, China.",2019,Journal of Computer Science and Technology,5,@ @ widespread use of smart phone and mobile internet social network user @ generated massive geo-tagged tweet photo and video to form lot of informative trajectory @ reveal not only @ spatio-temporal dynamic @ @ @ activity in @ physical world @ existing spatial trajectory query study mainly focus on analyzing @ spatio-temporal property of @ user trajectory @ leaving @ understanding of @ activity largely untouched @ in @ @ @ incorporate @ semantics of @ activity information embedded in trajectory @ query modelling and processing @ @ aim of providing end user more informative and meaningful @ @ to @ end @ propose a novel trajectory query @ not only considers @ spatio-temporal closeness @ @ more importantly leverage a proven technique in text mining field probabilistic topic modelling to capture @ semantic relatedness of @ activity @ @ data and query @ to support efficient query processing @ design a hierarchical grid-based index by integrating @ probabilistic topic distribution on @ substructure of trajectory and @ spatio-temporal extent at @ corresponding level of @ index hierarchy @ @ specialized structure enables a top-down search algorithm to traverse @ index @ pruning unqualified trajectory in spatial and topical dimension simultaneously @ @ experimental @ on real-world datasets demonstrate @ good efficiency and scalability performance of @ proposed index and trajectory search method @ @ science @ medium llc science @ china @ 
722,"Big Data Analytics, Text Mining and Modern English Language","The modern English Language took centuries to convert from old English. The word ‘hath’ of old English for example, has taken centuries to become ‘have’ in the modern English Language. If these changes had not been occurred there would not have been the possibility of modern words. A text written in fifteen century can be difficult to read and if we go back a couple of more centuries, it would be like reading a different language. In this paper, we have used the text mining techniques to analyze the old and modern English languages. We have introduced the Common-Words Counting algorithm that identifies common words of 15th century that diminishes gradually in the later centuries. We computed the speed of linguistic changes and identified the reasons behind them. For this purpose, 34000 text books were downloaded from Project Gutenberg of different authors, between 15th to 19th centuries. These books were categorized into five centuries in the range from 15th to 19th centuries. We selected most common words from the books of 15th century and calculated their frequencies in other centuries. We calculated the sum of Term Frequency-Inverse Document Frequency (TF-IDF) of these words and proved that frequencies of words were decreasing from 15th century to 19th century with some words even disappeared in other centuries, such as ‘doth’, ‘hath’, punt, guise and ‘selfe’. We calculated the speed of changing of words using the slope formula. We proved that the words were changing during each century with the speed of changing of words being the lowest during 16th – 17th centuries and the highest during 18th – 19th centuries which shows that the old words or their spellings were changed to the modern words during 18th – 19th centuries. The industrialization, modernization, and British Empire invasion were the key factors, which changed the old English language into modern English language. © 2018, Springer Nature B.V.",2019,Journal of Grid Computing,3,@ modern english language took century to convert @ old english @ @ word hath of old english @ example ha taken century to become @ in @ modern english language @ if @ change @ not @ occurred @ would not @ @ @ possibility of modern word @ a text written in fifteen century @ @ difficult to read and if @ go back a couple of more century @ would @ like reading a different language @ in @ @ @ @ used @ text mining technique to analyze @ old and modern english language @ @ @ introduced @ common-words counting algorithm @ identifies common word of th century @ diminishes gradually in @ later century @ @ computed @ speed of linguistic change and identified @ reason behind @ @ @ @ purpose text book @ downloaded @ project gutenberg of different author @ th to th century @ @ book @ categorized @ five century in @ range @ th to th century @ @ selected @ common word @ @ book of th century and calculated @ frequency in @ century @ @ calculated @ sum of term frequency-inverse document frequency @ tf-idf @ of @ word and proved @ frequency of word @ decreasing @ th century to th century @ some word even disappeared in @ century @ a doth hath punt guise and selfe @ @ calculated @ speed of changing of word @ @ slope formula @ @ proved @ @ word @ changing @ @ century @ @ speed of changing of word @ @ lowest @ th th century and @ highest @ th th century @ @ @ @ old word @ @ spelling @ changed to @ modern word @ th th century @ @ industrialization modernization and british empire invasion @ @ key factor @ changed @ old english language @ modern english language @ @ nature b @ v @ 
723,Learning document representation via topic-enhanced LSTM model,"Document representation plays an important role in the fields of text mining, natural language processing, and information retrieval. Traditional approaches to document representation may suffer from the disregard of the correlations or order of words in a document, due to unrealistic assumption of word independence or exchangeability. Recently, long–short-term memory (LSTM) based recurrent neural networks have been shown effective in preserving local contextual sequential patterns of words in a document, but using the LSTM model alone may not be adequate to capture global topical semantics for learning document representation. In this work, we propose a new topic-enhanced LSTM model to deal with the document representation problem. We first employ an attention-based LSTM model to generate hidden representation of word sequence in a given document. Then, we introduce a latent topic modeling layer with similarity constraint on the local hidden representation, and build a tree-structured LSTM on top of the topic layer for generating semantic representation of the document. We evaluate our model in typical text mining applications, i.e., document classification, topic detection, information retrieval, and document clustering. Experimental results on real-world datasets show the benefit of our innovations over state-of-the-art baseline methods. © 2019 Elsevier B.V.",2019,Knowledge-Based Systems,12,document representation play @ important role in @ field of text mining natural language processing and information retrieval @ traditional approach to document representation may suffer @ @ disregard of @ correlation @ order of word in a document due to unrealistic assumption of word independence @ exchangeability @ recently long short-term memory @ lstm @ based recurrent neural network @ @ @ effective in preserving local contextual sequential pattern of word in a document @ @ @ lstm model alone may not @ adequate to capture global topical semantics @ learning document representation @ in @ work @ propose a @ topic-enhanced lstm model to deal @ @ document representation problem @ @ first employ @ attention-based lstm model to generate hidden representation of word sequence in a given document @ @ @ introduce a latent topic modeling layer @ similarity constraint on @ local hidden representation and build a tree-structured lstm on top of @ topic layer @ generating semantic representation of @ document @ @ evaluate @ model in typical text mining application i @ e @ document classification topic detection information retrieval and document clustering @ experimental @ on real-world datasets @ @ benefit of @ innovation @ state-of-the-art baseline method @ @ b @ v @ 
726,A joint text mining-rank size investigation of the rhetoric structures of the US Presidents’ speeches,"This work presents a text mining context and its use for a deep analysis of the messages delivered by politicians. Specifically, we deal with an expert systems-based exploration of the rhetoric dynamics of a large collection of US Presidents’ speeches, ranging from Washington to Trump. In particular, speeches are viewed as complex expert systems whose structures can be effectively analyzed through rank-size laws. The methodological contribution of the paper is twofold. First, we develop a text mining-based procedure for the construction of the dataset by using a web scraping routine on the Miller Center website – the repository site collecting the speeches. Second, we explore the implicit structure of the discourse data by implementing a rank-size procedure over the individual speeches, being the words of each speech ranked in terms of their frequencies. The scientific significance of the proposed combination of text-mining and rank-size approaches can be found in its flexibility and generality, which let it be reproducible to a wide set of expert systems and text mining contexts. The usefulness of the proposed method and of the speeches analysis is demonstrated by the findings themselves. Indeed, in terms of impact, it is worth noting that interesting conclusions of social, political and linguistic nature on how 45 United States Presidents, from April 30, 1789 till February 28, 2017 delivered political messages can be carried out. Indeed, the proposed analysis shows some remarkable regularities, not only inside a given speech, but also among different speeches. Moreover, under a purely methodological perspective, the presented contribution suggests possible ways of generating a linguistic decision-making algorithm. © 2019 Elsevier Ltd",2019,Expert Systems with Applications,4,@ work @ a text mining context and @ use @ a deep analysis of @ message delivered by politician @ specifically @ deal @ @ expert systems-based exploration of @ rhetoric dynamic of a @ collection of u president speech ranging @ washington to trump @ in particular speech @ viewed a complex expert system whose structure @ @ effectively analyzed @ rank-size law @ @ methodological contribution of @ @ is twofold @ first @ develop a text mining-based procedure @ @ construction of @ dataset by @ a web scraping routine on @ miller center website @ repository site collecting @ speech @ second @ explore @ implicit structure of @ discourse data by implementing a rank-size procedure @ @ individual speech @ @ word of @ speech ranked in term of @ frequency @ @ scientific significance of @ proposed combination of text-mining and rank-size approach @ @ found in @ flexibility and generality @ let @ @ reproducible to a wide set of expert system and text mining context @ @ usefulness of @ proposed method and of @ speech analysis is demonstrated by @ finding @ @ indeed in term of impact @ is worth noting @ interesting conclusion of social political and linguistic nature on @ united state president @ april till february delivered political message @ @ carried @ @ indeed @ proposed analysis @ some remarkable regularity not only inside a given speech @ @ among different speech @ moreover @ a purely methodological perspective @ presented contribution suggests possible way of generating a linguistic decision-making algorithm @ @ ltd
730,Event extraction and representation: A case study for the Portuguese language,"Text information extraction is an important natural language processing (NLP) task, which aims to automatically identify, extract, and represent information from text. In this context, event extraction plays a relevant role, allowing actions, agents, objects, places, and time periods to be identified and represented. The extracted information can be represented by specialized ontologies, supporting knowledge-based reasoning and inference processes. In this work, we will describe, in detail, our proposal for event extraction from Portuguese documents. The proposed approach is based on a pipeline of specialized natural language processing tools; namely, a part-of-speech tagger, a named entities recognizer, a dependency parser, semantic role labeling, and a knowledge extraction module. The architecture is language-independent, but its modules are language-dependent and can be built using adequate AI (i.e., rule-based or machine learning) methodologies. The developed system was evaluated with a corpus of Portuguese texts and the obtained results are presented and analysed. The current limitations and future work are discussed in detail. © 2019 by the authors.",2019,Information (Switzerland),2,text information extraction is @ important natural language processing @ nlp @ task @ aim to automatically identify extract and represent information @ text @ in @ context event extraction play a relevant role allowing action agent object place and time period to @ identified and represented @ @ extracted information @ @ represented by specialized ontology supporting knowledge-based reasoning and inference process @ in @ work @ @ describe in detail @ proposal @ event extraction @ portuguese document @ @ proposed approach is based on a pipeline of specialized natural language processing tool @ namely a part-of-speech tagger a named entity recognizer a dependency parser semantic role labeling and a knowledge extraction module @ @ architecture is language-independent @ @ module @ language-dependent and @ @ built @ adequate ai @ i @ e @ rule-based @ machine learning @ methodology @ @ developed system wa evaluated @ a corpus of portuguese text and @ obtained @ @ presented and analysed @ @ current limitation and future work @ discussed in detail @ by @ author @ 
734,A feature based opinion mining for product reviews using naive bayes and k-nearest neighbor classifiers,"The explosive growth in the technology of web like social media, directs more number of people to express their sentiments, feedbacks or opinions towards the service, events, individuals, topics, products or social issues. Since the data get bigger extensively in everyday life, this makes so difficult for customers, manufactures as well as for end users of online websites to derive a conclusion on the accessibility of big data available as reviews. Opinion mining is referred to as a natural language processing technique that gives out the knowledge extraction of viewpoints or attitudes from the review texts. Feature-Based Opinion mining system aims to find the main aspects or features of a specified entity and the sentiment expressed on that entity by using natural language processing and AI techniques. Most of the studies have been conducted on aspect based opinion mining but not any of the particular works have justified to be adequate for assessing the critical or majorfactors. The major factors with respect to aspect based opinion mining are implicit or implied aspects, explicit or direct aspects and multiple aspect based. The Aspect based opinion mining by considering all these critical factors helps in analyzing the aspect of a particular entity and its sentiment more accurately. In this paper, an explicit aspect based opinion mining is carried out using naive bayes and K-nearest neighborclassifiers to generate more accurate opinions for product reviews. The end user can easily get opinions based on the particular aspect of the entity and the proposed work proves that the sentiment classification using naive bayes classifier provide with more accuracy than using K-nearest neighbor classifier. © BEIESP.",2019,International Journal of Engineering and Advanced Technology,0,@ explosive growth in @ technology of web like social medium directs more number of people to express @ sentiment feedback @ opinion towards @ service event individual topic product @ social issue @ since @ data get bigger extensively in everyday life @ make @ difficult @ customer manufacture a well a @ end user of online website to derive a conclusion on @ accessibility of big data available a review @ opinion mining is referred to a a natural language processing technique @ give @ @ knowledge extraction of viewpoint @ attitude @ @ review text @ feature-based opinion mining system aim to find @ main aspect @ feature of a specified entity and @ sentiment expressed on @ entity by @ natural language processing and ai technique @ @ of @ study @ @ conducted on aspect based opinion mining @ not @ of @ particular work @ justified to @ adequate @ assessing @ critical @ majorfactors @ @ major factor @ respect to aspect based opinion mining @ implicit @ implied aspect explicit @ direct aspect and multiple aspect based @ @ aspect based opinion mining by considering @ @ critical factor help in analyzing @ aspect of a particular entity and @ sentiment more accurately @ in @ @ @ explicit aspect based opinion mining is carried @ @ naive bayes and k-nearest neighborclassifiers to generate more accurate opinion @ product review @ @ end user @ easily get opinion based on @ particular aspect of @ entity and @ proposed work prof @ @ sentiment classification @ naive bayes classifier provide @ more accuracy @ @ k-nearest neighbor classifier @ beiesp @ 
735,A novel approach of sensitive data classification using convolution neural network and logistic regression,"Text Classification is a basic approach of text mining and natural language processing. In previous use, classifiers use human interface features like frequency base and n-gram features which are not able to find non-linearity in features and increase overlapping in features which directly impacts the performance of classifiers. In this paper, proposed convolution based approach refines the traditional features in layered approach by activation function. This process increase the effective pattern for learning which is learn by Logistic regression and optimized by boosting approach. In experiment, there is comparison of machine learning approach which uses traditional features and deep learning approach which refine the traditional approach for increasing non-linearity pattern. The results showed that proposed approach CNN-Logistic regression improves the accuracy significantly because of the improving pattern of features. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,1,text classification is a basic approach of text mining and natural language processing @ in previous use classifier use human interface feature like frequency base and n-gram feature @ @ not able to find non-linearity in feature and increase overlapping in feature @ directly impact @ performance of classifier @ in @ @ proposed convolution based approach refines @ traditional feature in layered approach by activation function @ @ process increase @ effective pattern @ learning @ is learn by logistic regression and optimized by boosting approach @ in experiment @ is comparison of machine learning approach @ us traditional feature and deep learning approach @ refine @ traditional approach @ increasing non-linearity pattern @ @ @ showed @ proposed approach cnn-logistic regression improves @ accuracy significantly @ of @ improving pattern of feature @ beiesp @ 
736,Special issue on the curative power of medical data,"With the massive amounts of medical data made available online, language technologies have proven to be indispensable in processing biomedical and molecular biology literature, health data or patient records. With huge amount of reports, evaluating their impact has long ceased to be a trivial task. Linking the contents of these documents to each other, as well as to specialized ontologies, could enable access to and the discovery of structured clinical information and could foster a major leap in natural language processing and in health research. The aim of this Special Issue, “Curative Power of Medical Data” in Data, is to gather innovative approaches for the exploitation of biomedical data using semantic web technologies and linked data by developing a community involvement in biomedical research. This Special Issue contains four surveys, which include a wide range of topics, from the analysis of biomedical articles writing style, to automatically generating tests from medical references, constructing a Gold standard biomedical corpus or the visualization of biomedical data. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.",2019,Data,0,@ @ massive amount of medical data made available online language technology @ proven to @ indispensable in processing biomedical and molecular biology literature health data @ patient record @ @ huge amount of report evaluating @ impact ha long ceased to @ a trivial task @ linking @ content of @ document to @ @ a well a to specialized ontology could enable access to and @ discovery of structured clinical information and could foster a major leap in natural language processing and in health research @ @ aim of @ special issue curative power of medical data in data is to gather innovative approach @ @ exploitation of biomedical data @ semantic web technology and linked data by developing a community involvement in biomedical research @ @ special issue contains four survey @ include a wide range of topic @ @ analysis of biomedical article writing style to automatically generating test @ medical reference constructing a gold standard biomedical corpus @ @ visualization of biomedical data @ by @ author @ licensee mdpi basel switzerland @ 
737,A comparative study on data crawling and extraction of climate change issues using machine learning technique,"Many different problems are triggered in different fields such as society, culture, and economy due to constant climate change problems. As time goes by, its influences are mounting and national attention is increasing. Therefore, it is necessary to understand various issues and improve policies on climate change. If it is possible to analyze information from media outlet data created on real time by using text mining technique, various climate change issues can be understood. In this comparative study, therefore will collect news article data related to climate change, identify issues utilizing text mining, and see complex information through the detailed analysis considering the characteristics of the text. We crawled news related to climate change issues and analyzed related keywords in terms of cause, result (phenomenon), and response. First, we extracted news related to climate change by using keyword-based document extraction method and Latent Dirichlet Allocation (LDA)-based document extraction method. In addition, we propose four related keyword analysis methods using Word2Vec, which is one of word embedding methods, and keyword frequency based method. Methods proposed in this comparative study are expected to be used in extracting and analyzing data on other specific issues not upcoming climate change issues. © BEIESP.",2019,International Journal of Innovative Technology and Exploring Engineering,1,many different problem @ triggered in different field @ a society culture and economy due to constant climate change problem @ a time go by @ influence @ mounting and national attention is increasing @ therefore @ is necessary to understand various issue and improve policy on climate change @ if @ is possible to analyze information @ medium outlet data created on real time by @ text mining technique various climate change issue @ @ understood @ in @ comparative study therefore @ collect news article data related to climate change identify issue utilizing text mining and see complex information @ @ detailed analysis considering @ characteristic of @ text @ @ crawled news related to climate change issue and analyzed related keywords in term of cause @ @ phenomenon @ and response @ first @ extracted news related to climate change by @ keyword-based document extraction method and latent dirichlet allocation @ lda @ based document extraction method @ in addition @ propose four related keyword analysis method @ word vec @ is @ of word embedding method and keyword frequency based method @ method proposed in @ comparative study @ expected to @ used in extracting and analyzing data on @ specific issue not upcoming climate change issue @ beiesp @ 
743,Task recommender system using semantic clustering to identify the right personnel,"Purpose: Text mining is growing in importance proportionate to the growth of unstructured data and its applications are increasing day by day from knowledge management to social media analysis. Mapping skillset of a candidate and requirements of job profile is crucial for conducting new recruitment as well as for performing internal task allocation in the organization. The automation in the process of selecting the candidates is essential to avoid bias or subjectivity, which may occur while shuffling through thousands of resumes and other informative documents. The system takes skillset in the form of documents to build the semantic space and then takes appraisals or resumes as input and suggests the persons appropriate to complete a task or job position and employees needing additional training. The purpose of this study is to extend the term-document matrix and achieve refined clusters to produce an improved recommendation. The study also focuses on achieving consistency in cluster quality in spite of increasing size of data set, to solve scalability issues. Design/methodology/approach: In this study, a synset-based document matrix construction method is proposed where semantically similar terms are grouped to reduce the dimension curse. An automated Task Recommendation System is proposed comprising synset-based feature extraction, iterative semantic clustering and mapping based on semantic similarity. Findings: The first step in knowledge extraction from the unstructured textual data is converting it into structured form either as Term frequency–Inverse document frequency (TF-IDF) matrix or synset-based TF-IDF. Once in structured form, a range of mining algorithms from classification to clustering can be applied. The algorithm gives a better feature vector representation and improved cluster quality. The synset-based grouping and feature extraction for resume data optimizes the candidate selection process by reducing entropy and error and by improving precision and scalability. Research limitations/implications: The productivity of any organization gets enhanced by assigning tasks to employees with a right set of skills. Efficient recruitment and task allocation can not only improve productivity but also cater to satisfy employee aspiration and identifying training requirements. Practical implications: Industries can use the approach to support different processes related to human resource management such as promotions, recruitment and training and, thus, manage the talent pool. Social implications: The task recommender system creates knowledge by following the steps of the knowledge management cycle and this methodology can be adopted in other similar knowledge management applications. Originality/value: The efficacy of the proposed approach and its enhancement is validated by carrying out experiments on the benchmarked dataset of resumes. The results are compared with existing techniques and show refined clusters. That is Absolute error is reduced by 30 per cent, precision is increased by 20 per cent and dimensions are lowered by 60 per cent than existing technique. Also, the proposed approach solves issue of scalability by producing improved recommendation for 1,000 resumes with reduced entropy. © 2019, Emerald Publishing Limited.",2019,VINE Journal of Information and Knowledge Management Systems,4,purpose @ text mining is growing in importance proportionate to @ growth of unstructured data and @ application @ increasing day by day @ knowledge management to social medium analysis @ mapping skillset of a candidate and requirement of job profile is crucial @ conducting @ recruitment a well a @ performing internal task allocation in @ organization @ @ automation in @ process of selecting @ candidate is essential to avoid bias @ subjectivity @ may occur @ shuffling @ thousand of resume and @ informative document @ @ system take skillset in @ form of document to build @ semantic space and @ take appraisal @ resume a input and suggests @ person appropriate to complete a task @ job position and employee needing additional training @ @ purpose of @ study is to extend @ term-document matrix and achieve refined cluster to produce @ improved recommendation @ @ study @ focus on achieving consistency in cluster quality in spite of increasing size of data set to solve scalability issue @ design methodology approach @ in @ study a synset-based document matrix construction method is proposed @ semantically similar term @ grouped to reduce @ dimension curse @ @ automated task recommendation system is proposed comprising synset-based feature extraction iterative semantic clustering and mapping based on semantic similarity @ finding @ @ first step in knowledge extraction @ @ unstructured textual data is converting @ @ structured form either a term frequency inverse document frequency @ tf-idf @ matrix @ synset-based tf-idf @ @ in structured form a range of mining algorithm @ classification to clustering @ @ applied @ @ algorithm give a better feature vector representation and improved cluster quality @ @ synset-based grouping and feature extraction @ resume data optimizes @ candidate selection process by reducing entropy and error and by improving precision and scalability @ research limitation implication @ @ productivity of @ organization get enhanced by assigning task to employee @ a right set of skill @ efficient recruitment and task allocation @ not only improve productivity @ @ cater to satisfy employee aspiration and identifying training requirement @ practical implication @ industry @ use @ approach to support different process related to human resource management @ a promotion recruitment and training and thus manage @ talent pool @ social implication @ @ task recommender system creates knowledge by following @ step of @ knowledge management cycle and @ methodology @ @ adopted in @ similar knowledge management application @ originality value @ @ efficacy of @ proposed approach and @ enhancement is validated by carrying @ experiment on @ benchmarked dataset of resume @ @ @ @ compared @ existing technique and @ refined cluster @ @ is absolute error is reduced by per cent precision is increased by per cent and dimension @ lowered by per cent @ existing technique @ @ @ proposed approach solves issue of scalability by producing improved recommendation @ resume @ reduced entropy @ emerald publishing limited @ 
747,Overview on Cache Transition Techniques For Semantic Graph Parsing For Optimizing Search Process Using Text Mining,"This paper elaborates the transition system that gives the standard transition-based dependency parsing techniques for generating the graph. It is essential to know the standard transition techniques for all graphical problems. Cache transition technique plays a vital role in optimizing the search process in various text mining applications. This paper provides an overview on cache transition technique for parsing semantic graphs for several Natural Language Processing (NLP) applications. According to this paper, the cache is having the fixed size m, by tree decomposition theory according to which there is a relationship between the parameter m and class of graphs produced by the theory. © Blue Eyes Intelligence Engineering & Sciences Publication.",2019,International Journal of Innovative Technology and Exploring Engineering,0,@ @ elaborates @ transition system @ give @ standard transition-based dependency parsing technique @ generating @ graph @ @ is essential to know @ standard transition technique @ @ graphical problem @ cache transition technique play a vital role in optimizing @ search process in various text mining application @ @ @ provides @ overview on cache transition technique @ parsing semantic graph @ several natural language processing @ nlp @ application @ according to @ @ @ cache is @ @ fixed size @ by tree decomposition theory according to @ @ is a relationship @ @ parameter @ and class of graph produced by @ theory @ blue eye intelligence engineering science publication @ 
749,Online visibility of software-related web sites: The case of biomedical text mining tools,"Internet, in general, and the WWW, in particular, have become an immediate, practical means of introducing software tools and resources, and most importantly, a key vehicle to attract the attention of the potential users. In this scenario, content organization as well as different development practices may affect the online visibility of the target resource. Therefore, the careful selection, organization and presentation of contents are critical to guarantee that the main features of the target tool can be easily discovered by potential visitors, while ensuring a proper indexation by automatic online systems and resource recognizers. Understanding how software is depicted in scientific manuscripts and comparing these texts with the corresponding online descriptions can help to improve the visibility of the target website. It is particularly relevant to be able to align online descriptions and those found in literature, and use the resulting knowledge to improve software indexing and grouping. Therefore, this paper presents a novel method for formally defining and mining software-related websites and related literature with the ultimate aim of improving the global online visibility of the software. As a proof of concept, the method was used to evaluate the online visibility of biomedical text mining tools. These tools have evolved considerably in the last decades, and are gathering together a heterogeneous development community as well as various user groups. For the most part, these tools are not easily discovered via general search engines. Hence, the proposed method enabled the identification of specific issues regarding the visibility of these online contents and the discussion of some possible improvements. © 2018 Elsevier Ltd",2019,Information Processing and Management,1,internet in general and @ www in particular @ become @ immediate practical mean of introducing software tool and resource and @ importantly a key vehicle to attract @ attention of @ potential user @ in @ scenario content organization a well a different development practice may affect @ online visibility of @ target resource @ therefore @ careful selection organization and presentation of content @ critical to guarantee @ @ main feature of @ target tool @ @ easily discovered by potential visitor @ ensuring a proper indexation by automatic online system and resource recognizers @ understanding @ software is depicted in scientific manuscript and comparing @ text @ @ corresponding online description @ help to improve @ visibility of @ target website @ @ is particularly relevant to @ able to align online description and @ found in literature and use @ resulting knowledge to improve software indexing and grouping @ therefore @ @ @ a novel method @ formally defining and mining software-related website and related literature @ @ ultimate aim of improving @ global online visibility of @ software @ a a proof of concept @ method wa used to evaluate @ online visibility of biomedical text mining tool @ @ tool @ evolved considerably in @ last decade and @ gathering together a heterogeneous development community a well a various user group @ @ @ @ part @ tool @ not easily discovered via general search engine @ hence @ proposed method enabled @ identification of specific issue regarding @ visibility of @ online content and @ discussion of some possible improvement @ @ ltd
750,VecText: Converting documents to vectors,"This paper introduces a software application VecText that is used to convert raw text data into a structured format suitable for various data mining tools. VecText supports most of the common operations needed for text data preprocessing as well as not very usual functions. Its graphical user interface enables user-friendly software employment without requiring specialized technical skills and knowledge of a particular programming language together with its library names and functions. The command line interface mode, where the options are specified using the command line parameters, enables incorporating the application into a more complicated data mining process integrating several software packages or performing multiple conversions in a batch. Besides introducing the tool, the paper also summarizes various techniques that are being applied when deriving a structured representation of texts in the form of a document-term matrix and compares several popular text mining frameworks and tools. © International Association of Engineers.",2019,IAENG International Journal of Computer Science,1,@ @ introduces a software application vectext @ is used to convert raw text data @ a structured format suitable @ various data mining tool @ vectext support @ of @ common operation needed @ text data preprocessing a well a not @ usual function @ @ graphical user interface enables user-friendly software employment without requiring specialized technical skill and knowledge of a particular programming language together @ @ library name and function @ @ command line interface mode @ @ option @ specified @ @ command line parameter enables incorporating @ application @ a more complicated data mining process integrating several software package @ performing multiple conversion in a batch @ besides introducing @ tool @ @ @ summarizes various technique @ @ @ applied @ deriving a structured representation of text in @ form of a document-term matrix and compare several popular text mining framework and tool @ international association of engineer @ 
754,Integrating LSA-based hierarchical conceptual space and machine learning methods for leveling the readability of domain-specific texts,"Text readability assessment is a challenging interdisciplinary endeavor with rich practical implications. It has long drawn the attention of researchers internationally, and the readability models since developed have been widely applied to various fields. Previous readability models have only made use of linguistic features employed for general text analysis and have not been sufficiently accurate when used to gauge domain-specific texts. In view of this, this study proposes a latent-semantic-analysis (LSA)-constructed hierarchical conceptual space that can be used to train a readability model to accurately assess domain-specific texts. Compared with a baseline reference using a traditional model, the new model improves by 13.88% to achieve 68.98% of accuracy when leveling social science texts, and by 24.61% to achieve 73.96% of accuracy when assessing natural science texts. We then combine the readability features developed for the current study with general linguistic features, and the accuracy of leveling social science texts improves by an even higher degree of 31.58% to achieve 86.68%, and that of natural science texts by 26.56% to achieve 75.91%. These results indicate that the readability features developed in this study can be used both to train a readability model for leveling domain-specific texts and also in combination with the more common linguistic features to enhance the efficacy of the model. Future research can expand the generalizability of the model by assessing texts from different fields and grade levels using the proposed method, thus enhancing the practical applications of this new method. © 2019 Cambridge University Press.",2019,Natural Language Engineering,7,text readability assessment is a challenging interdisciplinary endeavor @ rich practical implication @ @ ha long drawn @ attention of researcher internationally and @ readability model since developed @ @ widely applied to various field @ previous readability model @ only made use of linguistic feature employed @ general text analysis and @ not @ sufficiently accurate @ used to gauge domain-specific text @ in view of @ @ study proposes a latent-semantic-analysis @ lsa @ constructed hierarchical conceptual space @ @ @ used to train a readability model to accurately ass domain-specific text @ compared @ a baseline reference @ a traditional model @ @ model improves by @ to achieve @ of accuracy @ leveling social science text and by @ to achieve @ of accuracy @ assessing natural science text @ @ @ combine @ readability feature developed @ @ current study @ general linguistic feature and @ accuracy of leveling social science text improves by @ even higher degree of @ to achieve @ and @ of natural science text by @ to achieve @ @ @ @ indicate @ @ readability feature developed in @ study @ @ used @ to train a readability model @ leveling domain-specific text and @ in combination @ @ more common linguistic feature to enhance @ efficacy of @ model @ future research @ expand @ generalizability of @ model by assessing text @ different field and grade level @ @ proposed method thus enhancing @ practical application of @ @ method @ cambridge university @ @ 
760,The asymmetric effect of review valence on numerical rating: A viewpoint from a sentiment analysis of users of TripAdvisor,"Purpose: The basic assumption is that there is a symmetric relationship between review valence and rating, but what if review valence and rating were linked asymmetrically? There are few studies which have investigated the situations in which positive and negative online reviews exert different influences on ratings. This study considers brand strength as having an important moderating role because the average rating of existing reviews for a particular product is a heuristic cue for decision makers. Thus, the purpose of this paper is to argue that an asymmetric relationship between review content valence and numerical rating will depend on brand strength. Design/methodology/approach: The authors have conducted a sentiment analysis via text mining, using self-developed computer programs to retrieve a data set from the TripAdvisor website. Findings: This study finds there is an asymmetric relationship between review valence (verbal) and numerical rating. The authors further find brand strength to have an important moderating role. For a stronger brand, negative review content will have a greater impact on numerical ratings than positive review content, while for a weaker brand, positive review content will have a greater impact on numerical ratings than negative review content. Practical implications: Marketers could adopt sentiment analysis via text mining of online reviews as a valid measure or predictor of consumer satisfaction or numerical ratings. Strong brands should direct more attention to negative reviews, because in such reviews the negative impact transcends the positive. In contrast, weak brands should aim to exploit as many positive reviews as possible to minimize the impact of any negative reviews. Originality/value: This study finds there is an asymmetric relationship between review valence (verbal) and numerical rating and considers brand strength to play an important moderating role. The authors have used real data from the TripAdvisor website, which allow people to express themselves in an unsolicited manner, and linked these with the results from the sentiment analysis. © 2018, Emerald Publishing Limited.",2019,Online Information Review,9,purpose @ @ basic assumption is @ @ is a symmetric relationship @ review valence and rating @ @ if review valence and rating @ linked asymmetrically @ @ @ @ study @ @ investigated @ situation in @ positive and negative online review exert different influence on rating @ @ study considers brand strength a @ @ important moderating role @ @ average rating of existing review @ a particular product is a heuristic cue @ decision maker @ thus @ purpose of @ @ is to argue @ @ asymmetric relationship @ review content valence and numerical rating @ depend on brand strength @ design methodology approach @ @ author @ conducted a sentiment analysis via text mining @ self-developed computer program to retrieve a data set @ @ tripadvisor website @ finding @ @ study find @ is @ asymmetric relationship @ review valence @ verbal @ and numerical rating @ @ author @ find brand strength to @ @ important moderating role @ @ a stronger brand negative review content @ @ a greater impact on numerical rating @ positive review content @ @ a weaker brand positive review content @ @ a greater impact on numerical rating @ negative review content @ practical implication @ marketer could adopt sentiment analysis via text mining of online review a a valid measure @ predictor of consumer satisfaction @ numerical rating @ strong brand @ direct more attention to negative review @ in @ review @ negative impact transcends @ positive @ in contrast weak brand @ aim to exploit a many positive review a possible to minimize @ impact of @ negative review @ originality value @ @ study find @ is @ asymmetric relationship @ review valence @ verbal @ and numerical rating and considers brand strength to play @ important moderating role @ @ author @ used real data @ @ tripadvisor website @ allow people to express @ in @ unsolicited manner and linked @ @ @ @ @ @ sentiment analysis @ emerald publishing limited @ 
765,Automatically solving two-variable linear algebraic word problems using text mining,"The teaching and learning of algebraic word problems is a basic component of elementary education. Recently, to facilitate its learning, a few approaches for automatically solving algebraic and arithmetic word problems have been proposed. These systems generally use either natural language processing (NLP) or a combination of NLP and machine learning. However, they have low accuracy due to their large feature sets, extracted using limited preprocessing techniques. In this research work, we propose a template-based approach that was developed by following a two-step process. In the first step, we predict an equation template from a training dataset using NLP and a classification mechanism. The next step is to instantiate the predicted template with nouns and numbers through reasoning. To validate the proposed methodology, a prototype system was implemented. We then compared the proposed system with the existing systems using their respective datasets and the proposed dataset. The experimental results show improvement in accuracy, with an average precision of 80.6% and average recall of 83.5%. © 2018 John Wiley & Sons, Ltd",2019,Expert Systems,1,@ teaching and learning of algebraic word problem is a basic component of elementary education @ recently to facilitate @ learning a @ approach @ automatically solving algebraic and arithmetic word problem @ @ proposed @ @ system generally use either natural language processing @ nlp @ @ a combination of nlp and machine learning @ however @ @ low accuracy due to @ @ feature set extracted @ limited preprocessing technique @ in @ research work @ propose a template-based approach @ wa developed by following a two-step process @ in @ first step @ predict @ equation template @ a training dataset @ nlp and a classification mechanism @ @ next step is to instantiate @ predicted template @ noun and number @ reasoning @ to validate @ proposed methodology a prototype system wa implemented @ @ @ compared @ proposed system @ @ existing system @ @ respective datasets and @ proposed dataset @ @ experimental @ @ improvement in accuracy @ @ average precision of @ and average recall of @ @ john wiley son ltd
769,Extracting patterns from Twitter to promote biking,"Emphasis on non-motorized travel modes (for example, biking) reduces motorized trips and provides positive effects on the environment and the quality of human life. Understanding factors that influence people to biking or bike commuting can help decision makers, transportation planners, and bike commuting networks. Historically, conventional methods like surveys and crash data analyses were conducted to understand relevant factors. Survey and crash data analysis are difficult to perform in broad scale due to data availability and efforts. An innovative approach to determining these factors is to conduct social media mining to understand sentiments or motivations of bike commuters. People use terms (with hashtag at the beginning of the term) in Twitter, a popular social media network, to express their thoughts, activities or information. This study developed a framework for using Twitter data in understating the sentiments of the bikers with minimal effort. In this study, Twitter data associated with bike commuting hashtags were obtained for eight years (2009–2016). This study provided a framework of data collection and application of various natural language processing (NLP) tools (for example, text mining, sentiment analysis) to extract knowledge from the unstructured text data. Findings show that biking is associated with weather and seasonal patterns. The general sentiment towards biking is positive. However, negative sentiments are associated with bad weather, crime, and other challenges. The polarity scores indicate somewhat positiveness in the recent few years. The developed framework and the findings of this study will help planners and decision makers to promote biking on a broader scale. © 2018 International Association of Traffic and Safety Sciences",2019,IATSS Research,13,emphasis on non-motorized travel mode @ @ example biking @ reduces motorized trip and provides positive effect on @ environment and @ quality of human life @ understanding factor @ influence people to biking @ bike commuting @ help decision maker transportation planner and bike commuting network @ historically conventional method like survey and crash data analysis @ conducted to understand relevant factor @ survey and crash data analysis @ difficult to perform in broad scale due to data availability and effort @ @ innovative approach to determining @ factor is to conduct social medium mining to understand sentiment @ motivation of bike commuter @ people use term @ @ hashtag at @ beginning of @ term @ in twitter a popular social medium network to express @ thought activity @ information @ @ study developed a framework @ @ twitter data in understating @ sentiment of @ bikers @ minimal effort @ in @ study twitter data associated @ bike commuting hashtags @ obtained @ eight year @ @ @ @ study provided a framework of data collection and application of various natural language processing @ nlp @ tool @ @ example text mining sentiment analysis @ to extract knowledge @ @ unstructured text data @ finding @ @ biking is associated @ weather and seasonal pattern @ @ general sentiment towards biking is positive @ however negative sentiment @ associated @ bad weather crime and @ challenge @ @ polarity score indicate somewhat positiveness in @ recent @ year @ @ developed framework and @ finding of @ study @ help planner and decision maker to promote biking on a broader scale @ international association of traffic and safety science
774,Predicting social emotions from readers' perspective,"Due to the rapid development of Web, large numbers of documents assigned by readers' emotions have been generated through new portals. Comparing to the previous studies which focused on author's perspective, our research focuses on readers' emotions invoked by news articles. Our research provides meaningful assistance in social media application such as sentiment retrieval, opinion summarization and election prediction. In this paper, we predict the readers' emotion of news based on the social opinion network. More specifically, we construct the opinion network based on the semantic distance. The communities in the news network indicate specific events which are related to the emotions. Therefore, the opinion network serves as the lexicon between events and corresponding emotions. We leverage neighbor relationship in network to predict readers' emotions. As a result, our methods obtain better result than the state-of-the-art methods. Moreover, we developed a growing strategy to prune the network for practical application. The experiment verifies the rationality of the reduction for application. © 2010-2012 IEEE.",2019,IEEE Transactions on Affective Computing,7,due to @ rapid development of web @ number of document assigned by reader @ emotion @ @ generated @ @ portal @ comparing to @ previous study @ focused on author @ s perspective @ research focus on reader @ emotion invoked by news article @ @ research provides meaningful assistance in social medium application @ a sentiment retrieval opinion summarization and election prediction @ in @ @ @ predict @ reader @ emotion of news based on @ social opinion network @ more specifically @ construct @ opinion network based on @ semantic distance @ @ community in @ news network indicate specific event @ @ related to @ emotion @ therefore @ opinion network serf a @ lexicon @ event and corresponding emotion @ @ leverage neighbor relationship in network to predict reader @ emotion @ a a @ @ method obtain better @ @ @ state-of-the-art method @ moreover @ developed a growing strategy to prune @ network @ practical application @ @ experiment verifies @ rationality of @ reduction @ application @ @ @ 
779,Associative Feature Information Extraction Using Text Mining from Health Big Data,"With the development of big data computing technology, most documents in various areas, including politics, economics, society, culture, life, and public health, have been digitalized. The structure of conventional documents differs according to their authors or the organization that generated them. Therefore, policies and studies related to their efficient digitalization and use exist. Text mining is the technology used to classify, cluster, extract, search, and analyze data to find patterns or features in a set of unstructured or structured documents written in natural language. In this paper, a method for extracting associative feature information using text mining from health big data is proposed. Using health documents as raw data, health big data are created by means of the Web. The useful information contained in health documents is extracted through text mining. Health documents as raw data are collected through Web scraping and then saved in a file server. The collected raw data of health documents are sentence type, and thus morphological analysis is applied to create a corpus. The file server executes stop word removal, tagging, and the analysis of polysemous words in a preprocessing procedure to create a candidate corpus. TF-C-IDF is applied to the candidate corpus to evaluate the importance of words in a set of documents. The words classified as of high importance by TF-C-IDF are included in a set of keywords, and the transactions of each document are created. Using an Apriori mining algorithm, the association rules of keywords in the created transaction are analyzed and associative keywords are generated. TF-C-IDF weights and associative keywords are extracted from health big data as associative features. The proposed method is a base technology for creating added value in the healthcare industry in the era of the 4th industrial revolution. Its evaluation in terms of F-measure and efficiency showed its performance to be high. The method is expected to contribute to healthcare big data management and information search. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Wireless Personal Communications,22,@ @ development of big data computing technology @ document in various area including politics economics society culture life and public health @ @ digitalized @ @ structure of conventional document differs according to @ author @ @ organization @ generated @ @ therefore policy and study related to @ efficient digitalization and use exist @ text mining is @ technology used to classify cluster extract search and analyze data to find pattern @ feature in a set of unstructured @ structured document written in natural language @ in @ @ a method @ extracting associative feature information @ text mining @ health big data is proposed @ @ health document a raw data health big data @ created by mean of @ web @ @ useful information contained in health document is extracted @ text mining @ health document a raw data @ collected @ web scraping and @ saved in a file server @ @ collected raw data of health document @ sentence type and thus morphological analysis is applied to create a corpus @ @ file server executes stop word removal tagging and @ analysis of polysemous word in a preprocessing procedure to create a candidate corpus @ tf-c-idf is applied to @ candidate corpus to evaluate @ importance of word in a set of document @ @ word classified a of high importance by tf-c-idf @ included in a set of keywords and @ transaction of @ document @ created @ @ @ apriori mining algorithm @ association rule of keywords in @ created transaction @ analyzed and associative keywords @ generated @ tf-c-idf weight and associative keywords @ extracted @ health big data a associative feature @ @ proposed method is a base technology @ creating added value in @ healthcare industry in @ era of @ th industrial revolution @ @ evaluation in term of f-measure and efficiency showed @ performance to @ high @ @ method is expected to contribute to healthcare big data management and information search @ @ science @ medium llc part of @ nature @ 
788,A framework for information extraction from tables in biomedical literature,"The scientific literature is growing exponentially, and professionals are no more able to cope with the current amount of publications. Text mining provided in the past methods to retrieve and extract information from text; however, most of these approaches ignored tables and figures. The research done in mining table data still does not have an integrated approach for mining that would consider all complexities and challenges of a table. Our research is examining the methods for extracting numerical (number of patients, age, gender distribution) and textual (adverse reactions) information from tables in the clinical literature. We present a requirement analysis template and an integral methodology for information extraction from tables in clinical domain that contains 7 steps: (1) table detection, (2) functional processing, (3) structural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell selection and (7) syntactic processing and extraction. Our approach performed with the F-measure ranged between 82 and 92%, depending on the variable, task and its complexity. © 2019, The Author(s).",2019,International Journal on Document Analysis and Recognition,9,@ scientific literature is growing exponentially and professional @ no more able to cope @ @ current amount of publication @ text mining provided in @ past method to retrieve and extract information @ text @ however @ of @ approach ignored table and figure @ @ research done in mining table data still doe not @ @ integrated approach @ mining @ would consider @ complexity and challenge of a table @ @ research is examining @ method @ extracting numerical @ number of patient age gender distribution @ and textual @ adverse reaction @ information @ table in @ clinical literature @ @ @ a requirement analysis template and @ integral methodology @ information extraction @ table in clinical domain @ contains step @ @ @ table detection @ @ functional processing @ @ structural processing @ @ semantic tagging @ @ pragmatic processing @ @ cell selection and @ @ syntactic processing and extraction @ @ approach performed @ @ f-measure ranged @ and depending on @ variable task and @ complexity @ @ author @ s @ @ 
790,Constructing automatic domain-specific sentiment lexicon using KNN search via terms discrimination vectors,"Web textual data content is a viable source for decision-makers’ knowledge, so are text analytic applications. Sentiment analysis (SA) is one of text mining fields, in which text is analyzed to recognize text writer implied opinion. In this paper, a new approach had been presented for automatic Arabic language sentiment lexicon constructing. Popular KNN search algorithm is utilized for this objective. Cosine distance between seeds terms and corpus terms is employed in KNN search query. Generated lexicon terms are launched from sentiment seeds and seeds terms are augmented via Arabic-specific NLP-based algorithm, which is helped to enhance seeds terms selection process. Term discrimination vector (TDV) is the main part of KNN query inputs TDV components are computed for each corpus term and it is constituted by four term weight techniques. According to the experimental results, TDV accomplished better results than TF-IDF traditional method with lower computation cost. Also, constructed lexicons outperformed premade lexicons accuracy results. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group.",2019,International Journal of Computers and Applications,5,web textual data content is a viable source @ decision-makers knowledge @ @ text analytic application @ sentiment analysis @ sa @ is @ of text mining field in @ text is analyzed to recognize text writer implied opinion @ in @ @ a @ approach @ @ presented @ automatic arabic language sentiment lexicon constructing @ popular knn search algorithm is utilized @ @ objective @ cosine distance @ seed term and corpus term is employed in knn search query @ generated lexicon term @ launched @ sentiment seed and seed term @ augmented via arabic-specific nlp-based algorithm @ is helped to enhance seed term selection process @ term discrimination vector @ tdv @ is @ main part of knn query input tdv component @ computed @ @ corpus term and @ is constituted by four term weight technique @ according to @ experimental @ tdv accomplished better @ @ tf-idf traditional method @ lower computation cost @ @ constructed lexicon outperformed premade lexicon accuracy @ @ informa uk limited trading a taylor francis group @ 
795,Identifying comparative opinions in Arabic text in social media using machine learning techniques,"Social networking sites have become an integral part of everyday life, where people interact, cooperate and quarrel with each other. Social media also encourages them to express their opinions and share their comments about their lives’ events or about the product they use. Opinions can be direct without any comparison (I like ABC phone) or they can be comparative (X-phone’s camera is better than Y-phone). Comparative opinions are useful in many applications, e.g. marketing intelligence, product benchmarking, and e-commerce. The automatic mining of comparative opinions is an important text mining problem and an area of increasing interest for different languages. This paper focuses on identification of comparative sentence from non-comparative ones in Arabic texts. A corpus was developed consisting of YouTube comments. This paper describes research experiments that aimed to apply data/text mining algorithms, natural language processing and linguistic classification for Arabic comparative text discovery. The results of these experiments along with the analysis are also presented. The results were promising reaching to 91% accuracy for the detection of comparative opinions. © 2019, Springer Nature Switzerland AG.",2019,SN Applied Sciences,4,social networking site @ become @ integral part of everyday life @ people interact cooperate and quarrel @ @ @ @ social medium @ encourages @ to express @ opinion and share @ comment @ @ life event @ @ @ product @ use @ opinion @ @ direct without @ comparison @ i like abc phone @ @ @ @ @ comparative @ x-phone s camera is better @ y-phone @ @ comparative opinion @ useful in many application e @ g @ marketing intelligence product benchmarking and e-commerce @ @ automatic mining of comparative opinion is @ important text mining problem and @ area of increasing interest @ different language @ @ @ focus on identification of comparative sentence @ non-comparative @ in arabic text @ a corpus wa developed consisting of youtube comment @ @ @ describes research experiment @ aimed to apply data text mining algorithm natural language processing and linguistic classification @ arabic comparative text discovery @ @ @ of @ experiment along @ @ analysis @ @ presented @ @ @ @ promising reaching to accuracy @ @ detection of comparative opinion @ @ nature switzerland ag @ 
797,Comments Mining With TF-IDF: The Inherent Bias and Its Removal,"Text mining have gained great momentum in recent years, with user-generated content becoming widely available. One key use is comment mining, with much attention being given to sentiment analysis and opinion mining. An essential step in the process of comment mining is text pre-processing; a step in which each linguistic term is assigned with a weight that commonly increases with its appearance in the studied text, yet is offset by the frequency of the term in the domain of interest. A common practice is to use the well-known tf-idf formula to compute these weights. This paper reveals the bias introduced by between-participants' discourse to the study of comments in social media, and proposes an adjustment. We find that content extracted from discourse is often highly correlated, resulting in dependency structures between observations in the study, thus introducing a statistical bias. Ignoring this bias can manifest in a non-robust analysis at best and can lead to an entirely wrong conclusion at worst. We propose an adjustment to tf-idf that accounts for this bias. We illustrate the effects of both the bias and correction with with seven Facebook fan pages data, covering different domains, including news, finance, politics, sport, shopping, and entertainment. © 2019 IEEE.",2019,IEEE Transactions on Knowledge and Data Engineering,20,text mining @ gained great momentum in recent year @ user-generated content becoming widely available @ @ key use is comment mining @ much attention @ given to sentiment analysis and opinion mining @ @ essential step in @ process of comment mining is text pre-processing @ a step in @ @ linguistic term is assigned @ a weight @ commonly increase @ @ appearance in @ studied text yet is offset by @ frequency of @ term in @ domain of interest @ a common practice is to use @ well-known tf-idf formula to compute @ weight @ @ @ reveals @ bias introduced by between-participants @ discourse to @ study of comment in social medium and proposes @ adjustment @ @ find @ content extracted @ discourse is often highly correlated resulting in dependency structure @ observation in @ study thus introducing a statistical bias @ ignoring @ bias @ manifest in a non-robust analysis at best and @ lead to @ entirely wrong conclusion at worst @ @ propose @ adjustment to tf-idf @ account @ @ bias @ @ illustrate @ effect of @ @ bias and correction @ @ seven facebook fan page data covering different domain including news finance politics sport shopping and entertainment @ @ @ 
800,The cultural impact on social commerce: A sentiment analysis on Yelp ethnic restaurant reviews,"In social commerce, ethnic culture plays an important role in the content and quality perception of customer reviews. This study examined Japanese restaurant reviews in English at Yelp.com and those in Japanese at Yelp.co.jp from a cross-cultural perspective. Using bilingual text mining software, we demonstrate that Japanese customers have significantly different sentiment distribution patterns on four basic attributes of dining experience (food quality, service, ambiance, and price fairness) than Western customers. These findings shed insights on how review contents and ratings may vary between local and foreign customers at multi-national social commerce platforms. Our findings fill a research gap of cultural influence in social commerce. © 2018 Elsevier B.V.",2019,Information and Management,24,in social commerce ethnic culture play @ important role in @ content and quality perception of customer review @ @ study examined japanese restaurant review in english at yelp @ com and @ in japanese at yelp @ co @ jp @ a cross-cultural perspective @ @ bilingual text mining software @ demonstrate @ japanese customer @ significantly different sentiment distribution pattern on four basic attribute of dining experience @ food quality service ambiance and price fairness @ @ western customer @ @ finding shed insight on @ review content and rating may vary @ local and foreign customer at multi-national social commerce platform @ @ finding fill a research gap of cultural influence in social commerce @ @ b @ v @ 
801,Forecasting stock market movement direction using sentiment analysis and support vector machine,"Investor sentiment plays an important role on the stock market. User-generated textual content on the Internet provides a precious source to reflect investor psychology and predicts stock prices as a complement to stock market data. This paper integrates sentiment analysis into a machine learning method based on support vector machine. Furthermore, we take the day-of-week effect into consideration and construct more reliable and realistic sentiment indexes. Empirical results illustrate that the accuracy of forecasting the movement direction of the SSE 50 Index can be as high as 89.93% with a rise of 18.6% after introducing sentiment variables. And, meanwhile, our model helps investors make wiser decisions. These findings also imply that sentiment probably contains precious information about the asset fundamental values and can be regarded as one of the leading indicators of the stock market. © 2018 IEEE",2019,IEEE Systems Journal,28,investor sentiment play @ important role on @ stock market @ user-generated textual content on @ internet provides a precious source to reflect investor psychology and predicts stock price a a complement to stock market data @ @ @ integrates sentiment analysis @ a machine learning method based on support vector machine @ furthermore @ take @ day-of-week effect @ consideration and construct more reliable and realistic sentiment index @ empirical @ illustrate @ @ accuracy of forecasting @ movement direction of @ sse index @ @ a high a @ @ a rise of @ @ introducing sentiment variable @ and meanwhile @ model help investor make wiser decision @ @ finding @ imply @ sentiment probably contains precious information @ @ asset fundamental value and @ @ regarded a @ of @ leading indicator of @ stock market @ @
802,Arabic text clustering using improved clustering algorithms with dimensionality reduction,"Arabic Text document clustering is an important aspect for providing conjectural navigation and browsing techniques by organizing massive amounts of data into a small number of defined clusters. However, Words in form of vector are used for clustering methods is often unsatisfactory as it ignores relationships between important terms. Cluster analysis separates data into groups on clusters for the purposes of improved understanding or summarization. Clustering has a long history and many techniques developed in statistics, data mining, pattern recognition and other fields. This research proposes three approaches; Unsupervised, Semi Supervised techniques and Semi Supervised with dimensionality reduction to construct a clustering based classifier for Arabic text documents. Using k-means, incremental k-means, Threshold + k-means and k-means with dimensionality reduction, after document preprocessing removing stop words and gets the root for each term in each document. Then apply a term weighting method to get the weight of each term with respect to its document. Then apply a similarity measure method to each document and its similarity with other documents. And using F-measure, entropy and support vector machine (SVM) for calculate accuracy. The datasets are online dynamic datasets that are characterized by its availability and credibility on the internet. Arabic language is a challenging language when applied in an inference based algorithm. So, selecting the appropriate dataset is a principal factor in such research. The accuracy of those methods compared with other approaches and the proposed methods shows better accuracy and fewer errors for new classification test cases. Considering that the dimension reduction process is very sensitive because increasing the ratio of reduction can destroy important terms. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Cluster Computing,12,arabic text document clustering is @ important aspect @ providing conjectural navigation and browsing technique by organizing massive amount of data @ a small number of defined cluster @ however word in form of vector @ used @ clustering method is often unsatisfactory a @ ignores relationship @ important term @ cluster analysis separate data @ group on cluster @ @ purpose of improved understanding @ summarization @ clustering ha a long history and many technique developed in statistic data mining pattern recognition and @ field @ @ research proposes three approach @ unsupervised semi supervised technique and semi supervised @ dimensionality reduction to construct a clustering based classifier @ arabic text document @ @ k-means incremental k-means threshold k-means and k-means @ dimensionality reduction @ document preprocessing removing stop word and get @ root @ @ term in @ document @ @ apply a term weighting method to get @ weight of @ term @ respect to @ document @ @ apply a similarity measure method to @ document and @ similarity @ @ document @ and @ f-measure entropy and support vector machine @ svm @ @ calculate accuracy @ @ datasets @ online dynamic datasets @ @ characterized by @ availability and credibility on @ internet @ arabic language is a challenging language @ applied in @ inference based algorithm @ @ selecting @ appropriate dataset is a principal factor in @ research @ @ accuracy of @ method compared @ @ approach and @ proposed method @ better accuracy and fewer error @ @ classification test case @ considering @ @ dimension reduction process is @ sensitive @ increasing @ ratio of reduction @ destroy important term @ @ science @ medium llc part of @ nature @ 
807,Cross-language document summarization via extraction and ranking of multiple summaries,"The task of cross-language document summarization aims to produce a summary in a target language (e.g., Chinese) for a given document set in a different source language (e.g., English). Previous studies focus on ranking and selection of translated sentences in the target language. In this paper, we propose a new framework for addressing the task by extraction and ranking of multiple summaries in the target language. First, we extract multiple candidate summaries by proposing several schemes for improving the upper-bound quality of the summaries. Then, we propose a new ensemble ranking method for ranking the candidate summaries by making use of bilingual features. Extensive experiments have been conducted on a benchmark dataset and the results verify the effectiveness of our proposed framework, which outperforms a variety of baselines, including supervised baselines. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",2019,Knowledge and Information Systems,1,@ task of cross-language document summarization aim to produce a summary in a target language @ e @ g @ chinese @ @ a given document set in a different source language @ e @ g @ english @ @ previous study focus on ranking and selection of translated sentence in @ target language @ in @ @ @ propose a @ framework @ addressing @ task by extraction and ranking of multiple summary in @ target language @ first @ extract multiple candidate summary by proposing several scheme @ improving @ upper-bound quality of @ summary @ @ @ propose a @ ensemble ranking method @ ranking @ candidate summary by making use of bilingual feature @ extensive experiment @ @ conducted on a benchmark dataset and @ @ verify @ effectiveness of @ proposed framework @ outperforms a variety of baseline including supervised baseline @ springer-verlag london ltd @ part of @ nature @ 
809,Business environmental analysis for textual data using data mining and sentence-level classification,"Purpose: The purpose of this paper is to propose a methodology to analyze a large amount of unstructured textual data into categories of business environmental analysis frameworks. Design/methodology/approach: This paper uses machine learning to classify a vast amount of unstructured textual data by category of business environmental analysis framework. Generally, it is difficult to produce high quality and massive training data for machine-learning-based system in terms of cost. Semi-supervised learning techniques are used to improve the classification performance. Additionally, the lack of feature problem that traditional classification systems have suffered is resolved by applying semantic features by utilizing word embedding, a new technique in text mining. Findings: The proposed methodology can be used for various business environmental analyses and the system is fully automated in both the training and classifying phases. Semi-supervised learning can solve the problems with insufficient training data. The proposed semantic features can be helpful for improving traditional classification systems. Research limitations/implications: This paper focuses on classifying sentences that contain the information of business environmental analysis in large amount of documents. However, the proposed methodology has a limitation on the advanced analyses which can directly help managers establish strategies, since it does not summarize the environmental variables that are implied in the classified sentences. Using the advanced summarization and recommendation techniques could extract the environmental variables among the sentences, and they can assist managers to establish effective strategies. Originality/value: The feature selection technique developed in this paper has not been used in traditional systems for business and industry, so that the whole process can be fully automated. It also demonstrates practicality so that it can be applied to various business environmental analysis frameworks. In addition, the system is more economical than traditional systems because of semi-supervised learning, and can resolve the lack of feature problem that traditional systems suffer. This work is valuable for analyzing environmental factors and establishing strategies for companies. © 2018, Emerald Publishing Limited.",2019,Industrial Management and Data Systems,5,purpose @ @ purpose of @ @ is to propose a methodology to analyze a @ amount of unstructured textual data @ category of @ environmental analysis framework @ design methodology approach @ @ @ us machine learning to classify a vast amount of unstructured textual data by category of @ environmental analysis framework @ generally @ is difficult to produce high quality and massive training data @ machine-learning-based system in term of cost @ semi-supervised learning technique @ used to improve @ classification performance @ additionally @ lack of feature problem @ traditional classification system @ suffered is resolved by applying semantic feature by utilizing word embedding a @ technique in text mining @ finding @ @ proposed methodology @ @ used @ various @ environmental analysis and @ system is fully automated in @ @ training and classifying phase @ semi-supervised learning @ solve @ problem @ insufficient training data @ @ proposed semantic feature @ @ helpful @ improving traditional classification system @ research limitation implication @ @ @ focus on classifying sentence @ contain @ information of @ environmental analysis in @ amount of document @ however @ proposed methodology ha a limitation on @ advanced analysis @ @ directly help manager establish strategy since @ doe not summarize @ environmental variable @ @ implied in @ classified sentence @ @ @ advanced summarization and recommendation technique could extract @ environmental variable among @ sentence and @ @ assist manager to establish effective strategy @ originality value @ @ feature selection technique developed in @ @ ha not @ used in traditional system @ @ and industry @ @ @ whole process @ @ fully automated @ @ @ demonstrates practicality @ @ @ @ @ applied to various @ environmental analysis framework @ in addition @ system is more economical @ traditional system @ of semi-supervised learning and @ resolve @ lack of feature problem @ traditional system suffer @ @ work is valuable @ analyzing environmental factor and establishing strategy @ company @ emerald publishing limited @ 
810,Finding eWOM customers from customer reviews,"Purpose: The purpose of this paper is to identify electronic word-of-mouth (eWOM) customers from customer reviews. Thus, firms can precisely leverage eWOM customers to increase their product sales. Design/methodology/approach: This research proposed a framework to analyze the content of consumer-generated product reviews. Specific algorithms were used to identify potential eWOM reviewers, and then an evaluation method was used to validate the relationship between product sales and the eWOM reviewers identified by the authors’ proposed method. Findings: The results corroborate that online product reviews that are made by the eWOM customers identified by the authors’ proposed method are more related to product sales than customer reviews that are made by non-eWOM customers and that the predictive power of the reviews generated by eWOM customers are significantly higher than the reviews generated by non-eWOM customers. Research limitations/implications: The proposed method is useful in the data set, which is based on one type of products. However, for other products, the validity must be tested. Previous eWOM customers may have no significant influence on product sales in the future. Therefore, the proposed method should be tested in the new market environment. Practical implications: By combining the method with the previous customer segmentation method, a new framework of customer segmentation is proposed to help firms understand customers’ value specifically. Originality/value: This study is the first to identify eWOM customers from online reviews and to evaluate the relationship between reviewers and product sales. © 2018, Emerald Publishing Limited.",2019,Industrial Management and Data Systems,2,purpose @ @ purpose of @ @ is to identify electronic word-of-mouth @ ewom @ customer @ customer review @ thus firm @ precisely leverage ewom customer to increase @ product sale @ design methodology approach @ @ research proposed a framework to analyze @ content of consumer-generated product review @ specific algorithm @ used to identify potential ewom reviewer and @ @ evaluation method wa used to validate @ relationship @ product sale and @ ewom reviewer identified by @ author proposed method @ finding @ @ @ corroborate @ online product review @ @ made by @ ewom customer identified by @ author proposed method @ more related to product sale @ customer review @ @ made by non-ewom customer and @ @ predictive power of @ review generated by ewom customer @ significantly higher @ @ review generated by non-ewom customer @ research limitation implication @ @ proposed method is useful in @ data set @ is based on @ type of product @ however @ @ product @ validity must @ tested @ previous ewom customer may @ no significant influence on product sale in @ future @ therefore @ proposed method @ @ tested in @ @ market environment @ practical implication @ by combining @ method @ @ previous customer segmentation method a @ framework of customer segmentation is proposed to help firm understand customer value specifically @ originality value @ @ study is @ first to identify ewom customer @ online review and to evaluate @ relationship @ reviewer and product sale @ emerald publishing limited @ 
816,Topic-based knowledge mining of online student reviews for strategic planning in universities,"Over the past few years, studies observe a continuous decline in university enrollment and retention rates resulting in millions of dollars in lost revenue. Past research shows that 95% of the students rely heavily on the positive word-of-mouth to select a college. Especially, with the advent of Web 2.0 tools, students are able to make informed decisions due to increased awareness through online reviews. Academic institutions can also leverage this information to understand and improve their students’ perception. The objective of this paper is to identify the current strengths, weaknesses, opportunities and threats (SWOT) of a university by analyzing online student reviews using text analytics. Our proposed approach integrates four different techniques: topic modeling, sentiment analysis, root cause and SWOT analyses. First, we introduce an ensemble of Latent Dirichlet Allocation (E-LDA) topic models to automatically identify the key features (topics) that are predominantly discussed by students and categorize each review sentence into the most related topic. We then detect the opinion associated with each sentence (positive, negative and neutral) using sentiment analysis. Finally, a topic-based opinion summary (TOS) for a university is established to identify its strengths and weaknesses from the students’ perspective, and the opportunities and threats are determined by analyzing the TOS of the competitors (or other similar institutions). A case study is used to illustrate the feasibility and application of the proposed approach. The results indicate that the proposed method provides an efficient and economic performance summary of a university and its competitors, and could help its leaders in recruitment and retention efforts. © 2018 Elsevier Ltd",2019,Computers and Industrial Engineering,14,@ @ past @ year study observe a continuous decline in university enrollment and retention rate resulting in million of dollar in lost revenue @ past research @ @ of @ student rely heavily on @ positive word-of-mouth to select a college @ especially @ @ advent of web @ tool student @ able to make informed decision due to increased awareness @ online review @ @ institution @ @ leverage @ information to understand and improve @ student perception @ @ objective of @ @ is to identify @ current strength weakness opportunity and threat @ swot @ of a university by analyzing online student review @ text analytics @ @ proposed approach integrates four different technique @ topic modeling sentiment analysis root cause and swot analysis @ first @ introduce @ ensemble of latent dirichlet allocation @ e-lda @ topic model to automatically identify @ key feature @ topic @ @ @ predominantly discussed by student and categorize @ review sentence @ @ @ related topic @ @ @ detect @ opinion associated @ @ sentence @ positive negative and neutral @ @ sentiment analysis @ finally a topic-based opinion summary @ tos @ @ a university is established to identify @ strength and weakness @ @ student perspective and @ opportunity and threat @ determined by analyzing @ tos of @ competitor @ @ @ similar institution @ @ a case study is used to illustrate @ feasibility and application of @ proposed approach @ @ @ indicate @ @ proposed method provides @ efficient and economic performance summary of a university and @ competitor and could help @ leader in recruitment and retention effort @ @ ltd
841,Who is answering whom? Finding “Reply-To” relations in group chats with deep bidirectional LSTM networks,"Social networks facilitate communication among Internet users while generating large volumes of online short-text conversations every day. This leads to a huge number of free-style asynchronous conversations where multiple users are involved and multiple topics are discussed at the same time in the same place, e.g., an instant group chat in WeChat. Here emerges an interesting problem: as a result of a large number of users and topics, the conversation structure may get into a mess, which often interferes with the acquisition of messages users are interested in. For example, when a user enters a conversation, (s)he usually does not want to read all the historical messages, but just hope to get the messages that are the most relevant to some messages (s)he cares about. Therefore, it is an essential task to understand the logical correlations among messages, which benefits text mining, natural language processing, and web intelligence techniques. In this paper, we focus on “reply-to” relations, such as Q&A between messages in group chats. At first, a model called LSTM-RT is presented to predict the “reply-to” relations between messages, which is based on deep bidirectional LSTM networks. Then, three versions of the LSTM-RT model are proposed. In detail, the first version is based on a non-siamese architecture, which processes ordered message pairs; The other two versions are end-to-end models, which are based on the word level and the sentence level, respectively. Finally, experimental results conducted on two real-world group chat data sets demonstrate the effectiveness of the proposed model. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Cluster Computing,1,social network facilitate communication among internet user @ generating @ volume of online short-text conversation every day @ @ lead to a huge number of free-style asynchronous conversation @ multiple user @ involved and multiple topic @ discussed at @ @ time in @ @ place e @ g @ @ instant group chat in wechat @ @ emerges @ interesting problem @ a a @ of a @ number of user and topic @ conversation structure may get @ a mess @ often interferes @ @ acquisition of message user @ interested in @ @ example @ a user enters a conversation @ s @ he usually doe not want to read @ @ historical message @ @ hope to get @ message @ @ @ @ relevant to some message @ s @ he care @ @ therefore @ is @ essential task to understand @ logical correlation among message @ benefit text mining natural language processing and web intelligence technique @ in @ @ @ focus on reply-to relation @ a q a @ message in group chat @ at first a model called lstm-rt is presented to predict @ reply-to relation @ message @ is based on deep bidirectional lstm network @ @ three version of @ lstm-rt model @ proposed @ in detail @ first version is based on a non-siamese architecture @ process ordered message pair @ @ @ @ version @ end-to-end model @ @ based on @ word level and @ sentence level respectively @ finally experimental @ conducted on @ real-world group chat data set demonstrate @ effectiveness of @ proposed model @ @ science @ medium llc part of @ nature @ 
853,Review on Natural Language Processing Trends and Techniques Using NLTK,"In modern age of information explosion, every day millions of gigabytes of data are generated in the form of documents, web pages, e-mail, social media text, blogs etc., so importance of effective and efficient Natural Language Processing techniques become crucial for an information retrieval system, text summarization, sentiment analysis, information extraction, named entity recognition, relationship extraction, social media monitoring, text mining, language translation program, and question answering system. Natural Language Processing is a computational technique applies different levels of linguistic analysis for representing natural language into a useful representation for further processing. NLP is recognized as a challenging task in computer science and artificial intelligence because understanding human natural language is not only depends on the words but how those words are linked together to form precise meaning is also considered. Regardless of language being one of the easiest concepts for human to learn, but for training computers to understand natural language is a difficult task due to the ambiguity of language syntax and semantics. Natural Language processing techniques involves processing documents or text which reduces storage space and also reduces the size of index and understanding the given information which satisfies user’s need. NLP techniques improve the performance of the information retrieval efficiency and effective documentation processes. Common dialect handling procedures incorporates tokenization, stop word expulsion, stemming, lemmatization, parts of discourse labeling, lumping and named substance recognizer which enhances execution of NLP applications. The Natural Language Toolkit is the best possible solution for learning the ropes of NLP domain. NLTK, a collection of application packages which encourage researchers and learners in natural language processing, computational linguistics and artificial intelligence. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,3,in modern age of information explosion every day million of gigabyte of data @ generated in @ form of document web page e-mail social medium text blog etc @ @ importance of effective and efficient natural language processing technique become crucial @ @ information retrieval system text summarization sentiment analysis information extraction named entity recognition relationship extraction social medium monitoring text mining language translation program and question answering system @ natural language processing is a computational technique applies different level of linguistic analysis @ representing natural language @ a useful representation @ @ processing @ nlp is recognized a a challenging task in computer science and artificial intelligence @ understanding human natural language is not only depends on @ word @ @ @ word @ linked together to form precise meaning is @ considered @ regardless of language @ @ of @ easiest concept @ human to learn @ @ training computer to understand natural language is a difficult task due to @ ambiguity of language syntax and semantics @ natural language processing technique involves processing document @ text @ reduces storage space and @ reduces @ size of index and understanding @ given information @ satisfies user s need @ nlp technique improve @ performance of @ information retrieval efficiency and effective documentation process @ common dialect handling procedure incorporates tokenization stop word expulsion stemming lemmatization part of discourse labeling lumping and named substance recognizer @ enhances execution of nlp application @ @ natural language toolkit is @ best possible solution @ learning @ rope of nlp domain @ nltk a collection of application package @ encourage researcher and learner in natural language processing computational linguistics and artificial intelligence @ @ nature singapore pte ltd @ 
854,Effectiveness of recent research approaches in natural language processing on data science-an insight,"With the exponentially increasing size and complexity of the data in present time, data quality has become a major concern with respect to data analytics. The potential capability of Natural Language Processing (NLP) is already known and being harnessed by various researchers to evolve up with some significant analytical process. However, there is less number of research works emphasizing on applying NLP over the data with complexity reported in current times in the area of big data. Therefore, the primary contribution of this manuscript is to review the most recent work towards NLP based approaches for data analysis where input data could be either text or non-textual too. The secondary contribution is to gauge the level of effectiveness from the existing research approach with NLP-based practices towards leveraging better data quality in data science. © Springer Nature Switzerland AG. 2019.",2019,Advances in Intelligent Systems and Computing,1,@ @ exponentially increasing size and complexity of @ data in @ time data quality ha become a major concern @ respect to data analytics @ @ potential capability of natural language processing @ nlp @ is already known and @ harnessed by various researcher to evolve up @ some significant analytical process @ however @ is le number of research work emphasizing on applying nlp @ @ data @ complexity reported in current time in @ area of big data @ therefore @ primary contribution of @ manuscript is to review @ @ recent work towards nlp based approach @ data analysis @ input data could @ either text @ non-textual too @ @ secondary contribution is to gauge @ level of effectiveness @ @ existing research approach @ nlp-based practice towards leveraging better data quality in data science @ @ nature switzerland ag @ @ 
858,Building Dengue Sensors for Brazil Using a Social Network and Text Mining,"The increasing use of Social Networks to share personal information has opened many resources to analyze the behaviour of one city, state or country. Topics related to politics, science, health alarms and others are shared by users everyday to monitor an prevent events. Natural Language Processing is the tool to analyze this text and get some insight using Machine Learning Techniques. In this work, Twitter is analyzed to detect social events because users are considered sensors. The analysis is performed over Brazilian tweets to detect dengue. The results show the utility of the proposal to recognize dengue epidemics in the Brazilian territory. © Springer Nature Switzerland AG 2019.",2019,Communications in Computer and Information Science,0,@ increasing use of social network to share personal information ha opened many resource to analyze @ behaviour of @ city state @ country @ topic related to politics science health alarm and others @ shared by user everyday to monitor @ prevent event @ natural language processing is @ tool to analyze @ text and get some insight @ machine learning technique @ in @ work twitter is analyzed to detect social event @ user @ considered sensor @ @ analysis is performed @ brazilian tweet to detect dengue @ @ @ @ @ utility of @ proposal to recognize dengue epidemic in @ brazilian territory @ @ nature switzerland ag @ 
859,Related stocks selection with data collaboration using text mining,"We propose an extended scheme for selecting related stocks for themed mutual funds. This scheme was designed to support fund managers who are building themed mutual funds. In our preliminary experiments, building a themed mutual fund was found to be quite difficult. Our scheme is a type of natural language processing method and based on words extracted according to their similarity to a theme using word2vec and our unique similarity based on co-occurrence in company information. We used data including investor relations and official websites as company information data. We also conducted several other experiments, including hyperparameter tuning, in our scheme. The scheme achieved a 172% higher F1 score and 21% higher accuracy than a standard method. Our research also showed the possibility that official websites are not necessary for our scheme, contrary to our preliminary experiments for assessing data collaboration. © 2019 by the authors.",2019,Information (Switzerland),2,@ propose @ extended scheme @ selecting related stock @ themed mutual fund @ @ scheme wa designed to support fund manager @ @ building themed mutual fund @ in @ preliminary experiment building a themed mutual fund wa found to @ quite difficult @ @ scheme is a type of natural language processing method and based on word extracted according to @ similarity to a theme @ word vec and @ unique similarity based on co-occurrence in company information @ @ used data including investor relation and official website a company information data @ @ @ conducted several @ experiment including hyperparameter tuning in @ scheme @ @ scheme achieved a higher f score and higher accuracy @ a standard method @ @ research @ showed @ possibility @ official website @ not necessary @ @ scheme contrary to @ preliminary experiment @ assessing data collaboration @ by @ author @ 
860,"The Joint International Symposium on Artificial Intelligence and Natural Language Processing, iSAI-NLP 2017","The proceedings contain 22 papers. The special focus in this conference is on Artificial Intelligence and Natural Language Processing. The topics include: Isan Dhamma Characters Segmentation and reading in Thai; ontology-assisted structural design flaw detection of object-oriented software; virtual reality application for animal cruelty education; anti-theft motorcycle system using face recognition by deep learning under concept on Internet of Things; balanced scorecard quality information dashboards model for competitive business advantage; criminal background check program with fingerprint; Jobs analysis for business intelligence skills requirements in the ASEAN region: A text mining study; the model of teenager’s internet usage behavior analysis using data mining; privacy-preserving reputation management in fully decentralized systems: Challenges and opportunities; cloud-based services for cooperative robot learning of 3D object detection and recognition; empirical testing of a technique for assessing prior knowledge in the field of research training; problem content table construction based on extracting Sym-Multi-Word-Co from texts; Unpredictable disaster evacuation guide for weak people by real-time sensor network, SNS and maps; Detection of normal and abnormal ECG signal using ANN; dynamic relation-based analysis of objective interestingness measures in association rules mining; image encryption using cellular neural network and matrix transformation; improved term weighting factors for keyword extraction in hierarchical category structure and Thai text classification; ontology-based classifiers for wikipedia article quality classification; thai Named-Entity Recognition using variational long short-term memory with conditional random field; cardiac arrhythmia classification using Hjorth Descriptors.",2019,Advances in Intelligent Systems and Computing,0,@ proceeding contain @ @ @ special focus in @ conference is on artificial intelligence and natural language processing @ @ topic include @ isan dhamma character segmentation and reading in thai @ ontology-assisted structural design flaw detection of object-oriented software @ virtual reality application @ animal cruelty education @ anti-theft motorcycle system @ face recognition by deep learning @ concept on internet of thing @ balanced scorecard quality information dashboard model @ competitive @ advantage @ criminal background check program @ fingerprint @ job analysis @ @ intelligence skill requirement in @ asean region @ a text mining study @ @ model of teenager s internet usage behavior analysis @ data mining @ privacy-preserving reputation management in fully decentralized system @ challenge and opportunity @ cloud-based service @ cooperative robot learning of @ object detection and recognition @ empirical testing of a technique @ assessing prior knowledge in @ field of research training @ problem content table construction based on extracting sym-multi-word-co @ text @ unpredictable disaster evacuation guide @ weak people by real-time sensor network sn and map @ detection of normal and abnormal ecg signal @ ann @ dynamic relation-based analysis of objective interestingness measure in association rule mining @ image encryption @ cellular neural network and matrix transformation @ improved term weighting factor @ keyword extraction in hierarchical category structure and thai text classification @ ontology-based classifier @ wikipedia article quality classification @ thai named-entity recognition @ variational long short-term memory @ conditional random field @ cardiac arrhythmia classification @ hjorth descriptor @ 
865,Text mining method for building new business strategies: Focusing on the neurosurgical robot,"In any time, it has been essential to acquire knowledge of customer needs and global trends of technological progress for proper selection and concentration strategy planning, which is decisive for long-term growth of the company. However, with the change in innovation paradigm, the methods used for its acquisition have also changed. With the era of big data, text mining that gains knowledge necessary for this planning from unstructured natural language with weak affinity with relational databases has attracted attention recently. However, in order to obtain highly accurate and reliable knowledge that can contribute to company decision-making, the current natural language processing algorithm is not sufficient. Current text mining method, which is limited to bird’s eye viewing type aimed at capturing the entire text data roughly, is unsuitable for finding out important knowledge written only in a very small part of the text data. Therefore, this paper presents the virtual case of a company planning a new neurosurgical robot project and applies pinpoint focus type text mining technique to acquiring technological knowledge from high-impact peer-reviewed academic journals. © Springer Nature Singapore Pte Ltd. 2019.",2019,Paradigm Shift in Technologies and Innovation Systems,0,in @ time @ ha @ essential to acquire knowledge of customer need and global trend of technological progress @ proper selection and concentration strategy planning @ is decisive @ long-term growth of @ company @ however @ @ change in innovation paradigm @ method used @ @ acquisition @ @ changed @ @ @ era of big data text mining @ gain knowledge necessary @ @ planning @ unstructured natural language @ weak affinity @ relational database ha attracted attention recently @ however in order to obtain highly accurate and reliable knowledge @ @ contribute to company decision-making @ current natural language processing algorithm is not sufficient @ current text mining method @ is limited to bird s eye viewing type aimed at capturing @ entire text data roughly is unsuitable @ finding @ important knowledge written only in a @ small part of @ text data @ therefore @ @ @ @ virtual case of a company planning a @ neurosurgical robot project and applies pinpoint focus type text mining technique to acquiring technological knowledge @ high-impact peer-reviewed @ journal @ @ nature singapore pte ltd @ @ 
866,A Neural Named Entity Recognition and Multi-Type Normalization Tool for Biomedical Text Mining,"The amount of biomedical literature is vast and growing quickly, and accurate text mining techniques could help researchers to efficiently extract useful information from the literature. However, existing named entity recognition models used by text mining tools such as tmTool and ezTag are not effective enough, and cannot accurately discover new entities. Also, the traditional text mining tools do not consider overlapping entities, which are frequently observed in multi-type named entity recognition results. We propose a neural biomedical named entity recognition and multi-type normalization tool called BERN. The BERN uses high-performance BioBERT named entity recognition models which recognize known entities and discover new entities. Also, probability-based decision rules are developed to identify the types of overlapping entities. Furthermore, various named entity normalization models are integrated into BERN for assigning a distinct identifier to each recognized entity. The BERN provides a Web service for tagging entities in PubMed articles or raw text. Researchers can use the BERN Web service for their text mining tasks, such as new named entity discovery, information retrieval, question answering, and relation extraction. The application programming interfaces and demonstrations of BERN are publicly available at https://bern.korea.ac.kr. © 2013 IEEE.",2019,IEEE Access,11,@ amount of biomedical literature is vast and growing quickly and accurate text mining technique could help researcher to efficiently extract useful information @ @ literature @ however existing named entity recognition model used by text mining tool @ a tmtool and eztag @ not effective enough and cannot accurately discover @ entity @ @ @ traditional text mining tool @ not consider overlapping entity @ @ frequently observed in multi-type named entity recognition @ @ @ propose a neural biomedical named entity recognition and multi-type normalization tool called bern @ @ bern us high-performance biobert named entity recognition model @ recognize known entity and discover @ entity @ @ probability-based decision rule @ developed to identify @ type of overlapping entity @ furthermore various named entity normalization model @ integrated @ bern @ assigning a distinct identifier to @ recognized entity @ @ bern provides a web service @ tagging entity in pubmed article @ raw text @ researcher @ use @ bern web service @ @ text mining task @ a @ named entity discovery information retrieval question answering and relation extraction @ @ application programming interface and demonstration of bern @ publicly available at http @ bern @ korea @ ac @ kr @ @ @ 
868,Keyphrase Extraction from Modern Standard Arabic Texts Based on Association Rules,"Keywords or Keyphrases constitute a very important kind of concepts which can be extracted from texts. They reflect the semantic contained in these texts and are useful in many tasks of Information Retrieval, Text mining and Natural Language Processing. Their extraction is a challenging problem to which researchers have an active interest. In this paper, an approach based on the Association Rules model is described for extracting keyphrases from modern standard Arabic texts. The experiments done and the results obtained are promising: the performance values of the proposed system (in terms of precision, recall and f-score) are higher than 60% and can exceed 70%. © 2019, Springer Nature Switzerland AG.",2019,Communications in Computer and Information Science,1,keywords @ keyphrases constitute a @ important kind of concept @ @ @ extracted @ text @ @ reflect @ semantic contained in @ text and @ useful in many task of information retrieval text mining and natural language processing @ @ extraction is a challenging problem to @ researcher @ @ active interest @ in @ @ @ approach based on @ association rule model is described @ extracting keyphrases @ modern standard arabic text @ @ experiment done and @ @ obtained @ promising @ @ performance value of @ proposed system @ in term of precision recall and f-score @ @ higher @ and @ exceed @ @ nature switzerland ag @ 
870,A data-driven approach to automatic extraction of professional figure profiles from Résumés,"The process of selecting and interviewing suitable candidates for a job position is time-consuming and labour-intensive. Despite the existence of software applications aimed at helping professional recruiters in the process, only recently with Industry 4.0 there has been a real interest in implementing autonomous and data-driven approaches that can provide insights and practical assistance to recruiters. In this paper, we propose a framework that is aimed at improving the performances of an Applicant Tracking System. More specifically, we exploit advanced Natural Language Processing and Text Mining techniques to automatically profile resources (i.e. candidates for a job) and offers by extracting relevant keywords and building a semantic representation of résumés and job opportunities. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ process of selecting and interviewing suitable candidate @ a job position is time-consuming and labour-intensive @ despite @ existence of software application aimed at helping professional recruiter in @ process only recently @ industry @ @ ha @ a real interest in implementing autonomous and data-driven approach @ @ provide insight and practical assistance to recruiter @ in @ @ @ propose a framework @ is aimed at improving @ performance of @ applicant tracking system @ more specifically @ exploit advanced natural language processing and text mining technique to automatically profile resource @ i @ e @ candidate @ a job @ and offer by extracting relevant keywords and building a semantic representation of résumés and job opportunity @ @ nature switzerland ag @ 
873,Medicinal plant information extraction system—A text mining-based approach,"In this paper, we have discussed on applying text mining techniques to extract information on health benefits of medicinal plant from text article. The presence of multi-term phrases and complex sentences, i.e., the sentences that include clauses, implicit semantic relations in a text article, has always raised the complexity of information mining process. We have proposed a simple pattern-based semi-supervised approach powered by NLP techniques to deal with these issues. We have evaluated our methodology for a set of web documents on medicinal plants. Performance (in terms of recall) of our method was observed to considerably higher than existing relation extraction methodologies. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,1,in @ @ @ @ discussed on applying text mining technique to extract information on health benefit of medicinal plant @ text article @ @ presence of multi-term phrase and complex sentence i @ e @ @ sentence @ include clause implicit semantic relation in a text article ha always raised @ complexity of information mining process @ @ @ proposed a simple pattern-based semi-supervised approach powered by nlp technique to deal @ @ issue @ @ @ evaluated @ methodology @ a set of web document on medicinal plant @ performance @ in term of recall @ of @ method wa observed to considerably higher @ existing relation extraction methodology @ @ nature singapore pte ltd @ 
875,Jointly Extract Entities and Their Relations from Biomedical Text,"Entity recognition and relation extraction have become an important part of knowledge acquisition, and which have been widely applied in various fields, such as Bioinformatics. However, prior state-of-the-art extraction models heavily rely on the external features obtained from hand-craft or natural language processing (NLP) tools. As a result, the performance of models depends directly on the accuracy of the obtained features. Moreover, current joint extraction approaches cannot effectively tackle the multi-head problem (i.e. an entity is related to multiple entities). In this paper, we firstly present a novel tagging scheme and then propose a joint approach based deep neural network for producing unique tagging sequences. Our approach can not only simultaneously perform entity resolution and relation extraction without any external features, but also effectively solve the multi-head problem. Besides, since arbitrary tokens may provide important cues for two components, we exploit self-attention to explicitly capture long-range dependencies among them and character embeddings to learn the features of lexical morphology, which make our method less susceptible to cascading errors. The results demonstrate that the joint method proposed outperforms the other state-of-the-art joint models. Our work is beneficial for biomedical text mining, and the construction of the biomedical knowledge base. © 2013 IEEE.",2019,IEEE Access,1,entity recognition and relation extraction @ become @ important part of knowledge acquisition and @ @ @ widely applied in various field @ a bioinformatics @ however prior state-of-the-art extraction model heavily rely on @ external feature obtained @ hand-craft @ natural language processing @ nlp @ tool @ a a @ @ performance of model depends directly on @ accuracy of @ obtained feature @ moreover current joint extraction approach cannot effectively tackle @ multi-head problem @ i @ e @ @ entity is related to multiple entity @ @ in @ @ @ firstly @ a novel tagging scheme and @ propose a joint approach based deep neural network @ producing unique tagging sequence @ @ approach @ not only simultaneously perform entity resolution and relation extraction without @ external feature @ @ effectively solve @ multi-head problem @ besides since arbitrary token may provide important cue @ @ component @ exploit self-attention to explicitly capture long-range dependency among @ and character embeddings to learn @ feature of lexical morphology @ make @ method le susceptible to cascading error @ @ @ demonstrate @ @ joint method proposed outperforms @ @ state-of-the-art joint model @ @ work is beneficial @ biomedical text mining and @ construction of @ biomedical knowledge base @ @ @ 
876,Topic Modeling Technique for Text Mining over Biomedical Text Corpora through Hybrid Inverse Documents Frequency and Fuzzy K-Means Clustering,"Text data plays an imperative role in the biomedical domain. As patient's data comprises of a huge amount of text documents in a non-standardized format. In order to obtain the relevant data, the text documents pose a lot of challenging issues for data processing. Topic modeling is one of the popular techniques for information retrieval based on themes from the biomedical documents. In topic modeling discovering the precise topics from the biomedical documents is a challenging task. Furthermore, in biomedical text documents, the redundancy puts a negative impact on the quality of text mining as well. Therefore, the rapid growth of unstructured documents entails machine learning techniques for topic modeling capable of discovering precise topics. In this paper, we proposed a topic modeling technique for text mining through hybrid inverse document frequency and machine learning fuzzy k-means clustering algorithm. The proposed technique ameliorates the redundancy issue and discovers precise topics from the biomedical text documents. The proposed technique generates local and global term frequencies through the bag-of-words (BOW) model. The global term weighting is calculated through the proposed hybrid inverse documents frequency and Local term weighting is computed with term frequency. The robust principal component analysis is used to remove the negative impact of higher dimensionality on the global term weights. Afterward, the classification and clustering for text mining are performed with a probability of topics in the documents. The classification is performed through discriminant analysis classifier whereas the clustering is done through the k-means clustering. The performance of clustering is evaluated with Calinsiki-Har-abasz (CH) index internal validation method. The proposed toping modeling technique is evaluated on six standard datasets namely Ohsumed, MuchMore Springer Corpus, GENIA corpus, Bioxtext, tweets and WSJ redundant corpus for experimentation. The proposed topic modeling technique exhibits high performance on classification and clustering in text mining compared to baseline topic models like FLSA, LDA, and LSA. Moreover, the execution time of the proposed topic modeling technique remains stable for different numbers of topics. © 2019 IEEE.",2019,IEEE Access,5,text data play @ imperative role in @ biomedical domain @ a patient @ s data comprises of a huge amount of text document in a non-standardized format @ in order to obtain @ relevant data @ text document pose a lot of challenging issue @ data processing @ topic modeling is @ of @ popular technique @ information retrieval based on theme @ @ biomedical document @ in topic modeling discovering @ precise topic @ @ biomedical document is a challenging task @ furthermore in biomedical text document @ redundancy put a negative impact on @ quality of text mining a well @ therefore @ rapid growth of unstructured document entail machine learning technique @ topic modeling capable of discovering precise topic @ in @ @ @ proposed a topic modeling technique @ text mining @ hybrid inverse document frequency and machine learning fuzzy k-means clustering algorithm @ @ proposed technique ameliorates @ redundancy issue and discovers precise topic @ @ biomedical text document @ @ proposed technique generates local and global term frequency @ @ bag-of-words @ bow @ model @ @ global term weighting is calculated @ @ proposed hybrid inverse document frequency and local term weighting is computed @ term frequency @ @ robust principal component analysis is used to remove @ negative impact of higher dimensionality on @ global term weight @ afterward @ classification and clustering @ text mining @ performed @ a probability of topic in @ document @ @ classification is performed @ discriminant analysis classifier whereas @ clustering is done @ @ k-means clustering @ @ performance of clustering is evaluated @ calinsiki-har-abasz @ ch @ index internal validation method @ @ proposed toping modeling technique is evaluated on six standard datasets namely ohsumed muchmore @ corpus genia corpus bioxtext tweet and wsj redundant corpus @ experimentation @ @ proposed topic modeling technique exhibit high performance on classification and clustering in text mining compared to baseline topic model like flsa lda and lsa @ moreover @ execution time of @ proposed topic modeling technique remains stable @ different number of topic @ @ @ 
880,Skip-gram-KR: Korean word embedding for semantic clustering,"Deep learning algorithms are used in various applications for pattern recognition, natural language processing, speech recognition, and so on. Recently, neural network-based natural language processing techniques use fixed length word embedding. Word embedding is a method of digitizing a word at a specific position into a low-dimensional dense vector with fixed length while preserving the similarity of the distribution of its surrounding words. Currently, the word embedding methods for foreign language are used for Korean words; however, existing word embedding methods are developed for English originally, so they do not reflect the order and structure of the Korean words. In this paper, we propose a word embedding method for Korean, which is called Skip-gram-KR, and a Korean affix tokenizer. Skip-gram-KR creates similar word training data through backward mapping and the two-word skipping method. The experiment results show the proposed method achieved the most accurate performance. © 2019 IEEE.",2019,IEEE Access,1,deep learning algorithm @ used in various application @ pattern recognition natural language processing speech recognition and @ on @ recently neural network-based natural language processing technique use fixed length word embedding @ word embedding is a method of digitizing a word at a specific position @ a low-dimensional dense vector @ fixed length @ preserving @ similarity of @ distribution of @ surrounding word @ currently @ word embedding method @ foreign language @ used @ korean word @ however existing word embedding method @ developed @ english originally @ @ @ not reflect @ order and structure of @ korean word @ in @ @ @ propose a word embedding method @ korean @ is called skip-gram-kr and a korean affix tokenizer @ skip-gram-kr creates similar word training data @ backward mapping and @ two-word skipping method @ @ experiment @ @ @ proposed method achieved @ @ accurate performance @ @ @ 
881,Analyzing trending technological areas of patents,"Analyzing technological areas of inventions in patent domain is an important stage to discover relationships and trends for decision making. The International Patent Classification (IPC) is used for classifying the patents according to their technological areas. However, these classifications are quite inconsistent in various aspects because of the complexity and they may not be available for all areas of technology specially the emerging areas. This work introduces methods that applied on unstructured patents texts for detecting accurate technological areas to which the invention relates, and identifies semantically meaningful communities/topics for a large collection of patent documents. A hybrid text mining techniques with scalable analytics service that involves natural language processing which built on top of big-data architecture are used to extract the significant technical areas. Community detection approach is applied for efficiently identifying communities/topics by clustering the network graph of technological areas of inventions. A comparison to the standard LDA clustering is presented. Finally, regression analysis methods are applied in order to discover the interesting trends. © Springer Nature Switzerland AG 2019.",2019,Communications in Computer and Information Science,0,analyzing technological area of invention in patent domain is @ important stage to discover relationship and trend @ decision making @ @ international patent classification @ ipc @ is used @ classifying @ patent according to @ technological area @ however @ classification @ quite inconsistent in various aspect @ of @ complexity and @ may not @ available @ @ area of technology specially @ emerging area @ @ work introduces method @ applied on unstructured patent text @ detecting accurate technological area to @ @ invention relates and identifies semantically meaningful community topic @ a @ collection of patent document @ a hybrid text mining technique @ scalable analytics service @ involves natural language processing @ built on top of big-data architecture @ used to extract @ significant technical area @ community detection approach is applied @ efficiently identifying community topic by clustering @ network graph of technological area of invention @ a comparison to @ standard lda clustering is presented @ finally regression analysis method @ applied in order to discover @ interesting trend @ @ nature switzerland ag @ 
882,Jobs analysis for business intelligence skills requirements in the ASEAN region: A text mining study,"This paper applies text mining and data mining techniques to analyse online job advertisements related to Business Intelligence (BI) in web portals around the ASEAN region. Several techniques are applied: data preprocessing, tokenization, association rules (AR) with FP-Growth and visualisation. The research goal is the profiles of BI skills required for industry and the market. The results would be useful for individual to prepare for their BI competence, for faculties to prepare for BI-related courses and curriculum, and for organizations to prepare for Human Resource (HR) and job advertisement purposes. © Springer Nature Switzerland AG 2019.",2019,Advances in Intelligent Systems and Computing,1,@ @ applies text mining and data mining technique to analyse online job advertisement related to @ intelligence @ bi @ in web portal around @ asean region @ several technique @ applied @ data preprocessing tokenization association rule @ ar @ @ fp-growth and visualisation @ @ research goal is @ profile of bi skill required @ industry and @ market @ @ @ would @ useful @ individual to prepare @ @ bi competence @ faculty to prepare @ bi-related course and curriculum and @ organization to prepare @ human resource @ hr @ and job advertisement purpose @ @ nature switzerland ag @ 
883,A text mining-based approach for analyzing information retrieval in Spanish: Music data collection as a case study,"This paper presents a text mining-based search approach aimed at information retrieval in the Spanish language. For this purpose, a tool has been developed in order to facilitate and automate the analysis and retrieval, allowing the user to apply different analyzers when carrying out a query, to index and delete documents stored in the system and to evaluate the recovery process. To this extent, a dataset consisting in 27 songs has been used as a case study. Different queries have been made to investigate about the best fitting approaches to the Spanish language and their suitability depending on the query text. © Springer Nature Switzerland AG 2019.",2019,Advances in Intelligent Systems and Computing,0,@ @ @ a text mining-based search approach aimed at information retrieval in @ spanish language @ @ @ purpose a tool ha @ developed in order to facilitate and automate @ analysis and retrieval allowing @ user to apply different analyzer @ carrying @ a query to index and delete document stored in @ system and to evaluate @ recovery process @ to @ extent a dataset consisting in song ha @ used a a case study @ different query @ @ made to investigate @ @ best fitting approach to @ spanish language and @ suitability depending on @ query text @ @ nature switzerland ag @ 
884,Data science: Opportunities to transform education,"The article concerns the issue of data science tools implementation, including the text mining and natural language processing algorithms for increasing the value of high education for development modern and technologically flexible society. Data science is the field of study that involves tools, algorithms, and knowledge of math and statistics to discover knowledge from the raw data. Data science is developing fast and penetrating all spheres of life. More people understand the importance of the science of data and the need for implementation in everyday life. Data science is used in business for business analytics and production, in sales for offerings and, for sales forecasting, in marketing for customizing customers, and recommendations on purchasing, digital marketing, in banking and insurance for risk assessment, fraud detection, scoring, and in medicine for disease forecasting, process automation and patient health monitoring, in tourism in the field of price analysis, flight safety, opinion mining etc. However, data science applications in education have been relatively limited, and many opportunities for advancing the fields still unexplored. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2019,CEUR Workshop Proceedings,6,@ article concern @ issue of data science tool implementation including @ text mining and natural language processing algorithm @ increasing @ value of high education @ development modern and technologically flexible society @ data science is @ field of study @ involves tool algorithm and knowledge of math and statistic to discover knowledge @ @ raw data @ data science is developing fast and penetrating @ sphere of life @ more people understand @ importance of @ science of data and @ need @ implementation in everyday life @ data science is used in @ @ @ analytics and production in sale @ offering and @ sale forecasting in marketing @ customizing customer and recommendation on purchasing digital marketing in banking and insurance @ risk assessment fraud detection scoring and in medicine @ disease forecasting process automation and patient health monitoring in tourism in @ field of price analysis flight safety opinion mining etc @ however data science application in education @ @ relatively limited and many opportunity @ advancing @ field still unexplored @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
885,Indicator Proposal for Measuring Regional Political Support for the Electoral Process on Twitter: The Case of Spain's 2015 and 2016 General Elections,"Research on electoral events in conjunction with social media provides opportunities to describe an interesting phenomenon that can be analyzed using sentiment analysis techniques. The goal of the study is to analyze the support of political parties during electoral periods from Twitter comments, including 250 000 tweets regarding the Spanish general elections of 2015 and 2016, respectively. Text mining and natural language processing techniques enable information analysis, and the methodology emphasizes good practices for large-scale data collection retrieved from Twitter through a quantitative analysis of text collection written in the Spanish language. After information extraction obtained in three Spanish regions defined by geolocation, as well as feature selection based on keywords of the main four political parties, we conducted an in-depth examination of Twitter users' support during the course of the election. By weighting the tendency of tweets, we were able to obtain a proposed indicator of support: the positiveness ratio (PR). The results suggest that PR is a feasible barometer to demonstrate the measurable patterns of support tendency regarding political parties and users' behavioral activity to track their affinity on Twitter. The findings indicate consistent support behavior by users toward traditional parties and optimistic users' behavior regarding emerging political parties. © 2013 IEEE.",2019,IEEE Access,2,research on electoral event in conjunction @ social medium provides opportunity to describe @ interesting phenomenon @ @ @ analyzed @ sentiment analysis technique @ @ goal of @ study is to analyze @ support of political party @ electoral period @ twitter comment including tweet regarding @ spanish general election of and respectively @ text mining and natural language processing technique enable information analysis and @ methodology emphasizes good practice @ large-scale data collection retrieved @ twitter @ a quantitative analysis of text collection written in @ spanish language @ @ information extraction obtained in three spanish region defined by geolocation a well a feature selection based on keywords of @ main four political party @ conducted @ in-depth examination of twitter user @ support @ @ course of @ election @ by weighting @ tendency of tweet @ @ able to obtain a proposed indicator of support @ @ positiveness ratio @ pr @ @ @ @ suggest @ pr is a feasible barometer to demonstrate @ measurable pattern of support tendency regarding political party and user @ behavioral activity to track @ affinity on twitter @ @ finding indicate consistent support behavior by user toward traditional party and optimistic user @ behavior regarding emerging political party @ @ @ 
887,Framework for real-world event detection through online social networking sites,"In recent few years, due to the exponential growth of users on online social networking sites (OSNs), mainly over micro-blogging sites like Twitter, the OSNs now resemble the real world very cohesively. The excess of continuously user-generated online textual data by OSNs that encapsulates almost all verticals of the real world has attracted many researchers who are working in the area of text mining, natural language processing (NLP), machine learning, and data mining. This paper discusses the feasibility of OSNs in detecting real-world events from the horizon of the virtual world formed over OSNs. Moreover, this paper also describes the framework for real-world event detection through online social networking sites. © Springer Nature Singapore Pte Ltd. 2019.",2019,Advances in Intelligent Systems and Computing,1,in recent @ year due to @ exponential growth of user on online social networking site @ osns @ mainly @ micro-blogging site like twitter @ osns now resemble @ real world @ cohesively @ @ excess of continuously user-generated online textual data by osns @ encapsulates almost @ vertical of @ real world ha attracted many researcher @ @ working in @ area of text mining natural language processing @ nlp @ machine learning and data mining @ @ @ discus @ feasibility of osns in detecting real-world event @ @ horizon of @ virtual world formed @ osns @ moreover @ @ @ describes @ framework @ real-world event detection @ online social networking site @ @ nature singapore pte ltd @ @ 
889,System of intellectual Ukrainian language processing,"The problem of high-quality automatic natural language processing is one of the most important problems in computational linguistics. Automatic natural language processing is used in information retrieval, in tasks of text generation and text recognition, in machine translation, in sentiment analysis and so on. All of these areas require specialized linguistic and mathematical models to represent the morphology, syntax, and semantics of text in a form that is convenient for automatic processing. The article describes a developed system that implements specific linguistic tasks related to the processing of Ukrainian language, that is text preprocessing, morphological and lexical analyzes of text. In order to create such a system, an analysis of the available natural language text-processing tools was carried out and the possibility of using them for text processing of Ukrainian language was examined. Also, the most appropriate text processing tools in Ukrainian language were selected. The basic stages of text preprocessing were considered in detail and algorithms of their program implementation were given. In addition, the results of the developed system were demonstrated. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2019,CEUR Workshop Proceedings,0,@ problem of high-quality automatic natural language processing is @ of @ @ important problem in computational linguistics @ automatic natural language processing is used in information retrieval in task of text generation and text recognition in machine translation in sentiment analysis and @ on @ @ of @ area require specialized linguistic and mathematical model to represent @ morphology syntax and semantics of text in a form @ is convenient @ automatic processing @ @ article describes a developed system @ implement specific linguistic task related to @ processing of ukrainian language @ is text preprocessing morphological and lexical analyzes of text @ in order to create @ a system @ analysis of @ available natural language text-processing tool wa carried @ and @ possibility of @ @ @ text processing of ukrainian language wa examined @ @ @ @ appropriate text processing tool in ukrainian language @ selected @ @ basic stage of text preprocessing @ considered in detail and algorithm of @ program implementation @ given @ in addition @ @ of @ developed system @ demonstrated @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
891,Detecting comments showing risk for suicide in YouTube,"Natural language processing (NLP) with Cantonese, a mixture of Traditional Chinese, borrowed characters to represent spoken terms, and English, is largely under developed. To apply NLP to detect social media posts showing suicide risk, which is a rare event in regular population, is even more challenging. This paper tried different text mining methods to classify comments in Cantonese on YouTube whether they indicate suicidal risk. Based on word vector feature, classification algorithms such as SVM, AdaBoost, Random Forest, and LSTM are employed to detect the comments’ risk level. To address the imbalance issue of the data, both re-sampling and focal loss methods are used. Based on improvement on both data and algorithm level, the LSTM algorithm can achieve more satisfied testing classification results (84.3% and 84.5% g-mean, respectively). The study demonstrates the potential of automatically detected suicide risk in Cantonese social media posts. © Springer Nature Switzerland AG 2019.",2019,Advances in Intelligent Systems and Computing,0,natural language processing @ nlp @ @ cantonese a mixture of traditional chinese borrowed character to represent spoken term and english is largely @ developed @ to apply nlp to detect social medium post showing suicide risk @ is a rare event in regular population is even more challenging @ @ @ tried different text mining method to classify comment in cantonese on youtube whether @ indicate suicidal risk @ based on word vector feature classification algorithm @ a svm adaboost random forest and lstm @ employed to detect @ comment risk level @ to address @ imbalance issue of @ data @ re-sampling and focal loss method @ used @ based on improvement on @ data and algorithm level @ lstm algorithm @ achieve more satisfied testing classification @ @ @ and @ g-mean respectively @ @ @ study demonstrates @ potential of automatically detected suicide risk in cantonese social medium post @ @ nature switzerland ag @ 
893,Semantic-Emotion Neural Network for Emotion Recognition from Text,"Emotion detection and recognition from text is a recent essential research area in Natural Language Processing (NLP) which may reveal some valuable input to a variety of purposes. Nowadays, writings take many forms of social media posts, micro-blogs, news articles, customer review, etc., and the content of these short-texts can be a useful resource for text mining to discover an unhide various aspects, including emotions. The previously presented models mainly adopted word embedding vectors that represent rich semantic/syntactic information and those models cannot capture the emotional relationship between words. Recently, some emotional word embeddings are proposed but it requires semantic and syntactic information vice versa. To address this issue, we proposed a novel neural network architecture, called SENN (Semantic-Emotion Neural Network) which can utilize both semantic/syntactic and emotional information by adopting pre-trained word representations. SENN model has mainly two sub-networks, the first sub-network uses bidirectional Long-Short Term Memory (BiLSTM) to capture contextual information and focuses on semantic relationship, the second sub-network uses the convolutional neural network (CNN) to extract emotional features and focuses on the emotional relationship between words from the text. We conducted a comprehensive performance evaluation for the proposed model using standard real-world datasets. We adopted the notion of Ekman's six basic emotions. The experimental results show that the proposed model achieves a significantly superior quality of emotion recognition with various state-of-the-art approaches and further can be improved by other emotional word embeddings. © 2013 IEEE.",2019,IEEE Access,15,emotion detection and recognition @ text is a recent essential research area in natural language processing @ nlp @ @ may reveal some valuable input to a variety of purpose @ nowadays writing take many form of social medium post micro-blogs news article customer review etc @ and @ content of @ short-texts @ @ a useful resource @ text mining to discover @ unhide various aspect including emotion @ @ @ presented model mainly adopted word embedding vector @ represent rich semantic syntactic information and @ model cannot capture @ emotional relationship @ word @ recently some emotional word embeddings @ proposed @ @ requires semantic and syntactic information vice versa @ to address @ issue @ proposed a novel neural network architecture called senn @ semantic-emotion neural network @ @ @ utilize @ semantic syntactic and emotional information by adopting pre-trained word representation @ senn model ha mainly @ sub-networks @ first sub-network us bidirectional long-short term memory @ bilstm @ to capture contextual information and focus on semantic relationship @ second sub-network us @ convolutional neural network @ cnn @ to extract emotional feature and focus on @ emotional relationship @ word @ @ text @ @ conducted a comprehensive performance evaluation @ @ proposed model @ standard real-world datasets @ @ adopted @ notion of ekman @ s six basic emotion @ @ experimental @ @ @ @ proposed model achieves a significantly superior quality of emotion recognition @ various state-of-the-art approach and @ @ @ improved by @ emotional word embeddings @ @ @ 
894,Ontology-Based Security Context Reasoning for Power IoT-Cloud Security Service,"For a variety of cyber-attacks occurring in a power IoT-Cloud environment, conventional security intrusion incident detection and response technologies typically use pattern- and behavior-based statistical methods. However, they cannot provide fundamental solutions for a security intrusion or attacks, which are becoming more intelligent and diverse as time passes. Therefore, an effective response method that can respond to security intrusions intelligently while using an access control technique based on ontology reasoning is required. This can be achieved by adopting a variety of intelligent reasoning technologies for security intrusion incidents of power systems, such as various reasoning technologies based on the ontology and semantic-web technologies being actively studied in the field of intelligent systems, and malicious code detection technologies based on an intelligent access control model, text mining, and natural language processing technologies. Accordingly, a security context ontology was modeled by analyzing the security vulnerabilities of a power system in a power IoT-Cloud environment, and security context inference rules were defined. Furthermore, this paper presents an appropriate power IoT-Cloud security service framework that can be used in a power IoT-Cloud environment. In addition, a security mechanism that can be efficiently operated in such an environment is implemented. In experiments conducted for this application, attack context scenarios that commonly occur were created using a smart meter as an example, which is an essential power system device. Inference rules were then composed for each attack stage to check the paths of attacks those that exploit the vulnerability of a smart meter system. As a result, it was confirmed that a high level attack detection results can be obtained based on the inference rules. © 2013 IEEE.",2019,IEEE Access,6,@ a variety of cyber-attacks occurring in a power iot-cloud environment conventional security intrusion incident detection and response technology typically use pattern and behavior-based statistical method @ however @ cannot provide fundamental solution @ a security intrusion @ attack @ @ becoming more intelligent and diverse a time pass @ therefore @ effective response method @ @ respond to security intrusion intelligently @ @ @ access control technique based on ontology reasoning is required @ @ @ @ achieved by adopting a variety of intelligent reasoning technology @ security intrusion incident of power system @ a various reasoning technology based on @ ontology and semantic-web technology @ actively studied in @ field of intelligent system and malicious code detection technology based on @ intelligent access control model text mining and natural language processing technology @ accordingly a security context ontology wa modeled by analyzing @ security vulnerability of a power system in a power iot-cloud environment and security context inference rule @ defined @ furthermore @ @ @ @ appropriate power iot-cloud security service framework @ @ @ used in a power iot-cloud environment @ in addition a security mechanism @ @ @ efficiently operated in @ @ environment is implemented @ in experiment conducted @ @ application attack context scenario @ commonly occur @ created @ a smart meter a @ example @ is @ essential power system device @ inference rule @ @ composed @ @ attack stage to check @ path of attack @ @ exploit @ vulnerability of a smart meter system @ a a @ @ wa confirmed @ a high level attack detection @ @ @ obtained based on @ inference rule @ @ @ 
895,Mining Scholarly Publications for Scientific Knowledge Graph Construction,"In this paper, we present a preliminary approach that uses a set of NLP and Deep Learning methods for extracting entities and relationships from research publications and then integrates them in a Knowledge Graph. More specifically, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, and (iii) analyse an automatically generated Knowledge Graph including 10, 425 entities and 25, 655 relationships in the field of Semantic Web. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,in @ @ @ @ a preliminary approach @ us a set of nlp and deep learning method @ extracting entity and relationship @ research publication and @ integrates @ in a knowledge graph @ more specifically @ @ i @ tackle @ challenge of knowledge extraction by employing several state-of-the-art natural language processing and text mining tool @ ii @ describe @ approach @ integrating entity and relationship generated by @ tool and @ iii @ analyse @ automatically generated knowledge graph including entity and relationship in @ field of semantic web @ @ nature switzerland ag @ 
896,Conceptualized phrase clustering with distributed k -means,"A vast majority of text mining and machine learning algorithms such as topic models, classification, clustering are based on statistical methods thus the semantics or meaning of the words or phrases are not considered. Interpretation of outputs generated by such algorithms are difficult for humans because of the absence of sufficient contextual information. Distributional semantics is a relatively new but active research area in natural language processing that quantifies semantic similarities between linguistic elements considering the context in which they occur. Conceptualization algorithms on the other hand enriches short text such as words and phrases. This paper proposes an approach that uses a map-reduce framework for combining these two techniques to generate conceptualized semantic clusters of phrases using distributional representation. Rigorous and systematic experiments on unstructured text datasets show that this approach can generate semantically rich and human interpretable concept clusters from large datasets. Further, the approach is scalable when dealing with high dimensional data since this method uses a map-reduce based framework for clustering. © 2019 - IOS Press and the authors. All rights reserved.",2019,Intelligent Decision Technologies,0,a vast majority of text mining and machine learning algorithm @ a topic model classification clustering @ based on statistical method thus @ semantics @ meaning of @ word @ phrase @ not considered @ interpretation of output generated by @ algorithm @ difficult @ human @ of @ absence of sufficient contextual information @ distributional semantics is a relatively @ @ active research area in natural language processing @ quantifies semantic similarity @ linguistic element considering @ context in @ @ occur @ conceptualization algorithm on @ @ hand enriches short text @ a word and phrase @ @ @ proposes @ approach @ us a map-reduce framework @ combining @ @ technique to generate conceptualized semantic cluster of phrase @ distributional representation @ rigorous and systematic experiment on unstructured text datasets @ @ @ approach @ generate semantically rich and human interpretable concept cluster @ @ datasets @ @ @ approach is scalable @ dealing @ high dimensional data since @ method us a map-reduce based framework @ clustering @ io @ and @ author @ @ right reserved @ 
897,Fast language-independent correction of interconnected typos to finding longest terms,"Triagers deal with bug reports in software triage systems like Bugzilla to prioritizing, finding duplicates, and assigning those to developers, which these processes should be automated, especially for substantial open source projects. These bug reports must be mined by text mining, information retrieval, and natural language processing techniques for automation processes. There are many typos in user bug reports which cause low accuracy for artificial intelligence techniques. These typos can be detected based on standard dictionaries, but correction of these typos needs human knowledge based on the context of bug reports. It is essential which neither Google Translator nor Microsoft Office Word can detect interconnected terms –a common type of typos in bug reports- having more than two meaningful terms. This research provides a novel language-independent approach for fast correction of interconnected typos based on natural language processing and human neural network structure to detect and correct interconnected typos — a new tree-based method proposed for term matching. Also, two algorithms proposed for a fast finding the longest meaningful term in an interconnected typo. A dataset is used including 180-kilo typos based on four famous bug report dataset of Android, Eclipse, Mozilla Firefox, and Open Office projects. Then proposed method evaluated on typos versus state of the art. The results show the runtime performance of the proposed method is as same as the related works, but the average length of words is improved and at least more than 57% of typos in the dataset can be classified as interconnected typos. © 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,0,triagers deal @ bug report in software triage system like bugzilla to prioritizing finding duplicate and assigning @ to developer @ @ process @ @ automated especially @ substantial open source project @ @ bug report must @ mined by text mining information retrieval and natural language processing technique @ automation process @ @ @ many typo in user bug report @ cause low accuracy @ artificial intelligence technique @ @ typo @ @ detected based on standard dictionary @ correction of @ typo need human knowledge based on @ context of bug report @ @ is essential @ neither google translator @ microsoft office word @ detect interconnected term a common type of typo in bug report @ more @ @ meaningful term @ @ research provides a novel language-independent approach @ fast correction of interconnected typo based on natural language processing and human neural network structure to detect and correct interconnected typo a @ tree-based method proposed @ term matching @ @ @ algorithm proposed @ a fast finding @ longest meaningful term in @ interconnected typo @ a dataset is used including kilo typo based on four famous bug report dataset of android eclipse mozilla firefox and open office project @ @ proposed method evaluated on typo versus state of @ art @ @ @ @ @ runtime performance of @ proposed method is a @ a @ related work @ @ average length of word is improved and at least more @ of typo in @ dataset @ @ classified a interconnected typo @ @ @ @ by @ author @ 
898,Sentiment Analysis on Automobile Brands Using Twitter Data,"User generated contents in a very big number is freely available on different social media sites now a day. Companies to increase their competitive advantages keep an eye on their competing companies and closely analyze the data that are generated by their customers on their social media sites. Analysis of sentiments is the quickest growing field that utilizes text mining, computational linguistics and natural language processing, linguistic mining of text and calculation to extricate valuable data to assist in decision making. The automobiles business is extremely competing and needs that supplier, automobile corporations, carefully analyze and address the views of consumers with a specific end goal to accomplish an upper hand in the market. It is a great way to analyze the views of consumers through the data of social media sites; what’s more, it is also helpful for automobiles companies to improve their goals and objectives of marketing. In this research, presents an analysis of sentiment on a case study of automobiles industry. Sentiment analysis and text mining are utilized to analyze and break down unstructured Twitter’s tweets to take out automobile classes’ polarity for example, Honda, Toyota, BMW, Audi, and Mercedes. According to the classification of the polarity, you notice that Audi has 87% of the positive tweets compared to 74% for BMW, 84% for Honda, 70% for Toyota and 81% for Mercedes. What’s more, the results demonstrate that Audi has negative polarity 18% against 10% for BMW, 20% for Mercedes, 15% for Honda and 25% for Toyota. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,1,user generated content in a @ big number is freely available on different social medium site now a day @ company to increase @ competitive advantage keep @ eye on @ competing company and closely analyze @ data @ @ generated by @ customer on @ social medium site @ analysis of sentiment is @ quickest growing field @ utilizes text mining computational linguistics and natural language processing linguistic mining of text and calculation to extricate valuable data to assist in decision making @ @ automobile @ is extremely competing and need @ supplier automobile corporation carefully analyze and address @ view of consumer @ a specific end goal to accomplish @ upper hand in @ market @ @ is a great way to analyze @ view of consumer @ @ data of social medium site @ @ s more @ is @ helpful @ automobile company to improve @ goal and objective of marketing @ in @ research @ @ analysis of sentiment on a case study of automobile industry @ sentiment analysis and text mining @ utilized to analyze and break down unstructured twitter s tweet to take @ automobile class polarity @ example honda toyota bmw audi and mercedes @ according to @ classification of @ polarity @ notice @ audi ha of @ positive tweet compared to @ bmw @ honda @ toyota and @ mercedes @ @ s more @ @ demonstrate @ audi ha negative polarity @ @ bmw @ mercedes @ honda and @ toyota @ @ nature singapore pte ltd @ 
899,Styleexplorer: A toolkit for textual writing style visualization,"The analysis of textual writing styles is a well-studied problem with ongoing and active research in fields like authorship attribution, author profiling, text segmentation or plagiarism detection. While many features have been proposed and shown to be effective to characterize authors or document types in terms of high-dimensional feature vectors, an intuitive, human-friendly view on the computed data is often lacking. For example, machine learning algorithms are able to attribute previously unseen documents to a set of known authors by utilizing those features, but a visualization of the most discriminating features is usually not provided. To this end, we present StyleExplorer, a freely available web tool that is able to extract textual features from documents and to visualize them in multiple variants. Besides analyzing single documents intrinsically, it is also possible to visually compare multiple documents in single views with respect to selected metrics, making it a valuable analysis tool for various tasks in natural language processing as well as for areas in the humanities that work and analyze textual data. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ analysis of textual writing style is a well-studied problem @ ongoing and active research in field like authorship attribution author profiling text segmentation @ plagiarism detection @ @ many feature @ @ proposed and @ to @ effective to characterize author @ document type in term of high-dimensional feature vector @ intuitive human-friendly view on @ computed data is often lacking @ @ example machine learning algorithm @ able to attribute @ unseen document to a set of known author by utilizing @ feature @ a visualization of @ @ discriminating feature is usually not provided @ to @ end @ @ styleexplorer a freely available web tool @ is able to extract textual feature @ document and to visualize @ in multiple variant @ besides analyzing single document intrinsically @ is @ possible to visually compare multiple document in single view @ respect to selected metric making @ a valuable analysis tool @ various task in natural language processing a well a @ area in @ humanity @ work and analyze textual data @ @ nature switzerland ag @ 
900,"Automatic de-identification of medical texts in Spanish: The Meddocan track, corpus, guidelines, methods and evaluation of results","There is an increasing interest in exploiting the content of electronic health records by means of natural language processing and text-mining technologies, as they can result in resources for improving patient health/safety, aid in clinical decision making, facilitate drug re-purposing or precision medicine. To share, re-distribute and make clinical narratives accessible for text mining research purposes, it is key to fulfill legal conditions and address restrictions related data protection and patient privacy. Thus, clinical records cannot be shared directly”as is”. A necessary precondition for accessing clinical records outside of hospitals is their de-identification or exhaustive removal/replacement of all mentioned privacy related protected health information phrases. Providing a proper evaluation scenario for automatic anonymization tools is key for approval of data redistribution. The construction of manually de-identified medical records is currently the main rate and cost-limiting step for secondary use applications of clinical data. This paper summarizes the settings, data and results of the first shared track on anonymization of medical documents in Spanish, the MEDDOCAN (Medical Document Anonymization) track. This track relied on a carefully constructed synthetic corpus of clinical case documents, the MEDDOCAN corpus, following annotation guidelines for sensitive data based on the analysis of the EU General Data Protection Regulation. A total of 18 teams (from the 51 registrations) submitted 63 runs for first sub-track 1 and 61 systems for the second sub-track. The top scoring systems were based on sophisticated deep learning approaches, representing strategies that can significantly reduce time and costs associated to accessing textual data containing privacy-related sensitive information. The results of this track might help in lowering the clinical data access hurdle for Spanish language technology developers, showing also potentials for similar settings using data in other languages or from different domains. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,10,@ is @ increasing interest in exploiting @ content of electronic health record by mean of natural language processing and text-mining technology a @ @ @ in resource @ improving patient health safety aid in clinical decision making facilitate drug re-purposing @ precision medicine @ to share re-distribute and make clinical narrative accessible @ text mining research purpose @ is key to fulfill legal condition and address restriction related data protection and patient privacy @ thus clinical record cannot @ shared directly a is @ a necessary precondition @ accessing clinical record outside of hospital is @ de-identification @ exhaustive removal replacement of @ mentioned privacy related protected health information phrase @ providing a proper evaluation scenario @ automatic anonymization tool is key @ approval of data redistribution @ @ construction of manually de-identified medical record is currently @ main rate and cost-limiting step @ secondary use application of clinical data @ @ @ summarizes @ setting data and @ of @ first shared track on anonymization of medical document in spanish @ meddocan @ medical document anonymization @ track @ @ track relied on a carefully constructed synthetic corpus of clinical case document @ meddocan corpus following annotation guideline @ sensitive data based on @ analysis of @ eu general data protection regulation @ a total of team @ @ @ registration @ submitted run @ first sub-track and system @ @ second sub-track @ @ top scoring system @ based on sophisticated deep learning approach representing strategy @ @ significantly reduce time and cost associated to accessing textual data containing privacy-related sensitive information @ @ @ of @ track might help in lowering @ clinical data access hurdle @ spanish language technology developer showing @ potential @ similar setting @ data in @ language @ @ different domain @ ceur-ws @ @ right reserved @ 
902,Text mining for word sentiment detection,"This work presents a novel approach for automatically generating a sentiment lexicon. We employ an unsupervised learning approach using several probabilistic and information theoretic models. While most of the unsupervised approaches require a set of seed words to begin their work, our methods differ from these by using no a priori knowledge. In addition, our models are effective with a diverse corpus rather than requiring a corpus for a limited domain. We demonstrate the effectiveness of our approaches by performing sentiment analysis on Amazon products reviews, comparing the various automatically-generated lexicons. Based on our cross validation results, we show that our lexicons outperform a widely-used sentiment lexicon on both balanced and unbalanced datasets. © Springer Nature Switzerland AG 2019.",2019,Communications in Computer and Information Science,0,@ work @ a novel approach @ automatically generating a sentiment lexicon @ @ employ @ unsupervised learning approach @ several probabilistic and information theoretic model @ @ @ of @ unsupervised approach require a set of seed word to begin @ work @ method differ @ @ by @ no a priori knowledge @ in addition @ model @ effective @ a diverse corpus rather @ requiring a corpus @ a limited domain @ @ demonstrate @ effectiveness of @ approach by performing sentiment analysis on amazon product review comparing @ various automatically-generated lexicon @ based on @ cross validation @ @ @ @ @ lexicon outperform a widely-used sentiment lexicon on @ balanced and unbalanced datasets @ @ nature switzerland ag @ 
903,Applying OWA Operator in the Semantic Processing for Automatic Keyphrase Extraction,"The automatic keyphrases extraction from texts is a useful task for many computational systems in the natural language processing and text mining fields. Although several solutions to this problem have been developed, the semantic analysis has been one of the linguistic features less exploited in the most reported proposal, causing that the obtained results still show low accuracy and performance rates. This paper presents an unsupervised method for keyphrase extraction, which is based on the use of lexical-syntactic patterns for extracting information from texts and a fuzzy modelling of topics. An OWA operator which combines several semantics measures has been applied in the topic modelling process. This new approach was evaluated with Inspec and 500N-KPCrowd datasets and compared with other reported systems, obtaining promising results. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ automatic keyphrases extraction @ text is a useful task @ many computational system in @ natural language processing and text mining field @ although several solution to @ problem @ @ developed @ semantic analysis ha @ @ of @ linguistic feature le exploited in @ @ reported proposal causing @ @ obtained @ still @ low accuracy and performance rate @ @ @ @ @ unsupervised method @ keyphrase extraction @ is based on @ use of lexical-syntactic pattern @ extracting information @ text and a fuzzy modelling of topic @ @ owa operator @ combine several semantics measure ha @ applied in @ topic modelling process @ @ @ approach wa evaluated @ inspec and n-kpcrowd datasets and compared @ @ reported system obtaining promising @ @ @ nature switzerland ag @ 
904,An open science system for text mining,"Text mining (TM) techniques can extract high-quality information from big data through complex system architectures. However, these techniques are usually difficult to discover, install, and combine. Further, modern approaches to Science (e.g. Open Science) introduce new requirements to guarantee reproducibility, repeatability, and re-usability of methods and results as well as their longevity and sustainability. In this paper, we present a distributed system (NLPHub) that publishes and combines several state-of-the-art text mining services for named entities, events, and keywords recognition. NLPHub makes the integrated methods compliant with Open Science requirements and manages heterogeneous access policies to the methods. In the paper, we assess the benefits and the performance of NLPHub on the I-CAB corpus1. Copyright © 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,0,text mining @ tm @ technique @ extract high-quality information @ big data @ complex system architecture @ however @ technique @ usually difficult to discover install and combine @ @ modern approach to science @ e @ g @ open science @ introduce @ requirement to guarantee reproducibility repeatability and re-usability of method and @ a well a @ longevity and sustainability @ in @ @ @ @ a distributed system @ nlphub @ @ publishes and combine several state-of-the-art text mining service @ named entity event and keywords recognition @ nlphub make @ integrated method compliant @ open science requirement and manages heterogeneous access policy to @ method @ in @ @ @ ass @ benefit and @ performance of nlphub on @ i-cab corpus @ @ @ @ @ by @ author @ 
905,Sentiment Analysis of Text Classification Algorithms Using Confusion Matrix,"Sentiment analysis on text mining has a vital role in the process of review classification. Text classification needs some techniques like natural language processing, text mining, and machine learning to get meaningful knowledge. This paper focuses on performance analysis of text classification algorithms commonly named Support vector machine, random forest and extreme Gradient Boosting by creating confusion matrices for training and testing applying features on a product review dataset. We did comparison research on the performance of the three algorithms by computing the confusion matrix for accuracy, positive and negative prediction values. We used unigram, bigram and trigrams for the future extraction on the three classifiers using different number of features with and without stop words to determine which algorithms works better in case of text mining for sentiment analysis. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,0,sentiment analysis on text mining ha a vital role in @ process of review classification @ text classification need some technique like natural language processing text mining and machine learning to get meaningful knowledge @ @ @ focus on performance analysis of text classification algorithm commonly named support vector machine random forest and extreme gradient boosting by creating confusion matrix @ training and testing applying feature on a product review dataset @ @ @ comparison research on @ performance of @ three algorithm by computing @ confusion matrix @ accuracy positive and negative prediction value @ @ used unigram bigram and trigram @ @ future extraction on @ three classifier @ different number of feature @ and without stop word to determine @ algorithm work better in case of text mining @ sentiment analysis @ @ nature singapore pte ltd @ 
906,An intelligent platform with automatic assessment and engagement features for active online discussions,"In a university context, discussion forums are mostly available in Learning and Management Systems (LMS) but are often ineffective in encouraging participation due to poorly designed user interface and the lack of motivating factors to participate. Our integrated platform with the Telegram mobile app and a web-based forum, is capable of automatic thoughtfulness assessment of questions and answers posted, using text mining and Natural Language Processing (NLP) methodologies. We trained and applied the Random Forest algorithm to provide instant thoughtfulness score prediction for the new posts contributed by the students, and prompted the students to improve on their posts, thereby invoking deeper thinking resulting in better quality contributions. In addition, the platform is designed with six features to ensure that students remain actively engaged on the platform. We report the performance of our platform based on our implementations for a university course in two runs, and compare with existing systems to show that by using our platform, students’ participation and engagement are highly improved, and the quality of posts will increase. Most importantly, our students’ performance in the course was shown to be positively correlated with their participation in the system. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in a university context discussion forum @ mostly available in learning and management system @ lm @ @ @ often ineffective in encouraging participation due to poorly designed user interface and @ lack of motivating factor to participate @ @ integrated platform @ @ telegram mobile app and a web-based forum is capable of automatic thoughtfulness assessment of question and answer posted @ text mining and natural language processing @ nlp @ methodology @ @ trained and applied @ random forest algorithm to provide instant thoughtfulness score prediction @ @ @ post contributed by @ student and prompted @ student to improve on @ post thereby invoking deeper thinking resulting in better quality contribution @ in addition @ platform is designed @ six feature to ensure @ student remain actively engaged on @ platform @ @ report @ performance of @ platform based on @ implementation @ a university course in @ run and compare @ existing system to @ @ by @ @ platform student participation and engagement @ highly improved and @ quality of post @ increase @ @ importantly @ student performance in @ course wa @ to @ positively correlated @ @ participation in @ system @ @ nature switzerland ag @ 
907,Mining scholarly data for fine-grained knowledge graph construction,"Knowledge graphs (KG) are large networks of entities and relationships, typically expressed as RDF triples, relevant to a specific domain or an organization. Scientific Knowledge Graphs (SKGs) focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. The next big challenge in this field regards the generation of SKGs that also contain an explicit representation of the knowledge presented in research publications. In this paper, we present a preliminary approach that uses a set of NLP and Deep Learning methods for extracting entities and relationships from research publications, and then integrates them in a KG. More specifically, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) analyze an automatically generated Knowledge Graph including 10 425 entities and 25 655 relationships derived from 12 007 publications in the field of Semantic Web, and iv) discuss some open problems that have not been solved yet. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,1,knowledge graph @ kg @ @ @ network of entity and relationship typically expressed a rdf triple relevant to a specific domain @ @ organization @ scientific knowledge graph @ skgs @ focus on @ scholarly domain and typically contain metadata describing research publication @ a author venue organization research topic and citation @ @ next big challenge in @ field regard @ generation of skgs @ @ contain @ explicit representation of @ knowledge presented in research publication @ in @ @ @ @ a preliminary approach @ us a set of nlp and deep learning method @ extracting entity and relationship @ research publication and @ integrates @ in a kg @ more specifically @ i @ tackle @ challenge of knowledge extraction by employing several state-of-the-art natural language processing and text mining tool ii @ describe @ approach @ integrating entity and relationship generated by @ tool iii @ analyze @ automatically generated knowledge graph including entity and relationship derived @ publication in @ field of semantic web and @ @ discus some open problem @ @ not @ solved yet @ ceur-ws @ @ right reserved @ 
908,Processing and Analysis of Russian Strategic Planning Programs,"In this paper, we present a project on the analysis of an extensive corpus of strategic planning documents, devoted to various aspects of the development of Russian regions. The main purposes of the project are: (1) to extract different aspects of goal setting and planning, (2) to form an ontology of goals and criteria of achieving these goals, (3) to measure the similarity between goals declared by federal and municipal subjects. Such unsupervised Natural Language Processing (NLP) methods as phrase chunking, word embeddings, and latent topic modeling are used for information extraction and ontology construction as well as similarity computation. The resulting ontology should serve in short-term as a helper tool for writing strategic planning documents and in long-term resolve the need to compose strategic planning documents completely by navigating through the ontology and selecting relevant goals and criteria. The resulting similarity measure between federal and municipal goals will serve as a navigation tool for further analysis. © 2019, Springer Nature Switzerland AG.",2019,Communications in Computer and Information Science,1,in @ @ @ @ a project on @ analysis of @ extensive corpus of strategic planning document devoted to various aspect of @ development of russian region @ @ main purpose of @ project @ @ @ @ to extract different aspect of goal setting and planning @ @ to form @ ontology of goal and criterion of achieving @ goal @ @ to measure @ similarity @ goal declared by federal and municipal subject @ @ unsupervised natural language processing @ nlp @ method a phrase chunking word embeddings and latent topic modeling @ used @ information extraction and ontology construction a well a similarity computation @ @ resulting ontology @ serve in short-term a a helper tool @ writing strategic planning document and in long-term resolve @ need to compose strategic planning document completely by navigating @ @ ontology and selecting relevant goal and criterion @ @ resulting similarity measure @ federal and municipal goal @ serve a a navigation tool @ @ analysis @ @ nature switzerland ag @ 
909,Enhanced Entity Extraction Using Big Data Mechanics,"With the advancements of new technologies, a large volume of digital data is getting generated every second from various internal and external sources like social networking, organizations and any business applications. Big data refers to enormous digital data that are high in volume, velocity, varieties. The traditional conventional approach fails to handle large data sets using their tools and techniques. Big data proved to be an effective mean for collecting, analyzing and processing data despite their size and data formats structured, semi-structured or unstructured. Large set of information and data are produced from different organizations and social activities. Text mining or text analytics plays a significant role in deriving relevant information from text in digital environment. Text mining includes technique like entity extraction which automatically extracts structured information from unstructured or semi-structured documents. This paper details how entity extraction is useful in processing human language texts by using natural language processing. Entity extraction based on method like part-of-speech tagging which helps in determining the noun, verb, adverb and adjectives associated with a sentence. Enhanced entity extraction method will be mainly useful for filtering entities based on their part-of-speeches by removing any ambiguities. Entity extraction focuses on relevant parts of a document and represents them in a structured manner. © Springer Nature Singapore Pte Ltd. 2019.",2019,Advances in Intelligent Systems and Computing,2,@ @ advancement of @ technology a @ volume of digital data is getting generated every second @ various internal and external source like social networking organization and @ @ application @ big data refers to enormous digital data @ @ high in volume velocity variety @ @ traditional conventional approach fails to handle @ data set @ @ tool and technique @ big data proved to @ @ effective mean @ collecting analyzing and processing data despite @ size and data format structured semi-structured @ unstructured @ @ set of information and data @ produced @ different organization and social activity @ text mining @ text analytics play a significant role in deriving relevant information @ text in digital environment @ text mining includes technique like entity extraction @ automatically extract structured information @ unstructured @ semi-structured document @ @ @ detail @ entity extraction is useful in processing human language text by @ natural language processing @ entity extraction based on method like part-of-speech tagging @ help in determining @ noun verb adverb and adjective associated @ a sentence @ enhanced entity extraction method @ @ mainly useful @ filtering entity based on @ part-of-speeches by removing @ ambiguity @ entity extraction focus on relevant part of a document and represents @ in a structured manner @ @ nature singapore pte ltd @ @ 
910,Sentence similarity using weighted path and similarity matrices,"Sentence similarity is the task of assessing how similar the two snippets of text are. Similarity techniques are used extensively in clustering, summarization, classification, plagiarism detection etc. Due to a small set of vocabularies, sentence similarity is considered to be a difficult problem in natural language processing. There are two issues in solving this problem: (1) Which similarity techniques to be used for word pair similarity and (2) How to generalize that to sentence pairs. We have used the weighted path, a WordNet-based similarity assessment, and the paraphrase database to obtain word pair similarity values. Thereafter, we extracted maximum values from the pairwise similarity matrix and computed a similarity value for a sentence pair. We have also incorporated a vector space model technique to form a robust similarity measure. Our method outperformed state-of-the-art methods on the STSS65 test dataset in Pearson's correlation of 87% compared to human similarity scores. Moreover, our approach performed on par with other methods on the STSS131 test data using the same test. Our approach outperforms all the other WordNet-based methods compared on both datasets. © TÜBİTAK",2019,Turkish Journal of Electrical Engineering and Computer Sciences,0,sentence similarity is @ task of assessing @ similar @ @ snippet of text @ @ similarity technique @ used extensively in clustering summarization classification plagiarism detection etc @ due to a small set of vocabulary sentence similarity is considered to @ a difficult problem in natural language processing @ @ @ @ issue in solving @ problem @ @ @ @ similarity technique to @ used @ word pair similarity and @ @ @ to generalize @ to sentence pair @ @ @ used @ weighted path a wordnet-based similarity assessment and @ paraphrase database to obtain word pair similarity value @ thereafter @ extracted maximum value @ @ pairwise similarity matrix and computed a similarity value @ a sentence pair @ @ @ @ incorporated a vector space model technique to form a robust similarity measure @ @ method outperformed state-of-the-art method on @ stss test dataset in pearson @ s correlation of compared to human similarity score @ moreover @ approach performed on par @ @ method on @ stss test data @ @ @ test @ @ approach outperforms @ @ @ wordnet-based method compared on @ datasets @ tübi tak
911,Extracting clinical information from electronic medical records,"As the adoption of Electronic Medical Records (EMRs) rises in the healthcare institutions, these resources’ importance increases because of the clinical information they contain about patients. However, the unstructured information in the form of the narrative present in those records makes it hard to extract and structure useful clinical information. This limits the potential of the EMRs, because the clinical information these records contain, can be used to perform important operations inside healthcare institutions such as searching, summarization, decision support and statistical analysis, as well as be used to support management decisions or serve for research. These operations can only be done if the clinical information from the narratives is properly extracted and structured. Usually, this extraction is made manually by healthcare practitioners, what is not efficient and is error-prone. This research uses Natural Language Processing (NLP) and Information Extraction (IE) techniques in order to develop a pipeline system that can extract and structure clinical information directly from the clinical narratives present in Portuguese EMRs, in an automated way, in order to help EMRs to fulfil their potential. © Springer Nature Switzerland AG 2019.",2019,Advances in Intelligent Systems and Computing,3,a @ adoption of electronic medical record @ emrs @ rise in @ healthcare institution @ resource importance increase @ of @ clinical information @ contain @ patient @ however @ unstructured information in @ form of @ narrative @ in @ record make @ hard to extract and structure useful clinical information @ @ limit @ potential of @ emrs @ @ clinical information @ record contain @ @ used to perform important operation inside healthcare institution @ a searching summarization decision support and statistical analysis a well a @ used to support management decision @ serve @ research @ @ operation @ only @ done if @ clinical information @ @ narrative is properly extracted and structured @ usually @ extraction is made manually by healthcare practitioner @ is not efficient and is error-prone @ @ research us natural language processing @ nlp @ and information extraction @ ie @ technique in order to develop a pipeline system @ @ extract and structure clinical information directly @ @ clinical narrative @ in portuguese emrs in @ automated way in order to help emrs to fulfil @ potential @ @ nature switzerland ag @ 
912,Short text mining: State of the art and research opportunities,"With the growing number of connected online users producing a tremendous amount of unstructured short-texts daily, understanding and mining these data becomes very useful for individuals, governments and companies for identifying the public users' attitudes towards different entities, such as products, services, events, places, organizations and topics. However, analyzing these short-texts using traditional methods becomes a significant challenge due to the shortness and sparsity nature of short-texts. To address such challenges, the literature introduced a broad spectrum of short-texts mining approaches and applications. Hence, this paper provides a comprehensive survey of this spectrum based on a criterion-based research strategy. The different mining techniques and approaches utilized in short-texts were highlighted along with their related issues and challenges. This paper surveyed a total of 1575 research papers published in the refereed conferences and journals in the area of short-texts mining were sur-veyed from 2006 until 2017, from which 187 primary studies were included and analyzed to constitute the source of the present paper. After a careful review of these articles, it is obvious that there are research gaps in other languages than English and Chinese, multi-languages, and in specific domain studies. © 2019 Mohamed Grida, Hasnaa Soliman and Mohamed Hassan.",2019,Journal of Computer Science,0,@ @ growing number of connected online user producing a tremendous amount of unstructured short-texts daily understanding and mining @ data becomes @ useful @ individual government and company @ identifying @ public user @ attitude towards different entity @ a product service event place organization and topic @ however analyzing @ short-texts @ traditional method becomes a significant challenge due to @ shortness and sparsity nature of short-texts @ to address @ challenge @ literature introduced a broad spectrum of short-texts mining approach and application @ hence @ @ provides a comprehensive survey of @ spectrum based on a criterion-based research strategy @ @ different mining technique and approach utilized in short-texts @ highlighted along @ @ related issue and challenge @ @ @ surveyed a total of research @ published in @ refereed conference and journal in @ area of short-texts mining @ sur-veyed @ @ @ @ primary study @ included and analyzed to constitute @ source of @ @ @ @ @ a careful review of @ article @ is obvious @ @ @ research gap in @ language @ english and chinese multi-languages and in specific domain study @ mohamed grida hasnaa soliman and mohamed hassan @ 
914,The Application of Artificial Intelligence Technologies as a Substitute for Reading and to Support and Enhance the Authoring of Scientific Review Articles,"To gain a comprehensive overview of new scientific findings with the enormous, ever-increasing amount of published information, we apply a new combinatorial approach that complements the process of reading scientific articles by supplementing artificial intelligence technologies. We present a combinatorial approach, which we illustrate in the form of a 'double funnel of artificial intelligence.' Our approach suggests to largely increase the amount of data at the beginning of the data collection process and to subsequently clean and enrich the data set in order to gain much more knowledge at the end of the procedure compared to a 'classical' literature review. We use natural language processing and text visualization techniques to uncover findings that are generally unbeknown to the human reader due to the inability to process very large amounts of text. By illustrating the individual steps using practical examples taken from use cases, we demonstrate the merits of our approach. With our methodology, we are able to reproduce findings from 'regular' review papers; however, we discover additional and new findings in different fields, such as data science or medicine. We also point out the limitations of our approach. Finally, we make suggestions as to how the methodology could be further developed. © 2013 IEEE.",2019,IEEE Access,2,to gain a comprehensive overview of @ scientific finding @ @ enormous ever-increasing amount of published information @ apply a @ combinatorial approach @ complement @ process of reading scientific article by supplementing artificial intelligence technology @ @ @ a combinatorial approach @ @ illustrate in @ form of a @ double funnel of artificial intelligence @ @ @ approach suggests to largely increase @ amount of data at @ beginning of @ data collection process and to subsequently clean and enrich @ data set in order to gain much more knowledge at @ end of @ procedure compared to a @ classical @ literature review @ @ use natural language processing and text visualization technique to uncover finding @ @ generally unbeknown to @ human reader due to @ inability to process @ @ amount of text @ by illustrating @ individual step @ practical example taken @ use case @ demonstrate @ merit of @ approach @ @ @ methodology @ @ able to reproduce finding @ @ regular @ review @ @ however @ discover additional and @ finding in different field @ a data science @ medicine @ @ @ point @ @ limitation of @ approach @ finally @ make suggestion a to @ @ methodology could @ @ developed @ @ @ 
917,Online community conflict decomposition with pseudo spatial permutation,"Online communities are composed of individuals sharing similar opinions or behavior in the virtual world. Facilitated by the fast development of social media platforms, the expansion of online communities have raised many attentions among the researchers, business analysts, and decision makers, leading to a growing list of literature studying the interactions especially conflicts in the online communities. A conflict is often initiated by one community which then attacks the other, leading to an adversarial relationship and worse social impacts. Many studies have examined the origins and process of online community conflict while failing to address the possible spatial effects in their models. In this paper, we explore the prediction of online community conflict by decomposing and analyzing its prediction error taking geography into accounts. Grounding on the previous natural language processing based model, we introduce pseudo spatial permutation to test the model expressiveness with geographical factors. Pseudo spatial permutation employs different geographical distributions to sample from and perturbs the model using the pseudo geographical information to examine the relationship between online community conflict and spatial distribution. Our analysis shows that the pseudo spatial permutation is an efficient approach to robustly test the conflict relation learned by the prediction model, and also reveals the necessity to incorporate geographical information into the prediction. In conclusion, this work provides a different aspect of analyzing the community conflict that does not solely rely on the textual communication. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,online community @ composed of individual sharing similar opinion @ behavior in @ virtual world @ facilitated by @ fast development of social medium platform @ expansion of online community @ raised many attention among @ researcher @ analyst and decision maker leading to a growing list of literature studying @ interaction especially conflict in @ online community @ a conflict is often initiated by @ community @ @ attack @ @ leading to @ adversarial relationship and worse social impact @ many study @ examined @ origin and process of online community conflict @ failing to address @ possible spatial effect in @ model @ in @ @ @ explore @ prediction of online community conflict by decomposing and analyzing @ prediction error taking geography @ account @ grounding on @ previous natural language processing based model @ introduce pseudo spatial permutation to test @ model expressiveness @ geographical factor @ pseudo spatial permutation employ different geographical distribution to sample @ and perturbs @ model @ @ pseudo geographical information to examine @ relationship @ online community conflict and spatial distribution @ @ analysis @ @ @ pseudo spatial permutation is @ efficient approach to robustly test @ conflict relation learned by @ prediction model and @ reveals @ necessity to incorporate geographical information @ @ prediction @ in conclusion @ work provides a different aspect of analyzing @ community conflict @ doe not solely rely on @ textual communication @ @ nature switzerland ag @ 
919,An information retrieval approach for text mining of medical records based on graph descriptor,"This paper describes a new method of data retrieval from free text documents in medical domain. Proposed approach creates the document summary and highlights most important keywords in the text. To achieve this result we process the document natural language text and build a descriptor as an internal representation of the document. This descriptor is a graph with concepts, relations between them, and concept points as a metric of relevance. By means of points in the descriptor the approach performs ambiguity resolution, selects most relevant concepts to display in the summary, and votes for keywords highlighting in the text. Besides the direct representation of identified information in the summary, this work proposes a way to provide extended summary by using additional knowledge about relations between medications, procedures, diseases and anatomy. The described approach helps to speed up analysis and decision making processes by means of providing aggregated summary for a document and highlighting most meaningful parts of the document's text. Experiment results demonstrate that automatic summary generation and keywords highlighting can be successfully performed by the proposed approach to achieve meaningful and highly relevant results. © 2019 The authors and IOS Press. All rights reserved.",2019,Frontiers in Artificial Intelligence and Applications,1,@ @ describes a @ method of data retrieval @ free text document in medical domain @ proposed approach creates @ document summary and highlight @ important keywords in @ text @ to achieve @ @ @ process @ document natural language text and build a descriptor a @ internal representation of @ document @ @ descriptor is a graph @ concept relation @ @ and concept point a a metric of relevance @ by mean of point in @ descriptor @ approach performs ambiguity resolution selects @ relevant concept to display in @ summary and vote @ keywords highlighting in @ text @ besides @ direct representation of identified information in @ summary @ work proposes a way to provide extended summary by @ additional knowledge @ relation @ medication procedure disease and anatomy @ @ described approach help to speed up analysis and decision making process by mean of providing aggregated summary @ a document and highlighting @ meaningful part of @ document @ s text @ experiment @ demonstrate @ automatic summary generation and keywords highlighting @ @ successfully performed by @ proposed approach to achieve meaningful and highly relevant @ @ @ author and io @ @ @ right reserved @ 
920,Bio-molecular event extraction by integrating multiple event-extraction systems,"Event extraction from biomedical text is a very important task in text mining and natural language processing. The overall task involves finding event-related expressions, classifying these into predefined categories and attaching arguments to these events. We perform event detection and event classification in one step using an ensemble of classifiers. For event argument extraction, we also use an ensemble of classification models. Our base models are developed using supervised machine learning that makes use of statistical, contextual and syntactic features. Our experimental result on the benchmark datasets of BioNLP-2011 shared task shows the recall, precision and F-measure values of 51.20%, 65.78% and 57.58%, respectively. © 2019, Indian Academy of Sciences.",2019,Sadhana - Academy Proceedings in Engineering Sciences,0,event extraction @ biomedical text is a @ important task in text mining and natural language processing @ @ overall task involves finding event-related expression classifying @ @ predefined category and attaching argument to @ event @ @ perform event detection and event classification in @ step @ @ ensemble of classifier @ @ event argument extraction @ @ use @ ensemble of classification model @ @ base model @ developed @ supervised machine learning @ make use of statistical contextual and syntactic feature @ @ experimental @ on @ benchmark datasets of bionlp shared task @ @ recall precision and f-measure value of @ @ and @ respectively @ indian academy of science @ 
921,RBNN application and simulation in big data set classification,"Text classification technology, an important basis for text mining and information retrieval, is mainly to determine the text category according to the text content under a predetermined set of categories. Traditional manual text categorization has gradually failed to meet the needs, while automatic text categorization based on artificial intelligence has become an important research direction in the field of natural language processing. To this end, this paper introduced the RBNN-based classification algorithm by considering the high dimensionality, non-linearity and complex correlation between feature items, and the theoretical and feasibility analysis were carried out so as to apply it to text feature dimension reduction. Also, the effects of the distribution density of the radial basis function in the radial basis neural network and the normalized form of the input data on the classification results were studied. Through the computer simulation experiment, the influence rule of distribution density of the radial basis function in the radial basis neural network and the normalized form of the input data on the training precision and test accuracy of the classification process were demonstrated in the form of curves, which provides guidance for the application of RBNN in pattern recognition. © 2019 - IOS Press and the authors. All rights reserved.",2019,Journal of Intelligent and Fuzzy Systems,2,text classification technology @ important basis @ text mining and information retrieval is mainly to determine @ text category according to @ text content @ a predetermined set of category @ traditional manual text categorization ha gradually failed to meet @ need @ automatic text categorization based on artificial intelligence ha become @ important research direction in @ field of natural language processing @ to @ end @ @ introduced @ rbnn-based classification algorithm by considering @ high dimensionality non-linearity and complex correlation @ feature item and @ theoretical and feasibility analysis @ carried @ @ a to apply @ to text feature dimension reduction @ @ @ effect of @ distribution density of @ radial basis function in @ radial basis neural network and @ normalized form of @ input data on @ classification @ @ studied @ @ @ computer simulation experiment @ influence rule of distribution density of @ radial basis function in @ radial basis neural network and @ normalized form of @ input data on @ training precision and test accuracy of @ classification process @ demonstrated in @ form of curve @ provides guidance @ @ application of rbnn in pattern recognition @ io @ and @ author @ @ right reserved @ 
922,Analyzing the dabiq magazine: The language and the propaganda structure of ISIS,"The Islamic State of Iraq and Sham (ISIS) still poses a significant concern worldwide due to its brutal attacks and unconventional recruitment strategy despite its recent defeat and loss of territory. ISIS distinguished itself from other notorious terrorist organizations regarding Techniques, Tactics, and Procedures (TTP). It has been observed that ISIS is highly capable of attracting foreign fighters through its improved “netwar” skills. Whereas its propaganda videos and images have been extensively analyzed, a systematic analysis of textual content is still lacking. Therefore, we examine the Dabig magazine to discover propagandist elements by performing natural language processing (NLP) and text mining methods. Namely, we first automatically detect three types of entities (person, location, organization) in each article for fifteen Dabiq issues. Then we build entity networks based on co-occurrence of entities to observe the entity relationships over time. We further employ topic modeling on all articles and calculate statistics for entities. We observe entities revolve around the term “jihad,” and the ISIS consistently seems to exploit the sources of Islam in their propaganda. The analysis also revealed that ISIS primarily targets Shiites by using derogatory language about their belief system and try to justify their attacks against them. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ islamic state of iraq and sham @ isi @ still pose a significant concern worldwide due to @ brutal attack and unconventional recruitment strategy despite @ recent defeat and loss of territory @ isi distinguished @ @ @ notorious terrorist organization regarding technique tactic and procedure @ ttp @ @ @ ha @ observed @ isi is highly capable of attracting foreign fighter @ @ improved netwar skill @ whereas @ propaganda video and image @ @ extensively analyzed a systematic analysis of textual content is still lacking @ therefore @ examine @ dabig magazine to discover propagandist element by performing natural language processing @ nlp @ and text mining method @ namely @ first automatically detect three type of entity @ person location organization @ in @ article @ fifteen dabiq issue @ @ @ build entity network based on co-occurrence of entity to observe @ entity relationship @ time @ @ @ employ topic modeling on @ article and calculate statistic @ entity @ @ observe entity revolve around @ term jihad and @ isi consistently seems to exploit @ source of islam in @ propaganda @ @ analysis @ revealed @ isi primarily target shiite by @ derogatory language @ @ belief system and try to justify @ attack @ @ @ @ nature switzerland ag @ 
923,Text classification algorithms: A survey,"In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed. © 2019 by the authors.",2019,Information (Switzerland),138,in recent year @ ha @ @ exponential growth in @ number of complex document and text @ require a deeper understanding of machine learning method to @ able to accurately classify text in many application @ many machine learning approach @ achieved surpassing @ in natural language processing @ @ success of @ learning algorithm relies on @ capacity to understand complex model and non-linear relationship within data @ however finding suitable structure architecture and technique @ text classification is a challenge @ researcher @ in @ @ a brief overview of text classification algorithm is discussed @ @ overview cover different text feature extraction dimensionality reduction method existing algorithm and technique and evaluation method @ finally @ limitation of @ technique and @ application in real-world problem @ discussed @ by @ author @ 
925,6-Tier Design Framework for Smart Solution as a Service (SSaaS) Using Big Data,"The data is floating everywhere as a result of availability of smart connected devices, and an attempt to decipher this ocean of data has made big data analytics a buzzword today. After industrial revolution, Internet revolution, we are witnessing third revolution, i.e., the Internet powered by IoT and Big Data. Extracting the value from this huge data and making the data useful through Big Data Analytics is the need of the hour. The world as a whole is facing many problems or challenges in multiple areas towards extracting valuable insight from the pile of digital data which is heterogeneous and distributed. The paper suggests a 6-tier framework using Text Mining, Natural Language Processing and Data Analytics to suggest Smart Solution(s) as a Service. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,0,@ data is floating everywhere a a @ of availability of smart connected device and @ attempt to decipher @ ocean of data ha made big data analytics a buzzword today @ @ industrial revolution internet revolution @ @ witnessing third revolution i @ e @ @ internet powered by iot and big data @ extracting @ value @ @ huge data and making @ data useful @ big data analytics is @ need of @ hour @ @ world a a whole is facing many problem @ challenge in multiple area towards extracting valuable insight @ @ pile of digital data @ is heterogeneous and distributed @ @ @ suggests a tier framework @ text mining natural language processing and data analytics to suggest smart solution @ s @ a a service @ @ nature singapore pte ltd @ 
927,Real Time Traffic Incident Detection by Using Twitter Stream Analysis,"Internet sites are sources of information for the detection of events, a special mention of traffic activity and accidental accidents or earthquake detection system. Because of the rapid growth of the last 20 years, there have been frequent traffic congestions in cities around the world. The increase in vehicles has caused a greater number of traffic events and, as a result, there are no common resources. We present a methodology for the acquisition, processing and classification of public Tweets with Natural Language Processing (NLP) techniques using the Vector Machine Support (SVM) algorithm, using text classification using social network data to detect incidents. Our view can detect tweets related to traffic, with an accuracy of 88.27%. In this document, we focus on a real-time monitoring system to detect traffic, for Twitter streams analysis by ranking of Twitter posts. We cannot even distinguish if an outdoor event throws traffic or not, multiplying the classification problem and correcting it by point 88.89%. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,1,internet site @ source of information @ @ detection of event a special mention of traffic activity and accidental accident @ earthquake detection system @ @ of @ rapid growth of @ last year @ @ @ frequent traffic congestion in city around @ world @ @ increase in vehicle ha caused a greater number of traffic event and a a @ @ @ no common resource @ @ @ a methodology @ @ acquisition processing and classification of public tweet @ natural language processing @ nlp @ technique @ @ vector machine support @ svm @ algorithm @ text classification @ social network data to detect incident @ @ view @ detect tweet related to traffic @ @ accuracy of @ @ in @ document @ focus on a real-time monitoring system to detect traffic @ twitter stream analysis by ranking of twitter post @ @ cannot even distinguish if @ outdoor event throw traffic @ not multiplying @ classification problem and correcting @ by point @ @ @ nature switzerland ag @ 
928,"Bag-of-words, bag-of-topics and word-to-vec based subject classification of text documents in Polish - A comparative study","This paper deals with the problem of classification of Polish language documents in terms of a subject category. We compare four state-of-the-art approaches to this task which differ primarily in the way the documents are represented by feature vectors. Two methods considered in the study use frequency-of-words or frequency-of-topics representation of the documents and rely on the Natural Language Processing (NLP) technology to pre-process the raw text. Two alternative methods do not involve the NLP technology. They construct feature vectors using vector representation of words (Word2Vec method) or using a frequency of topics derived from the raw text. These four approaches are evaluated using 3 corpora with 5, 34 and 25 subject categories respectively and with a different level of class discrimination. Results suggest that no single method outperforms other method in all tests, however tests with large number of training observations seem to favour the NLP-free Word2Vec methods. © Springer International Publishing AG, part of Springer Nature 2019.",2019,Advances in Intelligent Systems and Computing,6,@ @ deal @ @ problem of classification of polish language document in term of a subject category @ @ compare four state-of-the-art approach to @ task @ differ primarily in @ way @ document @ represented by feature vector @ @ method considered in @ study use frequency-of-words @ frequency-of-topics representation of @ document and rely on @ natural language processing @ nlp @ technology to pre-process @ raw text @ @ alternative method @ not involve @ nlp technology @ @ construct feature vector @ vector representation of word @ word vec method @ @ @ a frequency of topic derived @ @ raw text @ @ four approach @ evaluated @ corpus @ and subject category respectively and @ a different level of class discrimination @ @ suggest @ no single method outperforms @ method in @ test however test @ @ number of training observation seem to favour @ nlp-free word vec method @ @ international publishing ag part of @ nature @ 
931,Sentiment analysis of customer data,"This paper presents an application of sentiment analysis on customer feedback data in the area of heavy equipment repair services. Sentiment analysis is used as a part of a framework for text mining-based Customer Loyalty Improvement Recommender System (CLIRS). In order to provide business users of the system with accurate predictions for customer satisfaction metrics, the original algorithm for the opinion mining needed to be improved. The paper presents the background of the proposed approach, the current techniques used to mine text data and existing applications of sentiment analysis. We propose an aspect-based, taxonomy-driven approach for customized sentiment analysis. The contribution of this paper is the implementation and evaluation of the proposed methods that improve the accuracy and coverage of the opinion mining algorithm. The improvements are illustrated with examples covered by the algorithm in the customer dataset. The application of the proposed methods resulted in increasing the algorithm's accuracy from 92% to 96%, and coverage from 36% to 48%. This research is an attempt to handle well-known issues in natural language processing that are currently not handled by text mining algorithms, such as ambiguity and context, opinionated verbs/nouns, subject recognition from pronouns. This is significant because the proposed techniques are generalizable to any application that uses sentiment analysis algorithm. © 2019 - IOS Press and the authors. All rights reserved.",2019,Web Intelligence,1,@ @ @ @ application of sentiment analysis on customer feedback data in @ area of heavy equipment repair service @ sentiment analysis is used a a part of a framework @ text mining-based customer loyalty improvement recommender system @ clirs @ @ in order to provide @ user of @ system @ accurate prediction @ customer satisfaction metric @ original algorithm @ @ opinion mining needed to @ improved @ @ @ @ @ background of @ proposed approach @ current technique used to mine text data and existing application of sentiment analysis @ @ propose @ aspect-based taxonomy-driven approach @ customized sentiment analysis @ @ contribution of @ @ is @ implementation and evaluation of @ proposed method @ improve @ accuracy and coverage of @ opinion mining algorithm @ @ improvement @ illustrated @ example covered by @ algorithm in @ customer dataset @ @ application of @ proposed method resulted in increasing @ algorithm @ s accuracy @ to and coverage @ to @ @ research is @ attempt to handle well-known issue in natural language processing @ @ currently not handled by text mining algorithm @ a ambiguity and context opinionated verb noun subject recognition @ pronoun @ @ is significant @ @ proposed technique @ generalizable to @ application @ us sentiment analysis algorithm @ io @ and @ author @ @ right reserved @ 
933,Customized Visualization of Email Using Sentimental and Impact Analysis in R,"In our modern world of social interactions where the analysis of each content on social media is based on the impact of the sentiment it imposes on the forum. The proposed system is used to implement a more personalized and customized report of these impacts. Email is of main focus here where in out of all other social applications, responses to and from the email is the most traditional and ethical way to communicate online. Users share all information, through internet especially emails because of its fast transmission and is considered as the most professional medium. Hence the proposed model focus more on the subjective content of email processed on R libraries created for Natural language processing in a more customized way. Nowadays, crime rate in emails are increasing drastically. Spamming, phishing and email fraudulent are the ways of targeting common people. The sentimental analysis on the impact of the email received is analysed and visualized. The system also proposes a design for establishing a framework that detects the suspicious one by comparing the mail with keywords and also reveals the level of suspiciousness in the particular mail. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,0,in @ modern world of social interaction @ @ analysis of @ content on social medium is based on @ impact of @ sentiment @ imposes on @ forum @ @ proposed system is used to implement a more personalized and customized report of @ impact @ email is of main focus @ @ in @ of @ @ social application response to and @ @ email is @ @ traditional and ethical way to communicate online @ user share @ information @ internet especially email @ of @ fast transmission and is considered a @ @ professional medium @ hence @ proposed model focus more on @ subjective content of email processed on r library created @ natural language processing in a more customized way @ nowadays crime rate in email @ increasing drastically @ spamming phishing and email fraudulent @ @ way of targeting common people @ @ sentimental analysis on @ impact of @ email received is analysed and visualized @ @ system @ proposes a design @ establishing a framework @ detects @ suspicious @ by comparing @ mail @ keywords and @ reveals @ level of suspiciousness in @ particular mail @ @ nature singapore pte ltd @ 
934,"Generation of Hindi word embeddings and their utilization in ranking documents using negative sampling architecture, t-SNE visualization and TF-IDF based weighted average of vectors","Hindi is the official language of India and has over 500 million speakers worldwide. Being a dominant language with a widespread impact, implies the need for development of technologies that cater to its native speakers. In this paper, a text mining based information retrieval model has been developed to generate Hindi word embeddings and their application ranking documents in order of relevance to an input query. Word embeddings are multi-dimensional vectors that can be created by utilizing the linguistic context of words in a large corpus. To generate the embeddings, a corpus was created from the Hindi Wikipedia dump, on which the skip-gram approach was applied using a neural network based negative sampling-architecture. The weighted average of each word embedding along with its tf-idf score generated the embeddings for each individual document. The cosine-similarity was then calculated between each document vector and the query vector. Using these similarity scores, the documents were ranked in descending order of relevance to the query. Highly relevant rankings were obtained in response to a query input. The results of the model were visualized using the t-SNE visualization method. The accuracy of this method proves that in the process of conversion of words to numeric vectors, the semantic context of the words was preserved. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,0,hindi is @ official language of india and ha @ million speaker worldwide @ @ a dominant language @ a widespread impact implies @ need @ development of technology @ cater to @ native speaker @ in @ @ a text mining based information retrieval model ha @ developed to generate hindi word embeddings and @ application ranking document in order of relevance to @ input query @ word embeddings @ multi-dimensional vector @ @ @ created by utilizing @ linguistic context of word in a @ corpus @ to generate @ embeddings a corpus wa created @ @ hindi wikipedia dump on @ @ skip-gram approach wa applied @ a neural network based negative sampling-architecture @ @ weighted average of @ word embedding along @ @ tf-idf score generated @ embeddings @ @ individual document @ @ cosine-similarity wa @ calculated @ @ document vector and @ query vector @ @ @ similarity score @ document @ ranked in descending order of relevance to @ query @ highly relevant ranking @ obtained in response to a query input @ @ @ of @ model @ visualized @ @ t-sne visualization method @ @ accuracy of @ method prof @ in @ process of conversion of word to numeric vector @ semantic context of @ word wa preserved @ @ nature switzerland ag @ 
935,Development of a kansei engineering artificial intelligence sightseeing application,"Kansei engineering methods and text mining methods were applied to the web application for assisting sightseeing travel at Kure city. Kansei engineering methods used here were a questionnaire and multivariate analyses for research inter-relations between aims of travels and interests. Text mining was done on logged tweets on Twitter. The number of tweets mentioned on Kure was around 3400 to 3700 tweets per day. The text mining reveals latest events and people’s interests in the daily basis. With Twitter text mining to conventional Kansei engineering methods, both general Kansei on sightseeing and rapidly changing interests are kept reflecting to the inference rules of the web-application. © Springer International Publishing AG, part of Springer Nature 2019.",2019,Advances in Intelligent Systems and Computing,0,kansei engineering method and text mining method @ applied to @ web application @ assisting sightseeing travel at kure city @ kansei engineering method used @ @ a questionnaire and multivariate analysis @ research inter-relations @ aim of travel and interest @ text mining wa done on logged tweet on twitter @ @ number of tweet mentioned on kure wa around to tweet per day @ @ text mining reveals latest event and people s interest in @ daily basis @ @ twitter text mining to conventional kansei engineering method @ general kansei on sightseeing and rapidly changing interest @ kept reflecting to @ inference rule of @ web-application @ @ international publishing ag part of @ nature @ 
937,Distributed Framework for Automating Opinion Discretization from Text Corpora on Facebook,"Nowadays, the consecutive increase of the volume of text corpora datasets and the countless research directions in general classification have created a great opportunity and an unprecedented demand for a comprehensive evaluation of the current achievement in the research of natural language processing. There are unfortunately few studies that have applied the combination of convolutional neural networks (CNN) and Apache Spark to the task of automating opinion discretization. In this paper, the authors propose a new distributed structure for solving an opinion classification problem in text mining by utilizing CNN models and big data technologies on Vietnamese text sources. The proposed framework consists of implementation concepts that are needed by a researcher to perform experiments on text discretization problems. It covers all the steps and components that are usually part of a completely practical text mining pipeline: Acquiring input data, processing, tokenizing it into a vectorial representation, applying machine learning algorithms, performing the trained models to unseen data, and evaluating their accuracy. The development of the framework started with a specific focus on binary text discretization, but soon expanded toward many other text-categorization-based problems, distributed language modeling and quantification. Several intensive assessments have been investigated to prove the robustness and efficiency of the proposed framework. Resulting in high accuracy (72.99% ± 3.64) from the experiments, one can conclude that it is feasible to perform our proposed distributed framework to the task of opinion discretization on Facebook. © 2013 IEEE.",2019,IEEE Access,2,nowadays @ consecutive increase of @ volume of text corpus datasets and @ countless research direction in general classification @ created a great opportunity and @ unprecedented demand @ a comprehensive evaluation of @ current achievement in @ research of natural language processing @ @ @ unfortunately @ study @ @ applied @ combination of convolutional neural network @ cnn @ and apache spark to @ task of automating opinion discretization @ in @ @ @ author propose a @ distributed structure @ solving @ opinion classification problem in text mining by utilizing cnn model and big data technology on vietnamese text source @ @ proposed framework consists of implementation concept @ @ needed by a researcher to perform experiment on text discretization problem @ @ cover @ @ step and component @ @ usually part of a completely practical text mining pipeline @ acquiring input data processing tokenizing @ @ a vectorial representation applying machine learning algorithm performing @ trained model to unseen data and evaluating @ accuracy @ @ development of @ framework started @ a specific focus on binary text discretization @ soon expanded toward many @ text-categorization-based problem distributed language modeling and quantification @ several intensive assessment @ @ investigated to prove @ robustness and efficiency of @ proposed framework @ resulting in high accuracy @ @ @ @ @ @ experiment @ @ conclude @ @ is feasible to perform @ proposed distributed framework to @ task of opinion discretization on facebook @ @ @ 
938,Data analysis through social media according to the classified crime,"The amount and variety of data generated through social media sites has increased along with the widespread use of social media sites. In addition, the data production rate has increased in the same way. The inclusion of personal information within these data makes it important to process the data and reach meaningful information within it. This process can be called intelligence and this meaningful information may be for commercial, academic, or security purposes. An example application is developed in this study for intelligence on Twitter. Crimes in Turkey are classified according to Turkish Statistical Institute criminal data and keywords are defined according to this data. A total of 150,000 tweet data in the Turkish language are collected from Twitter between specified dates and processed by Turkish Zemberek natural language processing. It is seen that 56% of the people are talking about terrorist attacks and bombing attacks on the study dates. The words “bomb,” “terror,” “attack,” “organization”, and “explode” have percentages of 24%, 12%, 8%, 6%, and 6%, respectively. Moreover, associations between words and situations are found. Correlations are important to create new subclusters like “terror” and “rape” in this study with 0.90 correlation. Bigger masses can be accessible by expanding keyword groups to have a clear picture of the real situation. © TÜBİTAK.",2019,Turkish Journal of Electrical Engineering and Computer Sciences,1,@ amount and variety of data generated @ social medium site ha increased along @ @ widespread use of social medium site @ in addition @ data production rate ha increased in @ @ way @ @ inclusion of personal information within @ data make @ important to process @ data and reach meaningful information within @ @ @ process @ @ called intelligence and @ meaningful information may @ @ commercial @ @ security purpose @ @ example application is developed in @ study @ intelligence on twitter @ crime in turkey @ classified according to turkish statistical institute criminal data and keywords @ defined according to @ data @ a total of tweet data in @ turkish language @ collected @ twitter @ specified date and processed by turkish zemberek natural language processing @ @ is seen @ of @ people @ talking @ terrorist attack and bombing attack on @ study date @ @ word bomb terror attack organization and explode @ percentage of and respectively @ moreover association @ word and situation @ found @ correlation @ important to create @ subclusters like terror and rape in @ study @ @ correlation @ bigger mass @ @ accessible by expanding keyword group to @ a clear picture of @ real situation @ tübi tak @ 
941,Leaving no stone unturned: Using machine learning based approaches for information extraction from full texts of a research data warehouse,"Data in healthcare and routine medical treatment is growing fast. Therefore and because of its variety, possible correlation within these are becoming even more complex. Popular tools for facilitating the daily routine for the clinical researchers are more often based on machine learning (ML) algorithms. Those tools might facilitate data management, data integration or even content classification. Besides commercial functionalities, there are many solutions which are developed by the user himself for his own, specific question of research or task. One of these tasks is described within this work: qualifying the Weber fracture, an ankle joint fracture, from radiological findings with the help of supervised machine learning algorithms. To do so, the findings were firstly processed with common natural language processing (NLP) methods. For the classifying part, we used the bags-of-words-approach to bring together the medical findings on the one hand, and the metadata of the findings on the other hand, and compared several common classifier to have the best results. In order to conduct this study, we used the data and the technology of the Enterprise Clinical Research Data Warehouse (ECRDW) from Hannover Medical School. This paper shows the implementation of machine learning and NLP techniques into the data warehouse integration process in order to provide consolidated, processed and qualified data to be queried for teaching and research purposes. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,data in healthcare and routine medical treatment is growing fast @ therefore and @ of @ variety possible correlation within @ @ becoming even more complex @ popular tool @ facilitating @ daily routine @ @ clinical researcher @ more often based on machine learning @ ml @ algorithm @ @ tool might facilitate data management data integration @ even content classification @ besides commercial functionality @ @ many solution @ @ developed by @ user @ @ @ @ specific question of research @ task @ @ of @ task is described within @ work @ qualifying @ weber fracture @ ankle joint fracture @ radiological finding @ @ help of supervised machine learning algorithm @ to @ @ @ finding @ firstly processed @ common natural language processing @ nlp @ method @ @ @ classifying part @ used @ bags-of-words-approach to bring together @ medical finding on @ @ hand and @ metadata of @ finding on @ @ hand and compared several common classifier to @ @ best @ @ in order to conduct @ study @ used @ data and @ technology of @ enterprise clinical research data warehouse @ ecrdw @ @ hannover medical school @ @ @ @ @ implementation of machine learning and nlp technique @ @ data warehouse integration process in order to provide consolidated processed and qualified data to @ queried @ teaching and research purpose @ @ nature switzerland ag @ 
942,Towards a statistical approach for user classification in twitter,"In this paper, we propose a novel technique for classifying user accounts on online social networks. The main purpose of our classification is to distinguish the patterns of users from those of organizations and individuals. The ability of distinguishing between the two account types is needed for developing recommendation engines, consumer products opinion mining tools, and information dissemination platforms. However, such a task is non-trivial. Classic and consolidated approaches of text mining use textual features from natural language processing for classification. Nevertheless, such approaches still have some drawbacks like the computational cost and time consumption. In this work, we propose a statistical approach based on post frequency, metadata of user profile, and popularity of posts so as to recognize the type of users without textual content. We performed a set of experiments over a twitter dataset and learn-based algorithms in classification task. Several supervised learning algorithms were tested. We achieved high f-measure results of 96.2% using imbalanced datasets and (GBRT), 1.9% were gains when we used imbalanced datasets with Synthetic Minority Oversampling technique and (RF), this yields 98.1%. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ propose a novel technique @ classifying user account on online social network @ @ main purpose of @ classification is to distinguish @ pattern of user @ @ of organization and individual @ @ ability of distinguishing @ @ @ account type is needed @ developing recommendation engine consumer product opinion mining tool and information dissemination platform @ however @ a task is non-trivial @ classic and consolidated approach of text mining use textual feature @ natural language processing @ classification @ nevertheless @ approach still @ some drawback like @ computational cost and time consumption @ in @ work @ propose a statistical approach based on post frequency metadata of user profile and popularity of post @ a to recognize @ type of user without textual content @ @ performed a set of experiment @ a twitter dataset and learn-based algorithm in classification task @ several supervised learning algorithm @ tested @ @ achieved high f-measure @ of @ @ imbalanced datasets and @ gbrt @ @ @ gain @ @ used imbalanced datasets @ synthetic minority oversampling technique and @ rf @ @ yield @ @ @ nature switzerland ag @ 
943,Machine learning methods for research highlight prediction in biomedical effects of nanomaterial application,"Recently, the studies on biomedical effects of nanomaterials have already achieved many progresses tightly relating to human health. But the large amounts of research achievements in nanomaterial journals bomb too massive data to clarify the research highlight prediction. Fortunately, automatic text mining methods can complete the extracting information from a large set of documents efficiently by machining learning methods. We used both Naive Bayes and K-means clustering algorithms on manually labeled research data sets. It is 88.62% by the Naive Bayes algorithm classification result of 5-folds cross validation on sampled libraries. By applying the optimized Naive Bayes classification model, we made the research highlight trend prediction based on research achievements of biomedical effects of nanomaterials in 22 cutting edge nanomaterial journals including 350,000 original literatures in period from 2000 to 2017. The data mining clarified the polymer nanomaterial is the most researched nanomaterial but with a decreasing trend. The research interests of metallic and carbon based nanomaterial follow the polymer one, and possess increasing trend. We could predict that the research highlight trend on biomedical effects of nanomaterials is focused on polymer, metallic and carbon based material systems in the near future. © 2018",2019,Pattern Recognition Letters,3,recently @ study on biomedical effect of nanomaterials @ already achieved many progress tightly relating to human health @ @ @ @ amount of research achievement in nanomaterial journal bomb too massive data to clarify @ research highlight prediction @ fortunately automatic text mining method @ complete @ extracting information @ a @ set of document efficiently by machining learning method @ @ used @ naive bayes and k-means clustering algorithm on manually labeled research data set @ @ is @ by @ naive bayes algorithm classification @ of fold cross validation on sampled library @ by applying @ optimized naive bayes classification model @ made @ research highlight trend prediction based on research achievement of biomedical effect of nanomaterials in cutting edge nanomaterial journal including original literature in period @ to @ @ data mining clarified @ polymer nanomaterial is @ @ researched nanomaterial @ @ a decreasing trend @ @ research interest of metallic and carbon based nanomaterial follow @ polymer @ and posse increasing trend @ @ could predict @ @ research highlight trend on biomedical effect of nanomaterials is focused on polymer metallic and carbon based material system in @ near future @ 
945,SCANCPECLENS: A Framework for Automatic Lexicon Generation and Sentiment Analysis of Micro Blogging Data on China Pakistan Economic Corridor,"With the growing availability of internet and opinion rich resources such as social networks and personal blogs, the task of mining public opinion and exploring facts has become more popular than ever before during the last decade. The latest trend has deeply transformed the way the governments interact with their citizens and offer them various services through continuous public engagement. The proposed framework SCANCPECLENS is an initiative to support performance assessment framework for e-government in Pakistan. The research takes into account the opinion of masses on one of the most crucial and widely discussed development projects, China Pakistan Economic Corridor (CPEC), considered as a game changer due to its promise of bringing economic prosperity to the region. The proposed framework suggests to use machine learning algorithms to automatically discover the public sentiment from microblogs on the matter nationally as well as internationally. We also present an automated way to create sentiment lexicon of positive, negative and neutral words on the subject. To the best of our knowledge, this theme has not been explored for opinion mining before and helps one in effectively assessing public satisfaction over government's policies in the CPEC region. The research is an initiative to discover new avenues of future research and direction for the government, policy making institutions and other stake holders and demonstrates the power of text mining as an effective tool to extract business value from vast amount of social media data. © 2013 IEEE.",2019,IEEE Access,1,@ @ growing availability of internet and opinion rich resource @ a social network and personal blog @ task of mining public opinion and exploring fact ha become more popular @ ever @ @ @ last decade @ @ latest trend ha deeply transformed @ way @ government interact @ @ citizen and offer @ various service @ continuous public engagement @ @ proposed framework scancpeclens is @ initiative to support performance assessment framework @ e-government in pakistan @ @ research take @ account @ opinion of mass on @ of @ @ crucial and widely discussed development project china pakistan economic corridor @ cpec @ considered a a game changer due to @ promise of bringing economic prosperity to @ region @ @ proposed framework suggests to use machine learning algorithm to automatically discover @ public sentiment @ microblogs on @ matter nationally a well a internationally @ @ @ @ @ automated way to create sentiment lexicon of positive negative and neutral word on @ subject @ to @ best of @ knowledge @ theme ha not @ explored @ opinion mining @ and help @ in effectively assessing public satisfaction @ government @ s policy in @ cpec region @ @ research is @ initiative to discover @ avenue of future research and direction @ @ government policy making institution and @ stake holder and demonstrates @ power of text mining a @ effective tool to extract @ value @ vast amount of social medium data @ @ @ 
946,"Using Twitter to Infer User Satisfaction with Public Transport: The Case of Santiago, Chile","User satisfaction is an important aspect to consider in any public transport system, and as such, regular and sound measurements of its levels are fundamental. However, typical evaluation schemes involve costly and time-consuming surveys. As a consequence, their frequency is not enough to properly and timely characterize the satisfaction of the users. In this paper, we propose a methodology, based on Twitter data, to capture the satisfaction of a large mass of users of public transport, allowing us to improve the characterization and location of their satisfaction level. We analyzed a massive volume of tweets referring to the public transport system in Santiago, Chile (Transantiago) using text mining techniques, such as sentiment analysis and topic modeling, in order to capture and group bus users' expressions. Results show that, although the level of detail and variety of answers obtained from surveys are higher than the ones obtained by our method, the amount of bus stops and bus services covered by the proposed scheme is larger. Moreover, the proposed methodology can be effectively used to diagnose problems in a timely manner, as it is able to identify and locate trends, and issues related to bus operating firms, whereas surveys tend to produce average answers. Based on the consistency and logic of the results, we argue that the proposed methodology can be used as a valuable complement to surveys, as both present different, but compatible characteristics. © 2013 IEEE.",2019,IEEE Access,2,user satisfaction is @ important aspect to consider in @ public transport system and a @ regular and sound measurement of @ level @ fundamental @ however typical evaluation scheme involve costly and time-consuming survey @ a a consequence @ frequency is not enough to properly and timely characterize @ satisfaction of @ user @ in @ @ @ propose a methodology based on twitter data to capture @ satisfaction of a @ mass of user of public transport allowing u to improve @ characterization and location of @ satisfaction level @ @ analyzed a massive volume of tweet referring to @ public transport system in santiago chile @ transantiago @ @ text mining technique @ a sentiment analysis and topic modeling in order to capture and group bus user @ expression @ @ @ @ although @ level of detail and variety of answer obtained @ survey @ higher @ @ @ obtained by @ method @ amount of bus stop and bus service covered by @ proposed scheme is larger @ moreover @ proposed methodology @ @ effectively used to diagnose problem in a timely manner a @ is able to identify and locate trend and issue related to bus operating firm whereas survey tend to produce average answer @ based on @ consistency and logic of @ @ @ argue @ @ proposed methodology @ @ used a a valuable complement to survey a @ @ different @ compatible characteristic @ @ @ 
947,Knowledge-enhanced document embeddings for text classification,"Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios. © 2018 Elsevier B.V.",2019,Knowledge-Based Systems,39,accurate semantic representation model @ essential in text mining application @ @ a successful application of @ text mining process @ text representation adopted must keep @ interesting pattern to @ discovered @ although competitive @ @ automatic text classification may @ achieved @ traditional bag of word @ representation model cannot provide satisfactory classification performance on hard setting @ richer text representation @ required @ in @ @ @ @ @ approach to represent document collection based on embedded representation of word and word sens @ @ bring together @ power of word sense disambiguation and @ semantic richness of word and word-sense embedded vector to construct embedded representation of document collection @ @ approach @ in semantically enhanced and low-dimensional representation @ @ overcome @ lack of interpretability of embedded vector @ is a drawback of @ kind of representation @ @ use of word sense embedded vector @ moreover @ experimental evaluation indicates @ @ use of @ proposed representation provides stable classifier @ strong quantitative @ especially in semantically-complex classification scenario @ @ b @ v @ 
948,An efficient preprocessing method for supervised sentiment analysis by converting sentences to numerical vectors: a twitter case study,"Along with the significant growth of social media, individuals and companies are increasingly receiving public opinions which direct their decisions. Opinion mining, which is considered as a sub-field of natural language processing, information retrieval, and text mining, is the process of understanding the users’ views from their comment, which have been represented as unstructured texts. Emergence of online social media has led to the production of a huge amount of user comments on websites, and thus, has raised opinion mining as a very useful and challenging problem. In this paper, an efficient preprocessing method for opinion mining is presented and will be used for analyzing users’ comments on Twitter social network. For this purpose, different text preprocessing techniques have been used on the dataset to achieve an acceptable standard text. Word2vec method which is a fast and accurate method have been also exploited to convert the words’ arrays to numerical vectors. Machine learning methods, with supervised learning approach, have been applied on the obtained data after this fast and accurate preprocessing phase. Python and RapidMiner have been used to implement different opinion mining methods and the results of these implementations have been compared and evaluated. The experimental results show that the combined use of the preprocessing method of this paper and support vector machine and artificial neural network have the highest accuracy compared to other methods. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Multimedia Tools and Applications,6,along @ @ significant growth of social medium individual and company @ increasingly receiving public opinion @ direct @ decision @ opinion mining @ is considered a a sub-field of natural language processing information retrieval and text mining is @ process of understanding @ user view @ @ comment @ @ @ represented a unstructured text @ emergence of online social medium ha led to @ production of a huge amount of user comment on website and thus ha raised opinion mining a a @ useful and challenging problem @ in @ @ @ efficient preprocessing method @ opinion mining is presented and @ @ used @ analyzing user comment on twitter social network @ @ @ purpose different text preprocessing technique @ @ used on @ dataset to achieve @ acceptable standard text @ word vec method @ is a fast and accurate method @ @ @ exploited to convert @ word array to numerical vector @ machine learning method @ supervised learning approach @ @ applied on @ obtained data @ @ fast and accurate preprocessing phase @ python and rapidminer @ @ used to implement different opinion mining method and @ @ of @ implementation @ @ compared and evaluated @ @ experimental @ @ @ @ combined use of @ preprocessing method of @ @ and support vector machine and artificial neural network @ @ highest accuracy compared to @ method @ @ science @ medium llc part of @ nature @ 
951,Exploring bigram character features for Arabic text clustering,"The vector space model (VSM) is an algebraic model that is widely used for data representation in text mining applications. However, the VSM poses a critical challenge, as it requires a high-dimensional feature space. Therefore, many feature selection techniques, such as employing roots or stems (i.e. words without infixes and prefixes, and/or suffixes) instead of using complete word forms, are proposed to tackle this space challenge problem. Recently, the literature shows that one more basic unit feature can be used to handle the textual features, which is the two-neighboring character form that we call microword. To evaluate this feature type, we measure the accuracy of the Arabic text clustering using two feature types: the complete word form and the microword form. Hence, the microword is two consecutive characters which are also known as the Bigram character feature. In the experiment, the principal component analysis (PCA) is used to reduce the feature vector dimensions while the k-means algorithm is used for the clustering purposes. The testing set includes 250 documents of five categories. The entire corpus contains 54,472 words, whereas the vocabulary contains 13,356 unique words. The experimental results show that the complete word form score accuracy is 97.2% while the two-character form score is 96.8%. In conclusion, the accuracies are almost the same; however, the two-character form uses a smaller vocabulary as well as less PCA subspaces. The study experiments might be a significant indication of the necessity to consider the Bigram character feature in the future text processing and natural language processing applications. © TÜBİTAK",2019,Turkish Journal of Electrical Engineering and Computer Sciences,2,@ vector space model @ vsm @ is @ algebraic model @ is widely used @ data representation in text mining application @ however @ vsm pose a critical challenge a @ requires a high-dimensional feature space @ therefore many feature selection technique @ a employing root @ stem @ i @ e @ word without infix and prefix and @ suffix @ instead of @ complete word form @ proposed to tackle @ space challenge problem @ recently @ literature @ @ @ more basic unit feature @ @ used to handle @ textual feature @ is @ two-neighboring character form @ @ call microword @ to evaluate @ feature type @ measure @ accuracy of @ arabic text clustering @ @ feature type @ @ complete word form and @ microword form @ hence @ microword is @ consecutive character @ @ @ known a @ bigram character feature @ in @ experiment @ principal component analysis @ pca @ is used to reduce @ feature vector dimension @ @ k-means algorithm is used @ @ clustering purpose @ @ testing set includes document of five category @ @ entire corpus contains word whereas @ vocabulary contains unique word @ @ experimental @ @ @ @ complete word form score accuracy is @ @ @ two-character form score is @ @ in conclusion @ accuracy @ almost @ @ @ however @ two-character form us a smaller vocabulary a well a le pca subspace @ @ study experiment might @ a significant indication of @ necessity to consider @ bigram character feature in @ future text processing and natural language processing application @ tübi tak
954,Collaborative Non-negative Matrix Factorization,"Non-negative matrix factorization is a machine learning technique that is used to decompose large data matrices imposing the non-negativity constraints on the factors. This technique has received a significant amount of attention as an important problem with many applications in different areas such as language modeling, text mining, clustering, music transcription, and neurobiology (gene separation). In this paper, we propose a new approach called Collaborative Non-negative Matrix Factorization ((formula presented)) which is based on the collaboration between several NMF (Non-negative Matrix Factorization) models. Our approach (formula presented)was validated on variant datasets and the experimental results show the effectiveness of the proposed approach. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,non-negative matrix factorization is a machine learning technique @ is used to decompose @ data matrix imposing @ non-negativity constraint on @ factor @ @ technique ha received a significant amount of attention a @ important problem @ many application in different area @ a language modeling text mining clustering music transcription and neurobiology @ gene separation @ @ in @ @ @ propose a @ approach called collaborative non-negative matrix factorization @ @ formula presented @ @ @ is based on @ collaboration @ several nmf @ non-negative matrix factorization @ model @ @ approach @ formula presented @ wa validated on variant datasets and @ experimental @ @ @ effectiveness of @ proposed approach @ @ nature switzerland ag @ 
955,GIDB: A knowledge database for the automated curation and multidimensional analysis of molecular signatures in gastrointestinal cancer,"Gastrointestinal (GI) cancer is common, characterized by high mortality, and includes oesophagus, gastric, liver, bile duct, pancreas, rectal and colon cancers. The insufficient specificity and sensitivity of biomarkers is still a key clinical hindrance for GI cancer diagnosis and successful treatment. The emergence of 'precision medicine', 'basket trial' and 'field cancerization' concepts calls for an urgent need and importance for the understanding of how organ system cancers occur at the molecular levels. Knowledge from both the literature and data available in public databases is informative in elucidating the molecular alterations underlying GI cancer. Currently, most available cancer databases have not offered a comprehensive discovery of gene-disease associations, molecular alterations and clinical information by integrated text mining and data mining in GI cancer. We develop GIDB, a panoptic knowledge database that attempts to automate the curation of molecular signatures using natural language processing approaches and multidimensional analyses. GIDB covers information on 8730 genes with both literature and data supporting evidence, 248 miRNAs, 58 lncRNAs, 320 copy number variations, 49 fusion genes and 2381 semantic networks. It presents a comprehensive database, not only in parallelizing supporting evidence and data integration for signatures associated with GI cancer but also in providing the timeline feature of major molecular discoveries. It highlights the most comprehensive overview, research hotspots and the development of historical knowledge of genes in GI cancer. Furthermore, GIDB characterizes genomic abnormalities in multilevel analysis, including simple somatic mutations, gene expression, DNA methylation and prognosis. GIDB offers a user-friendly interface and two customizable online tools (Heatmap and Network) for experimental researchers and clinicians to explore data and help them shorten the learning curve and broaden the scope of knowledge. More importantly, GIDB is an ongoing research project that will continue to be updated and improve the automated method for reducing manual work. © 2019 The Author(s) 2019. Published by Oxford University Press.",2019,Database,2,gastrointestinal @ gi @ cancer is common characterized by high mortality and includes oesophagus gastric liver bile duct pancreas rectal and colon cancer @ @ insufficient specificity and sensitivity of biomarkers is still a key clinical hindrance @ gi cancer diagnosis and successful treatment @ @ emergence of @ precision medicine @ @ basket trial @ and @ field cancerization @ concept call @ @ urgent need and importance @ @ understanding of @ organ system cancer occur at @ molecular level @ knowledge @ @ @ literature and data available in public database is informative in elucidating @ molecular alteration underlying gi cancer @ currently @ available cancer database @ not offered a comprehensive discovery of gene-disease association molecular alteration and clinical information by integrated text mining and data mining in gi cancer @ @ develop gidb a panoptic knowledge database @ attempt to automate @ curation of molecular signature @ natural language processing approach and multidimensional analysis @ gidb cover information on gene @ @ literature and data supporting evidence mirnas lncrnas copy number variation fusion gene and semantic network @ @ @ a comprehensive database not only in parallelizing supporting evidence and data integration @ signature associated @ gi cancer @ @ in providing @ timeline feature of major molecular discovery @ @ highlight @ @ comprehensive overview research hotspot and @ development of historical knowledge of gene in gi cancer @ furthermore gidb characterizes genomic abnormality in multilevel analysis including simple somatic mutation gene expression dna methylation and prognosis @ gidb offer a user-friendly interface and @ customizable online tool @ heatmap and network @ @ experimental researcher and clinician to explore data and help @ shorten @ learning curve and broaden @ scope of knowledge @ more importantly gidb is @ ongoing research project @ @ continue to @ updated and improve @ automated method @ reducing manual work @ @ author @ s @ @ published by oxford university @ @ 
956,Use of Pseudo Relevance Feedback for Patent Clustering with Fuzzy C-means,"Patent databases are meaningful resources for technology trend detection as they collect information on the recent key innovations; however the importance of wordings in patents and use of complex content are remarkable challenges in key word extraction in the text mining phase. Moreover patents share information by nature and depending on the criterion of classification such as materials or uses, one may belong to multiple classes. For clustering patents, this work proposes an updating fuzzy c-means clustering which employs pseudo relevance feedback originating from information retrieval in order to improve features extracted from the patent collection following feedbacks. The results show a noticeable improvement in clustering after applying pseudo relevance feedback clustering patents under topic contact lenses. Copyright © CIKM 2018.",2019,CEUR Workshop Proceedings,0,patent database @ meaningful resource @ technology trend detection a @ collect information on @ recent key innovation @ however @ importance of wording in patent and use of complex content @ remarkable challenge in key word extraction in @ text mining phase @ moreover patent share information by nature and depending on @ criterion of classification @ a material @ us @ may belong to multiple class @ @ clustering patent @ work proposes @ updating fuzzy c-means clustering @ employ pseudo relevance feedback originating @ information retrieval in order to improve feature extracted @ @ patent collection following feedback @ @ @ @ a noticeable improvement in clustering @ applying pseudo relevance feedback clustering patent @ topic contact lens @ @ cikm @ 
958,CBLNER: A Multi-models Biomedical Named Entity Recognition System Based on Machine Learning,"Biomedical named entities is fundamental recognition task in biomedical text mining. This paper developed a system for identifying biomedical entities with four models including CRF, LSTM, Bi-LSTM and BiLSTM-CRF. The system achieved the following performance in test data Genia V3.02: CRF with an F score of 75.91%, LSTM with an F score of 71.69%, BiLSTM with a F score of 74.37%, BiLSTM-CRF with a F score of 76.81%. Experimental results show the performance of BiLSTM-CRF model is better than other three models. Compared with CRF model, Bi-LSTM-CRF model has better recognition effect for biological entities in long text and entities that modified by modifiers. Therefore, CBLNER system lays a foundation for further relationship and event extraction, and could also provide reference for entity recognition research in other fields. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,biomedical named entity is fundamental recognition task in biomedical text mining @ @ @ developed a system @ identifying biomedical entity @ four model including crf lstm bi-lstm and bilstm-crf @ @ system achieved @ following performance in test data genia v @ @ crf @ @ f score of @ lstm @ @ f score of @ bilstm @ a f score of @ bilstm-crf @ a f score of @ @ experimental @ @ @ performance of bilstm-crf model is better @ @ three model @ compared @ crf model bi-lstm-crf model ha better recognition effect @ biological entity in long text and entity @ modified by modifier @ therefore cblner system lay a foundation @ @ relationship and event extraction and could @ provide reference @ entity recognition research in @ field @ @ nature switzerland ag @ 
959,Analyzing sentiment level of social media data based on SVM and Naïve Bayes algorithms,"Social media is a popular network through which users can share their reviews about various topics, news, products etc. People use internet to access or update reviews so it is necessary to express opinion. Twitter is a hugely valuable resource from which insights can be extracted by using text mining tools like sentiment analysis. Sentiment analysis is the task of identifying opinion from reviews. The system performs classification by combining Naïve Bayes (NB) and Support Vector Machine (SVM). The system is intended to measure the impact of ASEAN citizens’ social media based on their usage behavior. The system is developed for analyzing National Educational Rate, Business Rate and Crime Rate occurred in Malaysia, Singapore, Vietnam and our country, Myanmar. The system compares the performance of these two classifiers in accuracy, precision and recall. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,0,social medium is a popular network @ @ user @ share @ review @ various topic news product etc @ people use internet to access @ update review @ @ is necessary to express opinion @ twitter is a hugely valuable resource @ @ insight @ @ extracted by @ text mining tool like sentiment analysis @ sentiment analysis is @ task of identifying opinion @ review @ @ system performs classification by combining naïve bayes @ nb @ and support vector machine @ svm @ @ @ system is intended to measure @ impact of asean citizen social medium based on @ usage behavior @ @ system is developed @ analyzing national educational rate @ rate and crime rate occurred in malaysia singapore vietnam and @ country myanmar @ @ system compare @ performance of @ @ classifier in accuracy precision and recall @ @ nature singapore pte ltd @ 
960,The 2nd international workshop on narrative extraction from text: Text2story 2019,"Building upon the success of the first edition, we organize the second edition of the Text2Story Workshop on Narrative Extraction from Texts in conjunction with the 41st European Conference on Information Retrieval (ECIR 2019) on April 14, 2019. Our objective is to further consolidate the efforts of the community and reflect upon the progress made since the last edition. Although the understanding of natural language has improved over the last couple of years – with research works emerging on the grounds of information extraction and text mining – the problem of constructing consistent narrative structures is yet to be solved. It is expected that the state-of-the-art has been advancing in pursuit of methods that automatically identify, interpret and relate the different elements of narratives which are often spread among different sources. In the second edition of the workshop, we foster the discussion of recent advances in the link between Information Retrieval (IR) and formal narrative representations from text. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,building upon @ success of @ first edition @ organize @ second edition of @ text story workshop on narrative extraction @ text in conjunction @ @ st european conference on information retrieval @ ecir @ on april @ @ objective is to @ consolidate @ effort of @ community and reflect upon @ progress made since @ last edition @ although @ understanding of natural language ha improved @ @ last couple of year @ research work emerging on @ ground of information extraction and text mining @ problem of constructing consistent narrative structure is yet to @ solved @ @ is expected @ @ state-of-the-art ha @ advancing in pursuit of method @ automatically identify interpret and relate @ different element of narrative @ @ often spread among different source @ in @ second edition of @ workshop @ foster @ discussion of recent advance in @ link @ information retrieval @ ir @ and formal narrative representation @ text @ @ nature switzerland ag @ 
962,FoodBase corpus: A new resource of annotated food entities,"The existence of annotated text corpora is essential for the development of public health services and tools based on natural language processing (NLP) and text mining. Recently organized biomedical NLP shared tasks have provided annotated corpora related to different biomedical entities such as genes, phenotypes, drugs, diseases and chemical entities. These are needed to develop named-entity recognition (NER) models that are used for extracting entities from text and finding their relations. However, to the best of our knowledge, there are limited annotated corpora that provide information about food entities despite food and dietary management being an essential public health issue. Hence, we developed a new annotated corpus of food entities, named FoodBase. It was constructed using recipes extracted from Allrecipes, which is currently the largest food-focused social network. The recipes were selected from five categories: 'Appetizers and Snacks', 'Breakfast and Lunch', 'Dessert', 'Dinner' and 'Drinks'. Semantic tags used for annotating food entities were selected from the Hansard corpus. To extract and annotate food entities, we applied a rule-based food NER method called FoodIE. Since FoodIE provides a weakly annotated corpus, by manually evaluating the obtained results on 1000 recipes, we created a gold standard of FoodBase. It consists of 12 844 food entity annotations describing 2105 unique food entities. Additionally, we provided a weakly annotated corpus on an additional 21 790 recipes. It consists of 274 053 food entity annotations, 13 079 of which are unique. The FoodBase corpus is necessary for developing corpus-based NER models for food science, as a new benchmark dataset for machine learning tasks such as multi-class classification, multi-label classification and hierarchical multi-label classification. FoodBase can be used for detecting semantic differences/similarities between food concepts, and after all we believe that it will open a new path for learning food embedding space that can be used in predictive studies. © 2019 The Author(s) 2019. Published by Oxford University Press.",2019,Database,5,@ existence of annotated text corpus is essential @ @ development of public health service and tool based on natural language processing @ nlp @ and text mining @ recently organized biomedical nlp shared task @ provided annotated corpus related to different biomedical entity @ a gene phenotype drug disease and chemical entity @ @ @ needed to develop named-entity recognition @ ner @ model @ @ used @ extracting entity @ text and finding @ relation @ however to @ best of @ knowledge @ @ limited annotated corpus @ provide information @ food entity despite food and dietary management @ @ essential public health issue @ hence @ developed a @ annotated corpus of food entity named foodbase @ @ wa constructed @ recipe extracted @ allrecipes @ is currently @ largest food-focused social network @ @ recipe @ selected @ five category @ @ appetizer and snack @ @ breakfast and lunch @ @ dessert @ @ dinner @ and @ drink @ @ semantic tag used @ annotating food entity @ selected @ @ hansard corpus @ to extract and annotate food entity @ applied a rule-based food ner method called foodie @ since foodie provides a weakly annotated corpus by manually evaluating @ obtained @ on recipe @ created a gold standard of foodbase @ @ consists of food entity annotation describing unique food entity @ additionally @ provided a weakly annotated corpus on @ additional recipe @ @ consists of food entity annotation of @ @ unique @ @ foodbase corpus is necessary @ developing corpus-based ner model @ food science a a @ benchmark dataset @ machine learning task @ a multi-class classification multi-label classification and hierarchical multi-label classification @ foodbase @ @ used @ detecting semantic difference similarity @ food concept and @ @ @ believe @ @ @ open a @ path @ learning food embedding space @ @ @ used in predictive study @ @ author @ s @ @ published by oxford university @ @ 
963,"Syntactic, semantic and sentiment analysis: The joint effect on automated essay evaluation","Manual grading of essays by humans is time-consuming and likely to be susceptible to inconsistencies and inaccuracies. In recent years, an abundance of research has been done to automate essay evaluation processes, yet little has been done to take into consideration the syntax, semantic coherence and sentiments of the essay's text together. Our proposed system incorporates not just the rule-based grammar and surface level coherence check but also includes the semantic similarity of the sentences. We propose to use Graph-based relationships within the essay's content and polarity of opinion expressions. Semantic similarity is determined between each statement of the essay to form these Graph-based spatial relationships and novel features are obtained from it. Our algorithm uses 23 salient features with high predictive power, which is less than the current systems while considering every aspect to cover the dimensions that a human grader focuses on. Fewer features help us get rid of the redundancies of the data so that the predictions are based on more representative features and are robust to noisy data. The prediction of the scores is done with neural networks using the data released by the ASAP competition held by Kaggle. The resulting agreement between human grader's score and the system's prediction is measured using Quadratic Weighted Kappa (QWK). Our system produces a QWK of 0.793. © 2013 IEEE.",2019,IEEE Access,2,manual grading of essay by human is time-consuming and likely to @ susceptible to inconsistency and inaccuracy @ in recent year @ abundance of research ha @ done to automate essay evaluation process yet little ha @ done to take @ consideration @ syntax semantic coherence and sentiment of @ essay @ s text together @ @ proposed system incorporates not @ @ rule-based grammar and surface level coherence check @ @ includes @ semantic similarity of @ sentence @ @ propose to use graph-based relationship within @ essay @ s content and polarity of opinion expression @ semantic similarity is determined @ @ statement of @ essay to form @ graph-based spatial relationship and novel feature @ obtained @ @ @ @ algorithm us salient feature @ high predictive power @ is le @ @ current system @ considering every aspect to cover @ dimension @ a human grader focus on @ fewer feature help u get rid of @ redundancy of @ data @ @ @ prediction @ based on more representative feature and @ robust to noisy data @ @ prediction of @ score is done @ neural network @ @ data released by @ asap competition held by kaggle @ @ resulting agreement @ human grader @ s score and @ system @ s prediction is measured @ quadratic weighted kappa @ qwk @ @ @ system produce a qwk of @ @ @ @ 
965,Development of biomedical corpus enlargement platform using BERT for bio-entity recognition,"As the volume and availability of textual data dramatically increase in the current digital age, a major challenge is how to properly extract useful information online. A key component of the text mining pipeline is named entity recognition (NER) for extracting knowledge. Currently, there are many publicly available NER tools such as Stanford NLP, NLTK or Spacy python library. However, there is a problem of accurate unknown entity recognition. We focus on using deep learning for recognizing entities, as it has been shown to outperform traditional algorithms for big data in part of its ability for feature extraction and dealing with multi-dimensionality. In this paper, we applied the state-of-the-art language representation model termed BERT (Bidirectional Encoder Representations from Transformers) for NER classification, in order to enlarge the existing biomedical corpus for further machine learning processing. We used additional biomedical corpora for training, and then compared the results to a recent prior work. The end result is precision improvement of 2.24%, recall improvement of 3.55%, and F1-score improvement of 2.98%, in protein recognition of super-pathway of leucine, valine, and isoleucine biosynthesis. We also developed a prototype, in form of an internal web platform, for supporting bio-annotators and corpus enlargement purpose. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,a @ volume and availability of textual data dramatically increase in @ current digital age a major challenge is @ to properly extract useful information online @ a key component of @ text mining pipeline is named entity recognition @ ner @ @ extracting knowledge @ currently @ @ many publicly available ner tool @ a stanford nlp nltk @ spacy python library @ however @ is a problem of accurate unknown entity recognition @ @ focus on @ deep learning @ recognizing entity a @ ha @ @ to outperform traditional algorithm @ big data in part of @ ability @ feature extraction and dealing @ multi-dimensionality @ in @ @ @ applied @ state-of-the-art language representation model termed bert @ bidirectional encoder representation @ transformer @ @ ner classification in order to enlarge @ existing biomedical corpus @ @ machine learning processing @ @ used additional biomedical corpus @ training and @ compared @ @ to a recent prior work @ @ end @ is precision improvement of @ recall improvement of @ and f score improvement of @ in protein recognition of super-pathway of leucine valine and isoleucine biosynthesis @ @ @ developed a prototype in form of @ internal web platform @ supporting bio-annotators and corpus enlargement purpose @ @ nature switzerland ag @ 
966,Named Entity Recognition from Biomedical Texts Using a Fusion Attention-Based BiLSTM-CRF,"Biomedical named entity recognition (BNER) is the basis of biomedical text mining and one of the core sub-tasks of information extraction. Previous BNER models based on conventional machine learning rely on time-consuming feature engineering. Though most neural network methods improve the problems with automatic learning, they cannot pay attention to the significant areas when capturing features. In this paper, we propose an attention-based BiLSTM-CRF model. First, this model adopts a bidirectional long short-term memory network (BiLSTM) to obtain more complete context information. At the same time, the attention mechanism is proposed to improve the vector representations in BiLSTM. We design different attention weight redistribution methods and fuse them. It effectively prevents the significant information loss when extracting features. Finally, combining BiLSTM with conditional random field (CRF) layer effectively solves the problems of the inability to handle the strong dependence of tags in the sequence. With the simple architecture, our model achieves a reasonable performance on the JNLPBA corpus. It obtains an F1-score of 73.50. Our model can enhance the ability of the neural network to extract significant information and does not rely on any feature engineering, with only general pre-training word vectors. It makes our model have high portability and extendibility. © 2013 IEEE.",2019,IEEE Access,10,biomedical named entity recognition @ bner @ is @ basis of biomedical text mining and @ of @ core sub-tasks of information extraction @ previous bner model based on conventional machine learning rely on time-consuming feature engineering @ though @ neural network method improve @ problem @ automatic learning @ cannot pay attention to @ significant area @ capturing feature @ in @ @ @ propose @ attention-based bilstm-crf model @ first @ model adopts a bidirectional long short-term memory network @ bilstm @ to obtain more complete context information @ at @ @ time @ attention mechanism is proposed to improve @ vector representation in bilstm @ @ design different attention weight redistribution method and fuse @ @ @ effectively prevents @ significant information loss @ extracting feature @ finally combining bilstm @ conditional random field @ crf @ layer effectively solves @ problem of @ inability to handle @ strong dependence of tag in @ sequence @ @ @ simple architecture @ model achieves a reasonable performance on @ jnlpba corpus @ @ obtains @ f score of @ @ @ model @ enhance @ ability of @ neural network to extract significant information and doe not rely on @ feature engineering @ only general pre-training word vector @ @ make @ model @ high portability and extendibility @ @ @ 
967,Monitoring Public Participation in Multilateral Initiatives Using Social Media Intelligence,"Governments, multilateral agencies like the World Bank, United Nations, and Development Banks as well as other nonprofits are involved in a variety of developmental activities across the world. A lot of resources are spent to ensure proper consultations and post-implementation verification of results. But this does not completely ensure whether the objectives are achieved. The new web technologies provided methodologies and developed tools that allow the users to pool resources on projects over the Internet. Social media allowed real-time feedback for citizens, monitoring developmental initiatives of Governments and multilateral agencies. The role of technology ensures that the consultations and ongoing feedback can be captured, analyzed, and used in understating the stakeholder reactions to the project and its implementation. This helps in making necessary course corrections avoiding costly mistakes and overruns. In this paper, we model a tool to monitor, study, and analyze popular feedback, using forums, social media, surveys, and other crowdsourcing techniques. The feedback is gathered and analyzed using both quantitative and qualitative methods to understand what crowd is saying. The summation and visualization of patterns are automated using text mining and sentiment analysis tools including text analysis and tagging/annotation. These patterns provide insight into the popular feedback and sentiment effectively and accurately than the conventional method. The model is created by integrating such feedback channels. Data is collected and analyzed, and the results are presented using tools developed in open-source platform. © 2019, Springer Nature Singapore Pte Ltd.",2019,Lecture Notes on Data Engineering and Communications Technologies,0,government multilateral agency like @ world bank united nation and development bank a well a @ nonprofit @ involved in a variety of developmental activity across @ world @ a lot of resource @ spent to ensure proper consultation and post-implementation verification of @ @ @ @ doe not completely ensure whether @ objective @ achieved @ @ @ web technology provided methodology and developed tool @ allow @ user to pool resource on project @ @ internet @ social medium allowed real-time feedback @ citizen monitoring developmental initiative of government and multilateral agency @ @ role of technology ensures @ @ consultation and ongoing feedback @ @ captured analyzed and used in understating @ stakeholder reaction to @ project and @ implementation @ @ help in making necessary course correction avoiding costly mistake and overrun @ in @ @ @ model a tool to monitor study and analyze popular feedback @ forum social medium survey and @ crowdsourcing technique @ @ feedback is gathered and analyzed @ @ quantitative and qualitative method to understand @ crowd is saying @ @ summation and visualization of pattern @ automated @ text mining and sentiment analysis tool including text analysis and tagging annotation @ @ pattern provide insight @ @ popular feedback and sentiment effectively and accurately @ @ conventional method @ @ model is created by integrating @ feedback channel @ data is collected and analyzed and @ @ @ presented @ tool developed in open-source platform @ @ nature singapore pte ltd @ 
969,Automatic slide generation for scientific papers,"We describe our approach for automatically generating presentation slides for scientific papers using deep neural networks. Such slides can help authors have a starting point for their slide generation process. Extractive summarization techniques are applied to rank and select important sentences from the original document. Previous work identified important sentences based only on a limited number of features that were extracted from the position and structure of sentences in the paper. Our method extends previous work by (1) extracting a more comprehensive list of surface features, (2) considering semantic or meaning of the sentence, and (3) using context around the current sentence to rank the sentences. Once, the sentences are ranked, salient sentences are selected using Integer Linear Programming (ILP). Our results show the efficacy of our model for summarization and the slide generation task. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2019,CEUR Workshop Proceedings,0,@ describe @ approach @ automatically generating presentation slide @ scientific @ @ deep neural network @ @ slide @ help author @ a starting point @ @ slide generation process @ extractive summarization technique @ applied to rank and select important sentence @ @ original document @ previous work identified important sentence based only on a limited number of feature @ @ extracted @ @ position and structure of sentence in @ @ @ @ method extends previous work by @ @ extracting a more comprehensive list of surface feature @ @ considering semantic @ meaning of @ sentence and @ @ @ context around @ current sentence to rank @ sentence @ @ @ sentence @ ranked salient sentence @ selected @ integer linear programming @ ilp @ @ @ @ @ @ efficacy of @ model @ summarization and @ slide generation task @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
970,Deception detection in Arabic tweets and news,"The project Arabic Author Profiling for Cyber-Security (ARAP)1 aims at preventing cyber-threats using Machine Learning. To this end, they monitor social media to early detect threatening messages and, in such a case, to profile the authors behind. Profiling potential terrorists from messages shared in social media may allow detecting communities whose aim is to undermine the security of others. One of this framework's main challenges is recognizing false positives, such as potential threaten- ing messages that are actually deceptive, ironic or humorous. This paper focuses on the goal of detecting deceptive messages, which are intention- ally written trying to sound authentic. This task is performed on two different genres of Arabic texts: Twitter messages and news headlines. © Copyright 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,1,@ project arabic author profiling @ cyber-security @ arap @ aim at preventing cyber-threats @ machine learning @ to @ end @ monitor social medium to early detect threatening message and in @ a case to profile @ author behind @ profiling potential terrorist @ message shared in social medium may allow detecting community whose aim is to undermine @ security of others @ @ of @ framework @ s main challenge is recognizing false positive @ a potential threaten ing message @ @ actually deceptive ironic @ humorous @ @ @ focus on @ goal of detecting deceptive message @ @ intention ally written trying to sound authentic @ @ task is performed on @ different genre of arabic text @ twitter message and news headline @ @ @ @ @ by @ author @ 
971,A Baseline Approach for Early Detection of Signs of Anorexia and Self-harm in Reddit Posts,"This paper describes the systems developed by the BiTeM team for the CLEF eRisk Task 1 and 2, 2019. The goal was to predict the risk of anorexia and self-harm from user-generated content on Reddit. Several approaches based on supervised learning were used to estimate the risk of anorexia and self-harm. The systems were able to achieve low to moderate results. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,0,@ @ describes @ system developed by @ bitem team @ @ clef erisk task and @ @ goal wa to predict @ risk of anorexia and self-harm @ user-generated content on reddit @ several approach based on supervised learning @ used to estimate @ risk of anorexia and self-harm @ @ system @ able to achieve low to moderate @ @ ceur-ws @ @ right reserved @ 
973,Characteristics of a Highly Cited Article: A Machine Learning Perspective,"Machine learning (ML) is a fast-growing topic that enables the extraction of patterns from varying types of datasets, ranging from medical data to financial data. However, the application of the ML methodology to understand the key characteristics of highly cited research articles has not been thoroughly investigated, despite the potential practical guidance that ML can provide for researchers during the publication process. To address this research gap, an ML algorithm known as principal component (PC) analysis is used to detect patterns in highly and lowly cited papers. In this paper, eight features (number of citations, number of views, number of characters with no spaces, number of figures, number of tables, number of equations, number of authors, and title length) are extracted from highly and lowly cited papers, leading to eight PCs (PC1-PC8). PC1 shows that the numbers of citations are positively correlated with the character count and negatively correlated with the title length. PC2 shows that the number of tables is positively correlated with the title length. PC3 shows that the number of figures is positively correlated with the number of tables. PC4-PC8 rank the importance of individual features in the descending order: number of equations, number of characters with no spaces, number of figures, number of views, and then the number of authors. The results of the ML analysis provide interesting and valuable tips for researchers, students, and all academic and non-academic writers who are seeking to improve their citation rates. © 2013 IEEE.",2019,IEEE Access,6,machine learning @ ml @ is a fast-growing topic @ enables @ extraction of pattern @ varying type of datasets ranging @ medical data to financial data @ however @ application of @ ml methodology to understand @ key characteristic of highly cited research article ha not @ thoroughly investigated despite @ potential practical guidance @ ml @ provide @ researcher @ @ publication process @ to address @ research gap @ ml algorithm known a principal component @ pc @ analysis is used to detect pattern in highly and lowly cited @ @ in @ @ eight feature @ number of citation number of view number of character @ no space number of figure number of table number of equation number of author and title length @ @ extracted @ highly and lowly cited @ leading to eight pc @ pc pc @ @ pc @ @ @ number of citation @ positively correlated @ @ character count and negatively correlated @ @ title length @ pc @ @ @ number of table is positively correlated @ @ title length @ pc @ @ @ number of figure is positively correlated @ @ number of table @ pc pc rank @ importance of individual feature in @ descending order @ number of equation number of character @ no space number of figure number of view and @ @ number of author @ @ @ of @ ml analysis provide interesting and valuable tip @ researcher student and @ @ and non-academic writer @ @ seeking to improve @ citation rate @ @ @ 
974,From Web Crawled Text to Project Descriptions: Automatic Summarizing of Social Innovation Projects,"In the past decade, social innovation projects have gained the attention of policy makers, as they address important social issues in an innovative manner. A database of social innovation is an important source of information that can expand collaboration between social innovators, drive policy and serve as an important resource for research. Such a database needs to have projects described and summarized. In this paper, we propose and compare several methods (e.g. SVM-based, recurrent neural network based, ensambled) for describing projects based on the text that is available on project websites. We also address and propose a new metric for automated evaluation of summaries based on topic modelling. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,in @ past decade social innovation project @ gained @ attention of policy maker a @ address important social issue in @ innovative manner @ a database of social innovation is @ important source of information @ @ expand collaboration @ social innovator drive policy and serve a @ important resource @ research @ @ a database need to @ project described and summarized @ in @ @ @ propose and compare several method @ e @ g @ svm-based recurrent neural network based ensambled @ @ describing project based on @ text @ is available on project website @ @ @ address and propose a @ metric @ automated evaluation of summary based on topic modelling @ @ nature switzerland ag @ 
978,Can Machines Learn to Comprehend Scientific Literature?,"To measure the ability of a machine to understand professional-level scientific articles, we construct a scientific question answering task called PaperQA. The PaperQA task is based on more than 80 000 'fill-in-the-blank' type questions on articles from reputed scientific journals such as Nature and Science. We perform fine-grained linguistic analysis and evaluation to compare PaperQA and other conventional question and answering (QA) tasks on general literature (e.g., books, news articles, and Wikipedia texts). The results indicate that the PaperQA task is the most difficult QA task for both humans (lay people) and machines (deep-learning models). Moreover, humans generally outperform machines in conventional QA tasks, but we found that advanced deep-learning models outperform humans by 3%-13% on average in the PaperQA task. The PaperQA dataset used in this paper is publicly available at http://dmis.korea.ac.kr/downloads?id=PaperQA. © 2018 IEEE.",2019,IEEE Access,3,to measure @ ability of a machine to understand professional-level scientific article @ construct a scientific question answering task called paperqa @ @ paperqa task is based on more @ @ fill-in-the-blank @ type question on article @ reputed scientific journal @ a nature and science @ @ perform fine-grained linguistic analysis and evaluation to compare paperqa and @ conventional question and answering @ qa @ task on general literature @ e @ g @ book news article and wikipedia text @ @ @ @ indicate @ @ paperqa task is @ @ difficult qa task @ @ human @ lay people @ and machine @ deep-learning model @ @ moreover human generally outperform machine in conventional qa task @ @ found @ advanced deep-learning model outperform human by on average in @ paperqa task @ @ paperqa dataset used in @ @ is publicly available at http @ dmis @ korea @ ac @ kr downloads @ id paperqa @ @ @ 
979,Improving collaborative learning: Guiding knowledge exchange through the provision of information about learning partners and learning contents,"Various studies have revealed the positive influence of group awareness support on collaborative learning. They attributed this effect to the availability of information about learning partners but did not yet consider learners’ prior knowledge activation, possibly caused by additionally provided information about the content of learning material. Moreover, advanced technologies such as text-mining methods enable the automated identification of material-related content of learners’ prior knowledge. Providing co-learners with such information might further improve their topic selection and knowledge integration during knowledge exchange due to enhanced activation. Thus, to investigate the individual effects of both types of information and the effects of combining both, the present experimental study (N = 120) systematically varies information about learning partners (available/unavailable) and information about learning contents (specified/unspecified) in a 2 × 2 between-design. We found tendencies that learners with available partner-related information address more task-specific concepts in explanations, but detected neither effects of specified information about learning contents nor interaction effects. Further 2 × 2 mixed-designs that included co-learners’ prior knowledge levels as within-variables demonstrated that providing specified information about learning contents significantly improves questioning strategies and partner-related information significantly improves audience design in explanations. Finally, a mediation analysis suggested a significant indirect effect claiming that the knowledge integration level and partner modeling accuracy mediate the effect of available partner-related information on knowledge recall if this information is not content-specific. Hence, specified information about learning contents apparently guides metacognitive regulation with regard to identifying and filling own knowledge gaps. By contrast, plain partner-related information apparently evokes deeper cognitive elaboration. © 2018 Elsevier Ltd",2019,Computers and Education,15,various study @ revealed @ positive influence of group awareness support on collaborative learning @ @ attributed @ effect to @ availability of information @ learning partner @ @ not yet consider learner prior knowledge activation possibly caused by additionally provided information @ @ content of learning material @ moreover advanced technology @ a text-mining method enable @ automated identification of material-related content of learner prior knowledge @ providing co-learners @ @ information might @ improve @ topic selection and knowledge integration @ knowledge exchange due to enhanced activation @ thus to investigate @ individual effect of @ type of information and @ effect of combining @ @ @ experimental study @ n @ systematically varies information @ learning partner @ available unavailable @ and information @ learning content @ specified unspecified @ in a between-design @ @ found tendency @ learner @ available partner-related information address more task-specific concept in explanation @ detected neither effect of specified information @ learning content @ interaction effect @ @ mixed-designs @ included co-learners prior knowledge level a within-variables demonstrated @ providing specified information @ learning content significantly improves questioning strategy and partner-related information significantly improves audience design in explanation @ finally a mediation analysis suggested a significant indirect effect claiming @ @ knowledge integration level and partner modeling accuracy mediate @ effect of available partner-related information on knowledge recall if @ information is not content-specific @ hence specified information @ learning content apparently guide metacognitive regulation @ regard to identifying and filling @ knowledge gap @ by contrast plain partner-related information apparently evokes deeper cognitive elaboration @ @ ltd
982,Hybrid Words Representation for Airlines Sentiment Analysis,"Social media sentimental analysis is interesting field with the aim to analyze social conservation and determine deeper context as they apply to a topic or theme. However, it is challenging as tweets are unstructured, informal and noisy in nature. Also, it involves natural language complexities like words with same meanings (Polysemy). Most of the existing approaches mainly rely on clean textual data, however Twitter data is quite noisy in real life. Aiming to improve the performance, in this paper, we present hybrid words representation and Bi-directional Long Short Term Memory (BiLSTM) with attention modeling resulting in improvement in tweet quality by not only treating the noise within the textual context but also considers polysemy, semantics, syntax, out of vocabulary (OOV) words as well as words sentiments within a tweet. The proposed model overcomes the current limitations and improves the accuracy for tweets classification as showed by the evaluation of the model performed on real-world airline related datasets. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9,social medium sentimental analysis is interesting field @ @ aim to analyze social conservation and determine deeper context a @ apply to a topic @ theme @ however @ is challenging a tweet @ unstructured informal and noisy in nature @ @ @ involves natural language complexity like word @ @ meaning @ polysemy @ @ @ of @ existing approach mainly rely on clean textual data however twitter data is quite noisy in real life @ aiming to improve @ performance in @ @ @ @ hybrid word representation and bi-directional long short term memory @ bilstm @ @ attention modeling resulting in improvement in tweet quality by not only treating @ noise within @ textual context @ @ considers polysemy semantics syntax @ of vocabulary @ oov @ word a well a word sentiment within a tweet @ @ proposed model overcomes @ current limitation and improves @ accuracy @ tweet classification a showed by @ evaluation of @ model performed on real-world airline related datasets @ @ nature switzerland ag @ 
986,A survey of multiple types of text summarization with their satellite contents based on swarm intelligence optimization algorithms,"Due to the tremendous increment of data on the web, extracting the most important data as a conceptual brief would be valuable for certain users. Therefore, there is a massive enthusiasm concerning the generation of automatic text summary frameworks to constitute abstracts automatically from the text, web, and social network messages associated with their satellite content. This survey highlights, for the first time, how the swarm intelligence (SI) optimization techniques are performed to solve the text summarization task efficiently. Additionally, a convincing justification of why SI, especially Ant Colony Optimization (ACO), has been presented. Unfortunately, three types of text summarization tasks using SI indicate bit utilizing in the literature when contrasted with the other summarization techniques as machine learning and genetic algorithms, in spite of the fact that there are seriously promising outcomes of the SI methods. On the other hand, it has been noticed that the summarization task with multiple types has not been formalized as a multi-objective optimization (MOO) task before, despite that there are many objectives which can be considered. Moreover, the SI was not employed before to support the real-time summary approaches. Thus, a new model has been proposed to be adequate for achieving many objectives and to satisfy the real-time needs. Eventually, this study will enthuse researchers to further consider the various types of SI when solving the summarization tasks, particularly, in the short text summarization (STS) field. © 2018 Elsevier B.V.",2019,Knowledge-Based Systems,14,due to @ tremendous increment of data on @ web extracting @ @ important data a a conceptual brief would @ valuable @ certain user @ therefore @ is a massive enthusiasm concerning @ generation of automatic text summary framework to constitute abstract automatically @ @ text web and social network message associated @ @ satellite content @ @ survey highlight @ @ first time @ @ swarm intelligence @ si @ optimization technique @ performed to solve @ text summarization task efficiently @ additionally a convincing justification of @ si especially ant colony optimization @ aco @ ha @ presented @ unfortunately three type of text summarization task @ si indicate bit utilizing in @ literature @ contrasted @ @ @ summarization technique a machine learning and genetic algorithm in spite of @ fact @ @ @ seriously promising outcome of @ si method @ on @ @ hand @ ha @ noticed @ @ summarization task @ multiple type ha not @ formalized a a multi-objective optimization @ moo @ task @ despite @ @ @ many objective @ @ @ considered @ moreover @ si wa not employed @ to support @ real-time summary approach @ thus a @ model ha @ proposed to @ adequate @ achieving many objective and to satisfy @ real-time need @ eventually @ study @ enthuse researcher to @ consider @ various type of si @ solving @ summarization task particularly in @ short text summarization @ sts @ field @ @ b @ v @ 
987,A computational system based on ontologies to automate the mapping process of medical reports into structured databases,"We have developed, in collaboration with medical and computer experts, the ontology-based Medical Report Mapping Process to support the transformation of unstructured reports into a structured representation. Nevertheless, the techniques employed in this two-phase process must be performed individually and manually by computer instructions, which hinder their use by users not familiar with such language. Thereby, this work proposes a tool to automate and optimize this process by integrating its techniques in a computational system, which was built using a software engineering prototyping approach. This system was experimentally evaluated by applying it to a set of 100 textual reports. The first phase decreased the total number of phrases (853) and words (2520) by 82.25% (48) and 92.70% (184), respectively. In the second phase, 100% of the relevant pieces of information (previously established) present in the reports were transcribed. Also, the second phase was applied, using the same configuration as the first study, in another set with 250 textual reports, resulting in a mapping rate of 99.74%. The unprocessed and unmapped words, regarding both experimental evaluations, were recorded for later inclusion into the ontology. By using this system, efficient and scalable investigations can be performed from medical reports, contributing to generate new knowledge. Also, the system facilitates the definition of these structures due to the feasibility to analyze different sentences in unique phrase sets. © 2018 Elsevier Ltd",2019,Expert Systems with Applications,4,@ @ developed in collaboration @ medical and computer expert @ ontology-based medical report mapping process to support @ transformation of unstructured report @ a structured representation @ nevertheless @ technique employed in @ two-phase process must @ performed individually and manually by computer instruction @ hinder @ use by user not familiar @ @ language @ thereby @ work proposes a tool to automate and optimize @ process by integrating @ technique in a computational system @ wa built @ a software engineering prototyping approach @ @ system wa experimentally evaluated by applying @ to a set of textual report @ @ first phase decreased @ total number of phrase @ @ and word @ @ by @ @ @ and @ @ @ respectively @ in @ second phase of @ relevant piece of information @ @ established @ @ in @ report @ transcribed @ @ @ second phase wa applied @ @ @ configuration a @ first study in another set @ textual report resulting in a mapping rate of @ @ @ unprocessed and unmapped word regarding @ experimental evaluation @ recorded @ later inclusion @ @ ontology @ by @ @ system efficient and scalable investigation @ @ performed @ medical report contributing to generate @ knowledge @ @ @ system facilitates @ definition of @ structure due to @ feasibility to analyze different sentence in unique phrase set @ @ ltd
988,Tweets Competitive Sentimental Analysis of Android Mobile Brands to Understand Customer Experience,"With the dawn of the social media era the world has connected more than ever, every opinion, news and discussion is now online. Public opinion data is freely available and accessible through the API of the provider. Data mining, text mining and sentimental analysis provide insight of data. Companies hold official pages on micro-blogging websites like Twitter. This helps them to introduce products and keep in touch with customers. We choose the three Android phone selling brands which are Samsung, Oppo &, Nokia and do our analysis on the tweets posted on official page as a response to officially posted tweets or mentioned using hashtags “#” or mentioned tag “@”. We performed a competitive analysis on our finding to find similarities & differences. In the end, we provided recommendations on how to make a better competitive analysis strategy to win the market both on social media forum and in the sale market. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,0,@ @ dawn of @ social medium era @ world ha connected more @ ever every opinion news and discussion is now online @ public opinion data is freely available and accessible @ @ api of @ provider @ data mining text mining and sentimental analysis provide insight of data @ company hold official page on micro-blogging website like twitter @ @ help @ to introduce product and keep in touch @ customer @ @ choose @ three android phone selling brand @ @ samsung oppo nokia and @ @ analysis on @ tweet posted on official page a a response to officially posted tweet @ mentioned @ hashtags @ mentioned tag @ @ performed a competitive analysis on @ finding to find similarity difference @ in @ end @ provided recommendation on @ to make a better competitive analysis strategy to win @ market @ on social medium forum and in @ sale market @ @ nature singapore pte ltd @ 
991,Comparative analysis of scientific papers collections via topic modeling and co-authorship networks,"In this paper, the authors present an approach to benchmarking the collections of scientific journals based on the analysis of co-authorship graphs and a text models. The main methodical result is Comparative Topic Modeling (CTM) technique. The application of time series to the metrics of co-authorship graphs allowed trends in the development of author collaborations in scientific journals to be analyzed. A text model was created using machine learning methods. The content of journals was classified to determine the degree of authenticity both in various journals and their issues. Experiments was conducted on the archives of two journals in the field of Rheumatology. The authors used public data sets from the SNAP research laboratory at Stanford University to benchmark the co-authorship network metrics. The application of the research results is improving editorial strategies for development of co-authorship collaborations and scientific content excellence. © Springer Nature Switzerland AG 2019.",2019,Communications in Computer and Information Science,1,in @ @ @ author @ @ approach to benchmarking @ collection of scientific journal based on @ analysis of co-authorship graph and a text model @ @ main methodical @ is comparative topic modeling @ ctm @ technique @ @ application of time series to @ metric of co-authorship graph allowed trend in @ development of author collaboration in scientific journal to @ analyzed @ a text model wa created @ machine learning method @ @ content of journal wa classified to determine @ degree of authenticity @ in various journal and @ issue @ experiment wa conducted on @ archive of @ journal in @ field of rheumatology @ @ author used public data set @ @ snap research laboratory at stanford university to benchmark @ co-authorship network metric @ @ application of @ research @ is improving editorial strategy @ development of co-authorship collaboration and scientific content excellence @ @ nature switzerland ag @ 
992,Serial and Parallel Recurrent Convolutional Neural Networks for Biomedical Named Entity Recognition,"Identifying named entities from unstructured biomedical text is an important part of information extraction. The irrelevant words in long biomedical sentences and the complex composition of the entity make LSTM used in the general domain less effective. We find that emphasizing the local connection between words in a biomedical entity can improve performance. Based on the above observation, this paper proposes two novel neural network architectures combining bidirectional LSTM and CNN. In the first architecture S-CLSTM, a CNN structure is built on the top of bidirectional LSTM to keep both long dependencies in a sentence and local connection between words. The second architecture P-CLSTM combines bidirectional LSTM and CNN in parallel with the weighted loss to take advantage of the complementary features of two networks. Experimental results indicate that our architectures achieve significant improvements compared with baselines and other state-of-the-art approaches. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,identifying named entity @ unstructured biomedical text is @ important part of information extraction @ @ irrelevant word in long biomedical sentence and @ complex composition of @ entity make lstm used in @ general domain le effective @ @ find @ emphasizing @ local connection @ word in a biomedical entity @ improve performance @ based on @ @ observation @ @ proposes @ novel neural network architecture combining bidirectional lstm and cnn @ in @ first architecture s-clstm a cnn structure is built on @ top of bidirectional lstm to keep @ long dependency in a sentence and local connection @ word @ @ second architecture p-clstm combine bidirectional lstm and cnn in parallel @ @ weighted loss to take advantage of @ complementary feature of @ network @ experimental @ indicate @ @ architecture achieve significant improvement compared @ baseline and @ state-of-the-art approach @ @ nature switzerland ag @ 
993,A latent-dirichlet-allocation based extension for domain ontology of enterprise's technological innovation,"This paper proposed a method for building enterprise's technological innovation domain ontology automatically from plain text corpus based on Latent Dirichlet Allocation (LDA). The proposed method consisted of four modules: 1) introducing the seed ontology for domain of enterprise's technological innovation, 2) using Natural Language Processing (NLP) technique to preprocess the collected textual data, 3) mining domain specific terms from document collections based on LDA, 4) obtaining the relationship between the terms through the defined relevant rules. The experiments have been carried out to demonstrate the effectiveness of this method and the results indicated that many terms in domain of enterprise's technological innovation and the semantic relations between terms are discovered. The proposed method is a process of continuously cycles and iterations, that is the obtained objective ontology can be re-iterated as initial seed ontology. The constant knowledge acquisition in the domain of enterprise's technological innovation to update and perfect the initial seed ontology. © 2019.",2019,"International Journal of Computers, Communications and Control",5,@ @ proposed a method @ building enterprise @ s technological innovation domain ontology automatically @ plain text corpus based on latent dirichlet allocation @ lda @ @ @ proposed method consisted of four module @ @ introducing @ seed ontology @ domain of enterprise @ s technological innovation @ @ natural language processing @ nlp @ technique to preprocess @ collected textual data @ mining domain specific term @ document collection based on lda @ obtaining @ relationship @ @ term @ @ defined relevant rule @ @ experiment @ @ carried @ to demonstrate @ effectiveness of @ method and @ @ indicated @ many term in domain of enterprise @ s technological innovation and @ semantic relation @ term @ discovered @ @ proposed method is a process of continuously cycle and iteration @ is @ obtained objective ontology @ @ re-iterated a initial seed ontology @ @ constant knowledge acquisition in @ domain of enterprise @ s technological innovation to update and perfect @ initial seed ontology @ @ 
994,A new approach to improve online customer review analysis by a sentence level using vector similarity related text extraction,"Sentiment analysis is one of the Natural Language Processing (NLP) technique that consists in extracting emotions related to some raw texts. It’s usually used on social media posts and customer reviews to automatically understand if some users are positive or negative and why. The text mining is the process of converting unstructured text data into an extensive collection of analytics and visualization and model building. Online reviews of e-commerce giants are a paradigm that can be used to arrive at profitable results. Online reviews allow you to purchase information about the product, including its quality, performance, and recommendations, as it gives the buyer a clear image of the product. Favorable reviews by analyzing customer needs, manufacturers have realized the benefits of invisible energy online reviews. Reviewing proposed method sentences using text extraction from customers. They give positive and negative feedback from a variety of penalties, from which the syntax is extracted and analyzed.This work improves the text mining method using Vector Similarity RelatedText Extraction (VSRTE).This methodology not only classified the text into positive and negative sentiment but have also included emotions of customer’s reactions like usual, angry, and sentiments. This text extraction mining can help assess product quality to enable better decision-making for consumers and also predict their customer behavior. © 2019, Institute of Advanced Scientific Research, Inc.. All rights reserved.",2019,Journal of Advanced Research in Dynamical and Control Systems,0,sentiment analysis is @ of @ natural language processing @ nlp @ technique @ consists in extracting emotion related to some raw text @ @ s usually used on social medium post and customer review to automatically understand if some user @ positive @ negative and @ @ @ text mining is @ process of converting unstructured text data @ @ extensive collection of analytics and visualization and model building @ online review of e-commerce giant @ a paradigm @ @ @ used to arrive at profitable @ @ online review allow @ to purchase information @ @ product including @ quality performance and recommendation a @ give @ buyer a clear image of @ product @ favorable review by analyzing customer need manufacturer @ realized @ benefit of invisible energy online review @ reviewing proposed method sentence @ text extraction @ customer @ @ give positive and negative feedback @ a variety of penalty @ @ @ syntax is extracted and analyzed @ @ work improves @ text mining method @ vector similarity relatedtext extraction @ vsrte @ @ @ methodology not only classified @ text @ positive and negative sentiment @ @ @ included emotion of customer s reaction like usual angry and sentiment @ @ text extraction mining @ help ass product quality to enable better decision-making @ consumer and @ predict @ customer behavior @ institute of advanced scientific research inc @ @ @ right reserved @ 
996,When siri knows how you feel: Study of machine learning in automatic sentiment recognition from human speech,"Opinions and sentiments are essential to human activities and have a wide variety of applications. As many decision makers turn to social media due to large volume of opinion data available, efficient and accurate sentiment analysis is necessary to extract those data. Hence, text sentiment analysis has recently become a popular field and has attracted many researchers. However, extracting sentiments from audio speech remains a challenge. This project explored the possibility of applying supervised Machine Learning in recognizing sentiments in English utterances on a sentence level. In addition, the project also aimed to examine the effect of combining acoustic and linguistic features on classification accuracy. Six audio tracks were randomly selected to be training data from 40 YouTube videos (monologue) with strong presence of sentiments. Speakers expressed sentiments towards products, films, or political events. These sentiments were manually labelled as negative and positive based on independent judgment of three experimenters. A wide range of acoustic and linguistic features were then analyzed and extracted using sound editing and text mining tools, respectively. A novel approach was proposed, which used a simplified sentiment score to integrate linguistic features and estimate sentiment valence. This approach improved negation analysis and hence increased overall accuracy. Results showed that when both linguistic and acoustic features were used, accuracy of sentiment recognition improved significantly, and that excellent prediction was achieved when the four classifiers were trained, respectively, namely, kNN, SVM, Neural Network, and Naïve Bayes. Possible sources of error and inherent challenges of audio sentiment analysis were discussed to provide potential directions for future research. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,0,opinion and sentiment @ essential to human activity and @ a wide variety of application @ a many decision maker turn to social medium due to @ volume of opinion data available efficient and accurate sentiment analysis is necessary to extract @ data @ hence text sentiment analysis ha recently become a popular field and ha attracted many researcher @ however extracting sentiment @ audio speech remains a challenge @ @ project explored @ possibility of applying supervised machine learning in recognizing sentiment in english utterance on a sentence level @ in addition @ project @ aimed to examine @ effect of combining acoustic and linguistic feature on classification accuracy @ six audio track @ randomly selected to @ training data @ youtube video @ monologue @ @ strong presence of sentiment @ speaker expressed sentiment towards product film @ political event @ @ sentiment @ manually labelled a negative and positive based on independent judgment of three experimenter @ a wide range of acoustic and linguistic feature @ @ analyzed and extracted @ sound editing and text mining tool respectively @ a novel approach wa proposed @ used a simplified sentiment score to integrate linguistic feature and estimate sentiment valence @ @ approach improved negation analysis and hence increased overall accuracy @ @ showed @ @ @ linguistic and acoustic feature @ used accuracy of sentiment recognition improved significantly and @ excellent prediction wa achieved @ @ four classifier @ trained respectively namely knn svm neural network and naïve bayes @ possible source of error and inherent challenge of audio sentiment analysis @ discussed to provide potential direction @ future research @ @ nature switzerland ag @ 
997,Approach for social media content-based analysis for vacation resorts,"The impact of social networks on our lives keeps increasing because they provide content, generated and controlled by users, that is constantly evolving. They aid us in spreading news, statements, ideas and comments very quickly. Social platforms are currently one of the richest sources of customer feedback on a variety of topics. A topic that is frequently discussed is the resort and holiday villages and the tourist services offered there. Customer comments are valuable to both travel planners and tour operators. The accumulation of opinions in the web space is a prerequisite for using and applying appropriate tools for their computer processing and for extracting useful knowledge from them. While working with unstructured data, such as social media messages, there isn't a universal text processing algorithm because each social network and its resources have their own characteristics. In this article, we propose a new approach for an automated analysis of a static set of historical data of user messages about holiday and vacation resorts, published on Twitter. The approach is based on natural language processing techniques and the application of machine learning methods. The experiments are conducted using software product RapidMiner. © 2019 University of Split. All rights reserved.",2019,Journal of Communications Software and Systems,2,@ impact of social network on @ life keep increasing @ @ provide content generated and controlled by user @ is constantly evolving @ @ aid u in spreading news statement idea and comment @ quickly @ social platform @ currently @ of @ richest source of customer feedback on a variety of topic @ a topic @ is frequently discussed is @ resort and holiday village and @ tourist service offered @ @ customer comment @ valuable to @ travel planner and tour operator @ @ accumulation of opinion in @ web space is a prerequisite @ @ and applying appropriate tool @ @ computer processing and @ extracting useful knowledge @ @ @ @ working @ unstructured data @ a social medium message @ @ @ t a universal text processing algorithm @ @ social network and @ resource @ @ @ characteristic @ in @ article @ propose a @ approach @ @ automated analysis of a static set of historical data of user message @ holiday and vacation resort published on twitter @ @ approach is based on natural language processing technique and @ application of machine learning method @ @ experiment @ conducted @ software product rapidminer @ university of split @ @ right reserved @ 
999,Sentiment analysis of Turkish tweets by data mining methods,"Twitter, has fast emerged as one of the most powerful social media sites which can sway opinions. Sentiment or opinion analysis has of late emerged one of the most researched and talked about subject in Natural Language Processing (NLP), thanks mainly to sites like Twitter. In the past, sentiment analysis models using Twitter data have been built to predict sales performance, rank products and merchants, public opinion polls, predict election results, political standpoints, predict box-office revenues for movies and even predict the stock market. This study proposes a general frame in R programming language to act as a gateway for the analysis of the tweets that portray emotions in a short and concentrated format. The target tweets include brief emotion descriptions and words that are not used with a proper format or grammatical structure. Majority of the work constituted in Turkish includes the data scope and the aim of preparing a data-set. There is no concrete and usable work done on Turkish Tweet sentiment analysis as a software client/web application. This study is a starting point on building up the next steps. The aim is to compare five different common machine learning methods (support vector machines, random forests, boosting, maximum entropy, and artificial neural networks) to classify Twitters sentiments. © IAEME Publication",2019,International Journal of Mechanical Engineering and Technology,1,twitter ha fast emerged a @ of @ @ powerful social medium site @ @ sway opinion @ sentiment @ opinion analysis ha of late emerged @ of @ @ researched and talked @ subject in natural language processing @ nlp @ thanks mainly to site like twitter @ in @ past sentiment analysis model @ twitter data @ @ built to predict sale performance rank product and merchant public opinion poll predict election @ political standpoint predict box-office revenue @ movie and even predict @ stock market @ @ study proposes a general frame in r programming language to act a a gateway @ @ analysis of @ tweet @ portray emotion in a short and concentrated format @ @ target tweet include brief emotion description and word @ @ not used @ a proper format @ grammatical structure @ majority of @ work constituted in turkish includes @ data scope and @ aim of preparing a data-set @ @ is no concrete and usable work done on turkish tweet sentiment analysis a a software client web application @ @ study is a starting point on building up @ next step @ @ aim is to compare five different common machine learning method @ support vector machine random forest boosting maximum entropy and artificial neural network @ to classify twitter sentiment @ iaeme publication
1001,Methods and models of intellectual processing of texts for building ontologies of software for medical terms identification in content classification,"The article investigates the problem of automated development of basic ontology. A method, algorithm and means for extracting knowledge from natural text are proposed. It is shown that such an algorithm should be multistage and include a hierarchical multi-level procedure for recognizing concepts, relationships, predicates and rules, which are introduced as a result of ontology. The analysis of the subject area is the search and analysis of various information systems analogues. The analysis of methods and criteria of information systems is carried out. The analysis of the system and its functionality is presented. This paper examines methods and models of intellectual text processing, the results of which are intended to build software ontologies, and are used during Ontology Learning, when it is necessary to improve, extend, modify an existing ontology model, or build ontology from basic ontology, having only text-collection collections as sources of knowledge. In the latter case, the task is particularly complex and requires the use of the full range of text mining methods (Text Mining as TM). The paper deals with the solution of TM problems at different stages of the PMT processing: obtaining information (identifying entities - concepts and terms, their properties, facts, events, establishing relationships between entities, in particular associative ones), categorization, and clustering, semantic annotation. Below we will consider the tools of automated analysis of natural language and software products implemented on their basis for filling the system. Copyright © 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,21,@ article investigates @ problem of automated development of basic ontology @ a method algorithm and mean @ extracting knowledge @ natural text @ proposed @ @ is @ @ @ @ algorithm @ @ multistage and include a hierarchical multi-level procedure @ recognizing concept relationship predicate and rule @ @ introduced a a @ of ontology @ @ analysis of @ subject area is @ search and analysis of various information system analogue @ @ analysis of method and criterion of information system is carried @ @ @ analysis of @ system and @ functionality is presented @ @ @ examines method and model of intellectual text processing @ @ of @ @ intended to build software ontology and @ used @ ontology learning @ @ is necessary to improve extend modify @ existing ontology model @ build ontology @ basic ontology @ only text-collection collection a source of knowledge @ in @ latter case @ task is particularly complex and requires @ use of @ full range of text mining method @ text mining a tm @ @ @ @ deal @ @ solution of tm problem at different stage of @ pmt processing @ obtaining information @ identifying entity concept and term @ property fact event establishing relationship @ entity in particular associative @ @ categorization and clustering semantic annotation @ @ @ @ consider @ tool of automated analysis of natural language and software product implemented on @ basis @ filling @ system @ @ @ @ @ by @ author @ 
1002,CANCROX: A cross-species cancer therapy database,"Cancer comprises a set of more than 200 diseases resulting from the uncontrolled growth of cells that invade tissues and organs, which can spread to other regions of the body. The types of cancer found in humans are also described in animal models, a fact that has raised the interest of the scientific community in comparative oncology studies. In this study, bioinformatics tools were used to implement a computational model that uses text mining and natural language processing to construct a reference database that relates human and canine genes potentially associated with cancer, defining genetic pathways and information about cancer and cancer therapies. The CANCROX reference database was constructed by processing the scientific literature and lists more than 1300 drugs and therapies used to treat cancer, in addition to over 10 000 combinations of these drugs, including 40 types of cancer. A user-friendly interface was developed that enables researchers to search for different types of information about therapies, drug combinations, genes and types of cancer. In addition, data visualization tools allow to explore and relate different drugs and therapies for the treatment of cancer, providing information for groups studying animal models, in this case the dog, as well as groups studying cancer in humans. © 2019 The Author(s) 2019. Published by Oxford University Press.",2019,Database,0,cancer comprises a set of more @ disease resulting @ @ uncontrolled growth of cell @ invade tissue and organ @ @ spread to @ region of @ body @ @ type of cancer found in human @ @ described in animal model a fact @ ha raised @ interest of @ scientific community in comparative oncology study @ in @ study bioinformatics tool @ used to implement a computational model @ us text mining and natural language processing to construct a reference database @ relates human and canine gene potentially associated @ cancer defining genetic pathway and information @ cancer and cancer therapy @ @ cancrox reference database wa constructed by processing @ scientific literature and list more @ drug and therapy used to treat cancer in addition to @ combination of @ drug including type of cancer @ a user-friendly interface wa developed @ enables researcher to search @ different type of information @ therapy drug combination gene and type of cancer @ in addition data visualization tool allow to explore and relate different drug and therapy @ @ treatment of cancer providing information @ group studying animal model in @ case @ dog a well a group studying cancer in human @ @ author @ s @ @ published by oxford university @ @ 
1003,Application of machine learning techniques in clinical information extraction,"A large number of medical research papers and clinical notes on disease diagnostic, treatment and prevention are increasing every day. This biomedical text provides a rich source of knowledge for biomedical research. However, this medical information is scattered in vast medical informatics literature in unstructured form. It is requisite to retrieve imperative information from these publications and discover new knowledge. A lot of research is done in biomedical text mining using different methods and techniques. Centre of i2b2 organized different challenges on natural language processing for medical text. In i2b2 2010, challenge tasks were focused on concept extraction, assertion classification and relation extraction, and in 2012, the task was temporal information extraction. In previous work, various machine learning techniques are found to be one of the effective techniques to extract clinical information from different types of medical data like discharge summary, physical notes. This paper presents the review of earlier work on different machine learning techniques and methods for medical research. The effectiveness of these techniques has been measured by precision, recall and F-score. This review will be useful for biomedical researchers to identify best techniques for the further research in clinical information extraction. © Springer Nature Switzerland AG 2019.",2019,Studies in Fuzziness and Soft Computing,2,a @ number of medical research @ and clinical note on disease diagnostic treatment and prevention @ increasing every day @ @ biomedical text provides a rich source of knowledge @ biomedical research @ however @ medical information is scattered in vast medical informatics literature in unstructured form @ @ is requisite to retrieve imperative information @ @ publication and discover @ knowledge @ a lot of research is done in biomedical text mining @ different method and technique @ centre of i b organized different challenge on natural language processing @ medical text @ in i b challenge task @ focused on concept extraction assertion classification and relation extraction and in @ task wa temporal information extraction @ in previous work various machine learning technique @ found to @ @ of @ effective technique to extract clinical information @ different type of medical data like discharge summary physical note @ @ @ @ @ review of earlier work on different machine learning technique and method @ medical research @ @ effectiveness of @ technique ha @ measured by precision recall and f-score @ @ review @ @ useful @ biomedical researcher to identify best technique @ @ @ research in clinical information extraction @ @ nature switzerland ag @ 
1004,Multi-level Modeling of Structural Elements of Natural Language Texts and Its Applications,"Methods of extracting knowledge in the analysis of large volumes of natural language texts are relevant for solving various problems in the field of analysis and generation of textual information, such as text analysis for extracting data, fact and semantics; presenting extracted information in a convenient for machine processing form (for example, ontology); classification and clustering texts, including thematic modeling; information retrieval (including thematic search, search based on the user model, ontology-based models, document sample based search); texts abstracting and annotating; developing of intelligent question-answering systems; generating texts of different types (fiction, marketing, weather forecasts etc.); as well as rewriting texts, preserving the meaning of the original text for presenting it to different target audiences. In order for such methods to work, it is necessary to construct and use models that adequately describe structural elements of the text on different levels (individual words, sentences, thematic text fragments), their characteristics and semantics, as well as relations between them, allowing to form higher-level structures. Such models should also take into account general characteristics of textual data: genre, purpose, target audience, scientific field and others. In this paper, authors review three main approaches to text modeling (structural, statistical and hybrid), their characteristics, pros and cons and applicability on different stages (knowledge extraction, storage and text generation) of solving problems in the field of analysis and generation of textual information. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,4,method of extracting knowledge in @ analysis of @ volume of natural language text @ relevant @ solving various problem in @ field of analysis and generation of textual information @ a text analysis @ extracting data fact and semantics @ presenting extracted information in a convenient @ machine processing form @ @ example ontology @ @ classification and clustering text including thematic modeling @ information retrieval @ including thematic search search based on @ user model ontology-based model document sample based search @ @ text abstracting and annotating @ developing of intelligent question-answering system @ generating text of different type @ fiction marketing weather forecast etc @ @ @ a well a rewriting text preserving @ meaning of @ original text @ presenting @ to different target audience @ in order @ @ method to work @ is necessary to construct and use model @ adequately describe structural element of @ text on different level @ individual word sentence thematic text fragment @ @ characteristic and semantics a well a relation @ @ allowing to form higher-level structure @ @ model @ @ take @ account general characteristic of textual data @ genre purpose target audience scientific field and others @ in @ @ author review three main approach to text modeling @ structural statistical and hybrid @ @ characteristic pro and con and applicability on different stage @ knowledge extraction storage and text generation @ of solving problem in @ field of analysis and generation of textual information @ @ nature switzerland ag @ 
1007,Assorted sentiment analysis model for natural crisis response and recovery using big data driven technology,"Social networking sites are generating big data from which we can fetch valuable data. Social media is increasingly used for communication during emergency situation caused by natural crisis and also used for helping related requests. During natural crisis situation, pool of big data is used to extract emergency request for moving forward to provide timely help. Though emergency responders and government agencies work together with the help of their respective framework of natural crisis response and recovery mechanism, the sentiment of the affected persons during natural crisis and after that determines the success of natural crisis response and recovery mechanism. In this research paper, we have proposed natural crisis response and recovery through analysis of assorted sentiment model with the help of big data driven approach. Assorted sentiment analysis model is a combination of various phases for finding sentiment from any textual content of social media rather than to find sentiment in one phases from textual content of social media. Assorted sentiment model would extract sentiment from covering many phases from textual content of social media. These phases are namely collection phase, preprocessing phase, and filtering phase. The proposed model gathers natural crisis data from Twitter and categorizes them according to the requirement of needy persons. The categorized natural crisis dataset is classified through SVM (Support Vector Machine) algorithm for finding analysis of sentiment of affected people. We have chosen SVM algorithm for classification because SVM performs better than other machine learning algorithm after applying feature generation method. Various feature like, Bag of Word, part of speech and lexicon are analyzed to identify best classification strategy for natural crisis data. Result shows that Bag of Word feature combined with SVM is suitable for analyzing the needs of people during natural crisis. This model helps rescue team and emergency responders to improve good approaches for making effective MIS (management information system) of frequently changing natural crisis situation. © 2019, Research Trend. All rights reserved.",2019,International Journal on Emerging Technologies,1,social networking site @ generating big data @ @ @ @ fetch valuable data @ social medium is increasingly used @ communication @ emergency situation caused by natural crisis and @ used @ helping related request @ @ natural crisis situation pool of big data is used to extract emergency request @ moving forward to provide timely help @ though emergency responder and government agency work together @ @ help of @ respective framework of natural crisis response and recovery mechanism @ sentiment of @ affected person @ natural crisis and @ @ determines @ success of natural crisis response and recovery mechanism @ in @ research @ @ @ proposed natural crisis response and recovery @ analysis of assorted sentiment model @ @ help of big data driven approach @ assorted sentiment analysis model is a combination of various phase @ finding sentiment @ @ textual content of social medium rather @ to find sentiment in @ phase @ textual content of social medium @ assorted sentiment model would extract sentiment @ covering many phase @ textual content of social medium @ @ phase @ namely collection phase preprocessing phase and filtering phase @ @ proposed model gather natural crisis data @ twitter and categorizes @ according to @ requirement of needy person @ @ categorized natural crisis dataset is classified @ svm @ support vector machine @ algorithm @ finding analysis of sentiment of affected people @ @ @ chosen svm algorithm @ classification @ svm performs better @ @ machine learning algorithm @ applying feature generation method @ various feature like bag of word part of speech and lexicon @ analyzed to identify best classification strategy @ natural crisis data @ @ @ @ bag of word feature combined @ svm is suitable @ analyzing @ need of people @ natural crisis @ @ model help rescue team and emergency responder to improve good approach @ making effective mi @ management information system @ of frequently changing natural crisis situation @ research trend @ @ right reserved @ 
1008,A graph-based approach to topic clustering of tourist attraction reviews,"A large volume of user reviews on tourist attractions can prohibit travel businesses from acquiring overall consumers’ expectations and consumers themselves from seeing the big picture and making thoughtful decisions on trip planning. Summarization of the reviews allows both parties to catch the main themes and underlying tones of the attractions. In this paper, we address the task of topic clustering, by applying a graph-based approach to group the reviews into clusters. To interpret the resulting review clusters, WordNet and Inverse Document Frequency (IDF) are utilized to extract keywords from each cluster which represents the topic. We evaluate the graph-based clustering approach against gold standard data annotated by human and the results are compared against Latent Dirichlet Allocation (LDA), a widely used algorithm for topic discovery. The approach is shown to be competitive to LDA in terms of clustering user reviews on tourist attractions. The graph-based approach, unlike LDA which requires the number of clusters as an input, can dynamically clusters the reviews into groups, revealing the number of clusters. © Springer Nature Switzerland AG 2019.",2019,Communications in Computer and Information Science,0,a @ volume of user review on tourist attraction @ prohibit travel @ @ acquiring overall consumer expectation and consumer @ @ seeing @ big picture and making thoughtful decision on trip planning @ summarization of @ review allows @ party to catch @ main theme and underlying tone of @ attraction @ in @ @ @ address @ task of topic clustering by applying a graph-based approach to group @ review @ cluster @ to interpret @ resulting review cluster wordnet and inverse document frequency @ idf @ @ utilized to extract keywords @ @ cluster @ represents @ topic @ @ evaluate @ graph-based clustering approach @ gold standard data annotated by human and @ @ @ compared @ latent dirichlet allocation @ lda @ a widely used algorithm @ topic discovery @ @ approach is @ to @ competitive to lda in term of clustering user review on tourist attraction @ @ graph-based approach unlike lda @ requires @ number of cluster a @ input @ dynamically cluster @ review @ group revealing @ number of cluster @ @ nature switzerland ag @ 
1009,A framework for sentiment analysis in Arabic text,"Over the last decade there has been an increase in number of E-mails or comments to a company via social media sites, to satisfy their customers, the company must take in to consideration these messages and comments and know whether the customers are satisfied with what the company offers or not. Several techniques have been proposed to analyze the sentiment of the comment writer. Dealing with the Arabic language is faced with many challenges, such as it is a morphologically rich language and how to return the word to its original root. In this paper the challenges of dealing with the Arabic language were reviewed and a framework was also established to analyze the comments in Arabic and classify it into positive, negative or neutral sentiment. The framework was trained and tested and then the conclusions were drawn based on its work. Copyright © 2019 Institute of Advanced Engineering and Science. All rights reserved.",2019,Indonesian Journal of Electrical Engineering and Computer Science,4,@ @ last decade @ ha @ @ increase in number of e-mail @ comment to a company via social medium site to satisfy @ customer @ company must take in to consideration @ message and comment and know whether @ customer @ satisfied @ @ @ company offer @ not @ several technique @ @ proposed to analyze @ sentiment of @ comment writer @ dealing @ @ arabic language is faced @ many challenge @ a @ is a morphologically rich language and @ to return @ word to @ original root @ in @ @ @ challenge of dealing @ @ arabic language @ reviewed and a framework wa @ established to analyze @ comment in arabic and classify @ @ positive negative @ neutral sentiment @ @ framework wa trained and tested and @ @ conclusion @ drawn based on @ work @ @ institute of advanced engineering and science @ @ right reserved @ 
1010,Semi-automatic description of named rivers and bays for their representation in a terminological knowledge base,"EcoLexicon (http://ecolexicon.ugr.es) is a terminological knowledge base on environmental science, whose design permits the geographic contextualization of data. For the geographic contextualization of landform concepts, this paper presents a semi-automatic method for extracting terms associated with named rivers (e.g., Pearl River) and named bays (e.g., San Francisco Bay). Terms were extracted from an English specialized corpus on Coastal Engineering, where named rivers and bays were automatically identified. Statistical procedures were applied for selecting terms, rivers, and bays in distributional semantic models to construct the conceptual structures underlying the usage of named rivers and bays in Coastal Engineering texts. The rivers sharing associated terms were also automatically clustered and represented in the same conceptual network. The same was done for named bays sharing associated terms. The results showed that the method successfully described the semantic frames for named rivers and bays with explanatory adequacy, according to the premises of Frame-based Terminology. Furthermore, the semantic networks unveiled that the named rivers and bays mentioned in the Coastal Engineering corpus are both thematically related to sediment concentration and sediment transport in rivers, sediment discharge into bays and seas, and the negative effects of sediment supply decrease on coastal erosion because of human activities. Copyright © 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,0,ecolexicon @ http @ ecolexicon @ ugr @ e @ is a terminological knowledge base on environmental science whose design permit @ geographic contextualization of data @ @ @ geographic contextualization of landform concept @ @ @ a semi-automatic method @ extracting term associated @ named river @ e @ g @ pearl river @ and named bay @ e @ g @ san francisco bay @ @ term @ extracted @ @ english specialized corpus on coastal engineering @ named river and bay @ automatically identified @ statistical procedure @ applied @ selecting term river and bay in distributional semantic model to construct @ conceptual structure underlying @ usage of named river and bay in coastal engineering text @ @ river sharing associated term @ @ automatically clustered and represented in @ @ conceptual network @ @ @ wa done @ named bay sharing associated term @ @ @ showed @ @ method successfully described @ semantic frame @ named river and bay @ explanatory adequacy according to @ premise of frame-based terminology @ furthermore @ semantic network unveiled @ @ named river and bay mentioned in @ coastal engineering corpus @ @ thematically related to sediment concentration and sediment transport in river sediment discharge @ bay and sea and @ negative effect of sediment supply decrease on coastal erosion @ of human activity @ @ @ @ @ by @ author @ 
1011,Transfer learning for biomedical named entity recognition with BioBert,"We apply a transfer learning approach to biomedical named entity recognition and compare it with traditional approaches (dictionary, CRF, BiLTSM). Specifically, we build models for adverse drug reaction recognition on three datasets. We tune a pre-trained transformer model, BioBERT, on these datasets and observe the absolute F1-score improvements of 6.93, 10.46 and 13.31. This shows that, with a relatively small amount of annotated data, transfer learning can help in specialized information extraction tasks. Copyright © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",2019,CEUR Workshop Proceedings,0,@ apply a transfer learning approach to biomedical named entity recognition and compare @ @ traditional approach @ dictionary crf biltsm @ @ specifically @ build model @ adverse drug reaction recognition on three datasets @ @ tune a pre-trained transformer model biobert on @ datasets and observe @ absolute f score improvement of @ @ and @ @ @ @ @ @ a relatively small amount of annotated data transfer learning @ help in specialized information extraction task @ @ @ @ @ by @ author @ use permitted @ creative common license attribution @ international @ cc by @ @ @ 
1012,Automatic stop word generation for mining software artifact using topic model with pointwise mutual information,"Mining software artifacts is a useful way to understand the source code of software projects. Topic modeling in particular has been widely used to discover meaningful information from software artifacts. However, software artifacts are unstructured and contain a mix of textual types within the natural text. These software artifact characteristics worsen the performance of topic modeling. Among several natural language preprocessing tasks, removing stop words to reduce meaningless and uninteresting terms is an efficient way to improve the quality of topic models. Although many approaches are used to generate effective stop words, the lists are outdated or too general to apply to mining software artifacts. In addition, the performance of the topic model is sensitive to the datasets used in the training for each approach. To resolve these problems, we propose an automatic stop word generation approach for topic models of software artifacts. By measuring topic coherence among words in the topic using Pointwise Mutual Information (PMI), we added words with a low PMI score to our stop words list for every topic modeling loop. Through our experiment, we proved that our stop words list results in a higher performance of the topic model than lists from other approaches. © 2019 The Institute of Electronics, Information and Communication Engineers.",2019,IEICE Transactions on Information and Systems,1,mining software artifact is a useful way to understand @ source code of software project @ topic modeling in particular ha @ widely used to discover meaningful information @ software artifact @ however software artifact @ unstructured and contain a mix of textual type within @ natural text @ @ software artifact characteristic worsen @ performance of topic modeling @ among several natural language preprocessing task removing stop word to reduce meaningless and uninteresting term is @ efficient way to improve @ quality of topic model @ although many approach @ used to generate effective stop word @ list @ outdated @ too general to apply to mining software artifact @ in addition @ performance of @ topic model is sensitive to @ datasets used in @ training @ @ approach @ to resolve @ problem @ propose @ automatic stop word generation approach @ topic model of software artifact @ by measuring topic coherence among word in @ topic @ pointwise mutual information @ pmi @ @ added word @ a low pmi score to @ stop word list @ every topic modeling loop @ @ @ experiment @ proved @ @ stop word list @ in a higher performance of @ topic model @ list @ @ approach @ @ institute of electronics information and communication engineer @ 
1013,"Poli2Sum@CL-SciSumm-19: Identify, classify, and summarize cited text spans by means of ensembles of supervised models","This paper presents the Poli2Sum approach to the 5th Computational Linguistics Scientific Document Summarization Shared Task (BIRNDL CL-SciSumm 2019). Given a set of reference papers and the set of papers citing them, the proposed approach has a threefold aim. (1a) Identify the text spans in the reference paper that are referenced by a specific citation in the citing papers. (1b) Assign a facet to each citation describing the semantics behind the citation. (2) Generate a summary of the reference paper consisting of the most relevant cited text spans. The Poli2Sum approach to tasks (1a) and (1b) relies on an ensemble of classification and regression models trained on the annotated pairs of cited and citing sentences. Facet assignment is based on the relative positions of the cited sentences locally to the corresponding section and globally in the entire paper. Task (2) is addressed by predicting the overlap (in terms of units of text) between the selected text spans and the summary generated by the domain experts. The output summary consists of the subset of sentences maximizing the predicted overlap score. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,1,@ @ @ @ poli sum approach to @ th computational linguistics scientific document summarization shared task @ birndl cl-scisumm @ @ given a set of reference @ and @ set of @ citing @ @ proposed approach ha a threefold aim @ @ a @ identify @ text span in @ reference @ @ @ referenced by a specific citation in @ citing @ @ @ b @ assign a facet to @ citation describing @ semantics behind @ citation @ @ @ generate a summary of @ reference @ consisting of @ @ relevant cited text span @ @ poli sum approach to task @ a @ and @ b @ relies on @ ensemble of classification and regression model trained on @ annotated pair of cited and citing sentence @ facet assignment is based on @ relative position of @ cited sentence locally to @ corresponding section and globally in @ entire @ @ task @ @ is addressed by predicting @ overlap @ in term of unit of text @ @ @ selected text span and @ summary generated by @ domain expert @ @ output summary consists of @ subset of sentence maximizing @ predicted overlap score @ ceur-ws @ @ right reserved @ 
1014,Ensemble learning to detect aggressiveness in Mexican Spanish tweets,"Comments published on social media often contain aggressive language that can have damaging effects on users. The severe consequences of this problem, combined with the large amount of data that users daily publish on the Web, require the development of algorithms capable of automatically detecting inappropriate online remarks. In this paper, we present our participation in IberLEF-2019: subtask MEX-A3T: Authorship and aggressiveness analysis in Twitter: case study in Mexican Spanish. Our main contribution is the development of a ensemble learning system to detect aggressiveness in tweets. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,2,comment published on social medium often contain aggressive language @ @ @ damaging effect on user @ @ severe consequence of @ problem combined @ @ @ amount of data @ user daily publish on @ web require @ development of algorithm capable of automatically detecting inappropriate online remark @ in @ @ @ @ @ participation in iberlef @ subtask mex-a t @ authorship and aggressiveness analysis in twitter @ case study in mexican spanish @ @ main contribution is @ development of a ensemble learning system to detect aggressiveness in tweet @ ceur-ws @ @ right reserved @ 
1015,Istex: A database of twenty million scientific papers with a mining tool which uses named entities,"Istex is a database of twenty million full text scientific papers bought by the French Government for the use of academic libraries. Papers are usually searched for by the title, authors, keywords or possibly the abstract. To authorize new types of queries of Istex, we implemented a system of named entity recognition on all papers and we offer users the possibility to run searches on these entities. After the presentation of the French Istex project, we detail in this paper the named entity recognition with CasEN, a cascade of graphs, implemented on the Unitex Software. CasEN exists in French, but not in English. The first challenge was to build a new cascade in a short time. The results of its evaluation showed a good Precision measure, even if the Recall was not very good. The Precision was very important for this project to ensure it did not return unwanted papers by a query. The second challenge was the implementation of Unitex to parse around twenty millions of documents. We used a dockerized application. Finally, we explain also how to query the resulting Named entities in the Istex website. © 2019 by the authors.",2019,Information (Switzerland),2,istex is a database of twenty million full text scientific @ bought by @ french government @ @ use of @ library @ @ @ usually searched @ by @ title author keywords @ possibly @ abstract @ to authorize @ type of query of istex @ implemented a system of named entity recognition on @ @ and @ offer user @ possibility to run search on @ entity @ @ @ presentation of @ french istex project @ detail in @ @ @ named entity recognition @ casen a cascade of graph implemented on @ unitex software @ casen exists in french @ not in english @ @ first challenge wa to build a @ cascade in a short time @ @ @ of @ evaluation showed a good precision measure even if @ recall wa not @ good @ @ precision wa @ important @ @ project to ensure @ @ not return unwanted @ by a query @ @ second challenge wa @ implementation of unitex to parse around twenty million of document @ @ used a dockerized application @ finally @ explain @ @ to query @ resulting named entity in @ istex website @ by @ author @ 
1016,French medical named entity recognition: A hybrid approach,"With the availability of a huge amount of medical textual documents in digital form, the automatic extraction of relevant information from these documents is becoming a very challenging task because of the volume and the heterogeneous structure of medical text, which contains complex vocabulary. Therefore, there is an urgent need for medical information extraction techniques. One of the most important of these techniques is named entity recognition (NER). In this paper, we propose a system of French medical NER using a hybrid approach. And since the medical domain contains various types of information, we have taken into account both clinical and biomedical data to generalise the performance of our proposed system. © 2019 Inderscience Enterprises Ltd.",2019,International Journal of Intelligent Enterprise,0,@ @ availability of a huge amount of medical textual document in digital form @ automatic extraction of relevant information @ @ document is becoming a @ challenging task @ of @ volume and @ heterogeneous structure of medical text @ contains complex vocabulary @ therefore @ is @ urgent need @ medical information extraction technique @ @ of @ @ important of @ technique is named entity recognition @ ner @ @ in @ @ @ propose a system of french medical ner @ a hybrid approach @ and since @ medical domain contains various type of information @ @ taken @ account @ clinical and biomedical data to generalise @ performance of @ proposed system @ inderscience enterprise ltd @ 
1017,Bug Severity Prediction Using a Hierarchical One-vs.-Remainder Approach,"Assigning severity level to reported bugs is a critical part of software maintenance to ensure an efficient resolution process. In many bug trackers, e.g.Â Bugzilla, this is a time consuming process, because bug reporters must manually assign one of seven severity levels to each bug. In addition, some bug types may be reported more often than others, leading to a disproportionate distribution of severity labels. Machine learning techniques can be used to predict the label of a newly reported bug automatically. However, learning from imbalanced data in a multi-class task remains one of the major difficulties for machine learning classifiers. In this paper, we propose a hierarchical classification approach that exploits class imbalance in the training data, to reduce classification bias. Specifically, we designed a classification tree that consists of multiple binary classifiers organised hierarchically, such that instances from the most dominant class are trained against the remaining classes but are not used for training the next level of the classification tree. We used FastText classifier to test and compare between the hierarchical and standard classification approaches. Based on 93,051 bug reports from 38 Eclipse open-source products, the hierarchical approach was shown to perform relatively well with 65 % Micro F-Score and 45 % Macro F-Score. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,assigning severity level to reported bug is a critical part of software maintenance to ensure @ efficient resolution process @ in many bug tracker e @ g @ â bugzilla @ is a time consuming process @ bug reporter must manually assign @ of seven severity level to @ bug @ in addition some bug type may @ reported more often @ others leading to a disproportionate distribution of severity label @ machine learning technique @ @ used to predict @ label of a newly reported bug automatically @ however learning @ imbalanced data in a multi-class task remains @ of @ major difficulty @ machine learning classifier @ in @ @ @ propose a hierarchical classification approach @ exploit class imbalance in @ training data to reduce classification bias @ specifically @ designed a classification tree @ consists of multiple binary classifier organised hierarchically @ @ instance @ @ @ dominant class @ trained @ @ remaining class @ @ not used @ training @ next level of @ classification tree @ @ used fasttext classifier to test and compare @ @ hierarchical and standard classification approach @ based on bug report @ eclipse open-source product @ hierarchical approach wa @ to perform relatively well @ micro f-score and macro f-score @ @ nature switzerland ag @ 
1018,Structure-Based Supervised Term Weighting and Regularization for Text Classification,"Text documents have rich information that can be useful for different tasks. How to utilise the rich information in texts effectively and efficiently for tasks such as text classification is still an active research topic. One approach is to weight the terms in a text document based on their relevance to the classification task at hand. Another approach is to utilise structural information in a text document to regularize learning so that the learned model is more accurate. An important question is, can we combine the two approaches to achieve better performance? This paper presents a novel method for utilising the rich information in texts. We use supervised term weighting, which utilises the class information in a set of pre-classified training documents, thus the resulting term weighting is class specific. We also use structured regularization, which incorporates structural information into the learning process. A graph is built for each class from the pre-classified training documents and structural information in the graphs is used to calculate the supervised term weights and to define the groups for structured regularization. Experimental results for six text classification tasks show the increase in text classification accuracy with the utilisation of structural information in text for both weighting and regularization. Using graph-based text representation for supervised term weighting and structured regularization can build a compact model with considerable improvement in the performance of text classification. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,text document @ rich information @ @ @ useful @ different task @ @ to utilise @ rich information in text effectively and efficiently @ task @ a text classification is still @ active research topic @ @ approach is to weight @ term in a text document based on @ relevance to @ classification task at hand @ another approach is to utilise structural information in a text document to regularize learning @ @ @ learned model is more accurate @ @ important question is @ @ combine @ @ approach to achieve better performance @ @ @ @ a novel method @ utilising @ rich information in text @ @ use supervised term weighting @ utilises @ class information in a set of pre-classified training document thus @ resulting term weighting is class specific @ @ @ use structured regularization @ incorporates structural information @ @ learning process @ a graph is built @ @ class @ @ pre-classified training document and structural information in @ graph is used to calculate @ supervised term weight and to define @ group @ structured regularization @ experimental @ @ six text classification task @ @ increase in text classification accuracy @ @ utilisation of structural information in text @ @ weighting and regularization @ @ graph-based text representation @ supervised term weighting and structured regularization @ build a compact model @ considerable improvement in @ performance of text classification @ @ nature switzerland ag @ 
1020,Mining annotators’ common knowledge for automatic text revision,"Many natural language understanding tasks require clean input textual data in order to train systems with the highest precision. Such data, usually collected from surveys or the web, are manually processed in order to remove morphosyntactic variability, spelling errors and incoherence in naming entities. Since these operations are conducted by domain experts and annotators, they are usually costly and time-consuming. Furthermore, this scenario is very common in industrial tasks where annotators are hired. In this context, we propose an innovative and simple method that extracts correction patterns, i.e., <expression, replacement> pairs, where expression is a matching string and replacement indicates how to re-write the matched string. Such tool can be used both to evaluate annotators (since it provides a deep understanding of their work) and to automatically revise the texts. We extensively tested our method in a multilingual setting, obtaining outstanding results over baseline approaches. © 2019 Inderscience Enterprises Ltd.",2019,"International Journal of Metadata, Semantics and Ontologies",0,many natural language understanding task require clean input textual data in order to train system @ @ highest precision @ @ data usually collected @ survey @ @ web @ manually processed in order to remove morphosyntactic variability spelling error and incoherence in naming entity @ since @ operation @ conducted by domain expert and annotator @ @ usually costly and time-consuming @ furthermore @ scenario is @ common in industrial task @ annotator @ hired @ in @ context @ propose @ innovative and simple method @ extract correction pattern i @ e @ expression replacement pair @ expression is a matching string and replacement indicates @ to re-write @ matched string @ @ tool @ @ used @ to evaluate annotator @ since @ provides a deep understanding of @ work @ and to automatically revise @ text @ @ extensively tested @ method in a multilingual setting obtaining outstanding @ @ baseline approach @ inderscience enterprise ltd @ 
1021,Information Mining from Criminal Judgments of Lahore High Court,"Since the last few years, computers have become a prominent part of the court of law. Courts generate an enormous amount of unstructured text on a daily basis. Extraction of the desired information from this unstructured legal text is one of the major issues. So, there is a need to develop an intelligent system that can automatically find useful and critical information from the available text. Such a system will help judges and lawyers in their judgments and case preparations, common people in understanding law, and finding appropriate lawyer for their legal issues. Therefore, in this research, Punjab University Legal Mining System (PULMS) is developed using three different supervised machine learning algorithms; conditional random field (CRF), maximum entropy (MaxEnt), and trigram N tag (TNT). To train the system, 304 criminal miscellaneous judgments of the Lahore High Court (LHC) of Pakistan are manually tagged for nine named entities (NE). After training, among three machine learning algorithms, the system achieved significant precision, recall, and f-measure using CRF which are 0.97, 0.87, and 0.89, respectively. © 2013 IEEE.",2019,IEEE Access,2,since @ last @ year computer @ become a prominent part of @ court of law @ court generate @ enormous amount of unstructured text on a daily basis @ extraction of @ desired information @ @ unstructured legal text is @ of @ major issue @ @ @ is a need to develop @ intelligent system @ @ automatically find useful and critical information @ @ available text @ @ a system @ help judge and lawyer in @ judgment and case preparation common people in understanding law and finding appropriate lawyer @ @ legal issue @ therefore in @ research punjab university legal mining system @ pulms @ is developed @ three different supervised machine learning algorithm @ conditional random field @ crf @ maximum entropy @ maxent @ and trigram n tag @ tnt @ @ to train @ system criminal miscellaneous judgment of @ lahore high court @ lhc @ of pakistan @ manually tagged @ nine named entity @ ne @ @ @ training among three machine learning algorithm @ system achieved significant precision recall and f-measure @ crf @ @ @ @ and @ respectively @ @ @ 
1022,Legal Data Mining from Civil Judgments,"Due to advent of computing, content digitization and its processing is being widely performed across the globe. Legal domain is amongst many of those areas that provide various opportunities for innovation and betterment by means of computational advancements. In Pakistan, since last couple of years, courts have been reporting judgments for public consumption. This reported data is of great importance for judges, lawyers and civilians in various aspects. As this data is growing at rapid rate, there is dire need to process this huge amount of data to better address the need of respective stakeholders. Therefore, in this study, our aim is to develop a machine learning system that can automatically extract information out of public reported judgments of Lahore High Court. This information, once extracted, can be utilized in betterment for society and policy making in Pakistan. This study takes the first step to achieve this goal by means of extracting various entities from legal judgments. Total ten entities are being extracted that include dates, case numbers, reference cases, person names, respondent names etc. In order to automatically extract these entities, primary requirement was to construct dataset using legal judgments. Hence, firstly annotation guidelines are prepared followed by preparation of annotated dataset for entity extraction. Finally, various algorithms including Markov models and Conditional Random Fields are applied on annotated dataset. Experiments show that these approaches achieve reasonable well results for legal data extraction. Primary contribution of this study is development of annotated dataset on civil judgments followed by training of various machine learning models to extract the potential information from a judgment. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,1,due to advent of computing content digitization and @ processing is @ widely performed across @ globe @ legal domain is amongst many of @ area @ provide various opportunity @ innovation and betterment by mean of computational advancement @ in pakistan since last couple of year court @ @ reporting judgment @ public consumption @ @ reported data is of great importance @ judge lawyer and civilian in various aspect @ a @ data is growing at rapid rate @ is dire need to process @ huge amount of data to better address @ need of respective stakeholder @ therefore in @ study @ aim is to develop a machine learning system @ @ automatically extract information @ of public reported judgment of lahore high court @ @ information @ extracted @ @ utilized in betterment @ society and policy making in pakistan @ @ study take @ first step to achieve @ goal by mean of extracting various entity @ legal judgment @ total ten entity @ @ extracted @ include date case number reference case person name respondent name etc @ in order to automatically extract @ entity primary requirement wa to construct dataset @ legal judgment @ hence firstly annotation guideline @ prepared followed by preparation of annotated dataset @ entity extraction @ finally various algorithm including markov model and conditional random field @ applied on annotated dataset @ experiment @ @ @ approach achieve reasonable well @ @ legal data extraction @ primary contribution of @ study is development of annotated dataset on civil judgment followed by training of various machine learning model to extract @ potential information @ a judgment @ @ nature singapore pte ltd @ 
1025,ELSA: A multilingual document summarization algorithm based on frequent itemsets and latent semantic analysis,"Sentence-based summarization aims at extracting concise summaries of collections of textual documents. Summaries consist of a worthwhile subset of document sentences. The most effective multilingual strategies rely on Latent Semantic Analysis (LSA) and on frequent itemset mining, respectively. LSA-based summarizers pick the document sentences that cover the most important concepts. Concepts are modeled as combinations of single-document terms and are derived from a term-by-sentence matrix by exploiting Singular Value Decomposition (SVD). Itemset-based summarizers pick the sentences that contain the largest number of frequent itemsets, which represent combinations of frequently co-occurring terms. The main drawbacks of existing approaches are (i) the inability of LSA to consider the correlation between combinations of multiple-document terms and the underlying concepts, (ii) the inherent redundancy of frequent itemsets because similar itemsets may be related to the same concept, and (iii) the inability of itemset-based summarizers to correlate itemsets with the underlying document concepts. To overcome the issues of both of the abovementioned algorithms, we propose a new summarization approach that exploits frequent itemsets to describe all of the latent concepts covered by the documents under analysis and LSA to reduce the potentially redundant set of itemsets to a compact set of uncorrelated concepts. The summarizer selects the sentences that cover the latent concepts with minimal redundancy. We tested the summarization algorithm on both multilingual and English-language benchmark document collections. The proposed approach performed significantly better than both itemset- and LSA-based summarizers, and better than most of the other state-of-the-art approaches. © 2019 Association for Computing Machinery.",2019,ACM Transactions on Information Systems,8,sentence-based summarization aim at extracting concise summary of collection of textual document @ summary consist of a worthwhile subset of document sentence @ @ @ effective multilingual strategy rely on latent semantic analysis @ lsa @ and on frequent itemset mining respectively @ lsa-based summarizers pick @ document sentence @ cover @ @ important concept @ concept @ modeled a combination of single-document term and @ derived @ a term-by-sentence matrix by exploiting singular value decomposition @ svd @ @ itemset-based summarizers pick @ sentence @ contain @ largest number of frequent itemsets @ represent combination of frequently co-occurring term @ @ main drawback of existing approach @ @ i @ @ inability of lsa to consider @ correlation @ combination of multiple-document term and @ underlying concept @ ii @ @ inherent redundancy of frequent itemsets @ similar itemsets may @ related to @ @ concept and @ iii @ @ inability of itemset-based summarizers to correlate itemsets @ @ underlying document concept @ to overcome @ issue of @ of @ abovementioned algorithm @ propose a @ summarization approach @ exploit frequent itemsets to describe @ of @ latent concept covered by @ document @ analysis and lsa to reduce @ potentially redundant set of itemsets to a compact set of uncorrelated concept @ @ summarizer selects @ sentence @ cover @ latent concept @ minimal redundancy @ @ tested @ summarization algorithm on @ multilingual and english-language benchmark document collection @ @ proposed approach performed significantly better @ @ itemset and lsa-based summarizers and better @ @ of @ @ state-of-the-art approach @ association @ computing machinery @ 
1026,Extracting temporal patterns from large-scale text corpus,"Knowledge, in practice, is time-variant and many relations are only valid for a certain period of time. This phenomenon highlights the importance of designing temporal patterns, i.e., indicating phrases and their temporal meanings, for temporal knowledge harvesting. However, pattern design is extremely laborious and time consuming even for a single relation. Therefore, in this work, we study the problem of temporal pattern extraction by automatically analysing a large-scale text corpus with a small number of seed temporal facts. The problem is challenging considering the ambiguous nature of natural language and the huge amount of documents we need to analyse in order to obtain highly representative temporal patterns. To this end, we introduce various techniques, including corpus annotation, pattern generation, scoring and clustering, to reduce ambiguity in the text corpus and improve both accuracy and coverage of the extracted patterns. We conduct extensive experiments on real world datasets and the experimental results verify the effectiveness of our proposals. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,knowledge in practice is time-variant and many relation @ only valid @ a certain period of time @ @ phenomenon highlight @ importance of designing temporal pattern i @ e @ indicating phrase and @ temporal meaning @ temporal knowledge harvesting @ however pattern design is extremely laborious and time consuming even @ a single relation @ therefore in @ work @ study @ problem of temporal pattern extraction by automatically analysing a large-scale text corpus @ a small number of seed temporal fact @ @ problem is challenging considering @ ambiguous nature of natural language and @ huge amount of document @ need to analyse in order to obtain highly representative temporal pattern @ to @ end @ introduce various technique including corpus annotation pattern generation scoring and clustering to reduce ambiguity in @ text corpus and improve @ accuracy and coverage of @ extracted pattern @ @ conduct extensive experiment on real world datasets and @ experimental @ verify @ effectiveness of @ proposal @ @ nature switzerland ag @ 
1027,Analysis and prediction about the relationship of foreign exchange market sentiment and exchange rate trend,"This paper aims at finding the relationship between the market sentiment and the market trend in the foreign exchange market, and predicting the future trend of the market in a specific time period. We analyze the market sentiment through the broadcast news, and use the polynomial naïve byes model to classify the sentiment of the news. We set several time windows, and use time series analysis to predict the future market trend within the time window. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,0,@ @ aim at finding @ relationship @ @ market sentiment and @ market trend in @ foreign exchange market and predicting @ future trend of @ market in a specific time period @ @ analyze @ market sentiment @ @ broadcast news and use @ polynomial naïve bye model to classify @ sentiment of @ news @ @ set several time window and use time series analysis to predict @ future market trend within @ time window @ @ nature switzerland ag @ 
1028,Sentiment Analysis of Social Media Data Using Bayesian Regularization ANN (BRANN) Architecture,"Of late, big data and big data analytics have fund applications in diverse fields. Social media and allied applications are one such domain for research, where artificial intelligence has shown unprecedented impact. In this paper, a mechanism has been proposed which can classify text data into classes of different sentiments. Data in the form of tweets have been used in this case. Pre-processing of raw data has been done prior to using it to train a neural network. A neural network is then trained using the categories of the data which are tweets that correspond to happy, neutral and sad moods of the Twitter users. The Bayesian regularization (BR) algorithm has been used for training the artificial neural network. It has been observed that this proposed technique achieves an accuracy of 98%. The mean square error is a mere 2% (approx). © Springer Nature Singapore Pte Ltd. 2019.",2019,Advances in Intelligent Systems and Computing,0,of late big data and big data analytics @ fund application in diverse field @ social medium and allied application @ @ @ domain @ research @ artificial intelligence ha @ unprecedented impact @ in @ @ a mechanism ha @ proposed @ @ classify text data @ class of different sentiment @ data in @ form of tweet @ @ used in @ case @ pre-processing of raw data ha @ done prior to @ @ to train a neural network @ a neural network is @ trained @ @ category of @ data @ @ tweet @ correspond to happy neutral and sad mood of @ twitter user @ @ bayesian regularization @ br @ algorithm ha @ used @ training @ artificial neural network @ @ ha @ observed @ @ proposed technique achieves @ accuracy of @ @ mean square error is a mere @ approx @ @ @ nature singapore pte ltd @ @ 
1029,Summarization Using Corpus Training and Machine Learning,"Automatic summarization could be used for finding useful data from a given speech or text. Automatic summarization requires a machine learning approach to find the most suitable sentences to be included in the summary. Since summarization is a human process, it requires a human-like thinking approach from a machine. Summarization could be used for automatically finding out the main highlights of a given article or speech. First, we start with sentence extraction. Then, we use the corpus to find relevant patterns or features according to which we could rank a sentence. We train a Naive Bayes Classifier according to those features. Then, we perform tests on the Naive Bayes Classifier for finding scores of each sentence. The summary from the original text is produced using a certain compression rate according to which the machine selects the n-best sentences. A neural network model was also trained to compare results. The paper ends with an evaluation of the procedure’s accuracy by testing it against different test cases of various lengths and varying compression rates. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,1,automatic summarization could @ used @ finding useful data @ a given speech @ text @ automatic summarization requires a machine learning approach to find @ @ suitable sentence to @ included in @ summary @ since summarization is a human process @ requires a human-like thinking approach @ a machine @ summarization could @ used @ automatically finding @ @ main highlight of a given article @ speech @ first @ start @ sentence extraction @ @ @ use @ corpus to find relevant pattern @ feature according to @ @ could rank a sentence @ @ train a naive bayes classifier according to @ feature @ @ @ perform test on @ naive bayes classifier @ finding score of @ sentence @ @ summary @ @ original text is produced @ a certain compression rate according to @ @ machine selects @ n-best sentence @ a neural network model wa @ trained to compare @ @ @ @ end @ @ evaluation of @ procedure s accuracy by testing @ @ different test case of various length and varying compression rate @ @ nature singapore pte ltd @ 
1030,Sentiment score analysis and topic modelling for GST implementation in India,"Sentiment analysis has been widely used as a powerful tool in the era of predictive mining. However, combining sentiment analysis with social network analytics enhances the predictability power of the same. This research work attempts to provide the mining of the sentiments extracted from Twitter social application for analysis of the current trending topic in India, i.e. Goods and Services Tax (GST) and its impact on different sectors of Indian economy. This work is carried out to gain a bigger perspective of the current sentiment based on the live reactions and opinions of the people instead of smaller, restricted polls typically done by media corporations. A variety of classifiers are implemented to get the best possible accuracy on the dataset. A novel method is proposed to analyse the sentiment of the tweets and its impact on various sectors. Further, the sector trend is also analysed through the stock market analyses and the mapping between the two is made. Furthermore, the accuracy of stated approach is compared with state-of-the-art classifiers like SVM, naïve Bayes and random forest and the results demonstrate accuracy of stated approach outperformed all the other three techniques. Along with this, topic modelling was also done to get a picture of trending topics that are linked to GST. LDA and text ranking algorithms were applied to get connected topics. © Springer Nature Singapore Pte Ltd. 2019",2019,Advances in Intelligent Systems and Computing,1,sentiment analysis ha @ widely used a a powerful tool in @ era of predictive mining @ however combining sentiment analysis @ social network analytics enhances @ predictability power of @ @ @ @ research work attempt to provide @ mining of @ sentiment extracted @ twitter social application @ analysis of @ current trending topic in india i @ e @ good and service tax @ gst @ and @ impact on different sector of indian economy @ @ work is carried @ to gain a bigger perspective of @ current sentiment based on @ live reaction and opinion of @ people instead of smaller restricted poll typically done by medium corporation @ a variety of classifier @ implemented to get @ best possible accuracy on @ dataset @ a novel method is proposed to analyse @ sentiment of @ tweet and @ impact on various sector @ @ @ sector trend is @ analysed @ @ stock market analysis and @ mapping @ @ @ is made @ furthermore @ accuracy of stated approach is compared @ state-of-the-art classifier like svm naïve bayes and random forest and @ @ demonstrate accuracy of stated approach outperformed @ @ @ three technique @ along @ @ topic modelling wa @ done to get a picture of trending topic @ @ linked to gst @ lda and text ranking algorithm @ applied to get connected topic @ @ nature singapore pte ltd @ 
1031,Mining user-generated content in an online smoking cessation community to identify smoking status: A machine learning approach,"Online smoking cessation communities help hundreds of thousands of smokers quit smoking and stay abstinent each year. Content shared by users of such communities may contain important information that could enable more effective and personally tailored cessation treatment recommendations. This study demonstrates a novel approach to determine individuals' smoking status by applying machine learning techniques to classify user-generated content in an online cessation community. Study data were from BecomeAnEX.org, a large, online smoking cessation community. We extracted three types of novel features from a post: domain-specific features, author-based features, and thread-based features. These features helped to improve the smoking status identification (quit vs. not) performance by 9.7% compared to using only text features of a post's content. In other words, knowledge from domain experts, data regarding the post author's patterns of online engagement, and other community member reactions to the post can help to determine the focal post author's smoking status, over and above the actual content of a focal post. We demonstrated that machine learning methods can be applied to user-generated data from online cessation communities to validly and reliably discern important user characteristics, which could aid decision support on intervention tailoring. © 2018 Elsevier B.V.",2019,Decision Support Systems,6,online smoking cessation community help hundred of thousand of smoker quit smoking and stay abstinent @ year @ content shared by user of @ community may contain important information @ could enable more effective and personally tailored cessation treatment recommendation @ @ study demonstrates a novel approach to determine individual @ smoking status by applying machine learning technique to classify user-generated content in @ online cessation community @ study data @ @ becomeanex @ org a @ online smoking cessation community @ @ extracted three type of novel feature @ a post @ domain-specific feature author-based feature and thread-based feature @ @ feature helped to improve @ smoking status identification @ quit v @ not @ performance by @ compared to @ only text feature of a post @ s content @ in @ word knowledge @ domain expert data regarding @ post author @ s pattern of online engagement and @ community member reaction to @ post @ help to determine @ focal post author @ s smoking status @ and @ @ actual content of a focal post @ @ demonstrated @ machine learning method @ @ applied to user-generated data @ online cessation community to validly and reliably discern important user characteristic @ could aid decision support on intervention tailoring @ @ b @ v @ 
1032,An efficient sentiment mining approach on social media networks,"In today’s competitive environment, there is an essential need to collect and analyze data from social media, news, and other data streams that concern processing of huge amounts of data. A large number of posts, news, and blogs include opinions about product, service, and different issues. To accomplish an upper advantage, it is regularly important to listen and comprehend what individuals are saying in regard to contenders’ item, benefit, and distinctive issues. We proposed a sentiment mining technique for social media analytics to identify influential opinions. Our aim to mine and to compress every one of the people surveys of an item as well as the polarity of subjective topics which isn’t determine combined with opinion mining previously. Proposed task is performed in three steps: Firstly, mining item includes that have been remarked on by clients; secondly, recognize the supposition sentences in each survey and choosing whether every opinion sentence positive or negative or neutral; and finally, we summarize the results based on real datasets. © Springer Nature Singapore Pte Ltd. 2019.",2019,Advances in Intelligent Systems and Computing,0,in today s competitive environment @ is @ essential need to collect and analyze data @ social medium news and @ data stream @ concern processing of huge amount of data @ a @ number of post news and blog include opinion @ product service and different issue @ to accomplish @ upper advantage @ is regularly important to listen and comprehend @ individual @ saying in regard to contender item benefit and distinctive issue @ @ proposed a sentiment mining technique @ social medium analytics to identify influential opinion @ @ aim to mine and to compress every @ of @ people survey of @ item a well a @ polarity of subjective topic @ @ t determine combined @ opinion mining @ @ proposed task is performed in three step @ firstly mining item includes @ @ @ remarked on by client @ secondly recognize @ supposition sentence in @ survey and choosing whether every opinion sentence positive @ negative @ neutral @ and finally @ summarize @ @ based on real datasets @ @ nature singapore pte ltd @ @ 
1033,Survey on sentiment analysis methods for reputation evaluation,"Sentiment Analysis gathered huge attention in recent years. In this field, sentiments are analyzed and aggregated from the text. There are certain relevant sub-areas in research. This survey mainly concentrates on aspect-level (product feature) sentiment analysis. The aspects of the products are the noun phrases of the sentences. It is necessary to identify the goal and aggregate sentiments on entities in order to find the aspects of the entities. The detailed overview of study is given in such a way that the incredible evolution was already made in finding the target corresponding to the sentiment. The recent solutions are based on the aspect detection and extraction. In a detailed study, a performance report and evaluation related to the data sets are mentioned. In a variety of existing methods, an attempt is made to use the shared data values to standardize the evaluation methodology. The future research is in the direction of sentiment analysis which mainly concentrates on aspect centric reputation of online products. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,2,sentiment analysis gathered huge attention in recent year @ in @ field sentiment @ analyzed and aggregated @ @ text @ @ @ certain relevant sub-areas in research @ @ survey mainly concentrate on aspect-level @ product feature @ sentiment analysis @ @ aspect of @ product @ @ noun phrase of @ sentence @ @ is necessary to identify @ goal and aggregate sentiment on entity in order to find @ aspect of @ entity @ @ detailed overview of study is given in @ a way @ @ incredible evolution wa already made in finding @ target corresponding to @ sentiment @ @ recent solution @ based on @ aspect detection and extraction @ in a detailed study a performance report and evaluation related to @ data set @ mentioned @ in a variety of existing method @ attempt is made to use @ shared data value to standardize @ evaluation methodology @ @ future research is in @ direction of sentiment analysis @ mainly concentrate on aspect centric reputation of online product @ @ nature singapore pte ltd @ 
1034,Sentiment score analysis for opinion mining,"Sentiment Analysis has been widely used as a powerful tool in the era of predictive mining. However, combining sentiment analysis with social network analytics enhances the predictability power of the same. This research work attempts to provide the mining of the sentiments extracted from Twitter Social App for analysis of the current trending topic in India, i.e., Goods and Services Tax (GST) and its impact on different sectors of Indian economy. This work is carried out to gain a bigger perspective of the current sentiment based on the live reactions and opinions of the people instead of smaller, restricted polls typically done by media corporations. A variety of classifiers are implemented to get the best possible accuracy on the dataset. A novel method is proposed to analyze the sentiment of the tweets and its impact on various sectors. Further the sector trend is also analyzed through the stock market analyses and the mapping between the two is made. Furthermore, the accuracy of stated approach is compared with state of art classifiers like SVM, Naïve Bayes, and Random forest and the results demonstrate accuracy of stated approach outperformed all the other three techniques. Also, a detailed analysis is presented in this manuscript regarding the effect of GST along with time series analysis followed by gender-wise analysis. © Springer Nature Singapore Pte Ltd 2019.",2019,Advances in Intelligent Systems and Computing,0,sentiment analysis ha @ widely used a a powerful tool in @ era of predictive mining @ however combining sentiment analysis @ social network analytics enhances @ predictability power of @ @ @ @ research work attempt to provide @ mining of @ sentiment extracted @ twitter social app @ analysis of @ current trending topic in india i @ e @ good and service tax @ gst @ and @ impact on different sector of indian economy @ @ work is carried @ to gain a bigger perspective of @ current sentiment based on @ live reaction and opinion of @ people instead of smaller restricted poll typically done by medium corporation @ a variety of classifier @ implemented to get @ best possible accuracy on @ dataset @ a novel method is proposed to analyze @ sentiment of @ tweet and @ impact on various sector @ @ @ sector trend is @ analyzed @ @ stock market analysis and @ mapping @ @ @ is made @ furthermore @ accuracy of stated approach is compared @ state of art classifier like svm naïve bayes and random forest and @ @ demonstrate accuracy of stated approach outperformed @ @ @ three technique @ @ a detailed analysis is presented in @ manuscript regarding @ effect of gst along @ time series analysis followed by gender-wise analysis @ @ nature singapore pte ltd @ 
1035,An evolutionary algorithm-based text categorization technique,"In general, most of the organizations generate unstructured data from which extraction of meaningful information becomes a difficult task. Preprocessing of unstructured data before mining helps to improve the efficiency of the mining algorithms. In this paper, text data is initially preprocessed using tokenization, stop word removal, and stemming operations and a bag-of-words is identified to characterize the text dataset. Next, improved strength pareto evolutionary algorithm-based genetic algorithm is applied to determine the more compact set of informative words for clustering of text documents efficiently. It is a bi-objective genetic algorithm used to approximate the pareto-optimal front exploring the search space for optimal solution. The external clustering index and number of words described in the documents are considered as two objective functions of the algorithm, and based on these functions chromosomes in the population are evaluated and the best chromosome in non-dominated pareto front of final population gives the optimal set of words sufficient for categorizartion of text dataset. © 2019, Springer Nature Singapore Pte Ltd.",2019,Advances in Intelligent Systems and Computing,0,in general @ of @ organization generate unstructured data @ @ extraction of meaningful information becomes a difficult task @ preprocessing of unstructured data @ mining help to improve @ efficiency of @ mining algorithm @ in @ @ text data is initially preprocessed @ tokenization stop word removal and stemming operation and a bag-of-words is identified to characterize @ text dataset @ next improved strength pareto evolutionary algorithm-based genetic algorithm is applied to determine @ more compact set of informative word @ clustering of text document efficiently @ @ is a bi-objective genetic algorithm used to approximate @ pareto-optimal front exploring @ search space @ optimal solution @ @ external clustering index and number of word described in @ document @ considered a @ objective function of @ algorithm and based on @ function chromosome in @ population @ evaluated and @ best chromosome in non-dominated pareto front of final population give @ optimal set of word sufficient @ categorizartion of text dataset @ @ nature singapore pte ltd @ 
1036,Abstractive text summarization using LSTM-CNN based deep learning,"Abstractive Text Summarization (ATS), which is the task of constructing summary sentences by merging facts from different source sentences and condensing them into a shorter representation while preserving information content and overall meaning. It is very difficult and time consuming for human beings to manually summarize large documents of text. In this paper, we propose an LSTM-CNN based ATS framework (ATSDL) that can construct new sentences by exploring more fine-grained fragments than sentences, namely, semantic phrases. Different from existing abstraction based approaches, ATSDL is composed of two main stages, the first of which extracts phrases from source sentences and the second generates text summaries using deep learning. Experimental results on the datasets CNN and DailyMail show that our ATSDL framework outperforms the state-of-the-art models in terms of both semantics and syntactic structure, and achieves competitive results on manual linguistic quality evaluation. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2019,Multimedia Tools and Applications,52,abstractive text summarization @ at @ @ is @ task of constructing summary sentence by merging fact @ different source sentence and condensing @ @ a shorter representation @ preserving information content and overall meaning @ @ is @ difficult and time consuming @ human @ to manually summarize @ document of text @ in @ @ @ propose @ lstm-cnn based at framework @ atsdl @ @ @ construct @ sentence by exploring more fine-grained fragment @ sentence namely semantic phrase @ different @ existing abstraction based approach atsdl is composed of @ main stage @ first of @ extract phrase @ source sentence and @ second generates text summary @ deep learning @ experimental @ on @ datasets cnn and dailymail @ @ @ atsdl framework outperforms @ state-of-the-art model in term of @ semantics and syntactic structure and achieves competitive @ on manual linguistic quality evaluation @ @ science @ medium llc part of @ nature @ 
1037,Identification of semantic patterns in full-text documents using neural network methods,"Processing and text mining are becoming increasingly possible thanks to the development of computer technology, as well as the development of artificial intelligence (machine learning). This article describes approaches to the analysis of texts in natural language using methods of morphological, syntactic and semantic analysis. Morphological and syntactic analysis of the text is carried out using the Pullenti system, which allows not only to normalize words, but also to distinguish named entities, their characteristics, and relationships between them. As a result, a semantic network of related named entities is built, such as people, positions, geographical names, business associations, documents, education, dates, etc. The word2vec technology is used to identify semantic patterns in the text based on the joint occurrence of terms. The possibility of joint use of the described technologies is being considered. Copyright © 2019 for this paper by its authors.",2019,CEUR Workshop Proceedings,1,processing and text mining @ becoming increasingly possible thanks to @ development of computer technology a well a @ development of artificial intelligence @ machine learning @ @ @ article describes approach to @ analysis of text in natural language @ method of morphological syntactic and semantic analysis @ morphological and syntactic analysis of @ text is carried @ @ @ pullenti system @ allows not only to normalize word @ @ to distinguish named entity @ characteristic and relationship @ @ @ a a @ a semantic network of related named entity is built @ a people position geographical name @ association document education date etc @ @ word vec technology is used to identify semantic pattern in @ text based on @ joint occurrence of term @ @ possibility of joint use of @ described technology is @ considered @ @ @ @ @ by @ author @ 
1038,Prioritized Named Entity Driven LDA for Document Clustering,"Topic modeling methods like LSI, pLSI, and LDA have been widely studied in text mining domain for various applications like document representation, document clustering/classification, information retrieval, etc. However, such unsupervised methods are effective over corpus with well separable topics. In real-world applications, topics might be of highly overlapping in nature. For example, a news corpus of different terror attacks has highly overlapping keywords across reporting of different terror events. In this paper, we propose a variant of LDA, named as Prioritized Named Entity driven LDA (PNE-LDA), which can address the issue of overlapping topics by prioritizing named entities related to the topics. From various experimental setups, it is observed that the proposed method outperforms its counterparts in entity driven overlapping topics. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,topic modeling method like lsi plsi and lda @ @ widely studied in text mining domain @ various application like document representation document clustering classification information retrieval etc @ however @ unsupervised method @ effective @ corpus @ well separable topic @ in real-world application topic might @ of highly overlapping in nature @ @ example a news corpus of different terror attack ha highly overlapping keywords across reporting of different terror event @ in @ @ @ propose a variant of lda named a prioritized named entity driven lda @ pne-lda @ @ @ address @ issue of overlapping topic by prioritizing named entity related to @ topic @ @ various experimental setup @ is observed @ @ proposed method outperforms @ counterpart in entity driven overlapping topic @ @ nature switzerland ag @ 
1039,A recommender system of medical reports leveraging cognitive computing and frame semantics,"During the last decades, a huge amount of data have been collected in clinical databases in the form of medical reports, laboratory results, treatment plans, etc., representing patients health status. Hence, digital information available for patient-oriented decision making has increased drastically but it is often not mined and analyzed in depth since: (i) medical documents are often unstructured and therefore difficult to analyze automatically, (ii) doctors traditionally rely on their experience to recognize an illness, give a diagnosis, and prescribe medications. However doctors experience can be limited by the cases they are treated so far and medication errors can occur frequently. In addition, it is generally hard and time-consuming inferring information for comparing unstructured data and evaluating similarities between heterogeneous resources. Technologies as Data Mining, Natural Language Processing, and Machine Learning can provide possibilities to explore and exploit potential knowledge from diagnosis history records and help doctors to prescribe medication correctly to decrease medication error effectively. In this paper, we design and implement a medical recommender system that is able to cluster a collection of medical reports on features detected by IBM Watson and Framester, two emerging tools from, respectively, Cognitive Computing and Frame Semantics, and then, giving a medical report from a specific patient as input, to recommend similar other medical reports from patients who had analogues symptoms. Experiments and results have proved the quality of the resulting clustering and recommendations, and the key role that these innovative services can play on the biomedical sector. The proposed system is able to classify new medical cases thus supporting physicians to take more correct and reliable actions about specific diagnosis and cares. © 2019, Springer International Publishing AG, part of Springer Nature.",2019,Intelligent Systems Reference Library,12,@ @ last decade a huge amount of data @ @ collected in clinical database in @ form of medical report laboratory @ treatment plan etc @ representing patient health status @ hence digital information available @ patient-oriented decision making ha increased drastically @ @ is often not mined and analyzed in depth since @ @ i @ medical document @ often unstructured and therefore difficult to analyze automatically @ ii @ doctor traditionally rely on @ experience to recognize @ illness give a diagnosis and prescribe medication @ however doctor experience @ @ limited by @ case @ @ treated @ far and medication error @ occur frequently @ in addition @ is generally hard and time-consuming inferring information @ comparing unstructured data and evaluating similarity @ heterogeneous resource @ technology a data mining natural language processing and machine learning @ provide possibility to explore and exploit potential knowledge @ diagnosis history record and help doctor to prescribe medication correctly to decrease medication error effectively @ in @ @ @ design and implement a medical recommender system @ is able to cluster a collection of medical report on feature detected by ibm watson and framester @ emerging tool @ respectively cognitive computing and frame semantics and @ giving a medical report @ a specific patient a input to recommend similar @ medical report @ patient @ @ analogue symptom @ experiment and @ @ proved @ quality of @ resulting clustering and recommendation and @ key role @ @ innovative service @ play on @ biomedical sector @ @ proposed system is able to classify @ medical case thus supporting physician to take more correct and reliable action @ specific diagnosis and care @ @ international publishing ag part of @ nature @ 
1040,Improved term weighting factors for keyword extraction in hierarchical category structure and Thai text classification,"Keyword extraction of complex hierarchical categories becomes a challenge in text mining since commonly used classification for flat categories results in low accuracy. This paper presents a method to improve keyword extraction from hierarchical categories considering terms occurred in category from a hierarchy as additional factors in term-weighting. The method is an enhancement of a basic TF-IDF calculation; thus, it can comfortably be used for keyword extraction and classification. By taking term frequency and inverse document frequency of categories hierarchically related to a focused category, we can determine how important terms are in their family categories. In this work, hierarchy relations used in calculation are sub-categories, supercategories and sibling-categories. From experiment results, we found that the proposed method gained higher accuracy for about 40% from a baseline in a classification task. © Springer Nature Switzerland AG 2019.",2019,Advances in Intelligent Systems and Computing,0,keyword extraction of complex hierarchical category becomes a challenge in text mining since commonly used classification @ flat category @ in low accuracy @ @ @ @ a method to improve keyword extraction @ hierarchical category considering term occurred in category @ a hierarchy a additional factor in term-weighting @ @ method is @ enhancement of a basic tf-idf calculation @ thus @ @ comfortably @ used @ keyword extraction and classification @ by taking term frequency and inverse document frequency of category hierarchically related to a focused category @ @ determine @ important term @ in @ family category @ in @ work hierarchy relation used in calculation @ sub-categories supercategories and sibling-categories @ @ experiment @ @ found @ @ proposed method gained higher accuracy @ @ @ a baseline in a classification task @ @ nature switzerland ag @ 
1042,PreMedOnto: A Computer Assisted Ontology for Precision Medicine,"This paper proposes an ontology learning framework that combines text mining, information extraction and retrieval. The proposed model takes advantage of existing structured knowledge by reusing terms and concepts from other ontologies. We further apply the methodology to create a detailed ontology for the emerging precision medicine (PM) domain by collecting a corpus of relevant articles and mapping its frequent terms to existing concepts. The resulting ontology consists of 543 annotated classes. The ontology was also tested for effectiveness by applying two evaluation frameworks to validate its design and quality. The results demonstrate that the ontology learning system is able to capture and represent the semantics of the PM domain with high precision and significance. Moreover, the computer-assisted construction process reduced dependency on expert knowledge. The developed PreMedOnto ontology could be further used to enhance the potentials of other NLP applications in the PM domain. © 2019, Springer Nature Switzerland AG.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ proposes @ ontology learning framework @ combine text mining information extraction and retrieval @ @ proposed model take advantage of existing structured knowledge by reusing term and concept @ @ ontology @ @ @ apply @ methodology to create a detailed ontology @ @ emerging precision medicine @ pm @ domain by collecting a corpus of relevant article and mapping @ frequent term to existing concept @ @ resulting ontology consists of annotated class @ @ ontology wa @ tested @ effectiveness by applying @ evaluation framework to validate @ design and quality @ @ @ demonstrate @ @ ontology learning system is able to capture and represent @ semantics of @ pm domain @ high precision and significance @ moreover @ computer-assisted construction process reduced dependency on expert knowledge @ @ developed premedonto ontology could @ @ used to enhance @ potential of @ nlp application in @ pm domain @ @ nature switzerland ag @ 
1043,Text feature extraction and selection based on attention mechanism,"Selecting features that represent a particular corpus is important to the success of many machine learning and text mining applications. However, the previous attention-based work only focused on feature augmentation in the lexical level, lacking the exploration of feature enhancement in the sentence level. In this paper, we exploit a novel feature extraction and selection model for information retrieval, denoted by Dynamic Feature Generation Network (DFGN). In sentence dimension, features are firstly extracted by a variety of different attention mechanisms, then dynamically filtered by thresholds automatically learned. Different kinds of characteristics are distilled according to specific tasks, enhancing the practicability and robustness of the model. DFGN relies solely on the text itself, requires no external feature engineering. Our approach outperforms previous work on multiple well-known answer selection datasets. Through the analysis of the experiments, we prove that DFGN provides excellent retrieval and interpretative abilities. © Springer Nature Switzerland AG 2019.",2019,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,selecting feature @ represent a particular corpus is important to @ success of many machine learning and text mining application @ however @ previous attention-based work only focused on feature augmentation in @ lexical level lacking @ exploration of feature enhancement in @ sentence level @ in @ @ @ exploit a novel feature extraction and selection model @ information retrieval denoted by dynamic feature generation network @ dfgn @ @ in sentence dimension feature @ firstly extracted by a variety of different attention mechanism @ dynamically filtered by threshold automatically learned @ different kind of characteristic @ distilled according to specific task enhancing @ practicability and robustness of @ model @ dfgn relies solely on @ text @ requires no external feature engineering @ @ approach outperforms previous work on multiple well-known answer selection datasets @ @ @ analysis of @ experiment @ prove @ dfgn provides excellent retrieval and interpretative ability @ @ nature switzerland ag @ 
1044,A Multi-domain Named Entity Recognition Method Based on Part-of-Speech Attention Mechanism,"Named entity recognition is an important and basic work in text mining. To overcome the shortcomings of existing multi-domain named entity recognition methods, a multi-domain named entity recognition method based on the part-of-speech attention mechanism, called BiLSTM-ATTENTION-CRF, was proposed in this paper. The domain dictionary was constructed to represent multi-domain semantic information and the BiLSTM network was used to capture the grammatical and syntactic features, as well as multi-domain semantic features in context information. A part-of-speech attention mechanism was designed to obtain the contribution weight of part-of-speech for entity recognition. Finally, a group of experiments were performed on the multi-domain dataset to compare various fusion strategies of multi-level entity information. The experimental results show that BiLSTM-ATTENTION-CRF has a high precision and recall rate, and can effectively recognizes the multi-domain named entities. © 2019, Springer Nature Singapore Pte Ltd.",2019,Communications in Computer and Information Science,1,named entity recognition is @ important and basic work in text mining @ to overcome @ shortcoming of existing multi-domain named entity recognition method a multi-domain named entity recognition method based on @ part-of-speech attention mechanism called bilstm-attention-crf wa proposed in @ @ @ @ domain dictionary wa constructed to represent multi-domain semantic information and @ bilstm network wa used to capture @ grammatical and syntactic feature a well a multi-domain semantic feature in context information @ a part-of-speech attention mechanism wa designed to obtain @ contribution weight of part-of-speech @ entity recognition @ finally a group of experiment @ performed on @ multi-domain dataset to compare various fusion strategy of multi-level entity information @ @ experimental @ @ @ bilstm-attention-crf ha a high precision and recall rate and @ effectively recognizes @ multi-domain named entity @ @ nature singapore pte ltd @ 
1045,NaCTeM-UoM @ CL-SciSumm 2019,"This paper introduces the National Centre for Text Mining - University of Manchester systems submitted in CL-SciSumm 2019 Shared Task at BIRNDL 2019 Workshop. CL-SciSumm shared tasks focus on the identification of cited passages across scientific publications, and the subsequent summarisation of scientific articles based on their cited extracts. More specifically Task 1A is directed at the identification of cited text spans in the reference paper, based on the provided citation passages, while Task 1B concerns the classification of the citation passages based on their function in the text. For Task 2, the identified cited text spans are used in order to generate an informed summary for the reference paper. We participated in both tasks described above. We looked into supervised and semi-supervised approaches and explored the potential of adapting bidirectional transformers for each task.We further formalised Task 1A as a similarity ranking problem and implemented bilateral multi-perspective matching for natural language sentences. © 2019 CEUR-WS. All rights reserved.",2019,CEUR Workshop Proceedings,0,@ @ introduces @ national centre @ text mining university of manchester system submitted in cl-scisumm shared task at birndl workshop @ cl-scisumm shared task focus on @ identification of cited passage across scientific publication and @ subsequent summarisation of scientific article based on @ cited extract @ more specifically task a is directed at @ identification of cited text span in @ reference @ based on @ provided citation passage @ task b concern @ classification of @ citation passage based on @ function in @ text @ @ task @ identified cited text span @ used in order to generate @ informed summary @ @ reference @ @ @ participated in @ task described @ @ @ looked @ supervised and semi-supervised approach and explored @ potential of adapting bidirectional transformer @ @ task @ @ @ formalised task a a a similarity ranking problem and implemented bilateral multi-perspective matching @ natural language sentence @ ceur-ws @ @ right reserved @ 
1046,Application of sentiment lexicons on movies transcripts to detect violence in videos,"In the modern era of technological development, the emergence of Web 2.0 applications, related to social media, the dissemination of opinions, feelings, and participation in discussions on various issues have become very easy, which have led to a boom in text mining and natural language processing research. YouTube is one of the most popular social sites for video sharing. This may contain different types of unwanted content such as violence, which is the cause of many social problems, especially among children like aggression and bullying at home, in school and in public places. The research work reports performance of two different sentiment lexicons when they were applied on video transcripts to detect violence in YouTube videos. The automation of process to detect violence in videos can be helpful for censor boards that can use the technology to restrict violent video for a certain age group or can fully block entire video regardless of age. The models were built using the existing sentiment lexicons. The dataset consists of 100 English video transcripts collected from the web and was annotated manually as violent and non-violent. Various experiments were performed on the dataset using English SentiWordNet (ESWN) and Vader Package with different text preprocessing settings. The Vader package outperformed the ESWN by providing 75% accuracy. ESWN results for all POS tagging with 66% accuracy were better than its result for adjectives POS tagging with 58% accuracy. © 2013 The Science and Information (SAI) Organization.",2019,International Journal of Advanced Computer Science and Applications,2,in @ modern era of technological development @ emergence of web @ application related to social medium @ dissemination of opinion feeling and participation in discussion on various issue @ become @ easy @ @ led to a boom in text mining and natural language processing research @ youtube is @ of @ @ popular social site @ video sharing @ @ may contain different type of unwanted content @ a violence @ is @ cause of many social problem especially among child like aggression and bullying at home in school and in public place @ @ research work report performance of @ different sentiment lexicon @ @ @ applied on video transcript to detect violence in youtube video @ @ automation of process to detect violence in video @ @ helpful @ censor board @ @ use @ technology to restrict violent video @ a certain age group @ @ fully block entire video regardless of age @ @ model @ built @ @ existing sentiment lexicon @ @ dataset consists of english video transcript collected @ @ web and wa annotated manually a violent and non-violent @ various experiment @ performed on @ dataset @ english sentiwordnet @ eswn @ and vader package @ different text preprocessing setting @ @ vader package outperformed @ eswn by providing accuracy @ eswn @ @ @ po tagging @ accuracy @ better @ @ @ @ adjective po tagging @ accuracy @ @ science and information @ sai @ organization @ 
1047,The analytics of product-design requirements using dynamic internet data: application to Chinese smartphone market,"To accommodate the diverse users demands for consumer products, enterprises need to design and develop different lines of products according to different groups of users. Dynamic internet data, including product reviews, user attributes, and product configurations, are utilised to model users' stochastic product choice behaviours and mine the product design requirements of features, performance levels, and quantity. First, the web crawler is applied to collect internet data, and then the data are structured and the demand information is retrieved. Second, a product choice model is employed to capture the heterogeneity and correlation of user demands on product features. In particular, users' implicit requirements in terms of product function and performance are elicited from the text mining of product reviews. Third, incorporating various user requirements mined from dynamic internet data, graph theory analysis is introduced into design generation, product improvement, and market analysis. A case study on Chinese smartphones is presented, where the results show that the proposed method is practical and suitable for product-design analysis using the large volume of dynamic internet data. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2019,International Journal of Production Research,3,to accommodate @ diverse user demand @ consumer product enterprise need to design and develop different line of product according to different group of user @ dynamic internet data including product review user attribute and product configuration @ utilised to model user @ stochastic product choice behaviour and mine @ product design requirement of feature performance level and quantity @ first @ web crawler is applied to collect internet data and @ @ data @ structured and @ demand information is retrieved @ second a product choice model is employed to capture @ heterogeneity and correlation of user demand on product feature @ in particular user @ implicit requirement in term of product function and performance @ elicited @ @ text mining of product review @ third incorporating various user requirement mined @ dynamic internet data graph theory analysis is introduced @ design generation product improvement and market analysis @ a case study on chinese smartphones is presented @ @ @ @ @ @ proposed method is practical and suitable @ product-design analysis @ @ @ volume of dynamic internet data @ informa uk limited trading a taylor francis group @ 
1050,Automated narrative extraction from administrative records,"The U.S. Probation and Pretrial Services Office staff produce billions of pages of information on defendants’ and offenders’ profile and conduct. While it is critical for probation officers and district chiefs to have up-to-date knowledge on their clients to better assist and reduce risk of recidivism, the data are often stored in narrative texts in multiple large documents. As a result, these records remain mostly out of reach without the use of painstaking manual review. This paper describes an analytic prototype developed to automatically acquire structured information from natural language text in probation office documents through the application of PDF content extraction, text mining, and language analytics. Since serious mental illness is very prevalent in the U.S. corrections system, the first phase of the project focused on extracting information and constructing timelines from narrative text regarding the defendants’ mental health conditions, substance use and treatment history. Automated narrative extraction and the construction of an event timeline for defendants’ mental and emotional health history have allowed the probation office to have a better understanding of their client population and to perform analyses that were previously unavailable to the organization. This technical approach can be applied across organizations, legal institutions, clinical administrations, and government agencies that maintain large amounts of information in the form of free text narratives. Copyright © 2019 for this paper by The MITRE Corporation.",2019,CEUR Workshop Proceedings,0,@ u @ s @ probation and pretrial service office staff produce billion of page of information on defendant and offender profile and conduct @ @ @ is critical @ probation officer and district chief to @ up-to-date knowledge on @ client to better assist and reduce risk of recidivism @ data @ often stored in narrative text in multiple @ document @ a a @ @ record remain mostly @ of reach without @ use of painstaking manual review @ @ @ describes @ analytic prototype developed to automatically acquire structured information @ natural language text in probation office document @ @ application of pdf content extraction text mining and language analytics @ since serious mental illness is @ prevalent in @ u @ s @ correction system @ first phase of @ project focused on extracting information and constructing timeline @ narrative text regarding @ defendant mental health condition substance use and treatment history @ automated narrative extraction and @ construction of @ event timeline @ defendant mental and emotional health history @ allowed @ probation office to @ a better understanding of @ client population and to perform analysis @ @ @ unavailable to @ organization @ @ technical approach @ @ applied across organization legal institution clinical administration and government agency @ maintain @ amount of information in @ form of free text narrative @ @ @ @ @ by @ mitre corporation @ 
1051,Sentiment classification of customer’s reviews about automobiles in Roman Urdu,"Text mining is a broad field having sentiment mining as its important constituent in which we try to deduce the behavior of people towards a specific item, merchandise, politics, sports, social media comments, review sites, etc. Out of many issues in sentiment mining, analysis and classification, one major issue is that the reviews and comments can be in different languages, like English, Arabic, Urdu, etc. Handling each language according to its rules is a difficult task. A lot of research work has been done in English Language for sentiment analysis and classification but limited sentiment analysis work is being carried out on other regional languages, like Arabic, Urdu and Hindi. In this paper, Waikato Environment for Knowledge Analysis (WEKA) is used as a platform to execute different classification models for text classification of Roman Urdu text. Reviews dataset has been scrapped from different automobiles’ sites. These extracted Roman Urdu reviews, containing 1000 positive and 1000 negative reviews are then saved in WEKA attribute-relation file format (ARFF) as labeled examples. Training is done on 80% of this data and rest of it is used for testing purpose which is done using different models and results are analyzed in each case. The results show that Multinomial Naïve Bayes outperformed Bagging, Deep Neural Network, Decision Tree, Random Forest, AdaBoost, k-NN and SVM Classifiers in terms of more accuracy, precision, recall and F-measure. © 2019, Springer Nature Switzerland AG.",2019,Advances in Intelligent Systems and Computing,3,text mining is a broad field @ sentiment mining a @ important constituent in @ @ try to deduce @ behavior of people towards a specific item merchandise politics sport social medium comment review site etc @ @ of many issue in sentiment mining analysis and classification @ major issue is @ @ review and comment @ @ in different language like english arabic urdu etc @ handling @ language according to @ rule is a difficult task @ a lot of research work ha @ done in english language @ sentiment analysis and classification @ limited sentiment analysis work is @ carried @ on @ regional language like arabic urdu and hindi @ in @ @ waikato environment @ knowledge analysis @ weka @ is used a a platform to execute different classification model @ text classification of roman urdu text @ review dataset ha @ scrapped @ different automobile site @ @ extracted roman urdu review containing positive and negative review @ @ saved in weka attribute-relation file format @ arff @ a labeled example @ training is done on of @ data and rest of @ is used @ testing purpose @ is done @ different model and @ @ analyzed in @ case @ @ @ @ @ multinomial naïve bayes outperformed bagging deep neural network decision tree random forest adaboost k-nn and svm classifier in term of more accuracy precision recall and f-measure @ @ nature switzerland ag @ 
1053,Web page classification on news feeds using hybrid technique for extraction,"At the initial stage of web world, the number of websites hosted was handful, and from the users end, it was easy to maintain log file which consists of information like web pages URL or domain name, but as the number of web hosting increased gradually, it was found hard for the users to maintain such log files. Thus, the requirement exists which helped the users to search the information from the website easily which is now renowned as “Search Engines”; the only limitation found is that the users must be sound enough to give searching keywords in order to search relevant information, but in many cases, users have obtained irrelevant information from the web, and hence, looking into the current scenario of the Internet world, the number of websites has grown drastically holding various web pages within it. These web pages are observed to be published in structured or semi-structured manner which comprises of various multimedia contents, [(Nethra et al. in J Soft Comput 4:692–696, 2014) 1; (Kardan et al. in A novel approach for Keyword extraction in learning objects using text mining & WordNet. pp. 788–792, 2011) 2; (Menaka and Radha in Int J Adv Res Comput Sci Softw Eng 352:24–28, 2013) 3] so the chance to fetch wrong information also increased, and hence, there is a need to auto-categorize the web pages into some predetermined sections. The key point in this research is to recognize and allocate the news feeds into fixed sections of news like business, sports which enhance the reader’s accessibility towards relevant news by traversing appropriate category as per his/her choice, and this is done by adopting hybrid technique of URL analysis and content context analysis. The paper emphasis on a proposed model to perform classification on news feed related to various fields such as sports, health which starts with web crawling of URLs, scraping of news contents followed by the analysis carried out on account of generating keywords, weight calculation, and then at last identify the relevant category on the basis of contents fetched among various Indian news web portal like “The Times of India”, “Hindustan Times”, “The Guardian” [(Nethra et al. in J Soft Comput 4:692–696, 2014) 1; (Jena and Kamila in Int J Appl Innov Eng Manage 2, 2013) 4; (Ercan and Cicekli in Int J Inf Process Manage 43:1705–1714, 2007) 5]. © Springer Nature Singapore Pte Ltd. 2019.",2019,"Smart Innovation, Systems and Technologies",2,at @ initial stage of web world @ number of website hosted wa handful and @ @ user end @ wa easy to maintain log file @ consists of information like web page url @ domain name @ a @ number of web hosting increased gradually @ wa found hard @ @ user to maintain @ log file @ thus @ requirement exists @ helped @ user to search @ information @ @ website easily @ is now renowned a search engine @ @ only limitation found is @ @ user must @ sound enough to give searching keywords in order to search relevant information @ in many case user @ obtained irrelevant information @ @ web and hence looking @ @ current scenario of @ internet world @ number of website ha grown drastically holding various web page within @ @ @ web page @ observed to @ published in structured @ semi-structured manner @ comprises of various multimedia content @ nethra et al @ in j soft comput @ @ @ @ kardan et al @ in a novel approach @ keyword extraction in learning object @ text mining wordnet @ pp @ @ @ @ menaka and radha in int j adv @ comput sci softw eng @ @ @ @ chance to fetch wrong information @ increased and hence @ is a need to auto-categorize @ web page @ some predetermined section @ @ key point in @ research is to recognize and allocate @ news feed @ fixed section of news like @ sport @ enhance @ reader s accessibility towards relevant news by traversing appropriate category a per @ @ choice and @ is done by adopting hybrid technique of url analysis and content context analysis @ @ @ emphasis on a proposed model to perform classification on news feed related to various field @ a sport health @ start @ web crawling of url scraping of news content followed by @ analysis carried @ on account of generating keywords weight calculation and @ at last identify @ relevant category on @ basis of content fetched among various indian news web portal like @ time of india hindustan time @ guardian @ nethra et al @ in j soft comput @ @ @ @ jena and kamila in int j appl innov eng manage @ @ @ ercan and cicekli in int j inf process manage @ @ @ @ nature singapore pte ltd @ @ 
1056,Building the classical arabic named entity recognition corpus (Canercorpus),"The past decade has witnessed construction of the background information resources to overcome several challenges in text mining tasks. For non-English languages with poor knowledge sources such as Arabic, these challenges have become more salient especially for handling the natural language processing applications that require human annotation. In the Named Entity Recognition (NER) task, several researches have been introduced to address the complexity of Arabic in terms of morphological and syntactical variations. However, there are a small number of studies dealing with Classical Arabic (CA) that is the official language of Quran and Hadith. CA was also used for archiving the Islamic topics that contain a lot of useful information which could of great value if extracted. Therefore, in this paper, we introduce Classical Arabic Named Entity Recognition corpus as a new corpus of tagged data that can be useful for handling the issues in recognition of Arabic named entities. It is freely available and manual annotation by human experts, containing more than 7,000 Hadiths. Based on Islamic topics, we classify named entities into 20 types which include the specific-domain entities that have not been handled before such as Allah, Prophet, Paradise, Hell, and Religion. The differences between the standard and classical Arabic are described in details during this work. Moreover, the comprehensive statistical analysis is introduced to measure the factors that play important role in manual human annotation. © 2005 – ongoing JATIT & LLS.",2018,Journal of Theoretical and Applied Information Technology,0,@ past decade ha witnessed construction of @ background information resource to overcome several challenge in text mining task @ @ non-english language @ poor knowledge source @ a arabic @ challenge @ become more salient especially @ handling @ natural language processing application @ require human annotation @ in @ named entity recognition @ ner @ task several research @ @ introduced to address @ complexity of arabic in term of morphological and syntactical variation @ however @ @ a small number of study dealing @ classical arabic @ ca @ @ is @ official language of quran and hadith @ ca wa @ used @ archiving @ islamic topic @ contain a lot of useful information @ could of great value if extracted @ therefore in @ @ @ introduce classical arabic named entity recognition corpus a a @ corpus of tagged data @ @ @ useful @ handling @ issue in recognition of arabic named entity @ @ is freely available and manual annotation by human expert containing more @ hadith @ based on islamic topic @ classify named entity @ type @ include @ specific-domain entity @ @ not @ handled @ @ a allah prophet paradise hell and religion @ @ difference @ @ standard and classical arabic @ described in detail @ @ work @ moreover @ comprehensive statistical analysis is introduced to measure @ factor @ play important role in manual human annotation @ ongoing jatit lls @ 
1070,A new Persian text summarization approach based on natural language processing and graph similarity,"A significant amount of available information is stored in textual databases which contain a large collection of documents from different sources (such as news, articles, books, emails and web pages). The increasing visibility and importance of this class of information motivates us to work on having better automatic evaluation tools for textual resources. The automatic summarization of text is one of the ways to prevent the waste of users' time. The extractive text summarization consists of the extraction of the more important sentences with the purpose of shortening input text while maintaining the topics covered and the subjects discussed. In this paper, we have tried to improve the accuracy of the extracted summaries by combining natural language processing and text mining techniques. By modifying the mentioned algorithms and sentence scoring measures, accuracy is increased as compared to the previously used techniques. Part of speech tagging is used for calculating coefficient of words' importance. Using this approach will in turn help us with picking the more meaningful words and phrases that will result in better accuracy of the system. Graph similarity's methods are used to select sentences. Changing weight of the selected sentences in each step leads to solve the redundancy problem. Standard evaluation measures such as ""Precision"" and ""Recall"" are used to evaluate results based on a Persian corpus. © 2018 Iranian Research Institute for Scientific Information and Documentation. All Rights Reserved.",2018,Iranian Journal of Information Processing Management,1,a significant amount of available information is stored in textual database @ contain a @ collection of document @ different source @ @ a news article book email and web page @ @ @ increasing visibility and importance of @ class of information motivates u to work on @ better automatic evaluation tool @ textual resource @ @ automatic summarization of text is @ of @ way to prevent @ waste of user @ time @ @ extractive text summarization consists of @ extraction of @ more important sentence @ @ purpose of shortening input text @ maintaining @ topic covered and @ subject discussed @ in @ @ @ @ tried to improve @ accuracy of @ extracted summary by combining natural language processing and text mining technique @ by modifying @ mentioned algorithm and sentence scoring measure accuracy is increased a compared to @ @ used technique @ part of speech tagging is used @ calculating coefficient of word @ importance @ @ @ approach @ in turn help u @ picking @ more meaningful word and phrase @ @ @ in better accuracy of @ system @ graph similarity @ s method @ used to select sentence @ changing weight of @ selected sentence in @ step lead to solve @ redundancy problem @ standard evaluation measure @ a @ precision @ and @ recall @ @ used to evaluate @ based on a persian corpus @ iranian research institute @ scientific information and documentation @ @ right reserved @ 
1079,A survey of Arabic text classification models,"There is a huge content of Arabic text available over online that requires an organization of these texts. As result, here are many applications of natural languages processing (NLP) that concerns with text organization. One of the is text classification (TC). TC helps to make dealing with unorganized text. However, it is easier to classify them into suitable class or labels. This paper is a survey of Arabic text classification. Also, it presents comparison among different methods in the classification of Arabic texts, where Arabic text is represented a complex text due to its vocabularies. Arabic language is one of the richest languages in the world, where it has many linguistic bases. The researche in Arabic language processing is very few compared to English. As a result, these problems represent challenges in the classification, and organization of specific Arabic text. Text classification (TC) helps to access the most documents, or information that has already classified into specific classes, or categories to one or more classes or categories. In addition, classification of documents facilitate search engine to decrease the amount of document to, and then to become easier to search and matching with queries. Copyright © 2018 Institute of Advanced Engineering and Science. All rights reserved.",2018,International Journal of Electrical and Computer Engineering,5,@ is a huge content of arabic text available @ online @ requires @ organization of @ text @ a @ @ @ many application of natural language processing @ nlp @ @ concern @ text organization @ @ of @ is text classification @ tc @ @ tc help to make dealing @ unorganized text @ however @ is easier to classify @ @ suitable class @ label @ @ @ is a survey of arabic text classification @ @ @ @ comparison among different method in @ classification of arabic text @ arabic text is represented a complex text due to @ vocabulary @ arabic language is @ of @ richest language in @ world @ @ ha many linguistic base @ @ researche in arabic language processing is @ @ compared to english @ a a @ @ problem represent challenge in @ classification and organization of specific arabic text @ text classification @ tc @ help to access @ @ document @ information @ ha already classified @ specific class @ category to @ @ more class @ category @ in addition classification of document facilitate search engine to decrease @ amount of document to and @ to become easier to search and matching @ query @ @ institute of advanced engineering and science @ @ right reserved @ 
1080,Improving word embedding coverage in less-resourced languages through multi-linguality and cross-linguality: A case study with aspect-based sentiment analysis,"In the era of deep learning-based systems, efficient input representation is one of the primary requisites in solving various problems related to Natural Language Processing (NLP), data mining, text mining, and the like. Absence of adequate representation for an input introduces the problem of data sparsity, and it poses a great challenge to solve the underlying problem. The problem is more intensified with resource-poor languages due to the absence of a sufficiently large corpus required to train a word embedding model. In this work, we propose an effective method to improve the word embedding coverage in less-resourced languages by leveraging bilingual word embeddings learned from different corpora. We train and evaluate deep Long Short Term Memory (LSTM)-based architecture and show the effectiveness of the proposed approach for two aspect-level sentiment analysis tasks (i.e., aspect term extraction and sentiment classification). The neural network architecture is further assisted by hand-crafted features for prediction. We apply the proposed model in two experimental setups: multi-lingual and cross-lingual. Experimental results show the effectiveness of the proposed approach against the state-of-the-art methods. © 2018 Association for Computing Machinery.",2018,ACM Transactions on Asian and Low-Resource Language Information Processing,3,in @ era of deep learning-based system efficient input representation is @ of @ primary requisite in solving various problem related to natural language processing @ nlp @ data mining text mining and @ like @ absence of adequate representation @ @ input introduces @ problem of data sparsity and @ pose a great challenge to solve @ underlying problem @ @ problem is more intensified @ resource-poor language due to @ absence of a sufficiently @ corpus required to train a word embedding model @ in @ work @ propose @ effective method to improve @ word embedding coverage in less-resourced language by leveraging bilingual word embeddings learned @ different corpus @ @ train and evaluate deep long short term memory @ lstm @ based architecture and @ @ effectiveness of @ proposed approach @ @ aspect-level sentiment analysis task @ i @ e @ aspect term extraction and sentiment classification @ @ @ neural network architecture is @ assisted by hand-crafted feature @ prediction @ @ apply @ proposed model in @ experimental setup @ multi-lingual and cross-lingual @ experimental @ @ @ effectiveness of @ proposed approach @ @ state-of-the-art method @ association @ computing machinery @ 
1081,Hand collecting and coding versus data-driven methods in technical and professional communication research,"Background: Qualitative technical communication research often produces datasets that are too large to manage effectively with hand-coded approaches. Text-mining methods, used carefully, may uncover patterns and provide results for larger datasets that are more easily reproduced and scaled. Research questions: 1. To what degree can hand collection results be replicated by automated data collection? 2. To what degree can hand-coded results be replicated by machine coding? 3. What are the affordances and limitations of each method? Literature review: We introduce the stages of data collection and analysis that researchers typically discuss in the literature, and show how researchers in technical communication and other fields have discussed the affordances and limitations of hand collection and coding versus automated methods throughout each stage. Research methodology: We utilize an existing dataset that was hand-collected and hand-coded. We discuss the collection and coding processes, and demonstrate how they might be replicated with web scraping and machine coding. Results/discussion: We found that web scraping demonstrated an obvious advantage of automated data collection: Speed. Machine coding was able to provide comparable outputs to hand coding for certain types of data; for more nuanced and verbally complex data, machine coding was less useful and less reliable. Conclusions: Our findings highlight the importance of considering the context of a particular project when weighing the affordances and limitations of hand collecting and coding over automated approaches. Ultimately, a mixed-methods approach that relies on a combination of hand coding and automated coding should prove to be the most productive for current and future kinds of technical communication work, in which close attention to the nuances of language is critical, but in which processing large amounts of data would yield significant benefits as well. © 2018 IEEE.",2018,IEEE Transactions on Professional Communication,7,background @ qualitative technical communication research often produce datasets @ @ too @ to manage effectively @ hand-coded approach @ text-mining method used carefully may uncover pattern and provide @ @ larger datasets @ @ more easily reproduced and scaled @ research question @ @ to @ degree @ hand collection @ @ replicated by automated data collection @ @ to @ degree @ hand-coded @ @ replicated by machine coding @ @ @ @ @ affordances and limitation of @ method @ literature review @ @ introduce @ stage of data collection and analysis @ researcher typically discus in @ literature and @ @ researcher in technical communication and @ field @ discussed @ affordances and limitation of hand collection and coding versus automated method throughout @ stage @ research methodology @ @ utilize @ existing dataset @ wa hand-collected and hand-coded @ @ discus @ collection and coding process and demonstrate @ @ might @ replicated @ web scraping and machine coding @ @ discussion @ @ found @ web scraping demonstrated @ obvious advantage of automated data collection @ speed @ machine coding wa able to provide comparable output to hand coding @ certain type of data @ @ more nuanced and verbally complex data machine coding wa le useful and le reliable @ conclusion @ @ finding highlight @ importance of considering @ context of a particular project @ weighing @ affordances and limitation of hand collecting and coding @ automated approach @ ultimately a mixed-methods approach @ relies on a combination of hand coding and automated coding @ prove to @ @ @ productive @ current and future kind of technical communication work in @ close attention to @ nuance of language is critical @ in @ processing @ amount of data would yield significant benefit a well @ @ @ 
1083,Emotion detection from text and speech: a survey,"Emotion recognition has emerged as an important research area which may reveal some valuable input to a variety of purposes. People express their emotions directly or indirectly through their speech, facial expressions, gestures or writings. Many different sources of information, such as speech, text and visual can be used to analyze emotions. Nowadays, writings take many forms of social media posts, micro-blogs, news articles, etc., and the content of these posts can be useful resource for text mining to discover and unhide various aspects, including emotions. Extracting emotions behind these postings is an immense and complicated task. To tackle this problem, researchers from diverse fields are trying to find an efficient way to more precisely detect human emotions from various sources, including text and speech. In this sense, different word-based and sentence-based techniques, machine learning, natural language processing methods, etc., have been used to achieve better accuracy. Analyzing emotions can be helpful in many different domains. One such domain is human computer interaction. With the help of emotion recognition, computers can make better decisions to help users. With the increase in popularity of robotic research, emotion recognition will also help making human–robot interaction more natural. This survey covers existing emotion detection research efforts, emotion models, emotion datasets, emotion detection techniques, their features, limitations and some possible future directions. We focus on reviewing research efforts analyzing emotions based on text and speech. We investigated different feature sets that have been used in existing methodologies. We summarize basic achievements in the field and highlight possible extensions for better outcome. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature.",2018,Social Network Analysis and Mining,48,emotion recognition ha emerged a @ important research area @ may reveal some valuable input to a variety of purpose @ people express @ emotion directly @ indirectly @ @ speech facial expression gesture @ writing @ many different source of information @ a speech text and visual @ @ used to analyze emotion @ nowadays writing take many form of social medium post micro-blogs news article etc @ and @ content of @ post @ @ useful resource @ text mining to discover and unhide various aspect including emotion @ extracting emotion behind @ posting is @ immense and complicated task @ to tackle @ problem researcher @ diverse field @ trying to find @ efficient way to more precisely detect human emotion @ various source including text and speech @ in @ sense different word-based and sentence-based technique machine learning natural language processing method etc @ @ @ used to achieve better accuracy @ analyzing emotion @ @ helpful in many different domain @ @ @ domain is human computer interaction @ @ @ help of emotion recognition computer @ make better decision to help user @ @ @ increase in popularity of robotic research emotion recognition @ @ help making human robot interaction more natural @ @ survey cover existing emotion detection research effort emotion model emotion datasets emotion detection technique @ feature limitation and some possible future direction @ @ focus on reviewing research effort analyzing emotion based on text and speech @ @ investigated different feature set @ @ @ used in existing methodology @ @ summarize basic achievement in @ field and highlight possible extension @ better outcome @ springer-verlag gmbh austria part of @ nature @ 
1085,COVER: a linguistic resource combining common sense and lexicographic information,"Lexical resources are fundamental to tackle many tasks that are central to present and prospective research in Text Mining, Information Retrieval, and connected to Natural Language Processing. In this article we introduce COVER, a novel lexical resource, along with COVERAGE, the algorithm devised to build it. In order to describe concepts, COVER proposes a compact vectorial representation that combines the lexicographic precision characterizing BabelNet and the rich common-sense knowledge featuring ConceptNet. We propose COVER as a reliable and mature resource, that has been employed in as diverse tasks as conceptual categorization, keywords extraction, and conceptual similarity. The experimental assessment is performed on the last task: we report and discuss the obtained results, pointing out future improvements. We conclude that COVER can be directly exploited to build applications, and coupled with existing resources, as well. © 2018, Springer Nature B.V.",2018,Language Resources and Evaluation,8,lexical resource @ fundamental to tackle many task @ @ central to @ and prospective research in text mining information retrieval and connected to natural language processing @ in @ article @ introduce cover a novel lexical resource along @ coverage @ algorithm devised to build @ @ in order to describe concept cover proposes a compact vectorial representation @ combine @ lexicographic precision characterizing babelnet and @ rich common-sense knowledge featuring conceptnet @ @ propose cover a a reliable and mature resource @ ha @ employed in a diverse task a conceptual categorization keywords extraction and conceptual similarity @ @ experimental assessment is performed on @ last task @ @ report and discus @ obtained @ pointing @ future improvement @ @ conclude @ cover @ @ directly exploited to build application and coupled @ existing resource a well @ @ nature b @ v @ 
1087,A study on online travel reviews through intelligent data analysis,"The purpose of this paper is to show the application of a set of intelligent data analysis techniques to about 7 million of online travel reviews, with the aim of automatically extracting useful information. The reviews, collected from two popular online tourism-related review platforms, are all those posted by reviewers about one specific Italian location, from 2010 to 2017. To carry out the study, the following methodology is applied: a preliminary statistical analysis is performed to acquire general knowledge about the datasets, such as the geographical distribution of reviewers, their activities, and a comparison among the time of visits and the average scores of the reviews. Then, Natural Language Processing techniques are applied to extract and compare the most frequent words used in the two platforms. Finally, an Association Rule Learning algorithm is applied, to extract preferred destinations for distinct groups of reviewers, by mining interesting associations among the countries of origin of the reviewers and the most frequent destinations visited. By elaborating the available data, it is possible to automatically disclose valuable information for consumers and providers. The information automatically extracted can be exploited, for example, to build a recommender system for customers or a market analysis tool for service providers. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",2018,Information Technology and Tourism,5,@ purpose of @ @ is to @ @ application of a set of intelligent data analysis technique to @ million of online travel review @ @ aim of automatically extracting useful information @ @ review collected @ @ popular online tourism-related review platform @ @ @ posted by reviewer @ @ specific italian location @ to @ to carry @ @ study @ following methodology is applied @ a preliminary statistical analysis is performed to acquire general knowledge @ @ datasets @ a @ geographical distribution of reviewer @ activity and a comparison among @ time of visit and @ average score of @ review @ @ natural language processing technique @ applied to extract and compare @ @ frequent word used in @ @ platform @ finally @ association rule learning algorithm is applied to extract preferred destination @ distinct group of reviewer by mining interesting association among @ country of origin of @ reviewer and @ @ frequent destination visited @ by elaborating @ available data @ is possible to automatically disclose valuable information @ consumer and provider @ @ information automatically extracted @ @ exploited @ example to build a recommender system @ customer @ a market analysis tool @ service provider @ springer-verlag gmbh germany part of @ nature @ 
1088,Keyword extraction from micro-blogs using collective weight,"The growth of social networking has increased the scope of expression on a public platform. Twitter alone, being one of the most trending social networking sites, generates a huge amount of text every minute. Twitter content analysis and summarization benefits many applications such as information retrieval, automatic indexing, automatic classification, automatic clustering, automatic filtering, etc. One of the most important tasks in analyzing tweets is automatic keyword extraction. Many existing graph-based keyword extraction approaches determine keywords purely based on centrality measure. However, various features such as frequency, centrality, position, and strength of the neighbours of the keyword also affect the importance of a keyword in tweets. Therefore, this paper proposes a novel unsupervised graph-based keyword extraction method called keywords from collective weights (KCW) which determines the importance of a keyword by collectively considering various influencing features. The KCW is based on node-edge rank centrality with node weight depending on various features. The model is validated with five data sets: Uri Attack, Harry Potter, IPL, Donald Trump and IPhone5. The result of KCW is compared with three existing models. It is observed from the experimental results that the proposed method is far better than the others. The performances are shown in terms of precision, recall, and F measure. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature.",2018,Social Network Analysis and Mining,4,@ growth of social networking ha increased @ scope of expression on a public platform @ twitter alone @ @ of @ @ trending social networking site generates a huge amount of text every minute @ twitter content analysis and summarization benefit many application @ a information retrieval automatic indexing automatic classification automatic clustering automatic filtering etc @ @ of @ @ important task in analyzing tweet is automatic keyword extraction @ many existing graph-based keyword extraction approach determine keywords purely based on centrality measure @ however various feature @ a frequency centrality position and strength of @ neighbour of @ keyword @ affect @ importance of a keyword in tweet @ therefore @ @ proposes a novel unsupervised graph-based keyword extraction method called keywords @ collective weight @ kcw @ @ determines @ importance of a keyword by collectively considering various influencing feature @ @ kcw is based on node-edge rank centrality @ node weight depending on various feature @ @ model is validated @ five data set @ uri attack harry potter ipl donald trump and iphone @ @ @ of kcw is compared @ three existing model @ @ is observed @ @ experimental @ @ @ proposed method is far better @ @ others @ @ performance @ @ in term of precision recall and f measure @ springer-verlag gmbh austria part of @ nature @ 
1089,Sentiment Discovery of Social Messages Using Self-Organizing Maps,"Introduction Predicting the sentiments and emotions of people from their texts is a critical issue in cognitive computing. The explosive growth of social network services has led to a tremendous increase of textual data, increasing the demand of the advanced analysis of these data. Sentiment analysis on textual social media data emerged in recent years to fulfill the needs of areas such as national security, business, politics, and economics; however, text messages from social networks are rather different from those of traditional text documents, especially in presentation style and lengths. Therefore, it is difficult but essential to develop an effective method to explore the sentiments of social messages. Methods In this study, we first applied a self-organizing map (SOM) algorithm to cluster social messages as well as sentiment keywords. An association discovery process was then applied to discover the associations between a message and some sentiment keywords, and the sentiment of a message was determined according to such associations. Results We performed experiments on collected Twitter messages and the results’ accuracy outperformed that of a similar approach. Conclusions A sentiment analysis approach based on SOMs was proposed. The associations between messages and keywords were derived using the proposed method. The novelty of this work arises from the adoption of association discovery process in sentiment analysis. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2018,Cognitive Computation,9,introduction predicting @ sentiment and emotion of people @ @ text is a critical issue in cognitive computing @ @ explosive growth of social network service ha led to a tremendous increase of textual data increasing @ demand of @ advanced analysis of @ data @ sentiment analysis on textual social medium data emerged in recent year to fulfill @ need of area @ a national security @ politics and economics @ however text message @ social network @ rather different @ @ of traditional text document especially in presentation style and length @ therefore @ is difficult @ essential to develop @ effective method to explore @ sentiment of social message @ method in @ study @ first applied a self-organizing map @ som @ algorithm to cluster social message a well a sentiment keywords @ @ association discovery process wa @ applied to discover @ association @ a message and some sentiment keywords and @ sentiment of a message wa determined according to @ association @ @ @ performed experiment on collected twitter message and @ @ accuracy outperformed @ of a similar approach @ conclusion a sentiment analysis approach based on som wa proposed @ @ association @ message and keywords @ derived @ @ proposed method @ @ novelty of @ work arises @ @ adoption of association discovery process in sentiment analysis @ @ science @ medium llc part of @ nature @ 
1090,Introduction to the special issue: Data-driven approaches to research and teaching in professional and technical communication,"The quest to understand the nuances of professional communication using computational tools have continued since, and many researchers in our field have embraced the new interdisciplinary approach now known as data science. Our quick metadata search on the journals and conference proceedings in technical and professional communication (TPC) revealed an increasing number of articles associated with terms commonly used in data science (e.g., big data, content analysis, text mining, sentiment analysis, topic modeling, network analysis) originating from numerous disciplines (e.g., corpus linguistics, computational linguistics, artificial intelligence, statistics, business analytics). Yet, the field of TPC is just beginning to embrace the power of data-driven approaches. This special issue extends Orr's work by taking a snapshot of current work in data-driven approaches to the study of TPC. © 2018 IEEE.",2018,IEEE Transactions on Professional Communication,4,@ quest to understand @ nuance of professional communication @ computational tool @ continued since and many researcher in @ field @ embraced @ @ interdisciplinary approach now known a data science @ @ quick metadata search on @ journal and conference proceeding in technical and professional communication @ tpc @ revealed @ increasing number of article associated @ term commonly used in data science @ e @ g @ big data content analysis text mining sentiment analysis topic modeling network analysis @ originating @ numerous discipline @ e @ g @ corpus linguistics computational linguistics artificial intelligence statistic @ analytics @ @ yet @ field of tpc is @ beginning to embrace @ power of data-driven approach @ @ special issue extends orr @ s work by taking a snapshot of current work in data-driven approach to @ study of tpc @ @ @ 
1091,Diacritic restoration of Turkish tweets with word2vec,"Social media platforms such as Twitter have grown at a tremendous pace in recent years and have become an important source of data providing information countless field. This situation was of interest to researchers and many studies on machine learning and natural language processing was conducted on social media data. However, the language is used in social media contains a very high amount of noisy data than the formal writing language. In this article, we present a study on diacritic restoration which is one of the important difficulties of social media text normalization in order to reduce the noise problem. Diacritic is a set of marks used to change the sound values of letters and is used on many languages besides Turkish. We suggest a 3-step model for this study to overcome the top of the diacritic restoration problem. In the first step, a candidate word generator produces possible word forms, in the second step the language validator chooses the correct word forms and at the final Word2vec is used to create vector representations of the words and make the most appropriate word choice by using cosine similarities. The proposed method was tested on both the 2 ad-hoc created datasets and the real dataset. Studies on small ad-hoc created dataset and real dataset provided a relative error reduction of 37.8% with an average performance of 94.5%. In addition, tests on more than 6 M words on large ad-hoc created dataset yielded a serious performance with an error rate of 3.9%. Furthermore, the proposed method was tested on the binary classification problem consisting of highway traffic data in order to evaluate the effects on classification performance, and a 3.1% increase in classification performance was achieved. © 2018 Karabuk University",2018,"Engineering Science and Technology, an International Journal",4,social medium platform @ a twitter @ grown at a tremendous pace in recent year and @ become @ important source of data providing information countless field @ @ situation wa of interest to researcher and many study on machine learning and natural language processing wa conducted on social medium data @ however @ language is used in social medium contains a @ high amount of noisy data @ @ formal writing language @ in @ article @ @ a study on diacritic restoration @ is @ of @ important difficulty of social medium text normalization in order to reduce @ noise problem @ diacritic is a set of mark used to change @ sound value of letter and is used on many language besides turkish @ @ suggest a step model @ @ study to overcome @ top of @ diacritic restoration problem @ in @ first step a candidate word generator produce possible word form in @ second step @ language validator chooses @ correct word form and at @ final word vec is used to create vector representation of @ word and make @ @ appropriate word choice by @ cosine similarity @ @ proposed method wa tested on @ @ ad-hoc created datasets and @ real dataset @ study on small ad-hoc created dataset and real dataset provided a relative error reduction of @ @ @ average performance of @ @ in addition test on more @ @ word on @ ad-hoc created dataset yielded a serious performance @ @ error rate of @ @ furthermore @ proposed method wa tested on @ binary classification problem consisting of highway traffic data in order to evaluate @ effect on classification performance and a @ increase in classification performance wa achieved @ karabuk university
1093,Using frame semantics for classifying and summarizing application store reviews,"Text mining techniques have been recently employed to classify and summarize user reviews on mobile application stores. However, due to the inherently diverse and unstructured nature of user-generated online textual data, text-based review mining techniques often produce excessively complicated models that are prone to overfitting. In this paper, we propose a novel approach, based on frame semantics, for app review mining. Semantic frames help to generalize from raw text (individual words) to more abstract scenarios (contexts). This lower-dimensional representation of text is expected to enhance the predictive capabilities of review mining techniques and reduce the chances of overfitting. Specifically, our analysis in this paper is two-fold. First, we investigate the performance of semantic frames in classifying informative user reviews into various categories of actionable software maintenance requests. Second, we propose and evaluate the performance of multiple summarization algorithms in generating concise and representative summaries of informative reviews. Three different datasets of app store reviews, sampled from a broad range of application domains, are used to conduct our experimental analysis. The results show that semantic frames can enable an efficient and accurate review classification process. However, in review summarization tasks, our results show that text-based summarization generates more comprehensive summaries than frame-based summarization. Finally, we introduces MARC 2.0, a review classification and summarization suite that implements the algorithms investigated in our analysis. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2018,Empirical Software Engineering,10,text mining technique @ @ recently employed to classify and summarize user review on mobile application store @ however due to @ inherently diverse and unstructured nature of user-generated online textual data text-based review mining technique often produce excessively complicated model @ @ prone to overfitting @ in @ @ @ propose a novel approach based on frame semantics @ app review mining @ semantic frame help to generalize @ raw text @ individual word @ to more abstract scenario @ context @ @ @ lower-dimensional representation of text is expected to enhance @ predictive capability of review mining technique and reduce @ chance of overfitting @ specifically @ analysis in @ @ is two-fold @ first @ investigate @ performance of semantic frame in classifying informative user review @ various category of actionable software maintenance request @ second @ propose and evaluate @ performance of multiple summarization algorithm in generating concise and representative summary of informative review @ three different datasets of app store review sampled @ a broad range of application domain @ used to conduct @ experimental analysis @ @ @ @ @ semantic frame @ enable @ efficient and accurate review classification process @ however in review summarization task @ @ @ @ text-based summarization generates more comprehensive summary @ frame-based summarization @ finally @ introduces marc @ a review classification and summarization suite @ implement @ algorithm investigated in @ analysis @ @ science @ medium llc part of @ nature @ 
1095,Latent association rule cluster based model to extract topics for classification and recommendation applications,"The quality of any text mining technique is highly dependent on the features that are used to represent the document collection. A classical form of document representation is the vector space model (VSM), according to which the documents are represented as vectors of weights that correspond to the features of the documents. The bag-of-words model is the most popular VSM approach due to its simplicity and general applicability, but this model does not include term dependency and has a high dimensionality. In the literature, several models for document representation have been proposed in order to capture the dependency of terms. Among them, the topic model representation is one of the most interesting approaches - since it describes the collection of documents in a way that reveals their internal structure and the interrelationships therein, and also provides a dimensionality reduction. However, even for topic models, the efficient extraction of information concerning the relations among terms for document representation is still a major research challenge. In order to address this issue, we proposed thelatent association rule cluster based model (LARCM). The LARCM is a non-probabilistic topic model that makes use of association rule clustering to build a document representation with low dimensionality in such a way that each feature (i.e., topic) is comprised of information concerning relations among the terms. We evaluated the interpretability of the topics obtained by using our proposed model against the ones provided by the traditional latent dirichlet allocation (LDA) model and the LDA model using a document representation that includes correlated terms (i.e., bag-of-related-words). The experimental results indicated that the LARCM provides topics with better interpretability than the LDA models. Additionally, we used the topics obtained by the LARCM in two different applications: text classification and page recommendation. With respect to text classification, the topics were used to improve document collection representation. Concerning page recommendation, topics were used as contextual information in context-aware recommender systems. Results have shown that the topics provided by the LARCM can be used to improve both applications. © 2018 Elsevier Ltd",2018,Expert Systems with Applications,9,@ quality of @ text mining technique is highly dependent on @ feature @ @ used to represent @ document collection @ a classical form of document representation is @ vector space model @ vsm @ according to @ @ document @ represented a vector of weight @ correspond to @ feature of @ document @ @ bag-of-words model is @ @ popular vsm approach due to @ simplicity and general applicability @ @ model doe not include term dependency and ha a high dimensionality @ in @ literature several model @ document representation @ @ proposed in order to capture @ dependency of term @ among @ @ topic model representation is @ of @ @ interesting approach since @ describes @ collection of document in a way @ reveals @ internal structure and @ interrelationship therein and @ provides a dimensionality reduction @ however even @ topic model @ efficient extraction of information concerning @ relation among term @ document representation is still a major research challenge @ in order to address @ issue @ proposed thelatent association rule cluster based model @ larcm @ @ @ larcm is a non-probabilistic topic model @ make use of association rule clustering to build a document representation @ low dimensionality in @ a way @ @ feature @ i @ e @ topic @ is comprised of information concerning relation among @ term @ @ evaluated @ interpretability of @ topic obtained by @ @ proposed model @ @ @ provided by @ traditional latent dirichlet allocation @ lda @ model and @ lda model @ a document representation @ includes correlated term @ i @ e @ bag-of-related-words @ @ @ experimental @ indicated @ @ larcm provides topic @ better interpretability @ @ lda model @ additionally @ used @ topic obtained by @ larcm in @ different application @ text classification and page recommendation @ @ respect to text classification @ topic @ used to improve document collection representation @ concerning page recommendation topic @ used a contextual information in context-aware recommender system @ @ @ @ @ @ topic provided by @ larcm @ @ used to improve @ application @ @ ltd
1098,Making sense of organization dynamics using text analysis,Being able to understand the implicit power structures and dynamics among members plays a crucial role for the management of the organization. This paper introduces a novel and comprehensive approach to analyzing organizational discourse data. The proposed approach provides a holistic view of the power structure implied by the communications. The paper contributes to the domain of text mining by integrating various text-mining techniques to demonstrating different aspects of a power structure within an organization. It also contributes to the domain of supply chain management by using the conventional communication discourse method as the guideline for the development of the tool. We applied the proposed approach to a seven-year collection of meeting minutes from a co-op and our findings were largely confirmed by members of the organization. We provide a roadmap of using the multi-aspect approach to analyzing organizational discourse data in supply networks. © 2017 Elsevier Ltd,2018,Expert Systems with Applications,2,@ able to understand @ implicit power structure and dynamic among member play a crucial role @ @ management of @ organization @ @ @ introduces a novel and comprehensive approach to analyzing organizational discourse data @ @ proposed approach provides a holistic view of @ power structure implied by @ communication @ @ @ contributes to @ domain of text mining by integrating various text-mining technique to demonstrating different aspect of a power structure within @ organization @ @ @ contributes to @ domain of supply chain management by @ @ conventional communication discourse method a @ guideline @ @ development of @ tool @ @ applied @ proposed approach to a seven-year collection of meeting minute @ a co-op and @ finding @ largely confirmed by member of @ organization @ @ provide a roadmap of @ @ multi-aspect approach to analyzing organizational discourse data in supply network @ @ ltd
1118,Intelligent asset allocation via market sentiment views,"The sentiment index of market participants has been extensively used for stock market prediction in recent years. Many financial information vendors also provide it as a service. However, utilizing market sentiment under the asset allocation framework has been rarely discussed. In this article, we investigate the role of market sentiment in an asset allocation problem. We propose to compute sentiment time series from social media with the help of sentiment analysis and text mining techniques. A novel neural network design, built upon an ensemble of evolving clustering and long short-term memory, is used to formalize sentiment information into market views. These views are later integrated into modern portfolio theory through a Bayesian approach. We analyze the performance of this asset allocation model from many aspects, such as stability of portfolios, computing of sentiment time series, and profitability in our simulations. Experimental results show that our model outperforms some of the most successful forecasting techniques. Thanks to the introduction of the evolving clustering method, the estimation accuracy of market views is significantly improved. © 2018 IEEE.",2018,IEEE Computational Intelligence Magazine,28,@ sentiment index of market participant ha @ extensively used @ stock market prediction in recent year @ many financial information vendor @ provide @ a a service @ however utilizing market sentiment @ @ asset allocation framework ha @ rarely discussed @ in @ article @ investigate @ role of market sentiment in @ asset allocation problem @ @ propose to compute sentiment time series @ social medium @ @ help of sentiment analysis and text mining technique @ a novel neural network design built upon @ ensemble of evolving clustering and long short-term memory is used to formalize sentiment information @ market view @ @ view @ later integrated @ modern portfolio theory @ a bayesian approach @ @ analyze @ performance of @ asset allocation model @ many aspect @ a stability of portfolio computing of sentiment time series and profitability in @ simulation @ experimental @ @ @ @ model outperforms some of @ @ successful forecasting technique @ thanks to @ introduction of @ evolving clustering method @ estimation accuracy of market view is significantly improved @ @ @ 
1119,Deep learning for affective computing: Text-based emotion recognition in decision support,"Emotions widely affect human decision-making. This fact is taken into account by affective computing with the goal of tailoring decision support to the emotional states of individuals. However, the accurate recognition of emotions within narrative documents presents a challenging undertaking due to the complexity and ambiguity of language. Performance improvements can be achieved through deep learning; yet, as demonstrated in this paper, the specific nature of this task requires the customization of recurrent neural networks with regard to bidirectional processing, dropout layers as a means of regularization, and weighted loss functions. In addition, we propose sent2affect, a tailored form of transfer learning for affective computing: here the network is pre-trained for a different task (i.e. sentiment analysis), while the output layer is subsequently tuned to the task of emotion recognition. The resulting performance is evaluated in a holistic setting across 6 benchmark datasets, where we find that both recurrent neural networks and transfer learning consistently outperform traditional machine learning. Altogether, the findings have considerable implications for the use of affective computing. © 2018 Elsevier B.V.",2018,Decision Support Systems,50,emotion widely affect human decision-making @ @ fact is taken @ account by affective computing @ @ goal of tailoring decision support to @ emotional state of individual @ however @ accurate recognition of emotion within narrative document @ a challenging undertaking due to @ complexity and ambiguity of language @ performance improvement @ @ achieved @ deep learning @ yet a demonstrated in @ @ @ specific nature of @ task requires @ customization of recurrent neural network @ regard to bidirectional processing dropout layer a a mean of regularization and weighted loss function @ in addition @ propose sent affect a tailored form of transfer learning @ affective computing @ @ @ network is pre-trained @ a different task @ i @ e @ sentiment analysis @ @ @ output layer is subsequently tuned to @ task of emotion recognition @ @ resulting performance is evaluated in a holistic setting across benchmark datasets @ @ find @ @ recurrent neural network and transfer learning consistently outperform traditional machine learning @ altogether @ finding @ considerable implication @ @ use of affective computing @ @ b @ v @ 
1120,Question categorization and classification using grammar based approach,"Question-answering has become one of the most popular information retrieval applications. Despite that most question-answering systems try to improve the user experience and the technology used in finding relevant results, many difficulties are still faced because of the continuous increase in the amount of web content. Questions Classification (QC) plays an important role in question-answering systems, with one of the major tasks in the enhancement of the classification process being the identification of questions types. A broad range of QC approaches has been proposed with the aim of helping to find a solution for the classification problems; most of these are approaches based on bag-of-words or dictionaries. In this research, we present an analysis of the different type of questions based on their grammatical structure. We identify different patterns and use machine learning algorithms to classify them. A framework is proposed for question classification using a grammar-based approach (GQCC) which exploits the structure of the questions. Our findings indicate that using syntactic categories related to different domain-specific types of Common Nouns, Numeral Numbers and Proper Nouns enable the machine learning algorithms to better differentiate between different question types. The paper presents a wide range of experiments the results show that the GQCC using J48 classifier has outperformed other classification methods with 90.1% accuracy. © 2018 Elsevier Ltd",2018,Information Processing and Management,27,question-answering ha become @ of @ @ popular information retrieval application @ despite @ @ question-answering system try to improve @ user experience and @ technology used in finding relevant @ many difficulty @ still faced @ of @ continuous increase in @ amount of web content @ question classification @ qc @ play @ important role in question-answering system @ @ of @ major task in @ enhancement of @ classification process @ @ identification of question type @ a broad range of qc approach ha @ proposed @ @ aim of helping to find a solution @ @ classification problem @ @ of @ @ approach based on bag-of-words @ dictionary @ in @ research @ @ @ analysis of @ different type of question based on @ grammatical structure @ @ identify different pattern and use machine learning algorithm to classify @ @ a framework is proposed @ question classification @ a grammar-based approach @ gqcc @ @ exploit @ structure of @ question @ @ finding indicate @ @ syntactic category related to different domain-specific type of common noun numeral number and proper noun enable @ machine learning algorithm to better differentiate @ different question type @ @ @ @ a wide range of experiment @ @ @ @ @ gqcc @ j classifier ha outperformed @ classification method @ @ accuracy @ @ ltd
1122,A new method for semantic similarity assessment using fuzzy formal concept analysis & fuzzy set similarity measure,"Measuring the accurate semantic similarity between the words is a major issue in various applications of artificial intelligence and computational linguistics areas such as natural language processing, text-mining, information retrieval and for development of semantic web. In the past, many approaches have been proposed and adopted to evaluate similarity by using the knowledge based systems such as WordNet and MeSH ontology. In this paper we have proposed a new method; based on hybridization approach in knowledge based system. In this we have used feature based method and fuzzy Set theory. In feature based approach, properties or features are used for measuring the similarity as compare to edge and content information approaches. Our approach is investigated on standard dataset like R&G, M&C and 353-TC, which shows prominent improvement in the judgment of semantic similarity score between the words. This approach can be further used among cross ontology and fuzzy ontology as it is based on the feature based measure and fuzzy set theory. © 2018, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.",2018,International Journal of Recent Technology and Engineering,0,measuring @ accurate semantic similarity @ @ word is a major issue in various application of artificial intelligence and computational linguistics area @ a natural language processing text-mining information retrieval and @ development of semantic web @ in @ past many approach @ @ proposed and adopted to evaluate similarity by @ @ knowledge based system @ a wordnet and mesh ontology @ in @ @ @ @ proposed a @ method @ based on hybridization approach in knowledge based system @ in @ @ @ used feature based method and fuzzy set theory @ in feature based approach property @ feature @ used @ measuring @ similarity a compare to edge and content information approach @ @ approach is investigated on standard dataset like r g @ c and tc @ @ prominent improvement in @ judgment of semantic similarity score @ @ word @ @ approach @ @ @ used among cross ontology and fuzzy ontology a @ is based on @ feature based measure and fuzzy set theory @ blue eye intelligence engineering and science publication @ @ right reserved @ 
1123,Classification of compressed and uncompressed text documents,"Computing the degree of closeness (similarity) between two sets of text documents is one of the core operations in many text mining applications like text classification, clustering and sentiment analysis. The efficiency of such applications mainly depends on the factors like selection of representation model, selection of the similarity metric and selection of learning algorithms. Among these three factors, selection of similarity measure is important since it contributes to the efficiency of most of the text mining applications. In this research article, an efficient similarity measure is proposed for computing the closeness between two sets of text documents. The proposed measure has the capacity of considering different real time situations like presence of a feature or absence of features for computing the degree of similarity between the documents. Furthermore, a compression modeling similarity measure is also proposed for text documents. Two different sets of experiments are conducted to validate the efficacy of the proposed similarity measures. Experimental results demonstrate that the f-measure score obtained from proposed similarity metric is better than the f-measure score of the existing state of the art techniques. © 2018 Elsevier B.V.",2018,Future Generation Computer Systems,7,computing @ degree of closeness @ similarity @ @ @ set of text document is @ of @ core operation in many text mining application like text classification clustering and sentiment analysis @ @ efficiency of @ application mainly depends on @ factor like selection of representation model selection of @ similarity metric and selection of learning algorithm @ among @ three factor selection of similarity measure is important since @ contributes to @ efficiency of @ of @ text mining application @ in @ research article @ efficient similarity measure is proposed @ computing @ closeness @ @ set of text document @ @ proposed measure ha @ capacity of considering different real time situation like presence of a feature @ absence of feature @ computing @ degree of similarity @ @ document @ furthermore a compression modeling similarity measure is @ proposed @ text document @ @ different set of experiment @ conducted to validate @ efficacy of @ proposed similarity measure @ experimental @ demonstrate @ @ f-measure score obtained @ proposed similarity metric is better @ @ f-measure score of @ existing state of @ art technique @ @ b @ v @ 
1125,A deep network model for paraphrase detection in short text messages,"This paper is concerned with paraphrase detection, i.e., identifying sentences that are semantically identical. The ability to detect similar sentences written in natural language is crucial for several applications, such as text mining, text summarization, plagiarism detection, authorship authentication and question answering. Recognizing this importance, we study in particular how to address the challenges with detecting paraphrases in user generated short texts, such as Twitter, which often contain language irregularity and noise, and do not necessarily contain as much semantic information as longer clean texts. We propose a novel deep neural network-based approach that relies on coarse-grained sentence modelling using a convolutional neural network (CNN) and a recurrent neural network (RNN) model, combined with a specific fine-grained word-level similarity matching model. More specifically, we develop a new architecture, called DeepParaphrase, which enables to create an informative semantic representation of each sentence by (1) using CNN to extract the local region information in form of important n-grams from the sentence, and (2) applying RNN to capture the long-term dependency information. In addition, we perform a comparative study on state-of-the-art approaches within paraphrase detection. An important insight from this study is that existing paraphrase approaches perform well when applied on clean texts, but they do not necessarily deliver good performance against noisy texts, and vice versa. In contrast, our evaluation has shown that the proposed DeepParaphrase-based approach achieves good results in both types of texts, thus making it more robust and generic than the existing approaches. © 2018",2018,Information Processing and Management,43,@ @ is concerned @ paraphrase detection i @ e @ identifying sentence @ @ semantically identical @ @ ability to detect similar sentence written in natural language is crucial @ several application @ a text mining text summarization plagiarism detection authorship authentication and question answering @ recognizing @ importance @ study in particular @ to address @ challenge @ detecting paraphrase in user generated short text @ a twitter @ often contain language irregularity and noise and @ not necessarily contain a much semantic information a longer clean text @ @ propose a novel deep neural network-based approach @ relies on coarse-grained sentence modelling @ a convolutional neural network @ cnn @ and a recurrent neural network @ rnn @ model combined @ a specific fine-grained word-level similarity matching model @ more specifically @ develop a @ architecture called deepparaphrase @ enables to create @ informative semantic representation of @ sentence by @ @ @ cnn to extract @ local region information in form of important n-grams @ @ sentence and @ @ applying rnn to capture @ long-term dependency information @ in addition @ perform a comparative study on state-of-the-art approach within paraphrase detection @ @ important insight @ @ study is @ existing paraphrase approach perform well @ applied on clean text @ @ @ not necessarily deliver good performance @ noisy text and vice versa @ in contrast @ evaluation ha @ @ @ proposed deepparaphrase-based approach achieves good @ in @ type of text thus making @ more robust and generic @ @ existing approach @ 
1132,Literature-based automated discovery of tumor suppressor p53 phosphorylation and inhibition by NEK2,"Scientific progress depends on formulating testable hypotheses informed by the literature. In many domains, however, this model is strained because the number of research papers exceeds human readability. Here, we developed computational assistance to analyze the biomedical literature by reading PubMed abstracts to suggest new hypotheses. The approach was tested experimentally on the tumor suppressor p53 by ranking its most likely kinases, based on all available abstracts. Many of the best-ranked kinases were found to bind and phosphorylate p53 (P value = 0.005), suggesting six likely p53 kinases so far. One of these, NEK2, was studied in detail. A known mitosis promoter, NEK2 was shown to phosphorylate p53 at Ser315 in vitro and in vivo and to functionally inhibit p53. These bona fide validations of text-based predictions of p53 phosphorylation, and the discovery of an inhibitory p53 kinase of pharmaceutical interest, suggest that automated reasoning using a large body of literature can generate valuable molecular hypotheses and has the potential to accelerate scientific discovery. © 2018 National Academy of Sciences. All Rights Reserved.",2018,Proceedings of the National Academy of Sciences of the United States of America,13,scientific progress depends on formulating testable hypothesis informed by @ literature @ in many domain however @ model is strained @ @ number of research @ exceeds human readability @ @ @ developed computational assistance to analyze @ biomedical literature by reading pubmed abstract to suggest @ hypothesis @ @ approach wa tested experimentally on @ tumor suppressor p by ranking @ @ likely kinase based on @ available abstract @ many of @ best-ranked kinase @ found to bind and phosphorylate p @ p value @ @ suggesting six likely p kinase @ far @ @ of @ nek wa studied in detail @ a known mitosis promoter nek wa @ to phosphorylate p at ser in vitro and in vivo and to functionally inhibit p @ @ bona fide validation of text-based prediction of p phosphorylation and @ discovery of @ inhibitory p kinase of pharmaceutical interest suggest @ automated reasoning @ a @ body of literature @ generate valuable molecular hypothesis and ha @ potential to accelerate scientific discovery @ national academy of science @ @ right reserved @ 
1140,A survey on sentiment analysis challenges,"With accelerated evolution of the internet as websites, social networks, blogs, online portals, reviews, opinions, recommendations, ratings, and feedback are generated by writers. This writer generated sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, and individuals. While this content is meant to be useful, a bulk of this writer generated content require using the text mining techniques and sentiment analysis. But there are several challenges facing the sentiment analysis and evaluation process. These challenges become obstacles in analyzing the accurate meaning of sentiments and detecting the suitable sentiment polarity. Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text. This paper presents a survey on the sentiment analysis challenges relevant to their approaches and techniques. © 2016 The Author",2018,Journal of King Saud University - Engineering Sciences,122,@ accelerated evolution of @ internet a website social network blog online portal review opinion recommendation rating and feedback @ generated by writer @ @ writer generated sentiment content @ @ @ book people hotel product research event etc @ @ sentiment become @ beneficial @ @ government and individual @ @ @ content is meant to @ useful a bulk of @ writer generated content require @ @ text mining technique and sentiment analysis @ @ @ @ several challenge facing @ sentiment analysis and evaluation process @ @ challenge become obstacle in analyzing @ accurate meaning of sentiment and detecting @ suitable sentiment polarity @ sentiment analysis is @ practice of applying natural language processing and text analysis technique to identify and extract subjective information @ text @ @ @ @ a survey on @ sentiment analysis challenge relevant to @ approach and technique @ @ author
1144,A two-phase sentiment analysis approach for judgement prediction,"Factual scenario analysis of a judgement is critical to judges during sentencing. With the increasing number of legal cases, professionals typically endure heavy workloads on a daily basis. Although a few previous studies have applied information technology to legal cases, according to our research, no prior studies have predicted a pending judgement using legal documents. In this article, we introduce an innovative solution to predict relevant rulings. The proposed approach employs text mining methods to extract features from precedents and applies a text classifier to automatically classify judgements according to sentiment analysis. This approach can assist legal experts or litigants in predicting possible judgements. Experimental results from a judgement data set reveal that our approach is a satisfactory method for judgement classification. © The Author(s) 2017.",2018,Journal of Information Science,14,factual scenario analysis of a judgement is critical to judge @ sentencing @ @ @ increasing number of legal case professional typically endure heavy workload on a daily basis @ although a @ previous study @ applied information technology to legal case according to @ research no prior study @ predicted a pending judgement @ legal document @ in @ article @ introduce @ innovative solution to predict relevant ruling @ @ proposed approach employ text mining method to extract feature @ precedent and applies a text classifier to automatically classify judgement according to sentiment analysis @ @ approach @ assist legal expert @ litigant in predicting possible judgement @ experimental @ @ a judgement data set reveal @ @ approach is a satisfactory method @ judgement classification @ @ author @ s @ @ 
1145,Feature selection for entity extraction from multiple biomedical corpora: A PSO-based approach,"Entity extraction is an important step in biomedical text mining. Among many other challenges, there are two very crucial issues, viz. determining the most applicable feature set so that the model can be precise and less complex, and adapting the system across multiple benchmark corpora. In this paper, we propose a novel method for feature selection using the search capability of particle swarm optimization. The compact feature set used for training the classifier yields much better results when compared to the baseline model, which was developed with a complete set of features. A large number of features suitable for named entity recognition task from biomedical domain are also developed in the current paper. The complete set of features is implemented by studying the properties of datasets and from the domain knowledge. We have used conditional random field, a robust classifier as the underlying learning algorithm which has shown success in solving similar kinds of problems. Our experiments on multiple benchmark corpora yield the level of performance which are at par the state-of-the-art techniques. © 2017, Springer-Verlag GmbH Germany.",2018,Soft Computing,11,entity extraction is @ important step in biomedical text mining @ among many @ challenge @ @ @ @ crucial issue viz @ determining @ @ applicable feature set @ @ @ model @ @ precise and le complex and adapting @ system across multiple benchmark corpus @ in @ @ @ propose a novel method @ feature selection @ @ search capability of particle swarm optimization @ @ compact feature set used @ training @ classifier yield much better @ @ compared to @ baseline model @ wa developed @ a complete set of feature @ a @ number of feature suitable @ named entity recognition task @ biomedical domain @ @ developed in @ current @ @ @ complete set of feature is implemented by studying @ property of datasets and @ @ domain knowledge @ @ @ used conditional random field a robust classifier a @ underlying learning algorithm @ ha @ success in solving similar kind of problem @ @ experiment on multiple benchmark corpus yield @ level of performance @ @ at par @ state-of-the-art technique @ springer-verlag gmbh germany @ 
1148,A new LSA and entropy-based approach for automatic text document summarization,"Automatic text document summarization is active research area in text mining field. In this article, the authors are proposing two new approaches (three models) for sentence selection, and a new entropy-based summary evaluation criteria. The first approach is based on the algebraic model, Singular Value Decomposition (SVD), i.e. Latent Semantic Analysis (LSA) and model is termed as proposed_model-1, and Second Approach is based on entropy that is further divided into proposed_ model-2 and proposed_model-3. In first proposed model, the authors are using right singular matrix, and second & third proposed models are based on Shannon entropy. The advantage of these models is that these are not a Length dominating model, giving better results, and low redundancy. Along with these three new models, an entropy-based summary evaluation criteria is proposed and tested. They are also showing that their entropy based proposed models statistically closer to DUC-2002's standard/ gold summary. In this article, the authors are using a dataset taken from Document Understanding Conference-2002. Copyright © 2018, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",2018,International Journal on Semantic Web and Information Systems,3,automatic text document summarization is active research area in text mining field @ in @ article @ author @ proposing @ @ approach @ three model @ @ sentence selection and a @ entropy-based summary evaluation criterion @ @ first approach is based on @ algebraic model singular value decomposition @ svd @ i @ e @ latent semantic analysis @ lsa @ and model is termed a proposed model and second approach is based on entropy @ is @ divided @ proposed  model and proposed model @ in first proposed model @ author @ @ right singular matrix and second third proposed model @ based on shannon entropy @ @ advantage of @ model is @ @ @ not a length dominating model giving better @ and low redundancy @ along @ @ three @ model @ entropy-based summary evaluation criterion is proposed and tested @ @ @ @ showing @ @ entropy based proposed model statistically closer to duc @ s standard gold summary @ in @ article @ author @ @ a dataset taken @ document understanding conference @ @ igi global @ copying @ distributing in print @ electronic form without written permission of igi global is prohibited @ 
1161,Improving sentiment scoring mechanism: a case study on airline services,"Purpose: The purpose of this paper is to investigate the effect of including letter repetition commonly found within social media text and its impact in determining the sentiment scores for two major airlines in Malaysia. Design/methodology/approach: A Sentiment Intensity Calculator (SentI-Cal) was developed by assigning individual weights to each letter repetition, and tested it using data collected from official Facebook pages of the airlines. Findings: Evaluation metrics indicate that SentI-Cal outperforms the baseline tool Semantic Orientation Calculator (SO-CAL), with an accuracy of 90.7 percent compared to 58.33 percent for SO-CAL. Practical implications: A more accurate sentiment score allows airline services to easily obtain a better understanding of the sentiments of their customers, hence providing opportunities in improving their airline services. Originality/value: Proposed mechanism calculates sentiment intensity of social media text by assigning individual weightage to each repeated letter and exclamation mark thus producing a more accurate sentiment score. © 2018, Emerald Publishing Limited.",2018,Industrial Management and Data Systems,3,purpose @ @ purpose of @ @ is to investigate @ effect of including letter repetition commonly found within social medium text and @ impact in determining @ sentiment score @ @ major airline in malaysia @ design methodology approach @ a sentiment intensity calculator @ senti-cal @ wa developed by assigning individual weight to @ letter repetition and tested @ @ data collected @ official facebook page of @ airline @ finding @ evaluation metric indicate @ senti-cal outperforms @ baseline tool semantic orientation calculator @ so-cal @ @ @ accuracy of @ percent compared to @ percent @ so-cal @ practical implication @ a more accurate sentiment score allows airline service to easily obtain a better understanding of @ sentiment of @ customer hence providing opportunity in improving @ airline service @ originality value @ proposed mechanism calculates sentiment intensity of social medium text by assigning individual weightage to @ repeated letter and exclamation mark thus producing a more accurate sentiment score @ emerald publishing limited @ 
1168,A product affective properties identification approach based on web mining in a crowdsourcing environment,"Affective product design, which aims to satisfy customer feelings as an aspect of product quality, has attracted more and more research attention. However, conventional product design relies on surveys and user experiments to collect user evaluations, which leads to the issues that (i) consumers can only express their feelings on design attributes specified by assigners; (ii) abundant online consumer resources are neglected; and (iii) a lack of further prioritisation and re-construction of affective design properties. This study aims to develop a product affective properties identification approach. Crowdsourcing platforms have the advantage of obtaining large numbers of free consumer comments and have been utilised as data sources. Web mining and text mining are deployed to capture the crowdsourced product review resources. Then product design knowledge hierarchy is established to find design properties, while sentiment analysis was undertaken to identify affections. With the help of domain ontology to connect design properties and corresponding affections, product affective properties can be identified. Furthermore, the identified affective properties are prioritised, so as to assist in design improvement and support decision making. To illustrate the proposed approach, a pilot study on iPhone 7 was conducted, in which influential affective properties have been identified and ranked. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2018,Journal of Engineering Design,5,affective product design @ aim to satisfy customer feeling a @ aspect of product quality ha attracted more and more research attention @ however conventional product design relies on survey and user experiment to collect user evaluation @ lead to @ issue @ @ i @ consumer @ only express @ feeling on design attribute specified by assigners @ @ ii @ abundant online consumer resource @ neglected @ and @ iii @ a lack of @ prioritisation and re-construction of affective design property @ @ study aim to develop a product affective property identification approach @ crowdsourcing platform @ @ advantage of obtaining @ number of free consumer comment and @ @ utilised a data source @ web mining and text mining @ deployed to capture @ crowdsourced product review resource @ @ product design knowledge hierarchy is established to find design property @ sentiment analysis wa undertaken to identify affection @ @ @ help of domain ontology to connect design property and corresponding affection product affective property @ @ identified @ furthermore @ identified affective property @ prioritised @ a to assist in design improvement and support decision making @ to illustrate @ proposed approach a pilot study on iphone wa conducted in @ influential affective property @ @ identified and ranked @ informa uk limited trading a taylor francis group @ 
1169,Introduction to the special issue on bibliometric-enhanced information retrieval and natural language processing for digital libraries (BIRNDL),"The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometric, information retrieval (IR), text mining, and natural language processing techniques can assist to address this challenge, but have yet to be widely used in digital libraries (DL). This special issue on bibliometric-enhanced information retrieval and natural language processing for digital libraries (BIRNDL) was compiled after the first joint BIRNDL workshop that was held at the joint conference on digital libraries (JCDL 2016) in Newark, New Jersey, USA. It brought together IR and DL researchers and professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric, and recommendation techniques that can advance the state of the art in scholarly document understanding, analysis, and retrieval at scale. This special issue includes 14 papers: four extended papers originating from the first BIRNDL workshop 2016 and the BIR workshop at ECIR 2016, four extended system reports of the CL-SciSumm Shared Task 2016 and six original research papers submitted via the open call for papers. © 2017, Springer-Verlag GmbH Germany.",2018,International Journal on Digital Libraries,15,@ @ scale of scholarly publication pose a challenge @ scholar in information seeking and sensemaking @ bibliometric information retrieval @ ir @ text mining and natural language processing technique @ assist to address @ challenge @ @ yet to @ widely used in digital library @ dl @ @ @ special issue on bibliometric-enhanced information retrieval and natural language processing @ digital library @ birndl @ wa compiled @ @ first joint birndl workshop @ wa held at @ joint conference on digital library @ jcdl @ in newark @ jersey usa @ @ brought together ir and dl researcher and professional to elaborate on @ approach in natural language processing information retrieval scientometric and recommendation technique @ @ advance @ state of @ art in scholarly document understanding analysis and retrieval at scale @ @ special issue includes @ @ four extended @ originating @ @ first birndl workshop and @ bir workshop at ecir four extended system report of @ cl-scisumm shared task and six original research @ submitted via @ open call @ @ @ springer-verlag gmbh germany @ 
1170,"The rise of big data science: A survey of techniques, methods and approaches in the field of natural language processing and network theory","The continuous creation of data has posed new research challenges due to its complexity, diversity and volume. Consequently, Big Data has increasingly become a fully recognised scientific field. This article provides an overview of the current research efforts in Big Data science, with particular emphasis on its applications, as well as theoretical foundation. © 2018 by the authors.",2018,Big Data and Cognitive Computing,3,@ continuous creation of data ha posed @ research challenge due to @ complexity diversity and volume @ consequently big data ha increasingly become a fully recognised scientific field @ @ article provides @ overview of @ current research effort in big data science @ particular emphasis on @ application a well a theoretical foundation @ by @ author @ 
1172,Topic and Sentiment Words Extraction in Cross-Domain Product Reviews,"Sentiment analysis is very popular in natural language processing and text mining. The traditional sentiment analysis methods use supervised and unsupervised classifiers in a single domain and achieve good results. When training data and test data come from different domains, these methods become poor. The problem of cross-domain opinion analysis is that it is not easy to get a large number of tagged data sets and it is impossible to tag all the data in the interesting domains. We propose an extraction method for topic and sentiment words based on conditional random field and syntactic structure to analyze the sentiment orientation of Chinese product reviews. We aim to extract topic and sentiment words from target domain and identify their sentiment orientation with one or a few topic and sentiment words being tagged in the source domain and words in the target domain without any tagged information. Our experimental results show that our method is effective in cross-domain sentiment analysis. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",2018,Wireless Personal Communications,0,sentiment analysis is @ popular in natural language processing and text mining @ @ traditional sentiment analysis method use supervised and unsupervised classifier in a single domain and achieve good @ @ @ training data and test data come @ different domain @ method become poor @ @ problem of cross-domain opinion analysis is @ @ is not easy to get a @ number of tagged data set and @ is impossible to tag @ @ data in @ interesting domain @ @ propose @ extraction method @ topic and sentiment word based on conditional random field and syntactic structure to analyze @ sentiment orientation of chinese product review @ @ aim to extract topic and sentiment word @ target domain and identify @ sentiment orientation @ @ @ a @ topic and sentiment word @ tagged in @ source domain and word in @ target domain without @ tagged information @ @ experimental @ @ @ @ method is effective in cross-domain sentiment analysis @ @ science @ medium llc part of @ nature @ 
1176,Exploring the fine-grained analysis and automatic detection of irony on Twitter,"To push the state of the art in text mining applications, research in natural language processing has increasingly been investigating automatic irony detection, but manually annotated irony corpora are scarce. We present the construction of a manually annotated irony corpus based on a fine-grained annotation scheme that allows for identification of different types of irony. We conduct a series of binary classification experiments for automatic irony recognition using a support vector machine (SVM) that exploits a varied feature set and compare this method to a deep learning approach that is based on an LSTM network and (pre-trained) word embeddings. Evaluation on a held-out corpus shows that the SVM model outperforms the neural network approach and benefits from combining lexical, semantic and syntactic information sources. A qualitative analysis of the classification output reveals that the classifier performance may be further enhanced by integrating implicit sentiment information and context- and user-based features. © 2018, The Author(s).",2018,Language Resources and Evaluation,7,to push @ state of @ art in text mining application research in natural language processing ha increasingly @ investigating automatic irony detection @ manually annotated irony corpus @ scarce @ @ @ @ construction of a manually annotated irony corpus based on a fine-grained annotation scheme @ allows @ identification of different type of irony @ @ conduct a series of binary classification experiment @ automatic irony recognition @ a support vector machine @ svm @ @ exploit a varied feature set and compare @ method to a deep learning approach @ is based on @ lstm network and @ pre-trained @ word embeddings @ evaluation on a held-out corpus @ @ @ svm model outperforms @ neural network approach and benefit @ combining lexical semantic and syntactic information source @ a qualitative analysis of @ classification output reveals @ @ classifier performance may @ @ enhanced by integrating implicit sentiment information and context and user-based feature @ @ author @ s @ @ 
1177,Product recommendation model based on dynamic window for feature opinion pair extraction in online reviews,"The product feature opinions included in online customer reviews play an important role in helping users make purchase decisions. However, there is no recommendation system which mines product feature opinion from customer comments as the main data source. Furthermore, the feature opinions extraction algorithms of supervised methods focus less on Chinese sentence structure, and the extraction rules are lack of dynamic adaptability. Therefore, this paper proposes a product recommendation model based on feature opinion pairs. Firstly, combining the analysis of Chinese sentence structure and the matching relationship of feature opinion, the dynamic window is used to extract feature opinion. Secondly, aggregating feature opinions based on feature tree for product comparison and product recommendation. Finally, proposing the indicator of emotional credibility for typical review display. Compared with the reference methods using static window, our model has high recall and F-score in feature opinion extraction, which shows that it can provide reliable data sources for product recommendation based on feature opinion pairs, thus help users make purchase decisions effectively. © 2018, Editorial Board of Journal of Systems Engineering Society of China. All right reserved.",2018,Xitong Gongcheng Lilun yu Shijian/System Engineering Theory and Practice,0,@ product feature opinion included in online customer review play @ important role in helping user make purchase decision @ however @ is no recommendation system @ mine product feature opinion @ customer comment a @ main data source @ furthermore @ feature opinion extraction algorithm of supervised method focus le on chinese sentence structure and @ extraction rule @ lack of dynamic adaptability @ therefore @ @ proposes a product recommendation model based on feature opinion pair @ firstly combining @ analysis of chinese sentence structure and @ matching relationship of feature opinion @ dynamic window is used to extract feature opinion @ secondly aggregating feature opinion based on feature tree @ product comparison and product recommendation @ finally proposing @ indicator of emotional credibility @ typical review display @ compared @ @ reference method @ static window @ model ha high recall and f-score in feature opinion extraction @ @ @ @ @ provide reliable data source @ product recommendation based on feature opinion pair thus help user make purchase decision effectively @ editorial board of journal of system engineering society of china @ @ right reserved @ 
1178,QuarryMeaning: A topic model application focused on Spanish documents,"This demo shows a standalone application that allows to easily train and test a topic model. The application includes filters for reducing noise in the results. On the one hand, a base stop-list is included, but it can be complemented with a non-relevant word list proposed by user, or obtained it by means of a contrastive approach using a reference corpus. On the other hand, words having a high semantic value can be considered using POS tags. We also include a visualization in word-clouds way, where ten topics can be shown, in order to analyze in detail the results. Finally, evaluation was carried out focusing topic model for classifying documents. Our model achieved levels of precision above 95% in the test set. © 2018 Sociedad Española para el Procesamiento del Lenguaje Natural.",2018,Procesamiento de Lenguaje Natural,0,@ demo @ a standalone application @ allows to easily train and test a topic model @ @ application includes filter @ reducing noise in @ @ @ on @ @ hand a base stop-list is included @ @ @ @ complemented @ a non-relevant word list proposed by user @ obtained @ by mean of a contrastive approach @ a reference corpus @ on @ @ hand word @ a high semantic value @ @ considered @ po tag @ @ @ include a visualization in word-clouds way @ ten topic @ @ @ in order to analyze in detail @ @ @ finally evaluation wa carried @ focusing topic model @ classifying document @ @ model achieved level of precision @ in @ test set @ sociedad española para el procesamiento del lenguaje natural @ 
1180,Semantic Twitter sentiment analysis based on a fuzzy thesaurus,"We define a new, fully automated and domain-independent method for building feature vectors from Twitter text corpus for machine learning sentiment analysis based on a fuzzy thesaurus and sentiment replacement. The proposed method measures the semantic similarity of Tweets with features in the feature space instead of using terms’ presence or frequency feature vectors. Thus, we account for the sentiment of the context instead of just counting sentiment words. We use sentiment replacement to reduce the dimensionality of the feature space and a fuzzy thesaurus to incorporate semantics. Experimental results show that sentiment replacement yields up to 35% reduction in the dimensionality of the feature space. Moreover, feature vectors developed based on a fuzzy thesaurus show improvement of sentiment classification performance with multinomial naïve Bayes and support vector machine classifiers with accuracies of 83 and 85%, respectively, on the Stanford testing dataset. Incorporating the fuzzy thesaurus resulted in the best accuracy compared to the baselines with an increase greater than 3%. Comparable results were obtained with a larger dataset, the STS-Gold, indicating the robustness of the proposed method. Furthermore, comparison of results with previous work shows that the proposed method outperforms other methods reported in the literature using the same benchmark data. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",2018,Soft Computing,9,@ define a @ fully automated and domain-independent method @ building feature vector @ twitter text corpus @ machine learning sentiment analysis based on a fuzzy thesaurus and sentiment replacement @ @ proposed method measure @ semantic similarity of tweet @ feature in @ feature space instead of @ term presence @ frequency feature vector @ thus @ account @ @ sentiment of @ context instead of @ counting sentiment word @ @ use sentiment replacement to reduce @ dimensionality of @ feature space and a fuzzy thesaurus to incorporate semantics @ experimental @ @ @ sentiment replacement yield up to reduction in @ dimensionality of @ feature space @ moreover feature vector developed based on a fuzzy thesaurus @ improvement of sentiment classification performance @ multinomial naïve bayes and support vector machine classifier @ accuracy of and respectively on @ stanford testing dataset @ incorporating @ fuzzy thesaurus resulted in @ best accuracy compared to @ baseline @ @ increase greater @ @ comparable @ @ obtained @ a larger dataset @ sts-gold indicating @ robustness of @ proposed method @ furthermore comparison of @ @ previous work @ @ @ proposed method outperforms @ method reported in @ literature @ @ @ benchmark data @ springer-verlag gmbh germany part of @ nature @ 
1182,Social networking data analysis tools & challenges,"Online Social Network's (OSN) considered a spark that burst the Big Data era. The unfolding of every event, breaking new or trend flows in real time inside OSN triggering a surge of opinionated networked content. An unprecedented scale of social relationships also diffuses across this vastly interconnected system affecting public behaviors and knowledge construction. Extracting intelligence from such data has becoming a quickly widening multidisciplinary area that demands the synergy of scientific tools and expertise. Key analysis practices include social network analysis, sentiment analysis, trend analysis and collaborative recommendation. Though, both their recent advent and the fact that science is still in the frontiers of processing human-generated data, provokes the need for an update and comprehensible taxonomy of the related research. In response to this chaotic emerging science of social data, this paper provides a sophisticated classification of state-of the-art frameworks considering the diversity of practices, methods and techniques. To the best of our knowledge, this is the first attempt that illustrated the entire spectrum of social data networking analysis and their associated frameworks. The survey demonstrates challenges and future directions with a focus on text mining and the promising avenue of computational intelligence. © 2016 Elsevier B.V.",2018,Future Generation Computer Systems,49,online social network @ s @ osn @ considered a spark @ burst @ big data era @ @ unfolding of every event breaking @ @ trend flow in real time inside osn triggering a surge of opinionated networked content @ @ unprecedented scale of social relationship @ diffuses across @ vastly interconnected system affecting public behavior and knowledge construction @ extracting intelligence @ @ data ha becoming a quickly widening multidisciplinary area @ demand @ synergy of scientific tool and expertise @ key analysis practice include social network analysis sentiment analysis trend analysis and collaborative recommendation @ though @ @ recent advent and @ fact @ science is still in @ frontier of processing human-generated data provokes @ need @ @ update and comprehensible taxonomy of @ related research @ in response to @ chaotic emerging science of social data @ @ provides a sophisticated classification of state-of the-art framework considering @ diversity of practice method and technique @ to @ best of @ knowledge @ is @ first attempt @ illustrated @ entire spectrum of social data networking analysis and @ associated framework @ @ survey demonstrates challenge and future direction @ a focus on text mining and @ promising avenue of computational intelligence @ @ b @ v @ 
1183,Text mining and sentiment analysis for predicting box office success,"After emerging online communications, text mining and sentiment analysis has been frequently applied into analyzing electronic word-of-mouth. This study aims to develop a domain-specific lexicon of sentiment analysis to predict box office success in Korea film market and validate the feasibility of the lexicon. Natural language processing, a machine learning algorithm, and a lexicon-based sentiment classification method are employed. To create a movie domain sentiment lexicon, 233,631 reviews of 147 movies with popularity ratings is collected by a XML crawling package in R program. We accomplished 81.69% accuracy in sentiment classification by the Korean sentiment dictionary including 706 negative words and 617 positive words. The result showed a stronger positive relationship with box office success and consumers’ sentiment as well as a significant positive effect in the linear regression for the predicting model. In addition, it reveals emotion in the user-generated content can be a more accurate clue to predict business success. © 2018 KSII.",2018,KSII Transactions on Internet and Information Systems,5,@ emerging online communication text mining and sentiment analysis ha @ frequently applied @ analyzing electronic word-of-mouth @ @ study aim to develop a domain-specific lexicon of sentiment analysis to predict box office success in korea film market and validate @ feasibility of @ lexicon @ natural language processing a machine learning algorithm and a lexicon-based sentiment classification method @ employed @ to create a movie domain sentiment lexicon review of movie @ popularity rating is collected by a xml crawling package in r program @ @ accomplished @ accuracy in sentiment classification by @ korean sentiment dictionary including negative word and positive word @ @ @ showed a stronger positive relationship @ box office success and consumer sentiment a well a a significant positive effect in @ linear regression @ @ predicting model @ in addition @ reveals emotion in @ user-generated content @ @ a more accurate clue to predict @ success @ ksii @ 
1187,Extracting clinical knowledge from electronic medical records,"As the adoption of Electronic Medical Records (EMRs) rises in the healthcare institutions, these resources' importance increases because of the clinical information they contain about patients. However, the unstructured information in the form of clinical narratives present in those records, makes it hard to extract and structure useful clinical knowledge. This unstructured information limits the potential of the EMRs, because the clinical information these records contain can be used to perform important tasks inside healthcare institutions such as searching, summarization, decision support and statistical analysis, as well as be used to support management decisions or serve for research. These tasks can only be done if the unstructured clinical information from the narratives is properly extracted, structured and transformed in clinical knowledge. Usually, this extraction is made manually by healthcare practitioners, which is not efficient and is error-prone. This research uses Natural Language Processing (NLP) and Information Extraction (IE) techniques, in order to develop a pipeline system that can extract clinical knowledge from unstructured clinical information present in Portuguese EMRs, in an automated way, in order to help EMRs to fulfil their potential. © International Association of Engineers.",2018,IAENG International Journal of Computer Science,3,a @ adoption of electronic medical record @ emrs @ rise in @ healthcare institution @ resource @ importance increase @ of @ clinical information @ contain @ patient @ however @ unstructured information in @ form of clinical narrative @ in @ record make @ hard to extract and structure useful clinical knowledge @ @ unstructured information limit @ potential of @ emrs @ @ clinical information @ record contain @ @ used to perform important task inside healthcare institution @ a searching summarization decision support and statistical analysis a well a @ used to support management decision @ serve @ research @ @ task @ only @ done if @ unstructured clinical information @ @ narrative is properly extracted structured and transformed in clinical knowledge @ usually @ extraction is made manually by healthcare practitioner @ is not efficient and is error-prone @ @ research us natural language processing @ nlp @ and information extraction @ ie @ technique in order to develop a pipeline system @ @ extract clinical knowledge @ unstructured clinical information @ in portuguese emrs in @ automated way in order to help emrs to fulfil @ potential @ international association of engineer @ 
1192,Extending Embedding Representation by Incorporating Latent Relations,"The semantic representation of words is a fundamental task in natural language processing and text mining. Learning word embedding has shown its power on various tasks. Most studies are aimed at generating embedding representation of a word based on encoding its context information. However, many latent relations, such as co-occurring associative patterns and semantic conceptual relations, are not well considered. In this paper, we propose an extensible model to incorporate these kinds of valuable latent relations to increase the semantic relatedness of word pairs by learning word embeddings. To assess the effectiveness of our model, we conduct experiments on both information retrieval and text classification tasks. The results indicate the effectiveness of our model as well as its flexibility on different tasks. © 2013 IEEE.",2018,IEEE Access,2,@ semantic representation of word is a fundamental task in natural language processing and text mining @ learning word embedding ha @ @ power on various task @ @ study @ aimed at generating embedding representation of a word based on encoding @ context information @ however many latent relation @ a co-occurring associative pattern and semantic conceptual relation @ not well considered @ in @ @ @ propose @ extensible model to incorporate @ kind of valuable latent relation to increase @ semantic relatedness of word pair by learning word embeddings @ to ass @ effectiveness of @ model @ conduct experiment on @ information retrieval and text classification task @ @ @ indicate @ effectiveness of @ model a well a @ flexibility on different task @ @ @ 
1199,GLTM: A Global and Local Word Embedding-Based Topic Model for Short Texts,"Short texts have become a kind of prevalent source of information, and discovering topical information from short text collections is valuable for many applications. Due to the length limitation, conventional topic models based on document-level word co-occurrence information often fail to distill semantically coherent topics from short text collections. On the other hand, word embeddings as a powerful tool have been successfully applied in natural language processing. Word embeddings trained on large corpus are encoded with general semantic and syntactic information of words, and hence they can be leveraged to guide topic modeling for short text collections as supplementary information for sparse co-occurrence patterns. However, word embeddings are trained on large external corpus and the encoded information is not necessarily suitable for training data set of topic models, which is ignored by most existing models. In this article, we propose a novel global and local word embedding-based topic model (GLTM) for short texts. In the GLTM, we train global word embeddings from large external corpus and employ the continuous skip-gram model with negative sampling (SGNS) to obtain local word embeddings. Utilizing both the global and local word embeddings, the GLTM can distill semantic relatedness information between words which can be further leveraged by Gibbs sampler in the inference process to strengthen semantic coherence of topics. Compared with five state-of-the-art short text topic models on four real-world short text collections, the proposed GLTM exhibits the superiority in most cases. © 2018 IEEE.",2018,IEEE Access,12,short text @ become a kind of prevalent source of information and discovering topical information @ short text collection is valuable @ many application @ due to @ length limitation conventional topic model based on document-level word co-occurrence information often fail to distill semantically coherent topic @ short text collection @ on @ @ hand word embeddings a a powerful tool @ @ successfully applied in natural language processing @ word embeddings trained on @ corpus @ encoded @ general semantic and syntactic information of word and hence @ @ @ leveraged to guide topic modeling @ short text collection a supplementary information @ sparse co-occurrence pattern @ however word embeddings @ trained on @ external corpus and @ encoded information is not necessarily suitable @ training data set of topic model @ is ignored by @ existing model @ in @ article @ propose a novel global and local word embedding-based topic model @ gltm @ @ short text @ in @ gltm @ train global word embeddings @ @ external corpus and employ @ continuous skip-gram model @ negative sampling @ sgns @ to obtain local word embeddings @ utilizing @ @ global and local word embeddings @ gltm @ distill semantic relatedness information @ word @ @ @ @ leveraged by gibbs sampler in @ inference process to strengthen semantic coherence of topic @ compared @ five state-of-the-art short text topic model on four real-world short text collection @ proposed gltm exhibit @ superiority in @ case @ @ @ 
1203,Long-term stock index forecasting based on text mining of regulatory disclosures,"Share valuations are known to adjust to new information entering the market, such as regulatory disclosures. We study whether the language of such news items can improve short-term and especially long-term (24 months) forecasts of stock indices. For this purpose, this work utilizes predictive models suited to high-dimensional data and specifically compares techniques for data-driven and knowledge-driven dimensionality reduction in order to avoid overfitting. Our experiments, based on 75,927 ad hoc announcements from 1996–2016, reveal the following results: in the long run, text-based models succeed in reducing forecast errors below baseline predictions from historic lags at a statistically significant level. Our research provides implications to business applications of decision-support in financial markets, especially given the growing prevalence of index ETFs (exchange traded funds). © 2018 Elsevier B.V.",2018,Decision Support Systems,25,share valuation @ known to adjust to @ information entering @ market @ a regulatory disclosure @ @ study whether @ language of @ news item @ improve short-term and especially long-term @ month @ forecast of stock index @ @ @ purpose @ work utilizes predictive model suited to high-dimensional data and specifically compare technique @ data-driven and knowledge-driven dimensionality reduction in order to avoid overfitting @ @ experiment based on ad hoc announcement @ reveal @ following @ @ in @ long run text-based model succeed in reducing forecast error @ baseline prediction @ historic lag at a statistically significant level @ @ research provides implication to @ application of decision-support in financial market especially given @ growing prevalence of index etf @ exchange traded fund @ @ @ b @ v @ 
1205,Semantic concept model using Wikipedia semantic features,"Wikipedia has become a high coverage knowledge source which has been used in many research areas such as natural language processing, text mining and information retrieval. Several methods have been introduced for extracting explicit or implicit relations from Wikipedia to represent semantics of concepts/words. However, the main challenge in semantic representation is how to incorporate different types of semantic relations to capture more semantic evidences of the associations of concepts. In this article, we propose a semantic concept model that incorporates different types of semantic features extracting from Wikipedia. For each concept that corresponds to an article, four semantic features are introduced: template links, categories, salient concepts and topics. The proposed model is based on the probability distributions that are defined for these semantic features of a Wikipedia concept. The template links and categories are the document-level features which are directly extracted from the structured information included in the article. On the other hand, the salient concepts and topics are corpus-level features which are extracted to capture implicit relations among concepts. For the salient concepts feature, the distributional-based method is utilised on the hypertext corpus to extract this feature for each Wikipedia concept. Then, the probability product kernel is used to improve the weight of each concept in this feature. For the topic feature, the Labelled latent Dirichlet allocation is adapted on the supervised multi-label of Wikipedia to train the probabilistic model of this feature. Finally, we used the linear interpolation for incorporating these semantic features into the probabilistic model to estimate the semantic relation probability of the specific concept over Wikipedia articles. The proposed model is evaluated on 12 benchmark datasets in three natural language processing tasks: measuring the semantic relatedness of concepts/words in general and in the biomedical domain, semantic textual relatedness measurement and measuring the semantic compositionality of noun compounds. The model is also compared with five methods that depends on separate semantic features in Wikipedia. Experimental results show that the proposed model achieves promising results in three tasks and outperforms the baseline methods in most of the evaluation datasets. This implies that incorporation of explicit and implicit semantic features is useful for representing semantics of concepts in Wikipedia. © The Author(s) 2017.",2018,Journal of Information Science,3,wikipedia ha become a high coverage knowledge source @ ha @ used in many research area @ a natural language processing text mining and information retrieval @ several method @ @ introduced @ extracting explicit @ implicit relation @ wikipedia to represent semantics of concept word @ however @ main challenge in semantic representation is @ to incorporate different type of semantic relation to capture more semantic evidence of @ association of concept @ in @ article @ propose a semantic concept model @ incorporates different type of semantic feature extracting @ wikipedia @ @ @ concept @ corresponds to @ article four semantic feature @ introduced @ template link category salient concept and topic @ @ proposed model is based on @ probability distribution @ @ defined @ @ semantic feature of a wikipedia concept @ @ template link and category @ @ document-level feature @ @ directly extracted @ @ structured information included in @ article @ on @ @ hand @ salient concept and topic @ corpus-level feature @ @ extracted to capture implicit relation among concept @ @ @ salient concept feature @ distributional-based method is utilised on @ hypertext corpus to extract @ feature @ @ wikipedia concept @ @ @ probability product kernel is used to improve @ weight of @ concept in @ feature @ @ @ topic feature @ labelled latent dirichlet allocation is adapted on @ supervised multi-label of wikipedia to train @ probabilistic model of @ feature @ finally @ used @ linear interpolation @ incorporating @ semantic feature @ @ probabilistic model to estimate @ semantic relation probability of @ specific concept @ wikipedia article @ @ proposed model is evaluated on benchmark datasets in three natural language processing task @ measuring @ semantic relatedness of concept word in general and in @ biomedical domain semantic textual relatedness measurement and measuring @ semantic compositionality of noun compound @ @ model is @ compared @ five method @ depends on separate semantic feature in wikipedia @ experimental @ @ @ @ proposed model achieves promising @ in three task and outperforms @ baseline method in @ of @ evaluation datasets @ @ implies @ incorporation of explicit and implicit semantic feature is useful @ representing semantics of concept in wikipedia @ @ author @ s @ @ 
1206,Automated comprehensive evaluation approach for user interface satisfaction based on concurrent think-aloud method,"The concurrent think-aloud protocol (CTA) is an effective method for collecting abundant product comments related to user satisfaction during the execution of evaluation tasks. However, manual analysis of these audio comments is time-consuming and labor-intensive. This paper aims to propose an approach for automated comprehensive evaluation of user interface (UI) satisfaction. It takes advantage of text mining and sentiment analysis (SA) techniques instead of manual analysis in order to assess user comments collected by the CTA. Based on the results of the SA, the proposed approach makes use of the analytic hierarchy process (AHP) method to evaluate the overall satisfaction and support developers for UI design improvements. In order to enhance the objectivity of evaluation, a sentiment matrix originating from text mining and SA on user comments is used to replace the criteria and the relative weights of the AHP method which were previously defined by experts. A comparison between the questionnaire survey method and the proposed approach in the empirical study suggested that the latter can efficiently evaluate UI satisfaction with high accuracy and provide designers abundant and specific information directly related to defects in design. It is argued that the proposed approach could be used as an automated framework for handling any type of comments. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",2018,Universal Access in the Information Society,0,@ concurrent think-aloud protocol @ cta @ is @ effective method @ collecting abundant product comment related to user satisfaction @ @ execution of evaluation task @ however manual analysis of @ audio comment is time-consuming and labor-intensive @ @ @ aim to propose @ approach @ automated comprehensive evaluation of user interface @ ui @ satisfaction @ @ take advantage of text mining and sentiment analysis @ sa @ technique instead of manual analysis in order to ass user comment collected by @ cta @ based on @ @ of @ sa @ proposed approach make use of @ analytic hierarchy process @ ahp @ method to evaluate @ overall satisfaction and support developer @ ui design improvement @ in order to enhance @ objectivity of evaluation a sentiment matrix originating @ text mining and sa on user comment is used to replace @ criterion and @ relative weight of @ ahp method @ @ @ defined by expert @ a comparison @ @ questionnaire survey method and @ proposed approach in @ empirical study suggested @ @ latter @ efficiently evaluate ui satisfaction @ high accuracy and provide designer abundant and specific information directly related to defect in design @ @ is argued @ @ proposed approach could @ used a @ automated framework @ handling @ type of comment @ springer-verlag gmbh germany part of @ nature @ 
1209,Improving text relatedness by incorporating phrase relatedness with word relatedness,"Text is composed of words and phrases. In the bag-of-words model, phrases in text are split into words. This may discard the semantics of phrases, which, in turn, may give an inconsistent relatedness score between 2 texts. Our objective is to apply phrase relatedness in conjunction with word relatedness on the text relatedness task to improve text relatedness performance. We adopt 2 existing word relatedness measures based on Google n-gram and Global Vectors for Word Representation, respectively, and incorporate them differently with an existing Google n-gram–based phrase relatedness method to compute text relatedness. The combination of Google n-gram–based word and phrase relatedness performs better than Google n-gram–based word relatedness alone, by achieving the higher weighted mean of Pearson's r, ie, 0.639 and 0.619, respectively, on the 14 data sets from the series of Semantic Evaluation workshops SemEval-2012, SemEval-2013, and SemEval-2015. Similarly, the combination of GloVe-based word relatedness and Google n-gram–based phrase relatedness performs better than GloVe-based word relatedness alone, by achieving the higher weighted mean of Pearson's r, ie, 0.619 and 0.605, respectively, on the same 14 data sets. On the SemEval-2012, SemEval-2013, and SemEval-2015 data sets, the text relatedness results obtained from the combination of Google n-gram–based word and phrase relatedness ranked 24, 3, and 31 out of 89, 90, and 73 text relatedness systems, respectively. © 2018 Wiley Periodicals, Inc.",2018,Computational Intelligence,1,text is composed of word and phrase @ in @ bag-of-words model phrase in text @ split @ word @ @ may discard @ semantics of phrase @ in turn may give @ inconsistent relatedness score @ text @ @ objective is to apply phrase relatedness in conjunction @ word relatedness on @ text relatedness task to improve text relatedness performance @ @ adopt existing word relatedness measure based on google n-gram and global vector @ word representation respectively and incorporate @ differently @ @ existing google n-gram based phrase relatedness method to compute text relatedness @ @ combination of google n-gram based word and phrase relatedness performs better @ google n-gram based word relatedness alone by achieving @ higher weighted mean of pearson @ s r ie @ and @ respectively on @ data set @ @ series of semantic evaluation workshop semeval semeval and semeval @ similarly @ combination of glove-based word relatedness and google n-gram based phrase relatedness performs better @ glove-based word relatedness alone by achieving @ higher weighted mean of pearson @ s r ie @ and @ respectively on @ @ data set @ on @ semeval semeval and semeval data set @ text relatedness @ obtained @ @ combination of google n-gram based word and phrase relatedness ranked and @ of and text relatedness system respectively @ wiley periodical inc @ 
1221,A real-time social network-based knowledge discovery system for decision making,"The increasing amount of data in social networks has complicated data processing and interpretation. Therefore, intelligent decision-support mechanisms that have the ability to automatically extract meaning from data and interpret the opinions of people in real time have become inevitable. In this study, an intelligent multilingual decision support system was implemented, and a new algorithm that employs text mining and sentiment analysis techniques was developed to automatically interpret the opinions of social network users about the places they plan to visit. The system can be used as a baseline for sentiment analysis in social networks and can be adapted to build new systems. In this study, we set our main focus on Turkish language and show the applicability of our approach for other languages through the experiments for English language. The dataset required for the implementation of text mining techniques was created based on the venue recommendations shared on Foursquare social media platform. As a result, a contribution was made to the way the social network users make decisions without reading thousands of recommendations. Our results show that the developed system achieves classification accuracy of 84.49% for Turkish and 95% for English. Finally, the most liked or disliked foods/beverages are correctly identified for 107 out of 128 venues. © 2018, © 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",2018,Automatika,1,@ increasing amount of data in social network ha complicated data processing and interpretation @ therefore intelligent decision-support mechanism @ @ @ ability to automatically extract meaning @ data and interpret @ opinion of people in real time @ become inevitable @ in @ study @ intelligent multilingual decision support system wa implemented and a @ algorithm @ employ text mining and sentiment analysis technique wa developed to automatically interpret @ opinion of social network user @ @ place @ plan to visit @ @ system @ @ used a a baseline @ sentiment analysis in social network and @ @ adapted to build @ system @ in @ study @ set @ main focus on turkish language and @ @ applicability of @ approach @ @ language @ @ experiment @ english language @ @ dataset required @ @ implementation of text mining technique wa created based on @ venue recommendation shared on foursquare social medium platform @ a a @ a contribution wa made to @ way @ social network user make decision without reading thousand of recommendation @ @ @ @ @ @ developed system achieves classification accuracy of @ @ turkish and @ english @ finally @ @ liked @ disliked food beverage @ correctly identified @ @ of venue @ @ author @ s @ @ published by informa uk limited trading a taylor francis group @ 
1223,An investigation of social media data during a product recall scandal,"As social media has become an important part of modern daily life, users often share product opinions online and these tend to spike when large companies undergo crises. This paper investigates customer online responses to a large company crisis by uncovering hidden insights in social media comments and presents a framework for handling social media data and crisis management. Analysis of textual Facebook data from users responding to the 2013 horsemeat scandal is presented. In this study, we used a novel comprehensive data analysis framework alongside a text-mining framework to objectively classify and understand customer perceptions during this horsemeat scandal. This framework provides an effective approach for investigating customer perception during a company crisis and measures the effectiveness of crisis management practices which the company has adopted. Our analyses show that social media can provide important insights into customer behaviour during crisis communications. © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2018,Enterprise Information Systems,15,a social medium ha become @ important part of modern daily life user often share product opinion online and @ tend to spike @ @ company undergo crisis @ @ @ investigates customer online response to a @ company crisis by uncovering hidden insight in social medium comment and @ a framework @ handling social medium data and crisis management @ analysis of textual facebook data @ user responding to @ horsemeat scandal is presented @ in @ study @ used a novel comprehensive data analysis framework alongside a text-mining framework to objectively classify and understand customer perception @ @ horsemeat scandal @ @ framework provides @ effective approach @ investigating customer perception @ a company crisis and measure @ effectiveness of crisis management practice @ @ company ha adopted @ @ analysis @ @ social medium @ provide important insight @ customer behaviour @ crisis communication @ informa uk limited trading a taylor francis group @ 
1234,Deep learning for sentiment analysis: A survey,"Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state-of-the-art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis. This article is categorized under: Fundamental Concepts of Data and Knowledge > Data Concepts Algorithmic Development > Text Mining. © 2018 Wiley Periodicals, Inc.",2018,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,294,deep learning ha emerged a a powerful machine learning technique @ learns multiple layer of representation @ feature of @ data and produce state-of-the-art prediction @ @ along @ @ success of deep learning in many application domain deep learning is @ used in sentiment analysis in recent year @ @ @ give @ overview of deep learning and @ provides a comprehensive survey of @ current application in sentiment analysis @ @ article is categorized @ @ fundamental concept of data and knowledge data concept algorithmic development text mining @ wiley periodical inc @ 
1235,An Attention-Based Approach for Chemical Compound and Drug Named Entity Recognition,"Recognizing chemical compound and drug name from unstructured data in the field of biomedical text mining is of great significance. The current popular approaches are based on CRF model which needs large amounts of hand-crafted features, and these approaches inevitably have the tagging non-consistency problem (the same mentions in a document are tagged different labels). In this paper, we propose an attention-based BiLSTM-CRF architecture to mitigate these aforementioned drawbacks. First, word embedding is obtained from vast amounts of unlabeled biomedical text. Then the characters of current word are fed to a BiLSTM layer to learn the character representation of this word. After this, word and character representations are transformed to another BiLSTM layer and the current adjacency context representation of this word is generated. Then we use attention mechanism to obtain the current word's context at document level on the basis of the adjacency context of all words in this document and the current word. At last, a CRF layer is used to predict the label sequence of this document according to the integration of the current adjacency context and the document-level context. Experimental results show that our method improves the consistency of mention's label in the same document, and it can also achieve better performance (an F-score of 90.77%) than the state-of-the-art methods on the BioCreative IV CHEMDNER corpus. © 2018, Science Press. All right reserved.",2018,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,3,recognizing chemical compound and drug name @ unstructured data in @ field of biomedical text mining is of great significance @ @ current popular approach @ based on crf model @ need @ amount of hand-crafted feature and @ approach inevitably @ @ tagging non-consistency problem @ @ @ mention in a document @ tagged different label @ @ in @ @ @ propose @ attention-based bilstm-crf architecture to mitigate @ aforementioned drawback @ first word embedding is obtained @ vast amount of unlabeled biomedical text @ @ @ character of current word @ fed to a bilstm layer to learn @ character representation of @ word @ @ @ word and character representation @ transformed to another bilstm layer and @ current adjacency context representation of @ word is generated @ @ @ use attention mechanism to obtain @ current word @ s context at document level on @ basis of @ adjacency context of @ word in @ document and @ current word @ at last a crf layer is used to predict @ label sequence of @ document according to @ integration of @ current adjacency context and @ document-level context @ experimental @ @ @ @ method improves @ consistency of mention @ s label in @ @ document and @ @ @ achieve better performance @ @ f-score of @ @ @ @ state-of-the-art method on @ biocreative @ chemdner corpus @ science @ @ @ right reserved @ 
1237,A survey on automatic detection of hate speech in text,"The scientific study of hate speech, from a computer science point of view, is recent. This survey organizes and describes the current state of the field, providing a structured overview of previous approaches, including core algorithms, methods, and main features used. This work also discusses the complexity of the concept of hate speech, defined in many platforms and contexts, and provides a unifying definition. This area has an unquestionable potential for societal impact, particularly in online communities and digital media platforms. The development and systematization of shared resources, such as guidelines, annotated datasets in multiple languages, and algorithms, is a crucial step in advancing the automatic detection of hate speech. © 2018 ACM.",2018,ACM Computing Surveys,103,@ scientific study of hate speech @ a computer science point of view is recent @ @ survey organizes and describes @ current state of @ field providing a structured overview of previous approach including core algorithm method and main feature used @ @ work @ discus @ complexity of @ concept of hate speech defined in many platform and context and provides a unifying definition @ @ area ha @ unquestionable potential @ societal impact particularly in online community and digital medium platform @ @ development and systematization of shared resource @ a guideline annotated datasets in multiple language and algorithm is a crucial step in advancing @ automatic detection of hate speech @ acm @ 
1238,Extracting semantic relations from the Quranic Arabic based on Arabic conjunctive patterns,"There is an immense need for information systems that rely on Arabic Quranic ontologies to provide a precise and comprehensive knowledge to the world. Since semantic relations are a vital component in any ontology and many applications in Natural Language Processing strongly depend on them, this motivates the development of our approach to extract semantic relations from the Quranic Arabic Corpus, written in Arabic script, and enrich the automatic construction of Quran ontology. We focus on semantic relations resulting from proposed conjunctive patterns which include two terms with the conjunctive AND enclosed in between. The strength of each relation is measured based on the correlation coefficient. Finally, we evaluate the significance of this method by using hypotheses testing and Student t-test. The obtained results are very promising since we combine an accurate Arabic grammar with strong statistical techniques to prove the existence and measure the strength of this type of semantic relations. © 2017 The Authors",2018,Journal of King Saud University - Computer and Information Sciences,9,@ is @ immense need @ information system @ rely on arabic quranic ontology to provide a precise and comprehensive knowledge to @ world @ since semantic relation @ a vital component in @ ontology and many application in natural language processing strongly depend on @ @ motivates @ development of @ approach to extract semantic relation @ @ quranic arabic corpus written in arabic script and enrich @ automatic construction of quran ontology @ @ focus on semantic relation resulting @ proposed conjunctive pattern @ include @ term @ @ conjunctive and enclosed in @ @ @ strength of @ relation is measured based on @ correlation coefficient @ finally @ evaluate @ significance of @ method by @ hypothesis testing and student t-test @ @ obtained @ @ @ promising since @ combine @ accurate arabic grammar @ strong statistical technique to prove @ existence and measure @ strength of @ type of semantic relation @ @ author
1239,An environment for collective perception based on fuzzy and semantic approaches,"This work proposes a software environment implementing a methodology for acquiring and exploiting the collective perception (CP) of Points of Interests (POIs) in a Smart City, which is meant to support decision makers in urban planning and management. This environment relies upon semantic knowledge discovery techniques and fuzzy computational approaches, including natural language processing, sentiment analysis, POI signatures and Fuzzy Cognitive Maps, turning them into a cohesive architectural blend in order to effectively gather the realistic perception of a user community towards given areas and attractions of a Smart City. The environment has been put to the test via a thorough experimentation against a massive user base of an online community with respect to a large metropolitan city (the City of Naples). Such an experimentation yielded consistent results, useful for providing decision makers with a clear awareness of the positive as well as critical aspects of urban areas, and thus helping them shape the measures to be taken for an improved city management and development. © 2018 De Gruyter Open Ltd. All rights reserved.",2018,Journal of Artificial Intelligence and Soft Computing Research,9,@ work proposes a software environment implementing a methodology @ acquiring and exploiting @ collective perception @ cp @ of point of interest @ poi @ in a smart city @ is meant to support decision maker in urban planning and management @ @ environment relies upon semantic knowledge discovery technique and fuzzy computational approach including natural language processing sentiment analysis poi signature and fuzzy cognitive map turning @ @ a cohesive architectural blend in order to effectively gather @ realistic perception of a user community towards given area and attraction of a smart city @ @ environment ha @ put to @ test via a thorough experimentation @ a massive user base of @ online community @ respect to a @ metropolitan city @ @ city of naples @ @ @ @ experimentation yielded consistent @ useful @ providing decision maker @ a clear awareness of @ positive a well a critical aspect of urban area and thus helping @ shape @ measure to @ taken @ @ improved city management and development @ de gruyter open ltd @ @ right reserved @ 
1241,SemRe-Rank: Improving automatic term extraction by incorporating semantic relatedness with personalised page rank,"Automatic Term Extraction (ATE) dealswith the extraction of terminology from a domain specific corpus, and has long been an established research area in data and knowledge acquisition. ATE remains a challenging task as it is known that there is no existing ATE methods that can consistently outperform others in any domain. This work adopts a refreshed perspective to this problem: instead of searching for such a 'one-size-fit-all' solution that may never exist, we propose to develop generic methods to 'enhance' existing ATE methods. We introduce SemRe-Rank, the first method based on this principle, to incorporate semantic relatedness-an often overlooked venue-into an existing ATE method to further improve its performance. SemRe-Rank incorporates word embeddings into a personalised PageRank process to compute 'semantic importance' scores for candidate terms from a graph of semantically related words (nodes), which are then used to revise the scores of candidate terms computed by a base ATE algorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on four datasets of diverse nature, it is shown to have achieved widespread improvement over all base methods and across all datasets, with up to 15 percentage points when measured by the Precision in the top ranked K candidate terms (the average for a set of K's), or up to 28 percentage points in F1 measured at a K that equals to the expected real terms in the candidates (F1 in short). Compared to an alternative approach built on the well-known TextRank algorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision at top K, or up to 17 points in F1. © 2018 ACM.",2018,ACM Transactions on Knowledge Discovery from Data,11,automatic term extraction @ ate @ dealswith @ extraction of terminology @ a domain specific corpus and ha long @ @ established research area in data and knowledge acquisition @ ate remains a challenging task a @ is known @ @ is no existing ate method @ @ consistently outperform others in @ domain @ @ work adopts a refreshed perspective to @ problem @ instead of searching @ @ a @ one-size-fit-all @ solution @ may never exist @ propose to develop generic method to @ enhance @ existing ate method @ @ introduce semre-rank @ first method based on @ principle to incorporate semantic relatedness-an often overlooked venue-into @ existing ate method to @ improve @ performance @ semre-rank incorporates word embeddings @ a personalised pagerank process to compute @ semantic importance @ score @ candidate term @ a graph of semantically related word @ node @ @ @ @ used to revise @ score of candidate term computed by a base ate algorithm @ extensively evaluated @ state-of-the-art base ate method on four datasets of diverse nature @ is @ to @ achieved widespread improvement @ @ base method and across @ datasets @ up to percentage point @ measured by @ precision in @ top ranked k candidate term @ @ average @ a set of k @ s @ @ up to percentage point in f measured at a k @ equal to @ expected real term in @ candidate @ f in short @ @ compared to @ alternative approach built on @ well-known textrank algorithm semre-rank @ potentially outperform by up to point in precision at top k @ up to point in f @ acm @ 
1242,CLASENTI: A class-specific sentiment analysis framework,"Arabic text sentiment analysis suffers from low accuracy due to Arabic-specific challenges (e.g., limited resources, morphological complexity, and dialects) and general linguistic issues (e.g., fuzziness, implicit sentiment, sarcasm, and spam). The limited resources problem requires efforts to build new and improved Arabic corpora and lexica. We propose a class-specific sentiment analysis (CLASENTI) framework. The framework includes a new annotation approach to build multi-faceted Arabic corpus and lexicon allowing for simultaneous annotation of different facets, including domains, dialects, linguistic issues, and polarity strengths. Each of these facets has multiple classes (e.g., the nine classes representing dialects found in the Arab world). The new corpus and lexicon annotations facilitate the development of new class-specific classification models and polarity strength calculation. For the new sentiment classification models, we propose a hybrid model combining corpus-based and lexicon-based models. The corpus-based model has two interrelated phases to build; (1) full-corpus classification models for all facets; and (2) class-specific models trained on filtered subsets of the corpus according to the performances of the full-corpus models. To calculate polarity strengths, the lexicon-based model filters the annotated lexicon based on the specific classes of the domain and dialect. As a case study, we collect and annotate 15274 reviews from various sources, including surveys, Facebook comments, and Twitter posts, pertaining to governmental services. In addition, we develop a new web-based application to apply the proposed framework on the case study. CLASENTI framework reaches up to 95% accuracy and 93% F1-Score surpassing the best-known sentiment classifiers implemented in Scikit-learn library that achieve 82% accuracy and 81% F1-Score for Arabic when tested on the same dataset. © 2018 ACM.",2018,ACM Transactions on Asian and Low-Resource Language Information Processing,6,arabic text sentiment analysis suffers @ low accuracy due to arabic-specific challenge @ e @ g @ limited resource morphological complexity and dialect @ and general linguistic issue @ e @ g @ fuzziness implicit sentiment sarcasm and spam @ @ @ limited resource problem requires effort to build @ and improved arabic corpus and lexica @ @ propose a class-specific sentiment analysis @ clasenti @ framework @ @ framework includes a @ annotation approach to build multi-faceted arabic corpus and lexicon allowing @ simultaneous annotation of different facet including domain dialect linguistic issue and polarity strength @ @ of @ facet ha multiple class @ e @ g @ @ nine class representing dialect found in @ arab world @ @ @ @ corpus and lexicon annotation facilitate @ development of @ class-specific classification model and polarity strength calculation @ @ @ @ sentiment classification model @ propose a hybrid model combining corpus-based and lexicon-based model @ @ corpus-based model ha @ interrelated phase to build @ @ @ full-corpus classification model @ @ facet @ and @ @ class-specific model trained on filtered subset of @ corpus according to @ performance of @ full-corpus model @ to calculate polarity strength @ lexicon-based model filter @ annotated lexicon based on @ specific class of @ domain and dialect @ a a case study @ collect and annotate review @ various source including survey facebook comment and twitter post pertaining to governmental service @ in addition @ develop a @ web-based application to apply @ proposed framework on @ case study @ clasenti framework reach up to accuracy and f score surpassing @ best-known sentiment classifier implemented in scikit-learn library @ achieve accuracy and f score @ arabic @ tested on @ @ dataset @ acm @ 
1243,A multiple criteria credit rating approach utilizing social media data,"Credit rating is a process for building a classification system for credit lenders to characterize current or potential credit borrowers. By such a process, financial institutions classify borrowers for lending decision by evaluating their financial and/or nonfinancial performances. Recently, use of social media data has emerged an important source of information. Accordingly, social media data can be very useful in evaluating companies’ credibility when financial or non-financial assessments are missing or unreliable as well as when credit analyzers’ subjective perceptions manipulate the decision. In this study, a multiple criteria credit rating approach is proposed to determine companies’ credibility level utilizing social media data as well as financial measures. Additionally, to strengthen the lender's interpretation and inference competency, ratings are represented with a risk distribution based on cumulative belief degrees. Sentiment analysis, a web mining and text classification method, is used to collect social media data on Twitter. Importance of criteria is revealed through pairwise comparisons. Companies’ performance scores and ratings are obtained by a cumulative belief degree approach. The proposed approach is applied to 64 companies. Results indicate that social media provides valuable information to determine companies’ creditability. However credit ratings tend to decrease when social media data is considered. © 2018 Elsevier B.V.",2018,Data and Knowledge Engineering,7,credit rating is a process @ building a classification system @ credit lender to characterize current @ potential credit borrower @ by @ a process financial institution classify borrower @ lending decision by evaluating @ financial and @ nonfinancial performance @ recently use of social medium data ha emerged @ important source of information @ accordingly social medium data @ @ @ useful in evaluating company credibility @ financial @ non-financial assessment @ missing @ unreliable a well a @ credit analyzer subjective perception manipulate @ decision @ in @ study a multiple criterion credit rating approach is proposed to determine company credibility level utilizing social medium data a well a financial measure @ additionally to strengthen @ lender @ s interpretation and inference competency rating @ represented @ a risk distribution based on cumulative belief degree @ sentiment analysis a web mining and text classification method is used to collect social medium data on twitter @ importance of criterion is revealed @ pairwise comparison @ company performance score and rating @ obtained by a cumulative belief degree approach @ @ proposed approach is applied to company @ @ indicate @ social medium provides valuable information to determine company creditability @ however credit rating tend to decrease @ social medium data is considered @ @ b @ v @ 
1245,Hierarchical partitioning of the output space in multi-label data,"Hierarchy Of Multi-label classifiERs (HOMER) is a multi-label learning algorithm that breaks the initial learning task to several, easier sub-tasks by first constructing a hierarchy of labels from a given label set and secondly employing a given base multi-label classifier (MLC) to the resulting sub-problems. The primary goal is to effectively address class imbalance and scalability issues that often arise in real-world multi-label classification problems. In this work, we present the general setup for a HOMER model and a simple extension of the algorithm that is suited for MLCs that output rankings. Furthermore, we provide a detailed analysis of the properties of the algorithm, both from an aspect of effectiveness and computational complexity. A secondary contribution involves the presentation of a balanced variant of the k means algorithm, which serves in the first step of the label hierarchy construction. We conduct extensive experiments on six real-world data sets, studying empirically HOMER's parameters and providing examples of instantiations of the algorithm with different clustering approaches and MLCs, The empirical results demonstrate a significant improvement over the given base MLC. © 2018 Elsevier B.V.",2018,Data and Knowledge Engineering,4,hierarchy of multi-label classifier @ homer @ is a multi-label learning algorithm @ break @ initial learning task to several easier sub-tasks by first constructing a hierarchy of label @ a given label set and secondly employing a given base multi-label classifier @ mlc @ to @ resulting sub-problems @ @ primary goal is to effectively address class imbalance and scalability issue @ often arise in real-world multi-label classification problem @ in @ work @ @ @ general setup @ a homer model and a simple extension of @ algorithm @ is suited @ mlcs @ output ranking @ furthermore @ provide a detailed analysis of @ property of @ algorithm @ @ @ aspect of effectiveness and computational complexity @ a secondary contribution involves @ presentation of a balanced variant of @ k mean algorithm @ serf in @ first step of @ label hierarchy construction @ @ conduct extensive experiment on six real-world data set studying empirically homer @ s parameter and providing example of instantiation of @ algorithm @ different clustering approach and mlcs @ empirical @ demonstrate a significant improvement @ @ given base mlc @ @ b @ v @ 
1246,Towards an Improvement of Bug Report Summarization Using Two-Layer Semantic Information,"Bug report summarization has been explored in past research to help developers comprehend important information for bug resolution process. As text mining technology advances, many summarization approaches have been proposed to provide substantial summaries on bug reports. In this paper, we propose an enhanced summarization approach called TSM by first extending a semantic model used in AUSUM with the anthropogenic and procedural information in bug reports and then integrating the extended semantic model with the shallow textual information used in BRC. We have conducted experiments with a dataset of realistic software projects. Compared with the baseline approaches BRC and AUSUM, TSM demonstrates the enhanced performance in achieving relative improvements of 34.3% and 7.4% in the F1 measure, respectively. The experimental results show that TSM can effectively improve the performance. © Copyright 2018 The Institute of Electronics Information and Communication Engineers.",2018,IEICE Transactions on Information and Systems,4,bug report summarization ha @ explored in past research to help developer comprehend important information @ bug resolution process @ a text mining technology advance many summarization approach @ @ proposed to provide substantial summary on bug report @ in @ @ @ propose @ enhanced summarization approach called tsm by first extending a semantic model used in ausum @ @ anthropogenic and procedural information in bug report and @ integrating @ extended semantic model @ @ shallow textual information used in brc @ @ @ conducted experiment @ a dataset of realistic software project @ compared @ @ baseline approach brc and ausum tsm demonstrates @ enhanced performance in achieving relative improvement of @ and @ in @ f measure respectively @ @ experimental @ @ @ tsm @ effectively improve @ performance @ @ @ institute of electronics information and communication engineer @ 
1259,Auto-generated aterials database of Curie and Neél temperatures via semisupervised relationship extraction,"Large auto-generated databases of magnetic materials properties have the potential for great utility in materials science research. This article presents an auto-generated database of 39,822 records containing chemical compounds and their associated Curie and Neél magnetic phase transition temperatures. The database was produced using natural language processing and semi-supervised quaternary relationship extraction, applied to a corpus of 68,078 chemistry and physics articles. Evaluation of the database shows an estimated overall precision of 73%. Therein, records processed with the text-mining toolkit, ChemDataExtractor, were assisted by a modified Snowball algorithm, whose original binary relationship extraction capabilities were extended to quaternary relationship extraction. Consequently, its machine learning component can now train with ≤ 500 seeds, rather than the 4,000 originally used. Data processed with the modified Snowball algorithm affords 82% precision. Database records are available in MongoDB, CSV and JSON formats which can easily be read using Python, R, Java and MatLab. This makes the database easy to query for tackling big-data materials science initiatives and provides a basis for magnetic materials discovery. © 2018 The Author(s).",2018,Scientific Data,23,@ auto-generated database of magnetic material property @ @ potential @ great utility in material science research @ @ article @ @ auto-generated database of record containing chemical compound and @ associated curie and neél magnetic phase transition temperature @ @ database wa produced @ natural language processing and semi-supervised quaternary relationship extraction applied to a corpus of chemistry and physic article @ evaluation of @ database @ @ estimated overall precision of @ therein record processed @ @ text-mining toolkit chemdataextractor @ assisted by a modified snowball algorithm whose original binary relationship extraction capability @ extended to quaternary relationship extraction @ consequently @ machine learning component @ now train @ seed rather @ @ originally used @ data processed @ @ modified snowball algorithm affords precision @ database record @ available in mongodb csv and json format @ @ easily @ read @ python r java and matlab @ @ make @ database easy to query @ tackling big-data material science initiative and provides a basis @ magnetic material discovery @ @ author @ s @ @ 
1262,Using news to predict Chinese medicinal material price index movements,"Purpose: The purpose of this paper is to propose an approach for predicting the movements of Chinese medicinal material price indexes using news based on text mining. Design/methodology/approach: A research framework and three major methods, namely, domain dictionary construction, market convergence time calculation and dimensionality reduction integrating semantic analysis, are proposed for the approach. The proposed approach is applied in practice for predicting the price index movements of the top ten Chinese medicinal materials that receive the greatest media attention. Findings: A set of experiments performed herein show that a predictive relationship exists between the news and the commodity market and that each of the three major methods improves the forecasting performance. Research limitations/implications: Because the field of Chinese medicinal materials lacks a corpus that can be used for sentiment analysis, the accuracy of a trained automatic sentiment classifier is lower than obtained by a manual method, which can cause the calculated convergence result to be inaccurate, thus affecting the final prediction model. The manual method of having people label news decreases the proposed method’s aspects of being intelligent and automatic. Practical implications: Using the method proposed herein to predict the trends in Chinese medicinal materials prices helps farmers arrange a reasonable planting plan to pursue their best interests. Social implications: The method proposed herein to predict the trends in the prices of Chinese medicinal materials is conducive to the government arranging planned drug availabilities in order to avoid disasters in which herbs are looted. Originality/value: The produced prediction result is meaningful in supporting farmers and investors to make better decisions in growing and trading Chinese medicinal material, which leads to financial returns on investments and the avoidance of severe losses. © 2018, Emerald Publishing Limited.",2018,Industrial Management and Data Systems,1,purpose @ @ purpose of @ @ is to propose @ approach @ predicting @ movement of chinese medicinal material price index @ news based on text mining @ design methodology approach @ a research framework and three major method namely domain dictionary construction market convergence time calculation and dimensionality reduction integrating semantic analysis @ proposed @ @ approach @ @ proposed approach is applied in practice @ predicting @ price index movement of @ top ten chinese medicinal material @ receive @ greatest medium attention @ finding @ a set of experiment performed herein @ @ a predictive relationship exists @ @ news and @ commodity market and @ @ of @ three major method improves @ forecasting performance @ research limitation implication @ @ @ field of chinese medicinal material lack a corpus @ @ @ used @ sentiment analysis @ accuracy of a trained automatic sentiment classifier is lower @ obtained by a manual method @ @ cause @ calculated convergence @ to @ inaccurate thus affecting @ final prediction model @ @ manual method of @ people label news decrease @ proposed method s aspect of @ intelligent and automatic @ practical implication @ @ @ method proposed herein to predict @ trend in chinese medicinal material price help farmer arrange a reasonable planting plan to pursue @ best interest @ social implication @ @ method proposed herein to predict @ trend in @ price of chinese medicinal material is conducive to @ government arranging planned drug availability in order to avoid disaster in @ herb @ looted @ originality value @ @ produced prediction @ is meaningful in supporting farmer and investor to make better decision in growing and trading chinese medicinal material @ lead to financial return on investment and @ avoidance of severe loss @ emerald publishing limited @ 
1290,Making progress with the automation of systematic reviews: Principles of the International Collaboration for the Automation of Systematic Reviews (ICASR),"Systematic reviews (SR) are vital to health care, but have become complicated and time-consuming, due to the rapid expansion of evidence to be synthesised. Fortunately, many tasks of systematic reviews have the potential to be automated or may be assisted by automation. Recent advances in natural language processing, text mining and machine learning have produced new algorithms that can accurately mimic human endeavour in systematic review activity, faster and more cheaply. Automation tools need to be able to work together, to exchange data and results. Therefore, we initiated the International Collaboration for the Automation of Systematic Reviews (ICASR), to successfully put all the parts of automation of systematic review production together. The first meeting was held in Vienna in October 2015. We established a set of principles to enable tools to be developed and integrated into toolkits. This paper sets out the principles devised at that meeting, which cover the need for improvement in efficiency of SR tasks, automation across the spectrum of SR tasks, continuous improvement, adherence to high quality standards, flexibility of use and combining components, the need for a collaboration and varied skills, the desire for open source, shared code and evaluation, and a requirement for replicability through rigorous and open evaluation. Automation has a great potential to improve the speed of systematic reviews. Considerable work is already being done on many of the steps involved in a review. The 'Vienna Principles' set out in this paper aim to guide a more coordinated effort which will allow the integration of work by separate teams and build on the experience, code and evaluations done by the many teams working across the globe. © 2018 The Author(s).",2018,Systematic Reviews,31,systematic review @ sr @ @ vital to health care @ @ become complicated and time-consuming due to @ rapid expansion of evidence to @ synthesised @ fortunately many task of systematic review @ @ potential to @ automated @ may @ assisted by automation @ recent advance in natural language processing text mining and machine learning @ produced @ algorithm @ @ accurately mimic human endeavour in systematic review activity faster and more cheaply @ automation tool need to @ able to work together to exchange data and @ @ therefore @ initiated @ international collaboration @ @ automation of systematic review @ icasr @ to successfully put @ @ part of automation of systematic review production together @ @ first meeting wa held in vienna in october @ @ established a set of principle to enable tool to @ developed and integrated @ toolkits @ @ @ set @ @ principle devised at @ meeting @ cover @ need @ improvement in efficiency of sr task automation across @ spectrum of sr task continuous improvement adherence to high quality standard flexibility of use and combining component @ need @ a collaboration and varied skill @ desire @ open source shared code and evaluation and a requirement @ replicability @ rigorous and open evaluation @ automation ha a great potential to improve @ speed of systematic review @ considerable work is already @ done on many of @ step involved in a review @ @ @ vienna principle @ set @ in @ @ aim to guide a more coordinated effort @ @ allow @ integration of work by separate team and build on @ experience code and evaluation done by @ many team working across @ globe @ @ author @ s @ @ 
1296,Rough Set Knowledge Discovery Based Open Domain Chinese Question Answering Retrieval,"In the information retrieval (IR) based open domain question answering system (QA system), the main principle is that first use the semantic tools and knowledgebase to get the semantic and knowledge information, then calculate the matching value of both semantic and knowledge. However, in some practical applications of Chinese question answering, because of the uncertainty of both the Chinese language representation and the Chinese knowledge representation, the current methods are not very effective. To solve this problem, a rough set knowledge discovery based Chinese question answering method is proposed in this paper. It uses the method of rough set equivalence partitioning to represent the rough set knowledge of the QA pairs, then uses the idea of attribute reduction to mine out the upper approximation representations of all the knowledge items. Based on the rough set QA knowledgebase, the knowledge match value of a QA pair can be calculated as a kind of knowledge item similarity. After all the knowledge similarities of one question and its answer candidates are given, the final matching values which combines rough set knowledge similarity with traditional sentence similarity can be used to rank the answer candidates. The experiment shows that the proposed method can improve the MAP and MRR compared with the baseline information retrieval methods. © 2018, Science Press. All right reserved.",2018,Jisuanji Yanjiu yu Fazhan/Computer Research and Development,1,in @ information retrieval @ ir @ based open domain question answering system @ qa system @ @ main principle is @ first use @ semantic tool and knowledgebase to get @ semantic and knowledge information @ calculate @ matching value of @ semantic and knowledge @ however in some practical application of chinese question answering @ of @ uncertainty of @ @ chinese language representation and @ chinese knowledge representation @ current method @ not @ effective @ to solve @ problem a rough set knowledge discovery based chinese question answering method is proposed in @ @ @ @ us @ method of rough set equivalence partitioning to represent @ rough set knowledge of @ qa pair @ us @ idea of attribute reduction to mine @ @ upper approximation representation of @ @ knowledge item @ based on @ rough set qa knowledgebase @ knowledge match value of a qa pair @ @ calculated a a kind of knowledge item similarity @ @ @ @ knowledge similarity of @ question and @ answer candidate @ given @ final matching value @ combine rough set knowledge similarity @ traditional sentence similarity @ @ used to rank @ answer candidate @ @ experiment @ @ @ proposed method @ improve @ map and mrr compared @ @ baseline information retrieval method @ science @ @ @ right reserved @ 
1297,Sentiment analysis: An automatic contextual analysis and ensemble clustering approach and comparison,"Product reviews are one of the most important resources to determine public sentiment. The existing literature on review sentiment analysis mostly utilizes supervised models, which usually suffer from domain-dependency and require expensive manual labelling effort to provide training data. This article addresses these issues by describing a completely automatic and unsupervised approach to sentiment analysis. The method consists of two phases, which are contextual analysis and unsupervised ensemble learning. In the implementation of both phases, a sentiment lexicon, SentiWordNet, is deployed. Using effective contextual procedures and modifying the base learning component (the k-means algorithm) results in developing a successful approach to sentiment analysis which can overcome the domain-dependency and the labelling cost problems. The results show that the proposed nonrandom initialization of k-means yields a significant improvement compared to other algorithms. In terms of accuracy and performance, the proposed method is effective compared to supervised and unsupervised approaches. We also introduce new sentiment analysis problems relating to Australian airlines and home builders which could be potential benchmark problems in the sentiment analysis field. Our experiments on datasets from different domains show that contextual analysis and the ensemble phases improve the clustering performance in term of accuracy, stability and generalizability. © 2018 Elsevier B.V.",2018,Data and Knowledge Engineering,17,product review @ @ of @ @ important resource to determine public sentiment @ @ existing literature on review sentiment analysis mostly utilizes supervised model @ usually suffer @ domain-dependency and require expensive manual labelling effort to provide training data @ @ article address @ issue by describing a completely automatic and unsupervised approach to sentiment analysis @ @ method consists of @ phase @ @ contextual analysis and unsupervised ensemble learning @ in @ implementation of @ phase a sentiment lexicon sentiwordnet is deployed @ @ effective contextual procedure and modifying @ base learning component @ @ k-means algorithm @ @ in developing a successful approach to sentiment analysis @ @ overcome @ domain-dependency and @ labelling cost problem @ @ @ @ @ @ proposed nonrandom initialization of k-means yield a significant improvement compared to @ algorithm @ in term of accuracy and performance @ proposed method is effective compared to supervised and unsupervised approach @ @ @ introduce @ sentiment analysis problem relating to australian airline and home builder @ could @ potential benchmark problem in @ sentiment analysis field @ @ experiment on datasets @ different domain @ @ contextual analysis and @ ensemble phase improve @ clustering performance in term of accuracy stability and generalizability @ @ b @ v @ 
1298,A graph based keyword extraction model using collective node weight,"In the recent times, a huge amount of text is being generated for social purposes on twitter social networking site. Summarizing and analysing of twitter content is an important task as it benefits many applications such as information retrieval, automatic indexing, automatic classification, automatic clustering, automatic filtering etc. One of the most important tasks in analyzing tweets is automatic keyword extraction. There are some graph based approaches for keyword extraction which determine keywords only based on centrality measure. However, the importance of a keyword in twitter depends on various parameters such as frequency, centrality, position and strength of neighbors of the keyword. Therefore, this paper proposes a novel unsupervised graph based keyword extraction method called Keyword Extraction using Collective Node Weight (KECNW) which determines the importance of a keyword by collectively taking various influencing parameters. The KECNW is based on Node Edge rank centrality with node weight depending on various parameters. The model is validated with five datasets: Uri Attack, American Election, Harry Potter, IPL and Donald Trump. The result of KECMW is compared with three existing models. It is observed from the experimental results that the proposed method is far better than the others. The performances are shown in terms of precision, recall and F-measure. © 2017",2018,Expert Systems with Applications,37,in @ recent time a huge amount of text is @ generated @ social purpose on twitter social networking site @ summarizing and analysing of twitter content is @ important task a @ benefit many application @ a information retrieval automatic indexing automatic classification automatic clustering automatic filtering etc @ @ of @ @ important task in analyzing tweet is automatic keyword extraction @ @ @ some graph based approach @ keyword extraction @ determine keywords only based on centrality measure @ however @ importance of a keyword in twitter depends on various parameter @ a frequency centrality position and strength of neighbor of @ keyword @ therefore @ @ proposes a novel unsupervised graph based keyword extraction method called keyword extraction @ collective node weight @ kecnw @ @ determines @ importance of a keyword by collectively taking various influencing parameter @ @ kecnw is based on node edge rank centrality @ node weight depending on various parameter @ @ model is validated @ five datasets @ uri attack american election harry potter ipl and donald trump @ @ @ of kecmw is compared @ three existing model @ @ is observed @ @ experimental @ @ @ proposed method is far better @ @ others @ @ performance @ @ in term of precision recall and f-measure @ 
1302,Analysis of document pre-processing effects in text and opinion mining,"Typically, textual information is available as unstructured data, which require processing so that data mining algorithms can handle such data; this processing is known as the pre-processing step in the overall text mining process. This paper aims at analyzing the strong impact that the pre-processing step has on most mining tasks. Therefore, we propose a methodology to vary distinct combinations of pre-processing steps and to analyze which pre-processing combination allows high precision. In order to show different combinations of pre-processing methods, experiments were performed by comparing some combinations such as stemming, term weighting, term elimination based on low frequency cut and stop words elimination. These combinations were applied in text and opinion mining tasks, from which correct classification rates were computed to highlight the strong impact of the pre-processing combinations. Additionally, we provide graphical representations from each pre-processing combination to show how visual approaches are useful to show the processing effects on document similarities and group formation (i.e., cohesion and separation). © 2018 by the authors.",2018,Information (Switzerland),11,typically textual information is available a unstructured data @ require processing @ @ data mining algorithm @ handle @ data @ @ processing is known a @ pre-processing step in @ overall text mining process @ @ @ aim at analyzing @ strong impact @ @ pre-processing step ha on @ mining task @ therefore @ propose a methodology to vary distinct combination of pre-processing step and to analyze @ pre-processing combination allows high precision @ in order to @ different combination of pre-processing method experiment @ performed by comparing some combination @ a stemming term weighting term elimination based on low frequency cut and stop word elimination @ @ combination @ applied in text and opinion mining task @ @ correct classification rate @ computed to highlight @ strong impact of @ pre-processing combination @ additionally @ provide graphical representation @ @ pre-processing combination to @ @ visual approach @ useful to @ @ processing effect on document similarity and group formation @ i @ e @ cohesion and separation @ @ by @ author @ 
1310,Teaching data science: an objective approach to curriculum validation,"Emerging careers in technology-focused fields such as data science coupled with necessary graduate outcomes mandate the need for a truly interdisciplinary pedagogical approach. However, the rapid pace of curriculum development in this field of inquiry has meant that curricula across universities has largely evolved in line with the internal disciplinary strengths of each institution rather than in response to the needs of graduates. To assist with the development of data science subjects the themes and content that contribute to each subject should be objectively validated. We propose the use of an objective test for data science curricula to quantify whether a particular degree programme maintains an interdisciplinary perspective unconstrained by single discipline bias. The test analyses a given curriculum and quantifies the subject components by category using natural language processing (NLP) techniques. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2018,Computer Science Education,5,emerging career in technology-focused field @ a data science coupled @ necessary graduate outcome mandate @ need @ a truly interdisciplinary pedagogical approach @ however @ rapid pace of curriculum development in @ field of inquiry ha meant @ curriculum across university ha largely evolved in line @ @ internal disciplinary strength of @ institution rather @ in response to @ need of graduate @ to assist @ @ development of data science subject @ theme and content @ contribute to @ subject @ @ objectively validated @ @ propose @ use of @ objective test @ data science curriculum to quantify whether a particular degree programme maintains @ interdisciplinary perspective unconstrained by single discipline bias @ @ test analysis a given curriculum and quantifies @ subject component by category @ natural language processing @ nlp @ technique @ informa uk limited trading a taylor francis group @ 
1315,Characterising text mining: A systematic mapping review of the Portuguese language,"Documents written in natural language constitute a major part of the artefacts produced during the software engineering life cycle. Studies indicate that more than 80% of enterprise data is stored in some sort of unstructured form, mainly as text. Therefore, the growth of user-generated content, especially from social media, provides a huge amount of data which allows discovering the experiences, opinions, and feelings of users. Text mining refers to the set of tools, techniques, and algorithms adopted to extract useful information from unstructured data. Considering that Portuguese ranks among the ten most spoken languages, and it is the second most common in Twitter, this study aims to map current primary studies that relate to the application of text mining for Portuguese. A systematic mapping method was applied and 6075 primary studies were retrieved up to the year 2014. A total of 203 studies were included, from which more than 60% analyse texts written in Brazilian variant. The majority of studies focus on the text classification task. Support vector machine and Naïve Bayes appear as main the algorithms. Folha de São Paulo and Público newspapers appear as main corpora, followed by the Portuguese Attorney General's Office corpus and Twitter. © The Institution of Engineering and Technology 2017.",2018,IET Software,5,document written in natural language constitute a major part of @ artefact produced @ @ software engineering life cycle @ study indicate @ more @ of enterprise data is stored in some sort of unstructured form mainly a text @ therefore @ growth of user-generated content especially @ social medium provides a huge amount of data @ allows discovering @ experience opinion and feeling of user @ text mining refers to @ set of tool technique and algorithm adopted to extract useful information @ unstructured data @ considering @ portuguese rank among @ ten @ spoken language and @ is @ second @ common in twitter @ study aim to map current primary study @ relate to @ application of text mining @ portuguese @ a systematic mapping method wa applied and primary study @ retrieved up to @ year @ a total of study @ included @ @ more @ analyse text written in brazilian variant @ @ majority of study focus on @ text classification task @ support vector machine and naïve bayes appear a main @ algorithm @ folha de são paulo and público newspaper appear a main corpus followed by @ portuguese attorney general @ s office corpus and twitter @ @ institution of engineering and technology @ 
1316,Sentiment analysis on Twitter: A text mining approach to the Syrian refugee crisis,"The use of social media has become an integral part of daily routine in modern society. Social media portals offer powerful public platforms where people can freely share their opinions and feelings about various topics with large crowds. In the current study, we investigated the public opinions and sentiments towards the Syrian refugee crisis, which has affected millions of people and has become a widely discussed, polarizing topic in social media around the world. To analyze public sentiments about the topic on Twitter, we collected a total of 2381,297 relevant tweets in two languages including Turkish and English. Turkish sentiments were considered important as Turkey has welcomed the largest number of Syrian refugees and Turkish tweets carried information to reflect public perception of a refugee hosting country first handedly. We performed a comparative sentiment analysis of retrieved tweets. The results indicated that the sentiments in Turkish tweets were significantly different from the sentiments in English tweets. We found that Turkish tweets carried slightly more positive sentiments towards Syrians and refugees than neutral and negative sentiments, nevertheless the sentiments of tweets were almost evenly distributed among the three major categories. On the other hand, the largest number of English tweets by a significant margin contained neutral sentiments, which was followed by the negative sentiments. In comparison to the ratio of positive sentiments in Turkish tweets, 35% of all Turkish tweets, the proportion of English tweets contained remarkably less positive sentiments towards Syrians and refugees, only 12% of all English tweets. © 2017 Elsevier Ltd",2018,Telematics and Informatics,67,@ use of social medium ha become @ integral part of daily routine in modern society @ social medium portal offer powerful public platform @ people @ freely share @ opinion and feeling @ various topic @ @ crowd @ in @ current study @ investigated @ public opinion and sentiment towards @ syrian refugee crisis @ ha affected million of people and ha become a widely discussed polarizing topic in social medium around @ world @ to analyze public sentiment @ @ topic on twitter @ collected a total of relevant tweet in @ language including turkish and english @ turkish sentiment @ considered important a turkey ha welcomed @ largest number of syrian refugee and turkish tweet carried information to reflect public perception of a refugee hosting country first handedly @ @ performed a comparative sentiment analysis of retrieved tweet @ @ @ indicated @ @ sentiment in turkish tweet @ significantly different @ @ sentiment in english tweet @ @ found @ turkish tweet carried slightly more positive sentiment towards syrian and refugee @ neutral and negative sentiment nevertheless @ sentiment of tweet @ almost evenly distributed among @ three major category @ on @ @ hand @ largest number of english tweet by a significant margin contained neutral sentiment @ wa followed by @ negative sentiment @ in comparison to @ ratio of positive sentiment in turkish tweet of @ turkish tweet @ proportion of english tweet contained remarkably le positive sentiment towards syrian and refugee only of @ english tweet @ @ ltd
1318,Recognizing irregular entities in biomedical text via deep neural networks,"Named entity recognition (NER) is an important task for biomedical text mining. Most prior work focused on recognizing regular entities that consist of continuous word sequences and are not overlapped with each other. In this paper, we propose a neural network model called Bi-LSTM-CRF that consists of bidirectional (Bi) long short-term memories (LSTMs) and conditional random fields (CRFs) to identify regular entities and the components of irregular entities. Then the components are combined to build final irregular entities according to manually designed rules. Furthermore, we propose a novel model called NerOne that consists of the Bi-LSTM-CRF network and another Bi-LSTM network. The Bi-LSTM-CRF network performs the same task as the aforementioned model, and the Bi-LSTM network determines whether two components should be combined. Therefore, NerOne automatically combines the components instead of using manually designed rules. We evaluate our models on two datasets for recognizing regular and irregular biomedical entities. Experimental results show that, with less feature engineering, the performances of our models are comparable with those of state-of-the-art systems. We show that the method of automatically combining the components is as effective as the method of manually designing rules. Our work can facilitate the research on biomedical text mining. © 2017",2018,Pattern Recognition Letters,15,named entity recognition @ ner @ is @ important task @ biomedical text mining @ @ prior work focused on recognizing regular entity @ consist of continuous word sequence and @ not overlapped @ @ @ @ in @ @ @ propose a neural network model called bi-lstm-crf @ consists of bidirectional @ bi @ long short-term memory @ lstms @ and conditional random field @ crfs @ to identify regular entity and @ component of irregular entity @ @ @ component @ combined to build final irregular entity according to manually designed rule @ furthermore @ propose a novel model called nerone @ consists of @ bi-lstm-crf network and another bi-lstm network @ @ bi-lstm-crf network performs @ @ task a @ aforementioned model and @ bi-lstm network determines whether @ component @ @ combined @ therefore nerone automatically combine @ component instead of @ manually designed rule @ @ evaluate @ model on @ datasets @ recognizing regular and irregular biomedical entity @ experimental @ @ @ @ le feature engineering @ performance of @ model @ comparable @ @ of state-of-the-art system @ @ @ @ @ method of automatically combining @ component is a effective a @ method of manually designing rule @ @ work @ facilitate @ research on biomedical text mining @ 
1319,Fuzzy Bag-of-Words Model for Document Representation,"One key issue in text mining and natural language processing is how to effectively represent documents using numerical vectors. One classical model is the Bag-of-Words (BoW). In a BoW-based vector representation of a document, each element denotes the normalized number of occurrence of a basis term in the document. To count the number of occurrence of a basis term, BoW conducts exact word matching, which can be regarded as a hard mapping from words to the basis term. BoW representation suffers from its intrinsic extreme sparsity, high dimensionality, and inability to capture high-level semantic meanings behind text data. To address the aforementioned issues, we propose a new document representation method named fuzzy Bag-of-Words (FBoW) in this paper. FBoW adopts a fuzzy mapping based on semantic correlation among words quantified by cosine similarity measures between word embeddings. Since word semantic matching instead of exact word string matching is used, the FBoW could encode more semantics into the numerical representation. In addition, we propose to use word clusters instead of individual words as basis terms and develop fuzzy Bag-of-WordClusters (FBoWC) models. Three variants under the framework of FBoWC are proposed based on three different similarity measures between word clusters and words, which are named as FBoWC mean FBoWC max, and FBoWC min, respectively. Document representations learned by the proposed FBoW and FBoWC are dense and able to encode high-level semantics. The task of document categorization is used to evaluate the performance of learned representation by the proposed FBoW and FBoWC methods. The results on seven real-word document classification datasets in comparison with six document representation learning methods have shown that our methods FBoW and FBoWC achieve the highest classification accuracies. © 1993-2012 IEEE.",2018,IEEE Transactions on Fuzzy Systems,55,@ key issue in text mining and natural language processing is @ to effectively represent document @ numerical vector @ @ classical model is @ bag-of-words @ bow @ @ in a bow-based vector representation of a document @ element denotes @ normalized number of occurrence of a basis term in @ document @ to count @ number of occurrence of a basis term bow conduct exact word matching @ @ @ regarded a a hard mapping @ word to @ basis term @ bow representation suffers @ @ intrinsic extreme sparsity high dimensionality and inability to capture high-level semantic meaning behind text data @ to address @ aforementioned issue @ propose a @ document representation method named fuzzy bag-of-words @ fbow @ in @ @ @ fbow adopts a fuzzy mapping based on semantic correlation among word quantified by cosine similarity measure @ word embeddings @ since word semantic matching instead of exact word string matching is used @ fbow could encode more semantics @ @ numerical representation @ in addition @ propose to use word cluster instead of individual word a basis term and develop fuzzy bag-of-wordclusters @ fbowc @ model @ three variant @ @ framework of fbowc @ proposed based on three different similarity measure @ word cluster and word @ @ named a fbowc mean fbowc max and fbowc min respectively @ document representation learned by @ proposed fbow and fbowc @ dense and able to encode high-level semantics @ @ task of document categorization is used to evaluate @ performance of learned representation by @ proposed fbow and fbowc method @ @ @ on seven real-word document classification datasets in comparison @ six document representation learning method @ @ @ @ method fbow and fbowc achieve @ highest classification accuracy @ @ @ 
1321,The state-of-the-art in twitter sentiment analysis: A review and benchmark evaluation,"Twitter has emerged as a major social media platform and generated great interest from sentiment analysis researchers. Despite this attention, state-of-the-art Twitter sentiment analysis approaches perform relatively poorly with reported classification accuracies often below 70%, adversely impacting applications of the derived sentiment information. In this research, we investigate the unique challenges presented by Twitter sentiment analysis and review the literature to determine how the devised approaches have addressed these challenges. To assess the state-of-the-art in Twitter sentiment analysis, we conduct a benchmark evaluation of 28 top academic and commercial systems in tweet sentiment classification across five distinctive data sets. We perform an error analysis to uncover the causes of commonly occurring classification errors. To further the evaluation, we apply select systems in an event detection case study. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of the next generation of approaches. © 2018 ACM 2158-656X/2018/08-ART5 $15.00",2018,ACM Transactions on Management Information Systems,30,twitter ha emerged a a major social medium platform and generated great interest @ sentiment analysis researcher @ despite @ attention state-of-the-art twitter sentiment analysis approach perform relatively poorly @ reported classification accuracy often @ adversely impacting application of @ derived sentiment information @ in @ research @ investigate @ unique challenge presented by twitter sentiment analysis and review @ literature to determine @ @ devised approach @ addressed @ challenge @ to ass @ state-of-the-art in twitter sentiment analysis @ conduct a benchmark evaluation of top @ and commercial system in tweet sentiment classification across five distinctive data set @ @ perform @ error analysis to uncover @ cause of commonly occurring classification error @ to @ @ evaluation @ apply select system in @ event detection case study @ finally @ summarize @ key trend and takeaway @ @ review and benchmark evaluation and provide suggestion to guide @ design of @ next generation of approach @ acm x art @ 
1322,"Understanding Infrastructure Resiliency in Chennai, India Using Twitter's Geotags and Texts: A Preliminary Study","Geotagging is the process of labeling data and information with geographical identification metadata, and text mining refers to the process of deriving information from text through data analytics. Geotagging and text mining are used to mine rich sources of social media data, such as video, website, text, and Quick Response (QR) code. They have been frequently used to model consumer behaviors and market trends. This study uses both techniques to understand the resilience of infrastructure in Chennai, India using data mined from the 2015 flood. This paper presents a conceptual study on the potential use of social media (Twitter in this case) to better understand infrastructure resiliency. Using feature-extraction techniques, the research team extracted Twitter data from tweets generated by the Chennai population during the flood. First, this study shows that these techniques are useful in identifying locations, defects, and failure intensities of infrastructure using the location metadata from geotags, words containing the locations, and the frequencies of tweets from each location. However, more efforts are needed to better utilize the texts generated from the tweets, including a better understanding of the cultural contexts of the words used in the tweets, the contexts of the words used to describe the incidents, and the least frequently used words. © 2018",2018,Engineering,1,geotagging is @ process of labeling data and information @ geographical identification metadata and text mining refers to @ process of deriving information @ text @ data analytics @ geotagging and text mining @ used to mine rich source of social medium data @ a video website text and quick response @ qr @ code @ @ @ @ frequently used to model consumer behavior and market trend @ @ study us @ technique to understand @ resilience of infrastructure in chennai india @ data mined @ @ flood @ @ @ @ a conceptual study on @ potential use of social medium @ twitter in @ case @ to better understand infrastructure resiliency @ @ feature-extraction technique @ research team extracted twitter data @ tweet generated by @ chennai population @ @ flood @ first @ study @ @ @ technique @ useful in identifying location defect and failure intensity of infrastructure @ @ location metadata @ geotags word containing @ location and @ frequency of tweet @ @ location @ however more effort @ needed to better utilize @ text generated @ @ tweet including a better understanding of @ cultural context of @ word used in @ tweet @ context of @ word used to describe @ incident and @ least frequently used word @ 
1323,Sentiment recognition in customer reviews using deep learning,"Deep learning has become popular in all aspect related to human judgments. Most machine learning techniques work well which includes text classification, text sequence learning, sentiment analysis, question-answer engine, etc. This paper has been focused on two objectives, firstly is to study the applicability of deep neural networks strategies for extracting sentiment present in social media data and customer reviews with effective training solutions. The second objective is to design deep networks that can be trained with these weakly supervised strategies in order to predict meaningful inferences. This paper presents the concept and steps of using deep learning for extraction sentiments from customer reviews. The extraction pulls out the features from the customer reviews using deep learning popular methods including Convolution neural networks (CNN) and Long Short-Term Memory (LSTM) architectures. The comparison of the results with tradition text classification method such as Naive Bayes(NB) and Support Vector Machine(SVM) using two data sets IMDB reviews and Amazon customer reviews have been presented. This work mainly focused on investigating the merit of using deep models for sentiment analysis in customer reviews. © 2018, IGI Global.",2018,International Journal of Enterprise Information Systems,6,deep learning ha become popular in @ aspect related to human judgment @ @ machine learning technique work well @ includes text classification text sequence learning sentiment analysis question-answer engine etc @ @ @ ha @ focused on @ objective firstly is to study @ applicability of deep neural network strategy @ extracting sentiment @ in social medium data and customer review @ effective training solution @ @ second objective is to design deep network @ @ @ trained @ @ weakly supervised strategy in order to predict meaningful inference @ @ @ @ @ concept and step of @ deep learning @ extraction sentiment @ customer review @ @ extraction pull @ @ feature @ @ customer review @ deep learning popular method including convolution neural network @ cnn @ and long short-term memory @ lstm @ architecture @ @ comparison of @ @ @ tradition text classification method @ a naive bayes @ nb @ and support vector machine @ svm @ @ @ data set imdb review and amazon customer review @ @ presented @ @ work mainly focused on investigating @ merit of @ deep model @ sentiment analysis in customer review @ igi global @ 
1325,Combining bag-of-words and sentiment features of annual reports to predict abnormal stock returns,"Automated textual analysis of firm-related documents has become an important decision support tool for stock market investors. Previous studies tended to adopt either dictionary-based or machine learning approach. Nevertheless, little is known about their concurrent use. Here we use the combination of financial indicators, readability, sentiment categories, and bag-of-words (BoW) to increase prediction accuracy. This paper aims to extract both sentiment and BoW information from the annual reports of US firms. The sentiment analysis is based on two commonly used dictionaries, namely a general dictionary Diction 7.0 and a finance-specific dictionary proposed by Loughran and McDonald (J Finance 66:35–65, 2011. doi:10.1111/j.1540-6261.2010.01625.x). The BoW are selected according to their tf–idf. We combine these features with financial indicators to predict abnormal stock returns using a multilayer perceptron neural network with dropout regularization and rectified linear units. We show that this method performs similarly as naïve Bayes and outperforms other machine learning algorithms (support vector machine, C4.5 decision tree, and k-nearest neighbour classifier) in predicting positive/negative abnormal stock returns in terms of ROC. We also show that the quality of the prediction significantly increased when using the correlation-based feature selection of BoW. This prediction performance is robust to industry categorization and event window. © 2017, The Natural Computing Applications Forum.",2018,Neural Computing and Applications,13,automated textual analysis of firm-related document ha become @ important decision support tool @ stock market investor @ previous study tended to adopt either dictionary-based @ machine learning approach @ nevertheless little is known @ @ concurrent use @ @ @ use @ combination of financial indicator readability sentiment category and bag-of-words @ bow @ to increase prediction accuracy @ @ @ aim to extract @ sentiment and bow information @ @ annual report of u firm @ @ sentiment analysis is based on @ commonly used dictionary namely a general dictionary diction @ and a finance-specific dictionary proposed by loughran and mcdonald @ j finance @ @ doi @ @ j @ @ @ @ x @ @ @ bow @ selected according to @ tf idf @ @ combine @ feature @ financial indicator to predict abnormal stock return @ a multilayer perceptron neural network @ dropout regularization and rectified linear unit @ @ @ @ @ method performs similarly a naïve bayes and outperforms @ machine learning algorithm @ support vector machine c @ decision tree and k-nearest neighbour classifier @ in predicting positive negative abnormal stock return in term of roc @ @ @ @ @ @ quality of @ prediction significantly increased @ @ @ correlation-based feature selection of bow @ @ prediction performance is robust to industry categorization and event window @ @ natural computing application forum @ 
1334,Opinion mining using ensemble text hidden Markov models for text classification,"With the rapid growth of social media, text mining is extensively utilized in practical fields, and opinion mining, also known as sentiment analysis, plays an important role in analyzing opinion and sentiment in texts. Methods in opinion mining generally depend on a sentiment lexicon, which is a set of predefined key words that express sentiment. Opinion mining requires proper sentiment words to be extracted in advance and has difficulty classifying sentences that imply an opinion without using any sentiment key words. This paper presents a new sentiment analysis method, based on text-based hidden Markov models (TextHMMs), for text classification that uses a sequence of words in training texts instead of a predefined sentiment lexicon. We sought to learn text patterns representing sentiment through ensemble TextHMMs. Our method defines hidden variables in TextHMMs by semantic cluster information in consideration of the co-occurrence of words, and thus calculates the sentiment orientation of sentences by fitted TextHMMs. To reflect diverse patterns, we applied an ensemble of TextHMM-based classifiers. In the experiments with a benchmark data set, we show that this method is superior to some existing methods and particularly has potential to classify implicit opinions. We also demonstrate the practicality of the proposed method in a real-life data set of online market reviews. © 2017 Elsevier Ltd",2018,Expert Systems with Applications,59,@ @ rapid growth of social medium text mining is extensively utilized in practical field and opinion mining @ known a sentiment analysis play @ important role in analyzing opinion and sentiment in text @ method in opinion mining generally depend on a sentiment lexicon @ is a set of predefined key word @ express sentiment @ opinion mining requires proper sentiment word to @ extracted in advance and ha difficulty classifying sentence @ imply @ opinion without @ @ sentiment key word @ @ @ @ a @ sentiment analysis method based on text-based hidden markov model @ texthmms @ @ text classification @ us a sequence of word in training text instead of a predefined sentiment lexicon @ @ sought to learn text pattern representing sentiment @ ensemble texthmms @ @ method defines hidden variable in texthmms by semantic cluster information in consideration of @ co-occurrence of word and thus calculates @ sentiment orientation of sentence by fitted texthmms @ to reflect diverse pattern @ applied @ ensemble of texthmm-based classifier @ in @ experiment @ a benchmark data set @ @ @ @ method is superior to some existing method and particularly ha potential to classify implicit opinion @ @ @ demonstrate @ practicality of @ proposed method in a real-life data set of online market review @ @ ltd
1347,Acronyms as an Integral Part of Multi-Word Term Recognition - A Token of Appreciation,"Term conflation is the process of linking together different variants of the same term. In automatic term recognition approaches, all term variants should be aggregated into a single normalized term representative, which is associated with a single domain-specific concept as a latent variable. In a previous study, we described FlexiTerm, an unsupervised method for recognition of multiword terms from a domain-specific corpus. It uses a range of methods to normalize three types of term variation - orthographic, morphological, and syntactic variations. Acronyms, which represent a highly productive type of term variation, were not supported. In this paper, we describe how the functionality of FlexiTerm has been extended to recognize acronyms and incorporate them into the term conflation process. The main contribution of this paper is not acronym recognition per se, but rather its integration with other types of term variation into the term conflation process. We evaluated the effects of term conflation in the context of information retrieval as one of its most prominent applications. On average, relative recall increased by 32 points, whereas index compression factor increased by 7% points. Therefore, evidence suggests that integration of acronyms provides nontrivial improvement of term conflation. © 2013 IEEE.",2018,IEEE Access,5,term conflation is @ process of linking together different variant of @ @ term @ in automatic term recognition approach @ term variant @ @ aggregated @ a single normalized term representative @ is associated @ a single domain-specific concept a a latent variable @ in a previous study @ described flexiterm @ unsupervised method @ recognition of multiword term @ a domain-specific corpus @ @ us a range of method to normalize three type of term variation orthographic morphological and syntactic variation @ acronym @ represent a highly productive type of term variation @ not supported @ in @ @ @ describe @ @ functionality of flexiterm ha @ extended to recognize acronym and incorporate @ @ @ term conflation process @ @ main contribution of @ @ is not acronym recognition per se @ rather @ integration @ @ type of term variation @ @ term conflation process @ @ evaluated @ effect of term conflation in @ context of information retrieval a @ of @ @ prominent application @ on average relative recall increased by point whereas index compression factor increased by point @ therefore evidence suggests @ integration of acronym provides nontrivial improvement of term conflation @ @ @ 
1355,Identifying self-admitted technical debt in open source projects using text mining,"Technical debt is a metaphor to describe the situation in which long-term code quality is traded for short-term goals in software projects. Recently, the concept of self-admitted technical debt (SATD) was proposed, which considers debt that is intentionally introduced, e.g., in the form of quick or temporary fixes. Prior work on SATD has shown that source code comments can be used to successfully detect SATD, however, most current state-of-the-art classification approaches of SATD rely on manual inspection of the source code comments. In this paper, we proposed an automated approach to detect SATD in source code comments using text mining. In our approach, we utilize feature selection to select useful features for classifier training, and we combine multiple classifiers from different source projects to build a composite classifier that identifies SATD comments in a target project. We investigate the performance of our approach on 8 open source projects that contain 212,413 comments. Our experimental results show that, on every target project, our approach outperforms the state-of-the-art and the baselines approaches in terms of F1-score. The F1-score achieved by our approach ranges between 0.518 - 0.841, with an average of 0.737, which improves over the state-of-the-art approach proposed by Potdar and Shihab by 499.19%. When compared with the text mining-based baseline approaches, our approach significantly improves the average F1-score by at least 58.49%. When compared with a natural language processing-based baseline, our approach also significantly improves its F1-score by 27.95%. Our proposed approach can be used by project personnel to effectively identify SATD with minimal manual effort. © 2017, Springer Science+Business Media New York.",2018,Empirical Software Engineering,35,technical debt is a metaphor to describe @ situation in @ long-term code quality is traded @ short-term goal in software project @ recently @ concept of self-admitted technical debt @ satd @ wa proposed @ considers debt @ is intentionally introduced e @ g @ in @ form of quick @ temporary fix @ prior work on satd ha @ @ source code comment @ @ used to successfully detect satd however @ current state-of-the-art classification approach of satd rely on manual inspection of @ source code comment @ in @ @ @ proposed @ automated approach to detect satd in source code comment @ text mining @ in @ approach @ utilize feature selection to select useful feature @ classifier training and @ combine multiple classifier @ different source project to build a composite classifier @ identifies satd comment in a target project @ @ investigate @ performance of @ approach on open source project @ contain comment @ @ experimental @ @ @ on every target project @ approach outperforms @ state-of-the-art and @ baseline approach in term of f score @ @ f score achieved by @ approach range @ @ @ @ @ average of @ @ improves @ @ state-of-the-art approach proposed by potdar and shihab by @ @ @ compared @ @ text mining-based baseline approach @ approach significantly improves @ average f score by at least @ @ @ compared @ a natural language processing-based baseline @ approach @ significantly improves @ f score by @ @ @ proposed approach @ @ used by project personnel to effectively identify satd @ minimal manual effort @ @ science @ medium @ york @ 
1356,Exploring convolutional neural networks and topic models for user profiling from drug reviews,"Pharmacovigilance, and generally applications of natural language processing models to healthcare, have attracted growing attention over the recent years. In particular, drug reactions can be extracted from user reviews posted on the Web, and automated processing of this information represents a novel and exciting approach to personalized medicine and wide-scale drug tests. In medical applications, demographic information regarding the authors of these reviews such as age and gender is of primary importance; however, existing studies usually either assume that this information is available or overlook the issue entirely. In this work, we propose and compare several approaches to automated mining of demographic information from user-generated texts. We compare modern natural language processing techniques, including extensions of topic models and convolutional neural networks (CNN). We apply single-task and multi-task learning approaches to this problem. Based on a real-world dataset mined from a health-related web site, we conclude that while CNNs perform best in terms of predicting demographic information by jointly learning different user attributes, topic models provide additional information and reflect gender-specific and age-specific symptom profiles that may be of interest for a researcher. © 2017, Springer Science+Business Media, LLC.",2018,Multimedia Tools and Applications,7,pharmacovigilance and generally application of natural language processing model to healthcare @ attracted growing attention @ @ recent year @ in particular drug reaction @ @ extracted @ user review posted on @ web and automated processing of @ information represents a novel and exciting approach to personalized medicine and wide-scale drug test @ in medical application demographic information regarding @ author of @ review @ a age and gender is of primary importance @ however existing study usually either assume @ @ information is available @ overlook @ issue entirely @ in @ work @ propose and compare several approach to automated mining of demographic information @ user-generated text @ @ compare modern natural language processing technique including extension of topic model and convolutional neural network @ cnn @ @ @ apply single-task and multi-task learning approach to @ problem @ based on a real-world dataset mined @ a health-related web site @ conclude @ @ cnns perform best in term of predicting demographic information by jointly learning different user attribute topic model provide additional information and reflect gender-specific and age-specific symptom profile @ may @ of interest @ a researcher @ @ science @ medium llc @ 
1358,An active transfer learning framework for protein-protein interaction extraction,"Protein-Protein Interaction Extraction (PPIE) from biomedical literatures is an important task in biomedical text mining and has achieved great success on public datasets. However, in real-world applications, the existing PPI extraction methods are limited to label effort. Therefore, transfer learning method is applied to reduce the cost of manual labeling. Current transfer learning methods suffer from negative transfer and lower performance. To tackle this problem, an improved TrAdaBoost algorithm is proposed, that is, relative distribution is introduced to initialize the weights of TrAdaBoost to overcome the negative transfer caused by domain differences. To make further improvement on the performance of transfer learning, an approach combining active learning with the improved TrAdaBoost is presented. The experimental results on publicly available PPI corpora show that our method outperforms TrAdaBoost and SVM when the labeled data is insufficient,and on document classification corpora, it also illustrates that the proposed approaches can achieve better performance than TrAdaBoost and TPTSVM in final, which verifies the effectiveness of our methods. © Copyright 2018 The Institute of Electronics, Information and Communication Engineers.",2018,IEICE Transactions on Information and Systems,0,protein-protein interaction extraction @ ppie @ @ biomedical literature is @ important task in biomedical text mining and ha achieved great success on public datasets @ however in real-world application @ existing ppi extraction method @ limited to label effort @ therefore transfer learning method is applied to reduce @ cost of manual labeling @ current transfer learning method suffer @ negative transfer and lower performance @ to tackle @ problem @ improved tradaboost algorithm is proposed @ is relative distribution is introduced to initialize @ weight of tradaboost to overcome @ negative transfer caused by domain difference @ to make @ improvement on @ performance of transfer learning @ approach combining active learning @ @ improved tradaboost is presented @ @ experimental @ on publicly available ppi corpus @ @ @ method outperforms tradaboost and svm @ @ labeled data is insufficient and on document classification corpus @ @ illustrates @ @ proposed approach @ achieve better performance @ tradaboost and tptsvm in final @ verifies @ effectiveness of @ method @ @ @ institute of electronics information and communication engineer @ 
1359,Enhancing multi-document summarization using concepts,"In this paper we propose a methodology to mine concepts from documents and use these concepts to generate an objective summary of all relevant documents. We use the conceptual graph (CG) formalism as proposed by Sowa to represent the concepts and their relationships in the documents. In the present work we have modified and extended the definition of the concept given by Sowa. The modified and extended definition is discussed in detail in section 2 of this paper. A CG of a set of relevant documents can be considered as a semantic network. The semantic network is generated by automatically extracting CG for each document and merging them into one. We discuss (i) generation of semantic network using CGs and (ii) generation of multi-document summary. Here we use restricted Boltzmann machines, a deep learning technique, for automatically extracting CGs. We have tested our methodology using MultiLing 2015 corpus. We have obtained encouraging results, which are comparable to those from the state of the art systems. © 2018, Indian Academy of Sciences.",2018,Sadhana - Academy Proceedings in Engineering Sciences,2,in @ @ @ propose a methodology to mine concept @ document and use @ concept to generate @ objective summary of @ relevant document @ @ use @ conceptual graph @ cg @ formalism a proposed by sowa to represent @ concept and @ relationship in @ document @ in @ @ work @ @ modified and extended @ definition of @ concept given by sowa @ @ modified and extended definition is discussed in detail in section of @ @ @ a cg of a set of relevant document @ @ considered a a semantic network @ @ semantic network is generated by automatically extracting cg @ @ document and merging @ @ @ @ @ discus @ i @ generation of semantic network @ cgs and @ ii @ generation of multi-document summary @ @ @ use restricted boltzmann machine a deep learning technique @ automatically extracting cgs @ @ @ tested @ methodology @ multiling corpus @ @ @ obtained encouraging @ @ @ comparable to @ @ @ state of @ art system @ indian academy of science @ 
1360,NLP-MTFLR: Document-Level Prioritization and Identification of Dominant Multi-word Named Products in Customer Reviews,"The accessibility to large amount of datasets in commercial domains has accentuated the importance of data mining in the last few years. Practitioners as well as researchers rely on them to reflect on the magnitude and effect of data-related problems that require solution in business environments. In recent years, the volume of online data submissions (e-commerce data) on products, services and organizations has increased exponentially. However, the submitted data are highly unstructured and largely dependent on language. Mining and extracting useful information from such data is a colossal task, as analysis of the data should include opinion word identification/extraction, aspect extraction and entity extraction. Of the three, the entity extraction is one of the governing approaches in text analysis and plays a major role in e-commerce, biomedical and automobile industries and supports the categorization of the records based on the entity names, generation of short summary on the entities and grouping of the similar records. The existing approaches in entity extraction are capable of recognizing and extracting single-word named entities. However, the product names are often given as a sequence of words (multiple words or multi-word named entities) and, therefore, cannot be recognized by the existing methods. To resolve this issue, this paper presents a novel approach of NLP-Modified Token-based Frequencies of Left and Right (NLP-MTFLR), which is considered as an effective approach to detect and extract the multi-word named products and dominant multi-word named product from the customer review corpus. Using this NLP-MTFLR approach, from the review corpus the subwords and multi-subwords are identified and mapped them with its multi-word named products to recognize dominant product of that corpus. With this dominant product identification, the proposed method reveals in that corpus that the identified dominant product is highly reviewed by the reviewers compared to other products. This NLP-MTFLR approach is achieved 97% accuracy, 77% precision, 89% recall and 82% F-score. © 2017, King Fahd University of Petroleum & Minerals.",2018,Arabian Journal for Science and Engineering,0,@ accessibility to @ amount of datasets in commercial domain ha accentuated @ importance of data mining in @ last @ year @ practitioner a well a researcher rely on @ to reflect on @ magnitude and effect of data-related problem @ require solution in @ environment @ in recent year @ volume of online data submission @ e-commerce data @ on product service and organization ha increased exponentially @ however @ submitted data @ highly unstructured and largely dependent on language @ mining and extracting useful information @ @ data is a colossal task a analysis of @ data @ include opinion word identification extraction aspect extraction and entity extraction @ of @ three @ entity extraction is @ of @ governing approach in text analysis and play a major role in e-commerce biomedical and automobile industry and support @ categorization of @ record based on @ entity name generation of short summary on @ entity and grouping of @ similar record @ @ existing approach in entity extraction @ capable of recognizing and extracting single-word named entity @ however @ product name @ often given a a sequence of word @ multiple word @ multi-word named entity @ and therefore cannot @ recognized by @ existing method @ to resolve @ issue @ @ @ a novel approach of nlp-modified token-based frequency of left and right @ nlp-mtflr @ @ is considered a @ effective approach to detect and extract @ multi-word named product and dominant multi-word named product @ @ customer review corpus @ @ @ nlp-mtflr approach @ @ review corpus @ subwords and multi-subwords @ identified and mapped @ @ @ multi-word named product to recognize dominant product of @ corpus @ @ @ dominant product identification @ proposed method reveals in @ corpus @ @ identified dominant product is highly reviewed by @ reviewer compared to @ product @ @ nlp-mtflr approach is achieved accuracy precision recall and f-score @ king fahd university of petroleum mineral @ 
1361,Bayesian Nonparametric Learning for Hierarchical and Sparse Topics,"This paper presents the Bayesian nonparametric (BNP) learning for hierarchical and sparse topics from natural language. Traditionally, the Indian buffet process provides the BNP prior on a binary matrix for an infinite latent feature model consisting of a flat layer of topics. The nested model paves an avenue to construct a tree model instead of a flat-layer model. This paper presents the nested Indian buffet process (nIBP) to achieve the sparsity and flexibility in topic model where the model complexity and topic hierarchy are learned from the groups of words. The mixed membership modeling is conducted by representing a document using the tree nodes or dishes that a document or a customer chooses according to the nIBP scenario. A tree stick-breaking process is implemented to select topic weights from a subtree for flexible topic modeling. Such an nIBP relaxes the constraint of adopting a single tree path in the nested Chinese restaurant process (nCRP) and, therefore, improves the variety of topic representation for heterogeneous documents. A Gibbs sampling procedure is developed to infer the nIBP topic model. Compared to the nested hierarchical Dirichlet process (nhDP), the compactness of the estimated topics in a tree using nIBP is improved. Experimental results show that the proposed nIBP reduces the error rate of nCRP and nhDP by 18% and 8% on Reuters task for document classification, respectively. © 2017 IEEE.",2018,IEEE/ACM Transactions on Audio Speech and Language Processing,6,@ @ @ @ bayesian nonparametric @ bnp @ learning @ hierarchical and sparse topic @ natural language @ traditionally @ indian buffet process provides @ bnp prior on a binary matrix @ @ infinite latent feature model consisting of a flat layer of topic @ @ nested model pave @ avenue to construct a tree model instead of a flat-layer model @ @ @ @ @ nested indian buffet process @ nibp @ to achieve @ sparsity and flexibility in topic model @ @ model complexity and topic hierarchy @ learned @ @ group of word @ @ mixed membership modeling is conducted by representing a document @ @ tree node @ dish @ a document @ a customer chooses according to @ nibp scenario @ a tree stick-breaking process is implemented to select topic weight @ a subtree @ flexible topic modeling @ @ @ nibp relaxes @ constraint of adopting a single tree path in @ nested chinese restaurant process @ ncrp @ and therefore improves @ variety of topic representation @ heterogeneous document @ a gibbs sampling procedure is developed to infer @ nibp topic model @ compared to @ nested hierarchical dirichlet process @ nhdp @ @ compactness of @ estimated topic in a tree @ nibp is improved @ experimental @ @ @ @ proposed nibp reduces @ error rate of ncrp and nhdp by and on reuters task @ document classification respectively @ @ @ 
1362,Employing fisher discriminant analysis for Arabic text classification,"Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. © 2017 Elsevier Ltd",2018,Computers and Electrical Engineering,12,fisher @ s discriminant analysis @ @ called linear discriminant analysis @ lda @ is a popular dimensionality reduction technique @ is widely used @ feature extraction @ lda aim at finding @ optimal linear transformation based on maximizing a class separability @ even though lda @ useful @ in various pattern recognition problem @ a face recognition le attention ha @ devoted to employing @ technique in arabic information retrieval task @ in particular @ sizable feature vector in textual data enforces to implement dimensionality reduction technique @ a lda @ in @ @ @ empirically investigated @ lda based method @ arabic text classification @ @ used a corpus @ contains document belonging to five category @ @ experimental @ showed @ @ performance of semantic loss lda based method wa almost @ @ a @ semantic rich singular value decomposition @ svd @ and @ is indication @ lda is a promising method @ text mining application @ @ ltd
1363,Corpus-based topic diffusion for short text clustering,"In this paper, we propose a novel corpus-based enrichment approach for short text clustering. Since sparseness brings about the problem of insufficient word co-occurrence and lack of context information, previous researches use external sources such as Wikipedia or WordNet to enrich the representation of short text documents, which requires extra resources and might lead to possible inconsistency. On the other hand, corpus-based approaches use no external information in mining short text data. By introducing a set of conjugate definitions to characterize the structures of topics and words, and by proposing a virtual generative procedure for short texts, we perform expansion on short text data. Specifically, new words which may not appear in a short text document were added with a virtual term frequency, and this virtual frequency is obtained from the posterior probabilities of new words given all the words in that document. The complete procedure can be regarded as mapping data points (documents) from the original feature space to a hidden semantic space (topic space). After performing semantic smoothing, data points are then mapped back to the original space. We conduct experiments on two short text datasets, and the results show that the proposed method can effectively address the sparseness problem. For these datasets, our method, using only a basic clustering algorithm, attains a comparable performance with methods based on enrichment with external information sources. © 2017 Elsevier B.V.",2018,Neurocomputing,20,in @ @ @ propose a novel corpus-based enrichment approach @ short text clustering @ since sparseness brings @ @ problem of insufficient word co-occurrence and lack of context information previous research use external source @ a wikipedia @ wordnet to enrich @ representation of short text document @ requires extra resource and might lead to possible inconsistency @ on @ @ hand corpus-based approach use no external information in mining short text data @ by introducing a set of conjugate definition to characterize @ structure of topic and word and by proposing a virtual generative procedure @ short text @ perform expansion on short text data @ specifically @ word @ may not appear in a short text document @ added @ a virtual term frequency and @ virtual frequency is obtained @ @ posterior probability of @ word given @ @ word in @ document @ @ complete procedure @ @ regarded a mapping data point @ document @ @ @ original feature space to a hidden semantic space @ topic space @ @ @ performing semantic smoothing data point @ @ mapped back to @ original space @ @ conduct experiment on @ short text datasets and @ @ @ @ @ proposed method @ effectively address @ sparseness problem @ @ @ datasets @ method @ only a basic clustering algorithm attains a comparable performance @ method based on enrichment @ external information source @ @ b @ v @ 
1369,Opinion-Aspect Relations in Cognizing Customer Feelings via Reviews,"Determining a consensus opinion on a product sold online is no longer easy, because assessments have become more and more numerous on the Internet. To address this problem, researchers have used various approaches, such as looking for feelings expressed in the documents and exploring the appearance and syntax of reviews. Aspect-based evaluation is the most important aspect of opinion mining, and researchers are becoming more interested in product aspect extraction; however, more complex algorithms are needed to address this issue precisely with large data sets. This paper introduces a method to extract and summarize product aspects and corresponding opinions from a large number of product reviews in a specific domain. We maximize the accuracy and usefulness of the review summaries by leveraging knowledge about product aspect extraction and providing both an appropriate level of detail and rich representation capabilities. The results show that the proposed system achieves F1-scores of 0.714 for camera reviews and 0.774 for laptop reviews. © 2013 IEEE.",2018,IEEE Access,9,determining a consensus opinion on a product sold online is no longer easy @ assessment @ become more and more numerous on @ internet @ to address @ problem researcher @ used various approach @ a looking @ feeling expressed in @ document and exploring @ appearance and syntax of review @ aspect-based evaluation is @ @ important aspect of opinion mining and researcher @ becoming more interested in product aspect extraction @ however more complex algorithm @ needed to address @ issue precisely @ @ data set @ @ @ introduces a method to extract and summarize product aspect and corresponding opinion @ a @ number of product review in a specific domain @ @ maximize @ accuracy and usefulness of @ review summary by leveraging knowledge @ product aspect extraction and providing @ @ appropriate level of detail and rich representation capability @ @ @ @ @ @ proposed system achieves f score of @ @ camera review and @ @ laptop review @ @ @ 
1370,"Advanced methods: Identification of promising high-tech solutions with semantic technologies: Energy, pharma, and other industries","A novel approach to trend monitoring and the identification of promising high-tech solutions is presented in the chapter. It is based on the ontology of a technology/market trend, Hype Cycles methodology, and semantic indicators which provide evidence of a maturity level of a technology as well as of emerging user needs (customer pains) in high-tech industries. This approach forms the basis for text mining software tools implemented in Semantic Hub platform. The algorithms behind these tools allow users to escape from getting too general or garbage results which make it impossible to identify promising technologies at early stages (early detection, weak signals). Besides, these algorithms provide high-quality results in the extraction of complex multiword terms which correspond to technological concepts and user pains forming a trend. The methodology and software developed as a result of this study are applicable to various industries with minor adjustments. © 2018 by World Scientific Publishing Europe Ltd. All rights reserved.",2018,Innovation Discovery: Network Analysis Of Research And Invention Activity For Technology Management,0,a novel approach to trend monitoring and @ identification of promising high-tech solution is presented in @ chapter @ @ is based on @ ontology of a technology market trend hype cycle methodology and semantic indicator @ provide evidence of a maturity level of a technology a well a of emerging user need @ customer pain @ in high-tech industry @ @ approach form @ basis @ text mining software tool implemented in semantic hub platform @ @ algorithm behind @ tool allow user to escape @ getting too general @ garbage @ @ make @ impossible to identify promising technology at early stage @ early detection weak signal @ @ besides @ algorithm provide high-quality @ in @ extraction of complex multiword term @ correspond to technological concept and user pain forming a trend @ @ methodology and software developed a a @ of @ study @ applicable to various industry @ minor adjustment @ by world scientific publishing europe ltd @ @ right reserved @ 
1373,Comparison of natural language processing tools for automatic Gene Ontology annotation of scientific literature,"Manual curation of scientific literature for ontology-based knowledge representation has proven infeasible and unscalable to the large and growing volume of scientific literature. Automated annotation solutions that leverage text mining and Natural Language Processing (NLP) have been developed to ameliorate the problem of literature curation. These NLP approaches use parsing, syntactical, and lexical analysis of text to recognize and annotate pieces of text with ontology concepts. Here, we conduct a comparison of four state of the art NLP tools at the task of recognizing Gene Ontology concepts from biomedical literature using the Colorado Richly Annotated Full-Text (CRAFT) corpus as a gold standard reference. We demonstrate the use of semantic similarity metrics to compare NLP tool annotations to the gold standard. © 2018 CEUR-WS.",2018,CEUR Workshop Proceedings,0,manual curation of scientific literature @ ontology-based knowledge representation ha proven infeasible and unscalable to @ @ and growing volume of scientific literature @ automated annotation solution @ leverage text mining and natural language processing @ nlp @ @ @ developed to ameliorate @ problem of literature curation @ @ nlp approach use parsing syntactical and lexical analysis of text to recognize and annotate piece of text @ ontology concept @ @ @ conduct a comparison of four state of @ art nlp tool at @ task of recognizing gene ontology concept @ biomedical literature @ @ colorado richly annotated full-text @ craft @ corpus a a gold standard reference @ @ demonstrate @ use of semantic similarity metric to compare nlp tool annotation to @ gold standard @ ceur-ws @ 
1376,An improved Urdu stemming algorithm for text mining based on multi-step hybrid approach,"Stemming is the basic operation in Natural language processing (NLP) to remove derivational and inflectional affixes without performing a morphological analysis. This practice is essential to extract the root or stem. In NLP domains, the stemmer is used to improve the process of information retrieval (IR), text classifications (TC), text mining (TM) and related applications. In particular, Urdu stemmers utilize only uni-gram words from the input text by ignoring bigrams, trigrams, and n-gram words. To improve the process and efficiency of stemming, bigrams and trigram words must be included. Despite this fact, there are a few developed methods for Urdu stemmers in the past studies. Therefore, in this paper, we proposed an improved Urdu stemmer, using hybrid approach divided into multi-step operation, to deal with unigram, bigram, and trigram features as well. To evaluate the proposed Urdu stemming method, we have used two corpora; word corpus and text corpus. Moreover, two different evaluation metrics have been applied to measure the performance of the proposed algorithm. The proposed algorithm achieved an accuracy of 92.97% and compression rate of 55%. These experimental results indicate that the proposed system can be used to increase the effectiveness and efficiency of the Urdu stemmer for better information retrieval and text mining applications. © 2018 Informa UK Limited, trading as Taylor & Francis Group.",2018,Journal of Experimental and Theoretical Artificial Intelligence,7,stemming is @ basic operation in natural language processing @ nlp @ to remove derivational and inflectional affix without performing a morphological analysis @ @ practice is essential to extract @ root @ stem @ in nlp domain @ stemmer is used to improve @ process of information retrieval @ ir @ text classification @ tc @ text mining @ tm @ and related application @ in particular urdu stemmer utilize only uni-gram word @ @ input text by ignoring bigram trigram and n-gram word @ to improve @ process and efficiency of stemming bigram and trigram word must @ included @ despite @ fact @ @ a @ developed method @ urdu stemmer in @ past study @ therefore in @ @ @ proposed @ improved urdu stemmer @ hybrid approach divided @ multi-step operation to deal @ unigram bigram and trigram feature a well @ to evaluate @ proposed urdu stemming method @ @ used @ corpus @ word corpus and text corpus @ moreover @ different evaluation metric @ @ applied to measure @ performance of @ proposed algorithm @ @ proposed algorithm achieved @ accuracy of @ and compression rate of @ @ experimental @ indicate @ @ proposed system @ @ used to increase @ effectiveness and efficiency of @ urdu stemmer @ better information retrieval and text mining application @ informa uk limited trading a taylor francis group @ 
1378,Text mining - A key lynchpin in the investment process: A survey,"Text mining applications in the investment process involves a complex interaction between computational linguistics, natural language processing (NLP) and the know-how of the financial aspects. Given the progress in big data and multimodal data fusion, this state-of-the-art survey provides a timely consolidation of this ever evolving topic, together with new perspectives on the acquisition, input, variable relevance, feature extraction, fusion, and decision making based on a conjoint treatment of text and standard financial variables. Such an insight is then used as a basis to introduce an overarching framework for text-based big data in the investment process. The proposed approach is both novel and flexible, making it possible to be seamlessly employed across a variety of investable assets, including stocks, credit instruments, rates, FX and market indices. Another unique aspect is its modularity, whereby both emerging techniques in signal processing and machine learning, as well as traditional econometric techniques, are readily incorporated and combined towards the informed decision. Another virtue of the proposed concept is its ability to identify the semantic nature (context) of the source, even for general text-based sources (financial reports, social media, market news) while at the same time maintaining investors intuition, as news do affect asset prices and market moves. An example of a recent stock market performance during a company takeover process demonstrates the advantages of the proposed framework. © 2018 The authors and IOS Press. All rights reserved.",2018,Frontiers in Artificial Intelligence and Applications,0,text mining application in @ investment process involves a complex interaction @ computational linguistics natural language processing @ nlp @ and @ know-how of @ financial aspect @ given @ progress in big data and multimodal data fusion @ state-of-the-art survey provides a timely consolidation of @ ever evolving topic together @ @ perspective on @ acquisition input variable relevance feature extraction fusion and decision making based on a conjoint treatment of text and standard financial variable @ @ @ insight is @ used a a basis to introduce @ overarching framework @ text-based big data in @ investment process @ @ proposed approach is @ novel and flexible making @ possible to @ seamlessly employed across a variety of investable asset including stock credit instrument rate fx and market index @ another unique aspect is @ modularity whereby @ emerging technique in signal processing and machine learning a well a traditional econometric technique @ readily incorporated and combined towards @ informed decision @ another virtue of @ proposed concept is @ ability to identify @ semantic nature @ context @ of @ source even @ general text-based source @ financial report social medium market news @ @ at @ @ time maintaining investor intuition a news @ affect asset price and market move @ @ example of a recent stock market performance @ a company takeover process demonstrates @ advantage of @ proposed framework @ @ author and io @ @ @ right reserved @ 
1380,Text Mining Approach to Analyse Stock Market Movement,"Stock Market (SM) is a significant sector of countries’ economy and represents a crucial role in the growth of their commerce and industry. Hence, discovering efficient ways to analyse and visualise stock market data is considered a significant issue in modern finance. The use of Data Mining (DM) techniques to predict stock market has been extensively studied using historical market prices but such approaches are constrained to make assessments within the scope of existing information, and thus they are not able to model any random behaviour of stock market or provide causes behind events. One area of limited success in stock market prediction comes from textual data, which is a rich source of information and analysing it may provide better understanding of random behaviours of the market. Text Mining (TM) combined with Random Forest (RF) algorithm offers a novel approach to study critical indicators, which contribute to the prediction of stock market abnormal movements. A Stock Market Random Forest-Text Mining system (SMRF-TM) is developed to mine the critical indicators related to the 2009 Dubai stock market debt standstill. Random forest is applied to classify the extracted features into a set of semantic classes, thus extending current approaches from three to eight classes: critical down, down, neutral, up, critical up, economic, social and political. The study demonstrates that Random Forest has outperformed the other classifiers and has achieved the best accuracy in classifying the bigram features extracted from the corpus. © 2018, Springer International Publishing AG.",2018,Advances in Intelligent Systems and Computing,1,stock market @ sm @ is a significant sector of country economy and represents a crucial role in @ growth of @ commerce and industry @ hence discovering efficient way to analyse and visualise stock market data is considered a significant issue in modern finance @ @ use of data mining @ dm @ technique to predict stock market ha @ extensively studied @ historical market price @ @ approach @ constrained to make assessment within @ scope of existing information and thus @ @ not able to model @ random behaviour of stock market @ provide cause behind event @ @ area of limited success in stock market prediction come @ textual data @ is a rich source of information and analysing @ may provide better understanding of random behaviour of @ market @ text mining @ tm @ combined @ random forest @ rf @ algorithm offer a novel approach to study critical indicator @ contribute to @ prediction of stock market abnormal movement @ a stock market random forest-text mining system @ smrf-tm @ is developed to mine @ critical indicator related to @ dubai stock market debt standstill @ random forest is applied to classify @ extracted feature @ a set of semantic class thus extending current approach @ three to eight class @ critical down down neutral up critical up economic social and political @ @ study demonstrates @ random forest ha outperformed @ @ classifier and ha achieved @ best accuracy in classifying @ bigram feature extracted @ @ corpus @ @ international publishing ag @ 
1383,Detecting low back pain from clinical narratives using machine learning approaches,"Free-text clinical notes recorded during the patients’ visits in the Electronic Medical Record (EMR) system narrates clinical encounters, often using ‘SOAP’ notes (an acronym for subject, objective, assessment, and plan). The free-text notes represent a wealth of information for discovering insights, particularly in medical conditions such as pain and mental illness, where regular health metrics provide very little knowledge about the patients’ medical situations and reactions to treatments. In this paper, we develop a generic text-mining and decision support framework to diagnose chronic low back pain. The framework utilizes open-source algorithms for anonymization, natural language processing, and machine learning to classify low back pain patterns from unstructured free-text notes in the Electronic Medical Record (EMR) system as noted by the primary care physicians during patients’ visits. The initial results show a high accuracy for the limited thirty-four patient labelled data set that we used in this pilot study. We are currently processing a larger data set to test our approach. © Springer Nature Switzerland AG 2018.",2018,Communications in Computer and Information Science,2,free-text clinical note recorded @ @ patient visit in @ electronic medical record @ emr @ system narrates clinical encounter often @ soap note @ @ acronym @ subject objective assessment and plan @ @ @ free-text note represent a wealth of information @ discovering insight particularly in medical condition @ a pain and mental illness @ regular health metric provide @ little knowledge @ @ patient medical situation and reaction to treatment @ in @ @ @ develop a generic text-mining and decision support framework to diagnose chronic low back pain @ @ framework utilizes open-source algorithm @ anonymization natural language processing and machine learning to classify low back pain pattern @ unstructured free-text note in @ electronic medical record @ emr @ system a noted by @ primary care physician @ patient visit @ @ initial @ @ a high accuracy @ @ limited thirty-four patient labelled data set @ @ used in @ pilot study @ @ @ currently processing a larger data set to test @ approach @ @ nature switzerland ag @ 
1384,Distributional semantic phrase clustering and conceptualization using probabilistic knowledgebase,Distributional Semantics is an active research area in natural language processing (NLP) that develop methods for quantifying semantic similarities between linguistic elements in large samples of data. Short text conceptualization on the other hand is a technique for enriching short texts so that it become more interpretable. This is needed because most text mining tasks including topic modeling and clustering are based on statistical methods and won’t consider the semantics of text. This paper proposes a novel framework for combining distributional semantics and short text conceptualization for better interpretability of phrases in text data. Experiments on real-world datasets show that this method can better enrich phrases that are represented in distributional semantic spaces. © Springer Nature Singapore Pte Ltd. 2018.,2018,Communications in Computer and Information Science,0,distributional semantics is @ active research area in natural language processing @ nlp @ @ develop method @ quantifying semantic similarity @ linguistic element in @ sample of data @ short text conceptualization on @ @ hand is a technique @ enriching short text @ @ @ become more interpretable @ @ is needed @ @ text mining task including topic modeling and clustering @ based on statistical method and @ t consider @ semantics of text @ @ @ proposes a novel framework @ combining distributional semantics and short text conceptualization @ better interpretability of phrase in text data @ experiment on real-world datasets @ @ @ method @ better enrich phrase @ @ represented in distributional semantic space @ @ nature singapore pte ltd @ @ 
1385,Text Mining for Modeling Cyberattacks,"This chapter examines how natural language processing can be applied for building rich models for cybersecurity analytics. For this, it applies text mining to the natural-language content of Common Attack Pattern Enumeration and Classification (CAPECTM), a standardized corpus of cyberattack patterns. We adopt a vector-space model in which CAPEC attack patterns are treated as documents with term vectors. This provides a space in which to define distance measures, such as for retrieving attack patterns through term queries or finding clusters of related attack patterns. Analysis of clustering patterns, i.e., cluster hierarchies (clusters within clusters) is aided through tree visualization techniques. These analytic and visual techniques provide a range of capabilities for leveraging the content and relationships in CAPEC, e.g., for building more complex security models such as network attack graphs. © 2018 Elsevier B.V.",2018,Handbook of Statistics,2,@ chapter examines @ natural language processing @ @ applied @ building rich model @ cybersecurity analytics @ @ @ @ applies text mining to @ natural-language content of common attack pattern enumeration and classification @ capectm @ a standardized corpus of cyberattack pattern @ @ adopt a vector-space model in @ capec attack pattern @ treated a document @ term vector @ @ provides a space in @ to define distance measure @ a @ retrieving attack pattern @ term query @ finding cluster of related attack pattern @ analysis of clustering pattern i @ e @ cluster hierarchy @ cluster within cluster @ is aided @ tree visualization technique @ @ analytic and visual technique provide a range of capability @ leveraging @ content and relationship in capec e @ g @ @ building more complex security model @ a network attack graph @ @ b @ v @ 
1386,Visualizing the document pre-processing effects in text mining process,"Text mining is an important step to categorize textual data by using data mining techniques. As most obtained textual data is unstructured, it needs to be processed before applying mining algorithms – that process is known as pre-processing step in overall text mining process. Pre-processing step has important impact on mining. This paper aims at providing detailed analysis of the document pre-processing when employing multidimensional projection techniques to generate graphical representations of vector space models, which are computed from eight combinations of three steps: stemming, term weighting and term elimination based on low frequency cut. Experiments were made to show that the visual approach is useful to perceive the processing effects on document similarities and group formation (i.e., cohesion and separation). Additionally, quality measures were computed from graphical representations and compared with classification rates of a k-Nearest Neighbor and Naive Bayes classifiers, where the results highlights the importance of the pre-processing step in text mining. © Springer International Publishing AG 2018.",2018,Advances in Intelligent Systems and Computing,1,text mining is @ important step to categorize textual data by @ data mining technique @ a @ obtained textual data is unstructured @ need to @ processed @ applying mining algorithm @ process is known a pre-processing step in overall text mining process @ pre-processing step ha important impact on mining @ @ @ aim at providing detailed analysis of @ document pre-processing @ employing multidimensional projection technique to generate graphical representation of vector space model @ @ computed @ eight combination of three step @ stemming term weighting and term elimination based on low frequency cut @ experiment @ made to @ @ @ visual approach is useful to perceive @ processing effect on document similarity and group formation @ i @ e @ cohesion and separation @ @ additionally quality measure @ computed @ graphical representation and compared @ classification rate of a k-nearest neighbor and naive bayes classifier @ @ @ highlight @ importance of @ pre-processing step in text mining @ @ international publishing ag @ 
1388,Biomedical domain-oriented word embeddings via small background texts for biomedical text mining tasks,"Most word embedding methods are proposed with general purpose which take a word as a basic unit and learn embeddings by words’ external contexts. However, in the field of biomedical text mining, there are many biomedical entities and syntactic chunks which can enrich the semantic meaning of word embeddings. Furthermore, large scale background texts for training word embeddings are not available in some scenarios. Therefore, we propose a novel biomedical domain-specific word embeddings model based on maximum-margin (BEMM) to train word embeddings using small set of background texts, which incorporates biomedical domain information. Experimental results show that our word embeddings overall outperform other general-purpose word embeddings on some biomedical text mining tasks. © 2018, Springer International Publishing AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ word embedding method @ proposed @ general purpose @ take a word a a basic unit and learn embeddings by word external context @ however in @ field of biomedical text mining @ @ many biomedical entity and syntactic chunk @ @ enrich @ semantic meaning of word embeddings @ furthermore @ scale background text @ training word embeddings @ not available in some scenario @ therefore @ propose a novel biomedical domain-specific word embeddings model based on maximum-margin @ bemm @ to train word embeddings @ small set of background text @ incorporates biomedical domain information @ experimental @ @ @ @ word embeddings overall outperform @ general-purpose word embeddings on some biomedical text mining task @ @ international publishing ag @ 
1390,A text mining approach to capture user experience for new product development,"Insights into product design and development drive the strategies related to new products including price, demand, capacity, cost, and return strategies. To retrieve insights into human-product interactions, the user experience can be captured as the input of product design and development to enhance product quality and user satisfaction. However, it is critical to ascertain feedback on user experience to determine appropriate user preferences and needs. In this study, a text mining approach was developed to capture user experience in order to help product designers and developers to determine user purchase behavior and critical product features. The proposed text mining approach identifies key entities and expressions from full textual transcripts of user perceptions. Based on term frequency and emotional scales, this study provided the ordinary scale of categorical data. The emotional adjectives and functional terms of the data were analyzed to ascertain user experience and identify user segmentation. By extracting and associating key entities and expressions from user perceptions, we identified the features of wearable devices that influence user purchasing behavior. The results can assist designers to launch appropriate products and help developers to identify the key users that will enhance overall user satisfaction and company competitiveness. © International Journal of Industrial Engineering.",2018,International Journal of Industrial Engineering : Theory Applications and Practice,3,insight @ product design and development drive @ strategy related to @ product including price demand capacity cost and return strategy @ to retrieve insight @ human-product interaction @ user experience @ @ captured a @ input of product design and development to enhance product quality and user satisfaction @ however @ is critical to ascertain feedback on user experience to determine appropriate user preference and need @ in @ study a text mining approach wa developed to capture user experience in order to help product designer and developer to determine user purchase behavior and critical product feature @ @ proposed text mining approach identifies key entity and expression @ full textual transcript of user perception @ based on term frequency and emotional scale @ study provided @ ordinary scale of categorical data @ @ emotional adjective and functional term of @ data @ analyzed to ascertain user experience and identify user segmentation @ by extracting and associating key entity and expression @ user perception @ identified @ feature of wearable device @ influence user purchasing behavior @ @ @ @ assist designer to launch appropriate product and help developer to identify @ key user @ @ enhance overall user satisfaction and company competitiveness @ international journal of industrial engineering @ 
1391,"Exploring the cognitive, affective, and behavioral responses of Korean consumers toward mobile payment services: A text mining approach","The purpose of this study was to examine the cognitive, affective, and behavioral responses of Korean consumers toward mobile payment services based on the tri-component model by using a text-mining technique. Samsung Pay was chosen it is used in both online and offline transactions. We targeted social media data posted during the period between 1 July 2016 and 31 December 2016, which was about one year after Samsung Pay was launched. We conducted word frequency analysis, clustering analysis, and association rules using R programming. The results were the following. First, the 50 most frequently used words referenced the brand names of the mobile devices, payment methods, and the procedures and unique functions of Samsung Pay compared to other types of payment methods. Second, we classified the terms into 24 categories (11 categories of cognitive responses, 10 categories of affective responses, and 3 categories of behavioral responses) based on the tri-component model. The results of the clustering analysis based on the 24 categories showed a clear split between positive and negative responses at the macro level. The positive responses were further clustered into four groups, while the negative responses were fused into two groups at the micro level. Third, the association rules produced 65 rules, and we found that economic benefits played a great role in the positive feelings and continuous use of mobile payment services. This study offers valuable implications that may help mobile payment marketers with delivering services that correspond to consumer values and expectations, thus increasing consumer utility and satisfaction. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ purpose of @ study wa to examine @ cognitive affective and behavioral response of korean consumer toward mobile payment service based on @ tri-component model by @ a text-mining technique @ samsung pay wa chosen @ is used in @ online and offline transaction @ @ targeted social medium data posted @ @ period @ july and december @ wa @ @ year @ samsung pay wa launched @ @ conducted word frequency analysis clustering analysis and association rule @ r programming @ @ @ @ @ following @ first @ @ frequently used word referenced @ brand name of @ mobile device payment method and @ procedure and unique function of samsung pay compared to @ type of payment method @ second @ classified @ term @ category @ category of cognitive response category of affective response and category of behavioral response @ based on @ tri-component model @ @ @ of @ clustering analysis based on @ category showed a clear split @ positive and negative response at @ macro level @ @ positive response @ @ clustered @ four group @ @ negative response @ fused @ @ group at @ micro level @ third @ association rule produced rule and @ found @ economic benefit played a great role in @ positive feeling and continuous use of mobile payment service @ @ study offer valuable implication @ may help mobile payment marketer @ delivering service @ correspond to consumer value and expectation thus increasing consumer utility and satisfaction @ @ international publishing ag part of @ nature @ 
1392,Recommending valuable ideas in an open innovation community: A text mining approach to information overload problem,"Purpose: Open innovation communities are a growing trend across diverse industries because they provide opportunities of collaborating with customers and exploiting their knowledge effectively. Although open innovation communities can be strategic assets that can help firms innovate, firms nonetheless face the challenge of information overload incurred due to the characteristic of the community. The purpose of this paper is to mitigate the problem of information overload in an open innovation environment. Design/methodology/approach: This study chose MyStarbucksIdea.com (MSI) as a target open innovation community in which customers share their ideas. The authors analyzed a large data set collected from MSI utilizing text mining techniques including TF-IDF and sentiment analysis, while considering both term and non-term features of the data set. Those features were used to develop classification models to calculate the adoption probability of each idea. Findings: The results showed that term and non-term features play important roles in predicting the adoptability of ideas and the best classification accuracy was achieved by the hybrid classification models. In most cases, the precisions of classification models decreased as the number of recommendations increased, while the models’ recalls and F1s increased. Originality/value: This research dealt with the problem of information overload in an open innovation context. A large amount of customer opinions from an innovation community were examined and a recommendation system to mitigate the problem was proposed. Using the proposed system, the firm can get recommendations for ideas that could be valuable for its business innovation in the idea generation phase, thereby resolving the information overload and enhancing the effectiveness of open innovation. © 2018, Emerald Publishing Limited.",2018,Industrial Management and Data Systems,11,purpose @ open innovation community @ a growing trend across diverse industry @ @ provide opportunity of collaborating @ customer and exploiting @ knowledge effectively @ although open innovation community @ @ strategic asset @ @ help firm innovate firm nonetheless face @ challenge of information overload incurred due to @ characteristic of @ community @ @ purpose of @ @ is to mitigate @ problem of information overload in @ open innovation environment @ design methodology approach @ @ study chose mystarbucksidea @ com @ msi @ a a target open innovation community in @ customer share @ idea @ @ author analyzed a @ data set collected @ msi utilizing text mining technique including tf-idf and sentiment analysis @ considering @ term and non-term feature of @ data set @ @ feature @ used to develop classification model to calculate @ adoption probability of @ idea @ finding @ @ @ showed @ term and non-term feature play important role in predicting @ adoptability of idea and @ best classification accuracy wa achieved by @ hybrid classification model @ in @ case @ precision of classification model decreased a @ number of recommendation increased @ @ model recall and f s increased @ originality value @ @ research dealt @ @ problem of information overload in @ open innovation context @ a @ amount of customer opinion @ @ innovation community @ examined and a recommendation system to mitigate @ problem wa proposed @ @ @ proposed system @ firm @ get recommendation @ idea @ could @ valuable @ @ @ innovation in @ idea generation phase thereby resolving @ information overload and enhancing @ effectiveness of open innovation @ emerald publishing limited @ 
1399,Detecting aggressive behavior in discussion threads using text mining,"The detection of aggressive behavior in online discussion communities is of great interest, due to the large number of users, especially of young age, who are frequently exposed to such behaviors in social networks. Research on cyberbullying prevention focuses on the detection of potentially harmful messages and the development of intelligent systems for the identification of verbal aggressiveness expressed with insults and threats. Text mining techniques are among the most promising tools used so far in the field of aggressive sentiments detection in short texts, such as comments, reviews, tweets etc. This article presents a novel approach which employs sentiment analysis at message level, but considers the whole communication thread (i.e., users discussions) as the context of the aggressive behavior. The suggested approach is able to detect aggressive, inappropriate or antisocial behavior, under the prism of the discussion context. Key aspects of the approach are the monitoring and analysis of the most recently published comments, and the application of text classification techniques for detecting whether an aggressive action actually emerges in a discussion thread. Thorough experimental validation of the suggested approach in a dataset for cyberbullying detection tasks demonstrates its applicability and advantages compared to other approaches. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,@ detection of aggressive behavior in online discussion community is of great interest due to @ @ number of user especially of young age @ @ frequently exposed to @ behavior in social network @ research on cyberbullying prevention focus on @ detection of potentially harmful message and @ development of intelligent system @ @ identification of verbal aggressiveness expressed @ insult and threat @ text mining technique @ among @ @ promising tool used @ far in @ field of aggressive sentiment detection in short text @ a comment review tweet etc @ @ article @ a novel approach @ employ sentiment analysis at message level @ considers @ whole communication thread @ i @ e @ user discussion @ a @ context of @ aggressive behavior @ @ suggested approach is able to detect aggressive inappropriate @ antisocial behavior @ @ prism of @ discussion context @ key aspect of @ approach @ @ monitoring and analysis of @ @ recently published comment and @ application of text classification technique @ detecting whether @ aggressive action actually emerges in a discussion thread @ thorough experimental validation of @ suggested approach in a dataset @ cyberbullying detection task demonstrates @ applicability and advantage compared to @ approach @ @ nature switzerland ag @ 
1401,What affects patients’ online decisions: An empirical study of online appointment service based on text mining,"The emergence of online health communities enables patients’ comments on doctors to express their opinion on service and also make it possible for patients seeking doctors’ information before seeing doctor. Making appointment online and then go to see a doctor offline on schedule become popular in China due to its convenience. Both econometric estimations and text mining are used to explore the factors that influence patients’ selection of doctors in OAS. The results show that online satisfaction does affect patients to choose doctor, although offline attributes, such as doctor’s title and the tier level of hospital, are also considered. We find that overall satisfaction and review volume both have positive impacts on patients’ online decisions. As for the specific dimensions of satisfactions extracted from reviews, the service attitude, technical level, explanation clarity, and doctor ethics also positively affect the number of OAS. The moderating effect between doctor’s online recommendation and title is negative, as patients care more about doctor’s online reviews when she has a low title and vice versa. In addition, the results reveal that patients with high-risk disease are more sensitive to doctor’s review volume. Our findings can help doctors design their strategy of online appointment service, and also help online health communities refine their review system so that patients can express their attitudes more specifically. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ emergence of online health community enables patient comment on doctor to express @ opinion on service and @ make @ possible @ patient seeking doctor information @ seeing doctor @ making appointment online and @ go to see a doctor offline on schedule become popular in china due to @ convenience @ @ econometric estimation and text mining @ used to explore @ factor @ influence patient selection of doctor in oas @ @ @ @ @ online satisfaction doe affect patient to choose doctor although offline attribute @ a doctor s title and @ tier level of hospital @ @ considered @ @ find @ overall satisfaction and review volume @ @ positive impact on patient online decision @ a @ @ specific dimension of satisfaction extracted @ review @ service attitude technical level explanation clarity and doctor ethic @ positively affect @ number of oas @ @ moderating effect @ doctor s online recommendation and title is negative a patient care more @ doctor s online review @ @ ha a low title and vice versa @ in addition @ @ reveal @ patient @ high-risk disease @ more sensitive to doctor s review volume @ @ finding @ help doctor design @ strategy of online appointment service and @ help online health community refine @ review system @ @ patient @ express @ attitude more specifically @ @ nature switzerland ag @ 
1402,Introduction to the special issue on language in social media: Exploiting discourse and other contextual information,"Social media content is changing the way people interact with each other and share information, personal messages, and opinions about situations, objects, and past experiences. Most social media texts are short online conversational posts or comments that do not contain enough information for natural language processing (NLP) tools, as they are often accompanied by non-linguistic contextual information, including meta-data (e.g., the user’s profile, the social network of the user, and their interactions with other users). Exploiting such different types of context and their interactions makes the automatic processing of social media texts a challenging research task. Indeed, simply applying traditional text mining tools is clearly sub-optimal, as, typically, these tools take into account neither the interactive dimension nor the particular nature of this data, which shares properties with both spoken and written language. This special issue contributes to a deeper understanding of the role of these interactions to process social media data from a new perspective in discourse interpretation. This introduction first provides the necessary background to understand what context is from both the linguistic and computational linguistic perspectives, then presents the most recent context-based approaches to NLP for social media. We conclude with an overview of the papers accepted in this special issue, highlighting what we believe are the future directions in processing social media texts. © 2018, 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license.",2018,Computational Linguistics,2,social medium content is changing @ way people interact @ @ @ and share information personal message and opinion @ situation object and past experience @ @ social medium text @ short online conversational post @ comment @ @ not contain enough information @ natural language processing @ nlp @ tool a @ @ often accompanied by non-linguistic contextual information including meta-data @ e @ g @ @ user s profile @ social network of @ user and @ interaction @ @ user @ @ exploiting @ different type of context and @ interaction make @ automatic processing of social medium text a challenging research task @ indeed simply applying traditional text mining tool is clearly sub-optimal a typically @ tool take @ account neither @ interactive dimension @ @ particular nature of @ data @ share property @ @ spoken and written language @ @ special issue contributes to a deeper understanding of @ role of @ interaction to process social medium data @ a @ perspective in discourse interpretation @ @ introduction first provides @ necessary background to understand @ context is @ @ @ linguistic and computational linguistic perspective @ @ @ @ recent context-based approach to nlp @ social medium @ @ conclude @ @ overview of @ @ accepted in @ special issue highlighting @ @ believe @ @ future direction in processing social medium text @ association @ computational linguistics published @ a creative common attribution-noncommercial-noderivatives @ international @ cc by-nc-nd @ @ license @ 
1403,Gene prioritization using a deep learning approach,"The research of related genes is an extremely important issue in the study of human genes. Scientists could utilize related human genes to build huge, integrated gene regulatory network which enables scientists to discover the relationships between genes and diseases. In this research, we conduct the text mining through Natural Language Processing (NLP) by introducing a deep learning method. In order to obtain the association between genes based on the text information, we construct a parallel Convolutional Neural Network (CNN) with parallel convolutional layers and dynamic convolutional pooling layers to deduce the relationship — positive, negative, or nonexistent. Unlike other studies, we extract the positive and negative gene relationships, and apply the deep learning method to this field. For gene relationship recognition, the accuracy is 85.2%. The result of this algorithm turns out to be promising. © 2018 The authors and IOS Press. All rights reserved.",2018,Frontiers in Artificial Intelligence and Applications,0,@ research of related gene is @ extremely important issue in @ study of human gene @ scientist could utilize related human gene to build huge integrated gene regulatory network @ enables scientist to discover @ relationship @ gene and disease @ in @ research @ conduct @ text mining @ natural language processing @ nlp @ by introducing a deep learning method @ in order to obtain @ association @ gene based on @ text information @ construct a parallel convolutional neural network @ cnn @ @ parallel convolutional layer and dynamic convolutional pooling layer to deduce @ relationship positive negative @ nonexistent @ unlike @ study @ extract @ positive and negative gene relationship and apply @ deep learning method to @ field @ @ gene relationship recognition @ accuracy is @ @ @ @ of @ algorithm turn @ to @ promising @ @ author and io @ @ @ right reserved @ 
1404,In search of public agenda with text mining: An exploratory study of Agenda setting dynamics between the traditional media and wikipedia,"When Downs [1] proposed his famous issue-attention cycle in 1972, he thought that the mass media would report news and information that arouses people’s interests. This thought, however, is prone to challenges. With the prevalence of the Internet and, perhaps more importantly, the concept of Web 2.0, Wikipedia becomes another major source of information for the public. Given that Wikipedia allows anyone to edit the content, the details about a particular event or issue on pages of Wikipedia can be considered as a quasi-public agenda. Understanding this quasi-public agenda may help us evaluate different models of policy cycles, including the Downs’ famous issue-attention cycle. My study aims to assess the agenda setting dynamics among 5 major news outlets in the UK, as the traditional mass media, and Wikipedia, as a form of participatory journalism. By agenda, it refers to the choices of frames and sentiment. Using text mining techniques, my study assesses the choices of frames and sentiment adopted by the articles of the news outlets and the Wikipedia pages concerning with the issue Brexit. The timeline of the study is between the date when the Wikipedia page “Brexit” emerged and the date of the Brexit referendum. The study also explores the possible relationship between these agendas. Frame analysis of the news articles will be conducted through automatic text classification, whereas the frames on the Wikipedia pages will be analyzed through both text classification and clustering. Lexicon-based approach will be used for sentiment analysis. The relationship between the agendas will be explored through Granger-causality tests. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ down proposed @ famous issue-attention cycle in he thought @ @ mass medium would report news and information @ arouses people s interest @ @ thought however is prone to challenge @ @ @ prevalence of @ internet and perhaps more importantly @ concept of web @ wikipedia becomes another major source of information @ @ public @ given @ wikipedia allows anyone to edit @ content @ detail @ a particular event @ issue on page of wikipedia @ @ considered a a quasi-public agenda @ understanding @ quasi-public agenda may help u evaluate different model of policy cycle including @ down famous issue-attention cycle @ @ study aim to ass @ agenda setting dynamic among major news outlet in @ uk a @ traditional mass medium and wikipedia a a form of participatory journalism @ by agenda @ refers to @ choice of frame and sentiment @ @ text mining technique @ study ass @ choice of frame and sentiment adopted by @ article of @ news outlet and @ wikipedia page concerning @ @ issue brexit @ @ timeline of @ study is @ @ date @ @ wikipedia page brexit emerged and @ date of @ brexit referendum @ @ study @ explores @ possible relationship @ @ agenda @ frame analysis of @ news article @ @ conducted @ automatic text classification whereas @ frame on @ wikipedia page @ @ analyzed @ @ text classification and clustering @ lexicon-based approach @ @ used @ sentiment analysis @ @ relationship @ @ agenda @ @ explored @ granger-causality test @ @ nature switzerland ag @ 
1405,Text mining analysis of teaching evaluation questionnaires for the selection of outstanding teaching faculty members,"This paper was conducted in collaboration with the Office of Institutional Research at National Ilan University in Taiwan to analyze textual opinions found in teaching evaluation questionnaires and applied the analysis results to assisting the selection of outstanding teaching faculty members. The selection of outstanding teachers requires that selection committee members spend a large amount of time reviewing written data. Therefore, this paper develops a set of systems for the analysis of textual opinions in teaching evaluation questionnaires, providing reference materials for the selection committee. The teaching evaluation questionnaire is a form of educational data. This paper analyzes these data using educational data mining. In text mining, text sentiment analysis is a common textual data quantification method that can analyze the sentiment tendency of a text author. This paper uses a text sentiment analysis to quantify the students' textual opinions and to provide the selection committee with the sentiment tendency of students' comments on teaching faculty members. We analyze text sentiment separately for different classifiers by using the Chinese text sentiment analysis kit SnowNLP. We compare the efficacy of classifiers that do not take time series factors into consideration (naïve Bayes and fully connected neural network) to those that do [recurrent neural network (RNN), long short-term memory (LSTM) RNN, and attention RNN]. We found that classifiers that consider time series factors are more effective at analyzing text sentiment. Furthermore, adding LSTM cells and an attention mechanism to a traditional RNN classifier effectively improved its efficacy on long-sequence tasks. As a result, we chose the attention LSTM classifier-with a positive sentiment recognition rate of 97% and a negative sentiments recognition rate of 87%- A s our preferred text sentiment classifier. Finally, we set up an analytics server that will be modularized to facilitate its integration into the systems of different schools. © 2018 IEEE.",2018,IEEE Access,7,@ @ wa conducted in collaboration @ @ office of institutional research at national ilan university in taiwan to analyze textual opinion found in teaching evaluation questionnaire and applied @ analysis @ to assisting @ selection of outstanding teaching faculty member @ @ selection of outstanding teacher requires @ selection committee member spend a @ amount of time reviewing written data @ therefore @ @ develops a set of system @ @ analysis of textual opinion in teaching evaluation questionnaire providing reference material @ @ selection committee @ @ teaching evaluation questionnaire is a form of educational data @ @ @ analyzes @ data @ educational data mining @ in text mining text sentiment analysis is a common textual data quantification method @ @ analyze @ sentiment tendency of a text author @ @ @ us a text sentiment analysis to quantify @ student @ textual opinion and to provide @ selection committee @ @ sentiment tendency of student @ comment on teaching faculty member @ @ analyze text sentiment separately @ different classifier by @ @ chinese text sentiment analysis kit snownlp @ @ compare @ efficacy of classifier @ @ not take time series factor @ consideration @ naïve bayes and fully connected neural network @ to @ @ @ recurrent neural network @ rnn @ long short-term memory @ lstm @ rnn and attention rnn @ @ found @ classifier @ consider time series factor @ more effective at analyzing text sentiment @ furthermore adding lstm cell and @ attention mechanism to a traditional rnn classifier effectively improved @ efficacy on long-sequence task @ a a @ @ chose @ attention lstm classifier-with a positive sentiment recognition rate of and a negative sentiment recognition rate of a s @ preferred text sentiment classifier @ finally @ set up @ analytics server @ @ @ modularized to facilitate @ integration @ @ system of different school @ @ @ 
1406,"11th International Symposium on Natural Language Processing, SNLP-2016 and 1st Workshop in Intelligent Informatics and Smart Technology, 2016",The proceedings contain 22 papers. The special focus in this conference is on . The topics include: The NECTEC 2015 Thai open-domain automatic speech recognition system; handwritten character strings on medical prescription reading by using lexicon-driven; eye Region detection in fatigue monitoring for the military using AdaBoost algorithm; The feature extraction of ECG signal in myocardial infarction patients; the algorithm of static hand gesture recognition using rule-based classification; Robust watermarking for medical image authentication based on DWT with QR codes in telemedicine; intelligence planning for aerobic training using a genetic algorithm; palm’s lines detection and automatic palmistry prediction system; the algorithm for financial transactions on smartphones using two-factor authentication based on passwords and face recognition; design for a listening learner corpus for a listenability measurement method; medical instructional media in human body systems using 3D projection mapping; development of electromagnetic wave propagation in microstrip antenna using the novel finite element method; detection of diabetic retinopathy using image processing; The system validation documentation using text segmentation technique based on CMMI standards; a study of a Thai-English translation comparing on applying phrase-based and hierarchical phrase-based translation; english-Cebuano parallel language resource for statistical machine translation system; a part-of-speech-based exploratory text mining of students’ looking-back evaluation; finding key terms representing events from Thai Twitter; comparison of document clustering methods based on bees algorithm and firefly algorithm using Thai documents; the development of semi-automatic sentiment lexicon construction tool for Thai sentiment analysis.,2018,Advances in Intelligent Systems and Computing,0,@ proceeding contain @ @ @ special focus in @ conference is on @ @ topic include @ @ nectec thai open-domain automatic speech recognition system @ handwritten character string on medical prescription reading by @ lexicon-driven @ eye region detection in fatigue monitoring @ @ military @ adaboost algorithm @ @ feature extraction of ecg signal in myocardial infarction patient @ @ algorithm of static hand gesture recognition @ rule-based classification @ robust watermarking @ medical image authentication based on dwt @ qr code in telemedicine @ intelligence planning @ aerobic training @ a genetic algorithm @ palm s line detection and automatic palmistry prediction system @ @ algorithm @ financial transaction on smartphones @ two-factor authentication based on password and face recognition @ design @ a listening learner corpus @ a listenability measurement method @ medical instructional medium in human body system @ @ projection mapping @ development of electromagnetic wave propagation in microstrip antenna @ @ novel finite element method @ detection of diabetic retinopathy @ image processing @ @ system validation documentation @ text segmentation technique based on cmmi standard @ a study of a thai-english translation comparing on applying phrase-based and hierarchical phrase-based translation @ english-cebuano parallel language resource @ statistical machine translation system @ a part-of-speech-based exploratory text mining of student looking-back evaluation @ finding key term representing event @ thai twitter @ comparison of document clustering method based on bee algorithm and firefly algorithm @ thai document @ @ development of semi-automatic sentiment lexicon construction tool @ thai sentiment analysis @ 
1408,Data analysis: Opinion mining and sentiment analysis of opinionated unstructured data,"With the evolution of technology, there is also a huge increase in unstructured data. Now a day’s social media is an obvious source of current opinions and reviews and to extract the valuable suggestions on the basis of comments and opinions given on social network is very important. This paper includes data analysis and data mining with a special emphasis on Opinion mining and sentiment analysis. It is one of the most vigorous research areas in natural language processing and is also widely studied in areas like Web mining, and text mining. This paper tackles a comprehensive overview of last update in this field. For the very first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Users not only use the resources but also give their opinions and suggestions in the form of feedback for the improvement of the existing system. It is very important to analyze their opinions and to extract the valuable opinions and suggestions from the comments. This paper proposed algorithms of machine learning and lexicon based approaches to address the issue arised due to lack of analyzing tools. © Springer Nature Singapore Pte Ltd 2018.",2018,Communications in Computer and Information Science,2,@ @ evolution of technology @ is @ a huge increase in unstructured data @ now a day s social medium is @ obvious source of current opinion and review and to extract @ valuable suggestion on @ basis of comment and opinion given on social network is @ important @ @ @ includes data analysis and data mining @ a special emphasis on opinion mining and sentiment analysis @ @ is @ of @ @ vigorous research area in natural language processing and is @ widely studied in area like web mining and text mining @ @ @ tackle a comprehensive overview of last update in @ field @ @ @ @ first time in human history @ now @ a huge volume of opinionated data recorded in digital form @ analysis @ user not only use @ resource @ @ give @ opinion and suggestion in @ form of feedback @ @ improvement of @ existing system @ @ is @ important to analyze @ opinion and to extract @ valuable opinion and suggestion @ @ comment @ @ @ proposed algorithm of machine learning and lexicon based approach to address @ issue arised due to lack of analyzing tool @ @ nature singapore pte ltd @ 
1409,An automatic approach to generate corpus in Spanish,"A corpus is an indispensable linguistic resource for any application of natural language processing. Some corpora have been created manually or semi-automatically for a specific domain. In this paper, we present an automatic approach to generate corpus from digital information sources such as Wikipedia and web pages. The information extracted by Wikipedia is done by delimiting the domain, using a propagation algorithm to determine the categories associated with a domain region and a set of seeds to delimit the search. The information extracted from the web pages is carried out efficiently, determining the patterns associated with the structure of each page with the purpose of defining the quality of the extraction. © Springer Nature Switzerland AG 2018.",2018,Communications in Computer and Information Science,0,a corpus is @ indispensable linguistic resource @ @ application of natural language processing @ some corpus @ @ created manually @ semi-automatically @ a specific domain @ in @ @ @ @ @ automatic approach to generate corpus @ digital information source @ a wikipedia and web page @ @ information extracted by wikipedia is done by delimiting @ domain @ a propagation algorithm to determine @ category associated @ a domain region and a set of seed to delimit @ search @ @ information extracted @ @ web page is carried @ efficiently determining @ pattern associated @ @ structure of @ page @ @ purpose of defining @ quality of @ extraction @ @ nature switzerland ag @ 
1411,Ensemble text mining for sentiment analysis using deep learning based stacked auto encoder,"Text mining is the most widely used domain in the data mining and in the present world. Sentiment analysis is most one of the sub-domain in the present world. Using of various natural language processing algorithms (NLPS) and artificial intelligence algorithms the issues identified in the text mining with sentiment analysis is solved very rarely. Still there is a lack of compatibility based on the results. Previously the new ensemble approach is introduced which is merged with the features of text mining approach, NLP technique and artificial intelligence. This shows the result based on the documents mining with document or article belongs to which topic such as political, sports, technology and various domains. But due to the more computation time, lack of quality results and compatibility issues are identified. In this paper, the advanced ensemble approach to implement TSA using Stacked Auto Encoders (SAE) which is stochastic machine learning process that evolves over time in a probabilistic manner. So we propose a deep-learning and artificial intelligence-based TSA prediction method that comprises of a stacked auto encoder (SAE) model that is used to learn generic linguistic and text semantic features, and it is trained in a layer wise greedy fashion. Results show the performance of the algorithm. © Institute of Advanced Scientific Research, Inc.. All rights reserved.",2018,Journal of Advanced Research in Dynamical and Control Systems,1,text mining is @ @ widely used domain in @ data mining and in @ @ world @ sentiment analysis is @ @ of @ sub-domain in @ @ world @ @ of various natural language processing algorithm @ nlp @ and artificial intelligence algorithm @ issue identified in @ text mining @ sentiment analysis is solved @ rarely @ still @ is a lack of compatibility based on @ @ @ @ @ @ ensemble approach is introduced @ is merged @ @ feature of text mining approach nlp technique and artificial intelligence @ @ @ @ @ based on @ document mining @ document @ article belongs to @ topic @ a political sport technology and various domain @ @ due to @ more computation time lack of quality @ and compatibility issue @ identified @ in @ @ @ advanced ensemble approach to implement tsa @ stacked auto encoders @ sae @ @ is stochastic machine learning process @ evolves @ time in a probabilistic manner @ @ @ propose a deep-learning and artificial intelligence-based tsa prediction method @ comprises of a stacked auto encoder @ sae @ model @ is used to learn generic linguistic and text semantic feature and @ is trained in a layer wise greedy fashion @ @ @ @ performance of @ algorithm @ institute of advanced scientific research inc @ @ @ right reserved @ 
1412,Detection of typical progress patterns of industrial incidents by text mining technique,"To prevent accidents, it is very important to learn why and how past accidents occurred and escalated. The information of accidents is mostly recorded in natural language texts, which is not convenient to analyze the flow of events in the accidents. This paper proposes a method to recognize typical flow of events in a large set of text reports. By focusing two adjacent sentences, our system succeeded to detect typical pairs of predecessor word and successor word. Then we can recognize the typical flows of accidents. © Her Majesty the Queen in Right of United Kingdom 2018.",2018,Advances in Intelligent Systems and Computing,2,to prevent accident @ is @ important to learn @ and @ past accident occurred and escalated @ @ information of accident is mostly recorded in natural language text @ is not convenient to analyze @ flow of event in @ accident @ @ @ proposes a method to recognize typical flow of event in a @ set of text report @ by focusing @ adjacent sentence @ system succeeded to detect typical pair of predecessor word and successor word @ @ @ @ recognize @ typical flow of accident @ @ majesty @ queen in right of united kingdom @ 
1414,One single deep bidirectional LSTM network for word sense disambiguation of text data,"Due to recent technical and scientific advances, we have a wealth of information hidden in unstructured text data such as offline/online narratives, research articles, and clinical reports. To mine these data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,due to recent technical and scientific advance @ @ a wealth of information hidden in unstructured text data @ a offline online narrative research article and clinical report @ to mine @ data properly attributable to @ innate ambiguity a word sense disambiguation @ wsd @ algorithm @ avoid number of difficulty in natural language processing @ nlp @ pipeline @ however considering a @ number of ambiguous word in @ language @ technical domain @ may encounter limiting constraint @ proper deployment of existing wsd model @ @ @ attempt to address @ problem of one-classifier-per-one-word wsd algorithm by proposing a single bidirectional long short-term memory @ blstm @ network @ by considering sens and context sequence work on @ ambiguous word collectively @ evaluated on senseval benchmark @ @ @ @ of @ model is comparable @ top-performing wsd algorithm @ @ @ discus @ applying additional modification alleviates @ model fault and @ need @ more training data @ @ international publishing ag part of @ nature @ 
1416,Social choice theory based domain specific Hindi stop words list construction and its application in text mining,"In this paper, we have given an attempt to create domain specific Hindi stop words list using statistical and knowledge based techniques from prepared textual corpora of different domains. In order to remove the biased raking nature of each technique, Borda’s rule of vote ranking method has been employed for unbiased stop words list construction. We also propose a novel approach called netting ranked performance evaluation (NRPE) to evaluate prepared stop words lists, in which stop words removal is done in leading and trailing fashion based on ascending and descending order of terms. Further, using combined band net (CBN) performance, we demonstrate the ability of each technique in identifying of candidate stop words followed by selection of features for text mining models. The experimental results show that a technique selects good features for classification/clustering needs not necessarily finds the good stop words. Results also show that the final Borda’s lists gives normalized performance over individual technique. This approach guarantees candidate stop word removal, least information dissipation and text mining model performance enhancement. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,in @ @ @ @ given @ attempt to create domain specific hindi stop word list @ statistical and knowledge based technique @ prepared textual corpus of different domain @ in order to remove @ biased raking nature of @ technique borda s rule of vote ranking method ha @ employed @ unbiased stop word list construction @ @ @ propose a novel approach called netting ranked performance evaluation @ nrpe @ to evaluate prepared stop word list in @ stop word removal is done in leading and trailing fashion based on ascending and descending order of term @ @ @ combined band net @ cbn @ performance @ demonstrate @ ability of @ technique in identifying of candidate stop word followed by selection of feature @ text mining model @ @ experimental @ @ @ a technique selects good feature @ classification clustering need not necessarily find @ good stop word @ @ @ @ @ @ final borda s list give normalized performance @ individual technique @ @ approach guarantee candidate stop word removal least information dissipation and text mining model performance enhancement @ @ nature switzerland ag @ 
1418,Irony classification in Turkish text,"Millions of comments and thoughts have been shared every day in the websites since the Internet became a part of our life. These comments and thoughts are the subject of researches in the field of text mining. Irony is a language art that is common in the text resources on the Internet, it causes the classification algorithm to produce incorrect results in sentiment analysis studies. So, it is necessary to detect the irony to improve the sentiment analysis classification score. Classified irony result should be added to the sentiment analysis as a feature. In this study, irony is classified for Turkish sentiment analysis studies. A new algorithm for irony detection is developed with the help of a text mining methods and libraries which are analyzing Turkish texts morphologically and sentimentally. 13 different features were used and six different classification algorithms was employed in the study. After all, %88 classification rate was achieved by this algorithm.",2018,CEUR Workshop Proceedings,0,million of comment and thought @ @ shared every day in @ website since @ internet became a part of @ life @ @ comment and thought @ @ subject of research in @ field of text mining @ irony is a language art @ is common in @ text resource on @ internet @ cause @ classification algorithm to produce incorrect @ in sentiment analysis study @ @ @ is necessary to detect @ irony to improve @ sentiment analysis classification score @ classified irony @ @ @ added to @ sentiment analysis a a feature @ in @ study irony is classified @ turkish sentiment analysis study @ a @ algorithm @ irony detection is developed @ @ help of a text mining method and library @ @ analyzing turkish text morphologically and sentimentally @ different feature @ used and six different classification algorithm wa employed in @ study @ @ @ classification rate wa achieved by @ algorithm @ 
1420,Gene-disease-food relation extraction from biomedical database,"Through the past years, an incredible increase in the biomedical data amount presented on the web is enlarged due to the increased data volume in the medical and biological domains. Hence, the search for documents and information on the internet became increasingly complicated. In the current work, a new approach for information extraction using the Natural Language Processing (NLP) tools and ontology was proposed. It described a system to extract relations between the concepts from biomedical texts using morphological analysis and information extraction techniques. In the first step, the system segmented the input text into sentences. Each sentence is then segmented into words that were tagged with part-of-speech labels and concept classes (food, drug, and gene). A set of relation extraction rules (regular expression patterns) are applied on the annotated sentences. If a pattern matches, the concepts and relations are extracted. The system has been tested on a set of 700 MEDLINE abstracts. For performance evaluation, the precision, recall and F-score were calculated. The proposed approach created by information retrieval from MEDLINE to gather a set of abstracts related to a given domain. Then, these texts were annotated using an automaton and ontology via recognizing interesting concepts for morphological analysis. After the annotation step, some rules were summarizing in an automaton that help gene-disease-food relationships discovery. This work proposed an approach for identifying relations between medical concepts using NLP tools. An evaluation experiment reported good effectiveness results. © 2018, Springer International Publishing AG.",2018,Advances in Intelligent Systems and Computing,1,@ @ past year @ incredible increase in @ biomedical data amount presented on @ web is enlarged due to @ increased data volume in @ medical and biological domain @ hence @ search @ document and information on @ internet became increasingly complicated @ in @ current work a @ approach @ information extraction @ @ natural language processing @ nlp @ tool and ontology wa proposed @ @ described a system to extract relation @ @ concept @ biomedical text @ morphological analysis and information extraction technique @ in @ first step @ system segmented @ input text @ sentence @ @ sentence is @ segmented @ word @ @ tagged @ part-of-speech label and concept class @ food drug and gene @ @ a set of relation extraction rule @ regular expression pattern @ @ applied on @ annotated sentence @ if a pattern match @ concept and relation @ extracted @ @ system ha @ tested on a set of medline abstract @ @ performance evaluation @ precision recall and f-score @ calculated @ @ proposed approach created by information retrieval @ medline to gather a set of abstract related to a given domain @ @ @ text @ annotated @ @ automaton and ontology via recognizing interesting concept @ morphological analysis @ @ @ annotation step some rule @ summarizing in @ automaton @ help gene-disease-food relationship discovery @ @ work proposed @ approach @ identifying relation @ medical concept @ nlp tool @ @ evaluation experiment reported good effectiveness @ @ @ international publishing ag @ 
1422,Identifying Service Gaps from Public Patient Opinions Through Text Mining,"Nowadays, healthcare systems have become increasingly patient-centered and the unstructured, open-ended and patient-driven feedback has drawn a significant attention from medical and healthcare organizations. Based on this, we are motivated to harness various machine learning algorithms to process such a large amount of unstructured comments posted on public patient opinion sites. We first used sentiment analysis to automatically predict the concerns of patients from the training set which was already labelled. Then, with the help of the clustering, we extracted the hot topics related to a specific domain to reflect the service issues that patients concern most. Through experimental studies, the performance of different algorithms and the influence of different parameter were compared. Finally, refering to the survey and previous studies, the results were analyzed to obtain the conclusions. © 2018, Springer Nature Singapore Pte Ltd.",2018,Communications in Computer and Information Science,0,nowadays healthcare system @ become increasingly patient-centered and @ unstructured open-ended and patient-driven feedback ha drawn a significant attention @ medical and healthcare organization @ based on @ @ @ motivated to harness various machine learning algorithm to process @ a @ amount of unstructured comment posted on public patient opinion site @ @ first used sentiment analysis to automatically predict @ concern of patient @ @ training set @ wa already labelled @ @ @ @ help of @ clustering @ extracted @ hot topic related to a specific domain to reflect @ service issue @ patient concern @ @ @ experimental study @ performance of different algorithm and @ influence of different parameter @ compared @ finally refering to @ survey and previous study @ @ @ analyzed to obtain @ conclusion @ @ nature singapore pte ltd @ 
1423,Defining a gold standard for a Swedish sentiment lexicon: Towards higher-yield text mining in the digital humanities,"There is an increasing demand for multilingual sentiment analysis, and most work on sentiment lexicons is still carried out based on English lexicons like WordNet. In addition, many of the non-English sentiment lexicons that do exist have been compiled by (machine) translation from English resources, thereby arguably obscuring possible language-specific characteristics of sentiment-loaded vocabulary. In this paper we describe the creation from scratch of a gold standard for the sentiment annotation of Swedish terms as a first step towards the creation of a full-fledged sentiment lexicon for Swedish. © 2018 CEUR-WS. All rights reserved.",2018,CEUR Workshop Proceedings,1,@ is @ increasing demand @ multilingual sentiment analysis and @ work on sentiment lexicon is still carried @ based on english lexicon like wordnet @ in addition many of @ non-english sentiment lexicon @ @ exist @ @ compiled by @ machine @ translation @ english resource thereby arguably obscuring possible language-specific characteristic of sentiment-loaded vocabulary @ in @ @ @ describe @ creation @ scratch of a gold standard @ @ sentiment annotation of swedish term a a first step towards @ creation of a full-fledged sentiment lexicon @ swedish @ ceur-ws @ @ right reserved @ 
1425,Use of sentiment mining and online NMF for topic modeling through the analysis of patients online unstructured comments,"Patients have posted thousands of online reviews to assess their doctors’ performance. Mechanisms to collect unstructured feedback from patients of healthcare providers have become very common, but there are scarce researches on different analysis techniques to examine such feedback have not frequently been applied in this context. We apply text mining techniques to compare online physician reviews from RateMDs and Healthgrades, to measure the systematic similarities and differences in patient reviews between these two platforms. We use sentiment analysis techniques to categorize online patients’ reviews as either positive or negative descriptions of their health care. We apply a customized text mining technique, ONMF topic modeling to identify the major topics on two platforms. Our text mining techniques revealed research area on how to use big data and text mining techniques to help health care providers, and organizations hear patient voices to improve the health service quality. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,patient @ posted thousand of online review to ass @ doctor performance @ mechanism to collect unstructured feedback @ patient of healthcare provider @ become @ common @ @ @ scarce research on different analysis technique to examine @ feedback @ not frequently @ applied in @ context @ @ apply text mining technique to compare online physician review @ ratemds and healthgrades to measure @ systematic similarity and difference in patient review @ @ @ platform @ @ use sentiment analysis technique to categorize online patient review a either positive @ negative description of @ health care @ @ apply a customized text mining technique onmf topic modeling to identify @ major topic on @ platform @ @ text mining technique revealed research area on @ to use big data and text mining technique to help health care provider and organization hear patient voice to improve @ health service quality @ @ nature switzerland ag @ 
1427,Text classification techniques: A literature review,"Aim/Purpose The aim of this paper is to analyze various text classification techniques employed in practice, their strengths and weaknesses, to provide an improved awareness regarding various knowledge extraction possibilities in the field of data mining. Background Artificial Intelligence is reshaping text classification techniques to better acquire knowledge. However, in spite of the growth and spread of AI in all fields of research, its role with respect to text mining is not well understood yet. Methodology For this study, various articles written between 2010 and 2017 on “text classification techniques in AI”, selected from leading journals of computer science, were analyzed. Each article was completely read. The research problems related to text classification techniques in the field of AI were identified and techniques were grouped according to the algorithms involved. These algorithms were divided based on the learning procedure used. Finally, the findings were plotted as a tree structure for visualizing the relationship between learning procedures and algorithms. Contribution This paper identifies the strengths, limitations, and current research trends in text classification in an advanced field like AI. This knowledge is crucial for data scientists. They could utilize the findings of this study to devise customized data models. It also helps the industry to understand the operational efficiency of text mining techniques. It further contributes to reducing the cost of the projects and supports effective decision making. Findings It has been found more important to study and understand the nature of data before proceeding into mining. The automation of text classification process is required, with the increasing amount of data and need for accuracy. Another interesting research opportunity lies in building intricate text data models with deep learning systems. It has the ability to execute complex Natural Language Processing (NLP) tasks with semantic requirements. Recommendations Frame analysis, deception detection, narrative science where data expresses a for Practitioners story, healthcare applications to diagnose illnesses and conversation analysis are some of the recommendations suggested for practitioners. Recommendation Developing simpler algorithms in terms of coding and implementation, better for Researchers approaches for knowledge distillation, multilingual text refining, domain knowledge integration, subjectivity detection, and contrastive viewpoint summarization are some of the areas that could be explored by researchers. Impact on Society Text classification forms the base of data analytics and acts as the engine behind knowledge discovery. It supports state-of-the-art decision making, for example, predicting an event before it actually occurs, classifying a transaction as ‘Fraudulent’ etc. The results of this study could be used for developing applications dedicated to assisting decision making processes. These informed decisions will help to optimize resources and maximize benefits to the mankind. Future Research In the future, better methods for parameter optimization will be identified by selecting better parameters that reflects effective knowledge discovery. The role of streaming data processing is still rarely explored when it comes to text classification. © 2018 Informing Science Institute. All Rights Reserved.",2018,"Interdisciplinary Journal of Information, Knowledge, and Management",15,aim purpose @ aim of @ @ is to analyze various text classification technique employed in practice @ strength and weakness to provide @ improved awareness regarding various knowledge extraction possibility in @ field of data mining @ background artificial intelligence is reshaping text classification technique to better acquire knowledge @ however in spite of @ growth and spread of ai in @ field of research @ role @ respect to text mining is not well understood yet @ methodology @ @ study various article written @ and on text classification technique in ai selected @ leading journal of computer science @ analyzed @ @ article wa completely read @ @ research problem related to text classification technique in @ field of ai @ identified and technique @ grouped according to @ algorithm involved @ @ algorithm @ divided based on @ learning procedure used @ finally @ finding @ plotted a a tree structure @ visualizing @ relationship @ learning procedure and algorithm @ contribution @ @ identifies @ strength limitation and current research trend in text classification in @ advanced field like ai @ @ knowledge is crucial @ data scientist @ @ could utilize @ finding of @ study to devise customized data model @ @ @ help @ industry to understand @ operational efficiency of text mining technique @ @ @ contributes to reducing @ cost of @ project and support effective decision making @ finding @ ha @ found more important to study and understand @ nature of data @ proceeding @ mining @ @ automation of text classification process is required @ @ increasing amount of data and need @ accuracy @ another interesting research opportunity lie in building intricate text data model @ deep learning system @ @ ha @ ability to execute complex natural language processing @ nlp @ task @ semantic requirement @ recommendation frame analysis deception detection narrative science @ data express a @ practitioner story healthcare application to diagnose illness and conversation analysis @ some of @ recommendation suggested @ practitioner @ recommendation developing simpler algorithm in term of coding and implementation better @ researcher approach @ knowledge distillation multilingual text refining domain knowledge integration subjectivity detection and contrastive viewpoint summarization @ some of @ area @ could @ explored by researcher @ impact on society text classification form @ base of data analytics and act a @ engine behind knowledge discovery @ @ support state-of-the-art decision making @ example predicting @ event @ @ actually occurs classifying a transaction a fraudulent etc @ @ @ of @ study could @ used @ developing application dedicated to assisting decision making process @ @ informed decision @ help to optimize resource and maximize benefit to @ mankind @ future research in @ future better method @ parameter optimization @ @ identified by selecting better parameter @ reflects effective knowledge discovery @ @ role of streaming data processing is still rarely explored @ @ come to text classification @ informing science institute @ @ right reserved @ 
1429,Feature extraction in subject classification of text documents in Polish,"In this work we evaluate two different methods for deriving features for a subject classification of text documents. The first method uses the standard Bag-of-Words (BoW) approach, which represents the documents with vectors of frequencies of selected terms appearing in the documents. This method heavily relies on the natural language processing (NLP) tools to properly preprocess text in the grammar- and inflection-conscious way. The second approach is based on the word-embedding technique recently proposed by Mikolov and does not require any NLP preprocessing. In this method the words are represented as vectors in continuous space and this representation of words is used to construct the feature vectors of the documents. We evaluate these fundamentally different approaches in the task of classification of Polish language Wikipedia articles with 34 subject areas. Our study suggests that the word-embedding based features seem to outperform the standard NLP-based features providing sufficiently large training dataset is available. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,in @ work @ evaluate @ different method @ deriving feature @ a subject classification of text document @ @ first method us @ standard bag-of-words @ bow @ approach @ represents @ document @ vector of frequency of selected term appearing in @ document @ @ method heavily relies on @ natural language processing @ nlp @ tool to properly preprocess text in @ grammar and inflection-conscious way @ @ second approach is based on @ word-embedding technique recently proposed by mikolov and doe not require @ nlp preprocessing @ in @ method @ word @ represented a vector in continuous space and @ representation of word is used to construct @ feature vector of @ document @ @ evaluate @ fundamentally different approach in @ task of classification of polish language wikipedia article @ subject area @ @ study suggests @ @ word-embedding based feature seem to outperform @ standard nlp-based feature providing sufficiently @ training dataset is available @ @ international publishing ag part of @ nature @ 
1430,A survey on twitter sentimental analysis with machine learning techniques,"Sentiment or emotion behind a tweet from Twitter or a post from Facebook can help us answer what opinions or feedback a person has. With the advent of growing user-generated blogs, posts and reviews across various social media and online retails, calls for an understanding of these afore mentioned user data acts as a catalyst in building Recommender systems and drive business plans. User reviews on online retail stores influence buying behavior of customers and thus complements the ever-growing need of sentiment analysis. Machine Learning helps us to read between the lines of tweets by proving us with various algorithms like Naïve Bayes, SVM, etc. Sentiment Analysis uses Machine Learning and Natural Language Processing (NLP) to extract, classify and analyze tweets for sentiments (emotions). There are various packages and frameworks in R and Python that aid in Sentiment Analysis or Text Mining in general. © 2018 Authors.",2018,International Journal of Engineering and Technology(UAE),2,sentiment @ emotion behind a tweet @ twitter @ a post @ facebook @ help u answer @ opinion @ feedback a person ha @ @ @ advent of growing user-generated blog post and review across various social medium and online retail call @ @ understanding of @ afore mentioned user data act a a catalyst in building recommender system and drive @ plan @ user review on online retail store influence buying behavior of customer and thus complement @ ever-growing need of sentiment analysis @ machine learning help u to read @ @ line of tweet by proving u @ various algorithm like naïve bayes svm etc @ sentiment analysis us machine learning and natural language processing @ nlp @ to extract classify and analyze tweet @ sentiment @ emotion @ @ @ @ various package and framework in r and python @ aid in sentiment analysis @ text mining in general @ author @ 
1431,Ontology development through concept map and text analytics: The case of automotive safety ontology,"Ontology development is an expensive and time-consuming process. The development of real-world organizational ontology-based knowledge management systems is still in early stages. Some existing ontologies with simple tuples and properties are not designed for domain specific requirement, or does not utilize existing knowledge from organizational database or documents. Here we propose our concept map approach to first semi-automatically create a detailed level entities/concepts as a keyword list by applying natural language processing, including word dependency and POS tagging. Then this list can be used to extract entities/concepts for the same domain. This approach is applied to automotive safety domain. The results are further mapped to existing ontology and aggregated to form a concept map. We implement our approach in KNIME with Stanford NLP parser and generate a concept map from automotive safety complaint dataset. The final results expand the existing ontology, and also bridge the gap between ontology and real-world organization ontology-based knowledge management systems. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,ontology development is @ expensive and time-consuming process @ @ development of real-world organizational ontology-based knowledge management system is still in early stage @ some existing ontology @ simple tuples and property @ not designed @ domain specific requirement @ doe not utilize existing knowledge @ organizational database @ document @ @ @ propose @ concept map approach to first semi-automatically create a detailed level entity concept a a keyword list by applying natural language processing including word dependency and po tagging @ @ @ list @ @ used to extract entity concept @ @ @ domain @ @ approach is applied to automotive safety domain @ @ @ @ @ mapped to existing ontology and aggregated to form a concept map @ @ implement @ approach in knime @ stanford nlp parser and generate a concept map @ automotive safety complaint dataset @ @ final @ expand @ existing ontology and @ bridge @ gap @ ontology and real-world organization ontology-based knowledge management system @ @ international publishing ag part of @ nature @ 
1432,Enhancing web search through question classifier,"Question answering field has evolved alongside the Natural Language processing. There are several small-scale applications that use the linguistic, semantic, and syntactic interpretations of text and consume it in further processing. In case of a web search, knowing what the query is intended for can save hours of CPU processing and decrease response time tremendously. We have taken a small step in this direction by treating the query as a question and classifying it with the best suited classification algorithm. In this paper, we have tried to find out that when a perfect-informer of the question (knowing what is asked) is provided as an input for classification algorithms like SVN, Naïve Base, and decision tree, we want to observe their accuracy on the same data set of questions. In our experiment, we have used the concept of CRF to find question features that are relevant. CRF is a probabilistic model that treats features as observation sequence and emits all sequence labels with probability values. © 2018, Springer Nature Singapore Pte Ltd.",2018,"Smart Innovation, Systems and Technologies",1,question answering field ha evolved alongside @ natural language processing @ @ @ several small-scale application @ use @ linguistic semantic and syntactic interpretation of text and consume @ in @ processing @ in case of a web search knowing @ @ query is intended @ @ save hour of cpu processing and decrease response time tremendously @ @ @ taken a small step in @ direction by treating @ query a a question and classifying @ @ @ best suited classification algorithm @ in @ @ @ @ tried to find @ @ @ a perfect-informer of @ question @ knowing @ is asked @ is provided a @ input @ classification algorithm like svn naïve base and decision tree @ want to observe @ accuracy on @ @ data set of question @ in @ experiment @ @ used @ concept of crf to find question feature @ @ relevant @ crf is a probabilistic model @ treat feature a observation sequence and emits @ sequence label @ probability value @ @ nature singapore pte ltd @ 
1435,Language processing modelling notation – Orchestration of NLP microservices,The paper presents Language Processing Modelling Notation (LPMN). It is a formal language used to orchestrate a set of NLP microservices. The LPMN allows modeling and running complex workflows of language and machine learning tools. The scalability of the solution was achieved by a usage of message-oriented middleware. LPMN is used for developing text mining application with web-based interface and performing research experiments that requires a usage of NLP and machine learning tools. © Springer International Publishing AG 2018.,2018,Advances in Intelligent Systems and Computing,9,@ @ @ language processing modelling notation @ lpmn @ @ @ is a formal language used to orchestrate a set of nlp microservices @ @ lpmn allows modeling and running complex workflow of language and machine learning tool @ @ scalability of @ solution wa achieved by a usage of message-oriented middleware @ lpmn is used @ developing text mining application @ web-based interface and performing research experiment @ requires a usage of nlp and machine learning tool @ @ international publishing ag @ 
1436,Study on dynamic multi-document summarization system framework method,"This paper introduced a new Internet-based dynamic multi-document summarization system framework based on natural language processing and applied for the data management of wireless sensor networks that was capable of providing computational linguistics-related technical support. We mainly studied the related model of text mining for perceptual data. Wireless sensor networks had a large number of streaming data, with real-time characteristics. After basic node operation, it generated big data can be used for data mining. In order to apply the data mining technology to information processing of wireless sensor networks, this paper tried to find a similar such as DUC2008 abstract test samples, trained model and algorithm. The system integrated subsystems with different emphases to improve system performance, combined three innovative methods. Given that to date little research on dynamic multi-document summarization has been reported, this study had great significance. The results obtained by the new framework were compared with those from the TAC2008 evaluation, demonstrated that the new system's performance matched that of the best existing systems. © 2018 Taiwan Academic Network Management Committee. All Rights Reserved.",2018,Journal of Internet Technology,0,@ @ introduced a @ internet-based dynamic multi-document summarization system framework based on natural language processing and applied @ @ data management of wireless sensor network @ wa capable of providing computational linguistics-related technical support @ @ mainly studied @ related model of text mining @ perceptual data @ wireless sensor network @ a @ number of streaming data @ real-time characteristic @ @ basic node operation @ generated big data @ @ used @ data mining @ in order to apply @ data mining technology to information processing of wireless sensor network @ @ tried to find a similar @ a duc abstract test sample trained model and algorithm @ @ system integrated subsystem @ different emphasis to improve system performance combined three innovative method @ given @ to date little research on dynamic multi-document summarization ha @ reported @ study @ great significance @ @ @ obtained by @ @ framework @ compared @ @ @ @ tac evaluation demonstrated @ @ @ system @ s performance matched @ of @ best existing system @ taiwan @ network management committee @ @ right reserved @ 
1437,Content based fraudulent website detection using supervised machine learning techniques,"Fraudulent websites pose as legitimate sources of information, goods, product and services are propagating and resulted in loss of billions of dollars. Due to several undesirable impacts of Internet fraud and scam, several studies and approaches are focused to identify fraudulent Internet websites, yet none of them managed to offer an efficient solution to suppress these fraudulent activities. With this regard, this research proposes a fraudulent website detection model based on sentiment analysis of the textual contents of a given website, natural language processing and supervised machine learning techniques. The proposed model consists of four primary phases which are data acquisition phase, preprocessing phase, feature extraction phase and classification phase. Crawler is used to obtained data from Internet and data was cleaned to remove non-discriminative noises and reshape into desired format. Later, meaningful and discriminative patterns are extracted. Finally classification phase consists of supervised machine learning techniques to construct the fraudulent website detection model. This research employs 10-fold stratified cross validation technique in order to validate the performance of the proposed model. Experimental results show that the proposed fraudulent website detection model with cross validated accuracy of 97.67% and FPR of 3.49% achieved satisfactory results and served the aim of this research. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Advances in Intelligent Systems and Computing,3,fraudulent website pose a legitimate source of information good product and service @ propagating and resulted in loss of billion of dollar @ due to several undesirable impact of internet fraud and scam several study and approach @ focused to identify fraudulent internet website yet none of @ managed to offer @ efficient solution to suppress @ fraudulent activity @ @ @ regard @ research proposes a fraudulent website detection model based on sentiment analysis of @ textual content of a given website natural language processing and supervised machine learning technique @ @ proposed model consists of four primary phase @ @ data acquisition phase preprocessing phase feature extraction phase and classification phase @ crawler is used to obtained data @ internet and data wa cleaned to remove non-discriminative noise and reshape @ desired format @ later meaningful and discriminative pattern @ extracted @ finally classification phase consists of supervised machine learning technique to construct @ fraudulent website detection model @ @ research employ fold stratified cross validation technique in order to validate @ performance of @ proposed model @ experimental @ @ @ @ proposed fraudulent website detection model @ cross validated accuracy of @ and fpr of @ achieved satisfactory @ and served @ aim of @ research @ @ international publishing ag part of @ nature @ 
1438,Demo integration of text- and web-mining results in epidvis,"The new and emerging infectious diseases are an incising threat to countries due to globalisation, movement of passengers and international trade. In order to discover articles of potential importance to infectious disease emergence it is important to mine the Web with an accurate vocabulary. In this paper, we present a new methodology that combines text-mining results and visualisation approach in order to discover associations between hosts and symptoms related to emerging infectious disease outbreaks. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ and emerging infectious disease @ @ incising threat to country due to globalisation movement of passenger and international trade @ in order to discover article of potential importance to infectious disease emergence @ is important to mine @ web @ @ accurate vocabulary @ in @ @ @ @ a @ methodology @ combine text-mining @ and visualisation approach in order to discover association @ host and symptom related to emerging infectious disease outbreak @ @ international publishing ag part of @ nature @ 
1439,Classification of intangible social innovation concepts,"In social sciences, similarly to other fields, there is exponential growth of literature and textual data that people are no more able to cope with in a systematic manner. In many areas there is a need to catalogue knowledge and phenomena in a certain area. However, social science concepts and phenomena are complex and in many cases there is a dispute in the field between conflicting definitions. In this paper we present a method that catalogues a complex and disputed concept of social innovation by applying text mining and machine learning techniques. Recognition of social innovations is performed by decomposing a definitions into several more specific criteria (social objectives, social actor interactions, outputs and innovativeness). For each of these criteria, a machine learning-based classifier is created that checks whether certain text satisfies given criteria. The criteria can be successfully classified with an F1-score of 0.83–0.86. The presented method is flexible, since it allows combining criteria in a later stage in order to build and analyse the definition of choice. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,in social science similarly to @ field @ is exponential growth of literature and textual data @ people @ no more able to cope @ in a systematic manner @ in many area @ is a need to catalogue knowledge and phenomenon in a certain area @ however social science concept and phenomenon @ complex and in many case @ is a dispute in @ field @ conflicting definition @ in @ @ @ @ a method @ catalogue a complex and disputed concept of social innovation by applying text mining and machine learning technique @ recognition of social innovation is performed by decomposing a definition @ several more specific criterion @ social objective social actor interaction output and innovativeness @ @ @ @ of @ criterion a machine learning-based classifier is created @ check whether certain text satisfies given criterion @ @ criterion @ @ successfully classified @ @ f score of @ @ @ @ presented method is flexible since @ allows combining criterion in a later stage in order to build and analyse @ definition of choice @ @ international publishing ag part of @ nature @ 
1440,Sentimental analysis on cognitive data using R,"Internet is now vested with new form of societal interactive activities like social media, online portals, feeds, reviews, ratings, posts, critics etc., where people are able to post their expression-of-interest as tweets. Sentiment Analysis (SA) is used for better understanding of such linguistics tweets, extracting features, determine subjectivity and polarity of text located in these tweets. SA inherits text mining approach to process, investigate, and analyze idiosyncratic evidences from text. Now a days, SA was screamed as one of a predictor tool for improvement in knowledge management, revenue generation and decision-making in many businesses firms. The purpose of this work is to leverage a constructive tactic for SA towards dispensation of cognitive information, and seed pragmatic alley to researchers in cognitive science community. This study uses machine learning packages of R language over cognitive data to gain knowledge, discover sentiment polarity and better prediction over the data. To carry out a semantic study over cognitive data we thrived text from numerous numbers of social networking sites. This data was articulated in form of unstructured sentences, words and phrases in a document. Suitable linguistic features are captured to engender dissimilar sentiment polarity and analyze expression-of-interest of user. One of the most prevalent text classification method, Naïve bayes is applied over the text corpus to pinpoint the sentiment and assign its polarity. The connotation in this approaches are evaluated in terms of statistical measures precision, recall, f-measure, and accuracy, thereby these substantial outcomes help to arcade user behavior and predict future trends using SA. © The Author(s) 2018.",2018,SpringerBriefs in Applied Sciences and Technology,0,internet is now vested @ @ form of societal interactive activity like social medium online portal feed review rating post critic etc @ @ people @ able to post @ expression-of-interest a tweet @ sentiment analysis @ sa @ is used @ better understanding of @ linguistics tweet extracting feature determine subjectivity and polarity of text located in @ tweet @ sa inherits text mining approach to process investigate and analyze idiosyncratic evidence @ text @ now a day sa wa screamed a @ of a predictor tool @ improvement in knowledge management revenue generation and decision-making in many @ firm @ @ purpose of @ work is to leverage a constructive tactic @ sa towards dispensation of cognitive information and seed pragmatic alley to researcher in cognitive science community @ @ study us machine learning package of r language @ cognitive data to gain knowledge discover sentiment polarity and better prediction @ @ data @ to carry @ a semantic study @ cognitive data @ thrived text @ numerous number of social networking site @ @ data wa articulated in form of unstructured sentence word and phrase in a document @ suitable linguistic feature @ captured to engender dissimilar sentiment polarity and analyze expression-of-interest of user @ @ of @ @ prevalent text classification method naïve bayes is applied @ @ text corpus to pinpoint @ sentiment and assign @ polarity @ @ connotation in @ approach @ evaluated in term of statistical measure precision recall f-measure and accuracy thereby @ substantial outcome help to arcade user behavior and predict future trend @ sa @ @ author @ s @ @ 
1441,ALTAS: An intelligent text analysis system based on knowledge graphs,"This paper presents an intelligent text analysis system, called ALTAS, to support various text analysis tasks such as statistics analysis, sentiment analysis, text classification, and text clustering. The system contains four main components: knowledge graphs, text processing, text analysis and intelligent report. First, the system has built a semantic-rich knowledge base using several knowledge graph resources. A novel text processing and analysis framework based on knowledge graphs is developed and implemented. Given a text dataset, the text processing phase will do data cleaning, word segmentation and feature extraction for it. With the extracted features, the text analysis phase allows users to select a text mining task. We have implemented the proposed novel algorithm and several typical algorithms for each task. If users select multiple algorithms for the task, the intelligent report phase will automatically generate comparison results for users. Especially, the intelligent report phase also provides users a paper summary generating function on text mining problems. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ @ intelligent text analysis system called altas to support various text analysis task @ a statistic analysis sentiment analysis text classification and text clustering @ @ system contains four main component @ knowledge graph text processing text analysis and intelligent report @ first @ system ha built a semantic-rich knowledge base @ several knowledge graph resource @ a novel text processing and analysis framework based on knowledge graph is developed and implemented @ given a text dataset @ text processing phase @ @ data cleaning word segmentation and feature extraction @ @ @ @ @ extracted feature @ text analysis phase allows user to select a text mining task @ @ @ implemented @ proposed novel algorithm and several typical algorithm @ @ task @ if user select multiple algorithm @ @ task @ intelligent report phase @ automatically generate comparison @ @ user @ especially @ intelligent report phase @ provides user a @ summary generating function on text mining problem @ @ international publishing ag part of @ nature @ 
1442,Pico-nym cloud (PNC): A method to devise and peruse semantically related biological patterns,"Text mining works widely in the field of research techniques, which allow an individual to store text and its important terms in form of electronic document (.doc,.txt). Obliviously, one cannot remember such huge amount of text; moreover, the manual approach is more time-taking, unreliable, and accessible to that person only. Text mining techniques optimize this approach by extracting and storing this data. Computational comparison, file read, file write are more efficiently done. With the help of Pico-Nym Cloud (PNC), we generated more semantically similar, related, and significant patterns. The give, generate, and get sequence modeling is adopted. Over the other available Web applications, we present our application with improved stemming, relation, and average case consideration. This approach does not limit the displayed number of words as all the generated sets can be traversed with the GUI, with opted size of patterns. This PNC is highly applicable in bioinformatics, related information retrieval from document, sentimental analysis using social Web sites (Twitter and Facebook), query expansion (Google) and many more. © 2018, Springer Nature Singapore Pte Ltd.",2018,Advances in Intelligent Systems and Computing,0,text mining work widely in @ field of research technique @ allow @ individual to store text and @ important term in form of electronic document @ @ doc @ txt @ @ obliviously @ cannot remember @ huge amount of text @ moreover @ manual approach is more time-taking unreliable and accessible to @ person only @ text mining technique optimize @ approach by extracting and storing @ data @ computational comparison file read file write @ more efficiently done @ @ @ help of pico-nym cloud @ pnc @ @ generated more semantically similar related and significant pattern @ @ give generate and get sequence modeling is adopted @ @ @ @ available web application @ @ @ application @ improved stemming relation and average case consideration @ @ approach doe not limit @ displayed number of word a @ @ generated set @ @ traversed @ @ gui @ opted size of pattern @ @ pnc is highly applicable in bioinformatics related information retrieval @ document sentimental analysis @ social web site @ twitter and facebook @ query expansion @ google @ and many more @ @ nature singapore pte ltd @ 
1443,Document theme extraction using named-entity recognition,"The text mining can be implemented by term analysis of word or phrase. This term which describes the concepts of particular sentence is use to define the document theme. The new context-based mining technique is introduced which uses the concept-based mining model to analyze the terms present in sentence, document, and corpus levels. We find the theme of document like organization, medical, entertainment, sport, and so on. Context-based mining apply on statistical data as well as real-time data like Export data fromWikipedia. The theme of document is extracted by using Natural Language processing (NLP) for communication between computer and human languages and name entity recognition (NER) algorithm for identification of entity, entity chunking, and entity extraction. It used to get name entity in text such as person name, organization name, specific locations, time expressions, percentages quantities and so on. NLP and NER are used in context-based mining for finding name of entity and their relationship. Context Vector containing set of documents is used to extract the context of the document. Finally K-Mean algorithm is used for clustering to find inherent groupings of the text documents, then set of clusters are generated where each cluster exhibit high intra cluster similarity and low inter cluster similarity. The text document clustering is used to separate documents into groups or clusters based on their similarity so all groups define the distinct topics. © Springer Nature Singapore Pte Ltd. 2019.",2018,Advances in Intelligent Systems and Computing,1,@ text mining @ @ implemented by term analysis of word @ phrase @ @ term @ describes @ concept of particular sentence is use to define @ document theme @ @ @ context-based mining technique is introduced @ us @ concept-based mining model to analyze @ term @ in sentence document and corpus level @ @ find @ theme of document like organization medical entertainment sport and @ on @ context-based mining apply on statistical data a well a real-time data like export data fromwikipedia @ @ theme of document is extracted by @ natural language processing @ nlp @ @ communication @ computer and human language and name entity recognition @ ner @ algorithm @ identification of entity entity chunking and entity extraction @ @ used to get name entity in text @ a person name organization name specific location time expression percentage quantity and @ on @ nlp and ner @ used in context-based mining @ finding name of entity and @ relationship @ context vector containing set of document is used to extract @ context of @ document @ finally k-mean algorithm is used @ clustering to find inherent grouping of @ text document @ set of cluster @ generated @ @ cluster exhibit high intra cluster similarity and low inter cluster similarity @ @ text document clustering is used to separate document @ group @ cluster based on @ similarity @ @ group define @ distinct topic @ @ nature singapore pte ltd @ @ 
1447,Finding mentions of abbreviations and their definitions in Spanish clinical cases: the BARR2 shared task evaluation results,"A common characteristic of content generated by healthcare professionals, regardless the actual clinical discipline or language, is the widespread and frequent use of abbreviations, acronyms, telegraphic phrases and shorthand notes. Despite the well-known issues related to the ambiguity and misinterpretation of abbreviations, their use in practice is required to simplify and enable communication-avoiding repetition of long complex specialized medical terminologies. Moreover, clinical texts typically do not provide explicit abbreviation definitions. Thus the performance of clinical natural language processing and text mining systems is significantly affected by the previous recognition and definition resolution of medical abbreviations. To promote the development of such key components, we have organized the second Biomedical Abbreviation Recognition and Resolution (BARR2) track. The overall aim of this effort was to evaluate strategies for detecting automatically mentions of abbreviations in running text, as well as returning their corresponding definition given the corresponding context from Spanish clinical case studies. For this track, we constructed the Spanish clinical case corpus (SPACCC). This collection was exhaustively annotated by hand by domain experts with abbreviation mentions together with their corresponding definitions, resulting in the BARR2 corpus. A total of 5 teams submitted 26 runs for the two BARR2 subtasks: (a) the detection of explicit occurrences of abbreviation-definition pairs and (b) the resolution of abbreviations regardless whether their definition is mentioned within the actual document. Here we summarize the BARR2 track setting, the obtained results and the methodologies used by participating systems. The BARR2 task summary, resources and evaluation tool for testing systems beyond this campaign are available at: http://temu.bsc.es/BARR2. © 2018 CEUR-WS. All Rights Reserved.",2018,CEUR Workshop Proceedings,8,a common characteristic of content generated by healthcare professional regardless @ actual clinical discipline @ language is @ widespread and frequent use of abbreviation acronym telegraphic phrase and shorthand note @ despite @ well-known issue related to @ ambiguity and misinterpretation of abbreviation @ use in practice is required to simplify and enable communication-avoiding repetition of long complex specialized medical terminology @ moreover clinical text typically @ not provide explicit abbreviation definition @ thus @ performance of clinical natural language processing and text mining system is significantly affected by @ previous recognition and definition resolution of medical abbreviation @ to promote @ development of @ key component @ @ organized @ second biomedical abbreviation recognition and resolution @ barr @ track @ @ overall aim of @ effort wa to evaluate strategy @ detecting automatically mention of abbreviation in running text a well a returning @ corresponding definition given @ corresponding context @ spanish clinical case study @ @ @ track @ constructed @ spanish clinical case corpus @ spaccc @ @ @ collection wa exhaustively annotated by hand by domain expert @ abbreviation mention together @ @ corresponding definition resulting in @ barr corpus @ a total of team submitted run @ @ @ barr subtasks @ @ a @ @ detection of explicit occurrence of abbreviation-definition pair and @ b @ @ resolution of abbreviation regardless whether @ definition is mentioned within @ actual document @ @ @ summarize @ barr track setting @ obtained @ and @ methodology used by participating system @ @ barr task summary resource and evaluation tool @ testing system beyond @ campaign @ available at @ http @ temu @ bsc @ e barr @ ceur-ws @ @ right reserved @ 
1449,Analyzing the stemming paradigm,This paper discusses affix removal and statistical based Stemming algorithms in detail with stemmer-generated output from some Standard English text and dictionary. Comparative empirical studies of all these stemmers are also discussed here with respect to number of stem token generation from single root morphed word variants and computation time. First part of the paper deals with introductory discussion of stemming and lemmatization. Second part of the paper focuses on algorithms of affix and statistical based stemmers with their empirical output. Last part describes the steps of the comparative tool for the same. Finally conclusion section wraps up whole discussion about stemming. This paper can assist researchers working in the field of text mining. © Springer International Publishing AG 2018.,2018,"Smart Innovation, Systems and Technologies",1,@ @ discus affix removal and statistical based stemming algorithm in detail @ stemmer-generated output @ some standard english text and dictionary @ comparative empirical study of @ @ stemmer @ @ discussed @ @ respect to number of stem token generation @ single root morphed word variant and computation time @ first part of @ @ deal @ introductory discussion of stemming and lemmatization @ second part of @ @ focus on algorithm of affix and statistical based stemmer @ @ empirical output @ last part describes @ step of @ comparative tool @ @ @ @ finally conclusion section wrap up whole discussion @ stemming @ @ @ @ assist researcher working in @ field of text mining @ @ international publishing ag @ 
1450,Opinion argumentation based on combined Information Retrieval and topic modeling,"Argumentation mining is a text mining task, which aims at automatically detecting the argumentative structure concealed in a huge amount of text data. Previous researches in this field have focused on the classification of text sentences as arguments and the detection of relations between them. Different corpora have been used, such as newspaper articles and online debates. Due to the explosion of social networks, mi-croblogging platforms like Twitter and Facebook have become interesting tools to evaluate public opinion on different domains. In this work, we propose a new pipeline process to achieve the goal of argumentation mining, based on 70 millions of twitter-microblogs released from MC2 CLEF-2018 lab dealing with cultural events. Our approach is based on Information Retrieval protocol combined with sentiment analysis and topic modeling using Latent Dirichlet Allocation (LDA).",2018,CEUR Workshop Proceedings,0,argumentation mining is a text mining task @ aim at automatically detecting @ argumentative structure concealed in a huge amount of text data @ previous research in @ field @ focused on @ classification of text sentence a argument and @ detection of relation @ @ @ different corpus @ @ used @ a newspaper article and online debate @ due to @ explosion of social network mi-croblogging platform like twitter and facebook @ become interesting tool to evaluate public opinion on different domain @ in @ work @ propose a @ pipeline process to achieve @ goal of argumentation mining based on million of twitter-microblogs released @ mc clef lab dealing @ cultural event @ @ approach is based on information retrieval protocol combined @ sentiment analysis and topic modeling @ latent dirichlet allocation @ lda @ @ 
1451,Using knowledge graph to improve enterprise search experience,"FINRA has many millions of documents and database records that staff need to search through to find information relevant to regulatory activities. Searching across the large set of documents and structured database records using relevance ranked text search does not present items together that the users know are related. Relevance ranking discriminates using TF/IDF, and related techniques, but does not bring together items that are not related by relevance. The solution was to build a structured and navigable visual representation of the data returned by the underlying multiple query engines. Text mining and semantic web techniques were used extensively to build the enhanced metadata and create the linkages among the data objects needed in order to support the visual navigation paradigm. The resulting knowledge graph gives users the ability to see semantically related items. © 2018 CEUR-WS. All rights reserved.",2018,CEUR Workshop Proceedings,0,finra ha many million of document and database record @ staff need to search @ to find information relevant to regulatory activity @ searching across @ @ set of document and structured database record @ relevance ranked text search doe not @ item together @ @ user know @ related @ relevance ranking discriminates @ tf idf and related technique @ doe not bring together item @ @ not related by relevance @ @ solution wa to build a structured and navigable visual representation of @ data returned by @ underlying multiple query engine @ text mining and semantic web technique @ used extensively to build @ enhanced metadata and create @ linkage among @ data object needed in order to support @ visual navigation paradigm @ @ resulting knowledge graph give user @ ability to see semantically related item @ ceur-ws @ @ right reserved @ 
1453,News recommendation with CF-IDF+,"Traditionally, content-based recommendation is performed using term occurrences, which are leveraged in the TF-IDF method. This method is the defacto standard in text mining and information retrieval. Valuable additional information from domain ontologies, however, is not employed by default. The TF-IDF-based CF-IDF method successfully utilizes the semantics of a domain ontology for news recommendation by detecting ontological concepts instead of terms. However, like other semantics-based methods, CF-IDF fails to consider the different concept relationship types. In this paper, we extend CF-IDF to additionally take into account concept relationship types. Evaluation is performed using Ceryx, an extension to the Hermes news personalization framework. Using a custom news data set, our CF-IDF+ news recommender outperforms the CF-IDF and TF-IDF recommenders in terms of F1 and Kappa. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,traditionally content-based recommendation is performed @ term occurrence @ @ leveraged in @ tf-idf method @ @ method is @ defacto standard in text mining and information retrieval @ valuable additional information @ domain ontology however is not employed by default @ @ tf-idf-based cf-idf method successfully utilizes @ semantics of a domain ontology @ news recommendation by detecting ontological concept instead of term @ however like @ semantics-based method cf-idf fails to consider @ different concept relationship type @ in @ @ @ extend cf-idf to additionally take @ account concept relationship type @ evaluation is performed @ ceryx @ extension to @ hermes news personalization framework @ @ a custom news data set @ cf-idf news recommender outperforms @ cf-idf and tf-idf recommenders in term of f and kappa @ @ international publishing ag part of @ nature @ 
1454,LTSG: Latent topical skip-gram for mutually improving topic model and vector representations,"Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the results trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-the-art methods on various NLP tasks. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,topic model @ @ widely used in discovering latent topic @ @ shared across document in text mining @ vector representation word embeddings and topic embeddings map word and topic @ a low-dimensional and dense real-value vector space @ @ obtained high performance in nlp task @ however @ of @ existing model assume @ @ trained by @ of @ @ perfect correct and used a prior knowledge @ improving @ @ model @ some @ model use @ information trained @ external @ corpus to help improving smaller corpus @ in @ @ @ aim to build @ @ algorithm framework @ make topic model and vector representation mutually improve @ @ within @ @ corpus @ @ em-style algorithm framework is employed to iteratively optimize @ topic model and vector representation @ experimental @ @ @ @ model outperforms state-of-the-art method on various nlp task @ @ nature switzerland ag @ 
1455,Evaluating named-entity recognition approaches in plant molecular biology,"Text mining research is becoming an important topic in biology with the aim to extract biological entities from scientific papers in order to extend the biological knowledge. However, few thorough studies are developed for plant molecular biology data, especially rice, thus resulting a lack of datasets available to exploit advanced machine learning methods able to detect entities such as genes and proteins. In this article, we first developed a dataset from the Ozyzabase - a database of rice gene, and used it as the benchmark. Then, we evaluated the performance of two Name Entities Recognition (NER) methods for sequence tagging: a Long Short Term Memory (LSTM) model, combined with Conditional Random Fields (CRFs), and a hybrid method based on the dictionary lookup combining with some machine learning systems to improve result. We analyzed the performance of these methods when apply to the Oryzabase dataset and improved the results. On average, the result from LSTM-CRF reaching 86% in F1 is more exploitable. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,text mining research is becoming @ important topic in biology @ @ aim to extract biological entity @ scientific @ in order to extend @ biological knowledge @ however @ thorough study @ developed @ plant molecular biology data especially rice thus resulting a lack of datasets available to exploit advanced machine learning method able to detect entity @ a gene and protein @ in @ article @ first developed a dataset @ @ ozyzabase a database of rice gene and used @ a @ benchmark @ @ @ evaluated @ performance of @ name entity recognition @ ner @ method @ sequence tagging @ a long short term memory @ lstm @ model combined @ conditional random field @ crfs @ and a hybrid method based on @ dictionary lookup combining @ some machine learning system to improve @ @ @ analyzed @ performance of @ method @ apply to @ oryzabase dataset and improved @ @ @ on average @ @ @ lstm-crf reaching in f is more exploitable @ @ nature switzerland ag @ 
1456,A combinational classification for the customers of airline platform based on text mining,"Intelligent customer service platform has the ability of automatic communication and service based on natural language. It can achieve intelligent interaction between human and machine interaction, such as, providing users with higher quality services through instant messaging, Internet, telephone, text messaging, etc. This paper mainly explores the subdivision research of airline customers based on text classification. First of all, we extract the characteristics on text data of airline customer service platform through TF-IDF. Secondly, naive Bayes, SVM, KNN, and logistic regression are used to train the model. Thirdly, a combinational model based on the four algorithms is constructed; Finally, we use 10-fold cross validation to verify the testing results. And experiments show that the combinational algorithms are better than the original methods. © 2018 The authors and IOS Press. All rights reserved.",2018,Frontiers in Artificial Intelligence and Applications,0,intelligent customer service platform ha @ ability of automatic communication and service based on natural language @ @ @ achieve intelligent interaction @ human and machine interaction @ a providing user @ higher quality service @ instant messaging internet telephone text messaging etc @ @ @ mainly explores @ subdivision research of airline customer based on text classification @ first of @ @ extract @ characteristic on text data of airline customer service platform @ tf-idf @ secondly naive bayes svm knn and logistic regression @ used to train @ model @ thirdly a combinational model based on @ four algorithm is constructed @ finally @ use fold cross validation to verify @ testing @ @ and experiment @ @ @ combinational algorithm @ better @ @ original method @ @ author and io @ @ @ right reserved @ 
1457,A part-of-speech-based exploratory text mining of students’ looking-back evaluation,"In our lectures at universities, we observe that the students’ attitudes affects a lot to their achievements. In order to prove this observation based on data, we have been investigating to find effective methods that extract students’ attitudes from lecture data; such as examination score as an index to student’s achievement, attendance and homework data for his/her effort, and answer texts of the term-end questionnaire as information source of attitude. In this chapter, we take another approach to investigate the influences of words used in the answer texts of students on their achievements. We use a machine learning method called Support Vector Machine (SVM), which is a tool to create a model for classifying the given data into two groups by positive and negative training sample data. We apply SVM to the answer texts for analyzing the influences of parts of speech of words to the student’s achievement. Even though adjectives and adverbs are the same in the sense that they modify nouns and verbs, we found that adverbs affects much more than adjectives, as a result. From our experiences so far, we believe that analysis of answers to the evaluations of students toward themselves and lectures are very useful source of finding the students’ attitudes to learning. © Springer International Publishing AG 2018.",2018,Advances in Intelligent Systems and Computing,0,in @ lecture at university @ observe @ @ student attitude affect a lot to @ achievement @ in order to prove @ observation based on data @ @ @ investigating to find effective method @ extract student attitude @ lecture data @ @ a examination score a @ index to student s achievement attendance and homework data @ @ @ effort and answer text of @ term-end questionnaire a information source of attitude @ in @ chapter @ take another approach to investigate @ influence of word used in @ answer text of student on @ achievement @ @ use a machine learning method called support vector machine @ svm @ @ is a tool to create a model @ classifying @ given data @ @ group by positive and negative training sample data @ @ apply svm to @ answer text @ analyzing @ influence of part of speech of word to @ student s achievement @ even though adjective and adverb @ @ @ in @ sense @ @ modify noun and verb @ found @ adverb affect much more @ adjective a a @ @ @ @ experience @ far @ believe @ analysis of answer to @ evaluation of student toward @ and lecture @ @ useful source of finding @ student attitude to learning @ @ international publishing ag @ 
1458,Intelligent text mining model for english language using deep neural network,"Today there exist various sources that provide information in very massive amount to serve the demand over the internet, which creates huge collection of heterogeneous data. Thus existing data can be categorized as unstructured and structured data. In this paper we propose an idea of a tool which intelligently preprocesses the unstructured data by segmenting the whole document into number of sentences, using deep learning concepts with word2vec [11] and a Recurrent Neural Network [13]. At the beginning step we use word2vec which was introduced by Tomas Mikolov with his team at Google, to generate vectors of the inputted text content which will be further forwarded to Recurrent Neural Network. RNN takes this series of vectors as input and trained Data Cleaning Recurrent Neural Network model will perform preprocessing task (including cleaning of missing, grammatically incorrect, misspelled data) to produce structured results, which then passed into automatic summarization module to generate desired summary. © Springer International Publishing AG 2018.",2018,"Smart Innovation, Systems and Technologies",3,today @ exist various source @ provide information in @ massive amount to serve @ demand @ @ internet @ creates huge collection of heterogeneous data @ thus existing data @ @ categorized a unstructured and structured data @ in @ @ @ propose @ idea of a tool @ intelligently preprocesses @ unstructured data by segmenting @ whole document @ number of sentence @ deep learning concept @ word vec and a recurrent neural network @ at @ beginning step @ use word vec @ wa introduced by tomas mikolov @ @ team at google to generate vector of @ inputted text content @ @ @ @ forwarded to recurrent neural network @ rnn take @ series of vector a input and trained data cleaning recurrent neural network model @ perform preprocessing task @ including cleaning of missing grammatically incorrect misspelled data @ to produce structured @ @ @ passed @ automatic summarization module to generate desired summary @ @ international publishing ag @ 
1459,Art critics and art producers: Interaction through the text,"As well as the most areas of social life, the field of art is now extended to the cyberspace. In this study, we analyze online reviews of Russian art critics with two objectives. On the one hand, we investigate the patterns of the interactions between critics and artists (both contemporary and recognized ones) in the Russian Art. Since the Russian school of art critique is still in the process of formation, an analysis of web data we offer a significant contribution to the scope of Russian Art studies. On the other hand, we use social network analysis and text mining tools in order to gain more insights from the data and affirm the applicability of the modern tools to the classic research tasks. In this study we analyze data from the 5 Russian art magazines, in particular articles, authors and named entities from this texts. As a result, we explored different patterns of the critics production that could divide this area of web interaction both by geographical and textual characteristics of agents and articles. © Springer Nature Switzerland AG 2018.",2018,Communications in Computer and Information Science,0,a well a @ @ area of social life @ field of art is now extended to @ cyberspace @ in @ study @ analyze online review of russian art critic @ @ objective @ on @ @ hand @ investigate @ pattern of @ interaction @ critic and artist @ @ contemporary and recognized @ @ in @ russian art @ since @ russian school of art critique is still in @ process of formation @ analysis of web data @ offer a significant contribution to @ scope of russian art study @ on @ @ hand @ use social network analysis and text mining tool in order to gain more insight @ @ data and affirm @ applicability of @ modern tool to @ classic research task @ in @ study @ analyze data @ @ russian art magazine in particular article author and named entity @ @ text @ a a @ @ explored different pattern of @ critic production @ could divide @ area of web interaction @ by geographical and textual characteristic of agent and article @ @ nature switzerland ag @ 
1461,A Comparative Survey of Authorship Attribution on Short Arabic Texts,"In this paper, we deal with the problem of authorship attribution (AA) on short Arabic texts. So, we make a survey on a set of several features and classifiers that are employed for the task of AA. This investigation uses characters, character bigrams, character trigrams, character tetragrams, words, word bigrams and rare words. The AA is ensured by 4 different measures, 3 classifiers (Multi-Layer Perceptron (MLP), Support Vector Machines (SVM) and Linear Regression (LR)) and a new proposed fusion called VBF (i.e. Vote Based Fusion). The evaluation is done on short Arabic texts extracted from the AAAT dataset (AA of Ancient Arabic Texts). Although the task of AA is known to be difficult on short texts, the different results have revealed interesting information on the performances of the features and classification techniques on Arabic text data. For instance, character-based features appear to be better than word-based features for short texts. Furthermore, the proposed VBF fusion provided high performances with an accuracy of 90% of good AA, which is higher than the score of the original classifier using only one feature. Globally, the results of this investigation shed light on the efficiency and pertinency of several features and classifiers in AA of short Arabic texts. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,in @ @ @ deal @ @ problem of authorship attribution @ aa @ on short arabic text @ @ @ make a survey on a set of several feature and classifier @ @ employed @ @ task of aa @ @ investigation us character character bigram character trigram character tetragram word word bigram and rare word @ @ aa is ensured by different measure classifier @ multi-layer perceptron @ mlp @ support vector machine @ svm @ and linear regression @ lr @ @ and a @ proposed fusion called vbf @ i @ e @ vote based fusion @ @ @ evaluation is done on short arabic text extracted @ @ aaat dataset @ aa of ancient arabic text @ @ although @ task of aa is known to @ difficult on short text @ different @ @ revealed interesting information on @ performance of @ feature and classification technique on arabic text data @ @ instance character-based feature appear to @ better @ word-based feature @ short text @ furthermore @ proposed vbf fusion provided high performance @ @ accuracy of of good aa @ is higher @ @ score of @ original classifier @ only @ feature @ globally @ @ of @ investigation shed light on @ efficiency and pertinency of several feature and classifier in aa of short arabic text @ @ nature switzerland ag @ 
1464,Mining supervisor evaluation and peer feedback in performance appraisals,"Performance appraisal (PA) is an important HR process to periodically measure and evaluate every employee’s performance vis-a-vis the goals established by the organization. A PA process involves purposeful multi-step multi-modal communication between employees, their supervisors and their peers, such as self-appraisal, supervisor assessment and peer feedback. Analysis of the structured data and text produced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we apply text mining techniques to produce insights from PA text. First, we perform sentence classification to identify strengths, weaknesses and suggestions of improvements found in the supervisor assessments and then use clustering to discover broad categories among them. Next we use multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance. Finally, we propose a short-text summarization technique to produce a summary of peer feedback comments for a given employee and compare it with manual summaries. All techniques are illustrated using a real-life dataset of supervisor assessment and peer feedback text produced during the PA of 4528 employees in a large multi-national IT company. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,performance appraisal @ pa @ is @ important hr process to periodically measure and evaluate every employee s performance vis-a-vis @ goal established by @ organization @ a pa process involves purposeful multi-step multi-modal communication @ employee @ supervisor and @ peer @ a self-appraisal supervisor assessment and peer feedback @ analysis of @ structured data and text produced in pa is crucial @ measuring @ quality of appraisal and tracking actual improvement @ in @ @ @ apply text mining technique to produce insight @ pa text @ first @ perform sentence classification to identify strength weakness and suggestion of improvement found in @ supervisor assessment and @ use clustering to discover broad category among @ @ next @ use multi-class multi-label classification technique to match supervisor assessment to predefined broad perspective on performance @ finally @ propose a short-text summarization technique to produce a summary of peer feedback comment @ a given employee and compare @ @ manual summary @ @ technique @ illustrated @ a real-life dataset of supervisor assessment and peer feedback text produced @ @ pa of employee in a @ multi-national @ company @ @ nature switzerland ag @ 
1465,"The evolution of sentiment analysis—A review of research topics, venues, and top cited papers","Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990's. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis. © 2017 Elsevier Inc.",2018,Computer Science Review,141,sentiment analysis is @ of @ fastest growing research area in computer science making @ challenging to keep track of @ @ activity in @ area @ @ @ a computer-assisted literature review @ @ utilize @ text mining and qualitative coding and analyze @ @ scopus @ @ find @ @ root of sentiment analysis @ in @ study on public opinion analysis at @ beginning of th century and in @ text subjectivity analysis performed by @ computational linguistics community in @ s @ however @ outbreak of computer-based sentiment analysis only occurred @ @ availability of subjective text on @ web @ consequently of @ @ @ @ published @ @ sentiment analysis @ @ scattered to multiple publication venue and @ combined number of @ in @ top venue only represent ca @ of @ @ in total @ @ @ @ top cited @ @ google scholar and scopus and a taxonomy of research topic @ in recent year sentiment analysis ha shifted @ analyzing online product review to social medium text @ twitter and facebook @ many topic beyond product review like stock market election disaster medicine software engineering and cyberbullying extend @ utilization of sentiment analysis @ @ inc @ 
1466,Arabic text classification using deep learning technics,"Text classification is the process of gathering documents into classes and categories based on their contents. This process is becoming more important due to the huge textual information available online. The main problem in text classification is how to improve the classification accuracy. Many algorithms have been proposed and implemented to solve this problem in general. However, few studies have been carried out for categorizing and classifying Arabic text. Technically, the process of text classification follows two steps; the first step consists on selecting some special features from all the features available from the text by applying features selection, features reduction and features weighting techniques. And the second step applies classification algorithms on those chosen features. In this paper, we present an innovative method for Arabic text classification. We use an Arabic stemming algorithm to extract, select and reduce the features that we need. After that, we use the Term Frequency-Inverse Document Frequency technique as feature weighting technique. And finally, for the classification step, we use one of the deep learning algorithms that is very powerful in other field such as the image processing and pattern recognition, but still rarely used in text mining, this algorithm is the Convolutional Neural Networks. With this combination and some hyperparameter tuning in the Convolutional Neural Networks algorithm we can achieve excellent results on multiple benchmarks. Copyright ⓒ 2018 SERSC Australia.",2018,International Journal of Grid and Distributed Computing,12,text classification is @ process of gathering document @ class and category based on @ content @ @ process is becoming more important due to @ huge textual information available online @ @ main problem in text classification is @ to improve @ classification accuracy @ many algorithm @ @ proposed and implemented to solve @ problem in general @ however @ study @ @ carried @ @ categorizing and classifying arabic text @ technically @ process of text classification follows @ step @ @ first step consists on selecting some special feature @ @ @ feature available @ @ text by applying feature selection feature reduction and feature weighting technique @ and @ second step applies classification algorithm on @ chosen feature @ in @ @ @ @ @ innovative method @ arabic text classification @ @ use @ arabic stemming algorithm to extract select and reduce @ feature @ @ need @ @ @ @ use @ term frequency-inverse document frequency technique a feature weighting technique @ and finally @ @ classification step @ use @ of @ deep learning algorithm @ is @ powerful in @ field @ a @ image processing and pattern recognition @ still rarely used in text mining @ algorithm is @ convolutional neural network @ @ @ combination and some hyperparameter tuning in @ convolutional neural network algorithm @ @ achieve excellent @ on multiple benchmark @ @ sersc australia @ 
1469,A system analytics framework for detecting infrastructure-related topics in disasters using social sensing,"The objective of this paper is to propose and test a system analytics framework based on social sensing and text mining to detect topic evolution associated with the performance of infrastructure systems in disasters. Social media, like Twitter, as active channels of communication and information dissemination, provide insights into real-time information and first-hand experience from affected areas in mass emergencies. While the existing studies show the importance of social sensing in improving situational awareness and emergency response in disasters, the use of social sensing for detection and analysis of infrastructure systems and their resilience performance has been rather limited. This limitation is due to the lack of frameworks to model the events and topics (e.g., grid interruption and road closure) evolution associated with infrastructure systems (e.g., power, highway, airport, and oil) in times of disasters. The proposed framework detects infrastructure-related topics of the tweets posted in disasters and their evolutions by integrating searching relevant keywords, text lemmatization, Part-of-Speech (POS) tagging, TF-IDF vectorization, topic modeling by using Latent Dirichlet Allocation (LDA), and K-Means clustering. The application of the proposed framework was demonstrated in a study of infrastructure systems in Houston during Hurricane Harvey. In this case study, more than sixty thousand tweets were retrieved from 150-mile radius in Houston over 39 days. The analysis of topic detection and evolution from user-generated data were conducted, and the clusters of tweets pertaining to certain topics were mapped in networks over time. The results show that the proposed framework enables to summarize topics and track the movement of situations in different disaster phases. The analytics elements of the proposed framework can improve the recognition of infrastructure performance through text-based representation and provide evidence for decision-makers to take actionable measurements. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),15,@ objective of @ @ is to propose and test a system analytics framework based on social sensing and text mining to detect topic evolution associated @ @ performance of infrastructure system in disaster @ social medium like twitter a active channel of communication and information dissemination provide insight @ real-time information and first-hand experience @ affected area in mass emergency @ @ @ existing study @ @ importance of social sensing in improving situational awareness and emergency response in disaster @ use of social sensing @ detection and analysis of infrastructure system and @ resilience performance ha @ rather limited @ @ limitation is due to @ lack of framework to model @ event and topic @ e @ g @ grid interruption and road closure @ evolution associated @ infrastructure system @ e @ g @ power highway airport and oil @ in time of disaster @ @ proposed framework detects infrastructure-related topic of @ tweet posted in disaster and @ evolution by integrating searching relevant keywords text lemmatization part-of-speech @ po @ tagging tf-idf vectorization topic modeling by @ latent dirichlet allocation @ lda @ and k-means clustering @ @ application of @ proposed framework wa demonstrated in a study of infrastructure system in houston @ hurricane harvey @ in @ case study more @ sixty thousand tweet @ retrieved @ mile radius in houston @ day @ @ analysis of topic detection and evolution @ user-generated data @ conducted and @ cluster of tweet pertaining to certain topic @ mapped in network @ time @ @ @ @ @ @ proposed framework enables to summarize topic and track @ movement of situation in different disaster phase @ @ analytics element of @ proposed framework @ improve @ recognition of infrastructure performance @ text-based representation and provide evidence @ decision-makers to take actionable measurement @ @ international publishing ag part of @ nature @ 
1470,Semantic Predicates in the Business Language,"In recent years, the interest in the use of language for business has grown. It is recognized that the hidden persuasive linguistic potential improves the company’s positioning in the public consciousness. The language of the business world is multifarious: we try to identify its features and behaviour, considering the evolution that it has faced primarily with the globalization of markets. Business activities are so complex that they require the application of several disciplines at the same time and therefore the use of specific languages and technical terminology. In order to reach an efficient analysis of business language, this study explores the role of semantic predicates constructed from lexical and the syntactic structures in which they are placed within business communication contexts. From the point of view of LG framework, a set of lexical-syntactic structures defines the value of semantic predicates, while the arguments selected by each semantic predicate are given the value of actants, subjects included. The features of each verb are expressed by the application of the rules of co-occurrence and selection restriction, through which verbs select semantically their arguments to construct acceptable simple sentences. In this way, the entries belonging to electronic dictionaries should be classified presuming their similarity and proximity. Even if the list of semantic tags is not simply identifiable, grammars could be built for single sets of semantic predicates. LG descriptions assign correlated predicates and arguments by applying electronic dictionaries of Italian. Using NooJ environment and Italian linguistic resources to automatically processing natural language, we will process a corpus of business documents. We will show and describe the syntactic structures, semantic and syntactic properties of predicates, in order to build formal grammar for business language. © 2018, Springer International Publishing AG.",2018,Communications in Computer and Information Science,2,in recent year @ interest in @ use of language @ @ ha grown @ @ is recognized @ @ hidden persuasive linguistic potential improves @ company s positioning in @ public consciousness @ @ language of @ @ world is multifarious @ @ try to identify @ feature and behaviour considering @ evolution @ @ ha faced primarily @ @ globalization of market @ @ activity @ @ complex @ @ require @ application of several discipline at @ @ time and therefore @ use of specific language and technical terminology @ in order to reach @ efficient analysis of @ language @ study explores @ role of semantic predicate constructed @ lexical and @ syntactic structure in @ @ @ placed within @ communication context @ @ @ point of view of lg framework a set of lexical-syntactic structure defines @ value of semantic predicate @ @ argument selected by @ semantic predicate @ given @ value of actants subject included @ @ feature of @ verb @ expressed by @ application of @ rule of co-occurrence and selection restriction @ @ verb select semantically @ argument to construct acceptable simple sentence @ in @ way @ entry belonging to electronic dictionary @ @ classified presuming @ similarity and proximity @ even if @ list of semantic tag is not simply identifiable grammar could @ built @ single set of semantic predicate @ lg description assign correlated predicate and argument by applying electronic dictionary of italian @ @ nooj environment and italian linguistic resource to automatically processing natural language @ @ process a corpus of @ document @ @ @ @ and describe @ syntactic structure semantic and syntactic property of predicate in order to build formal grammar @ @ language @ @ international publishing ag @ 
1472,Classifying semantic types of legal sentences: Portability of machine learning models,"Legal contract analysis is an important research area. The classification of clauses or sentences enables valuable insights such as the extraction of rights and obligations. However, datasets consisting of contracts are quite rare, particularly regarding German language. Therefore this paper experiments the portability of machine learning (ML) models with regard to different document types. We trained different ML classifiers on the tenancy law of the German Civil Code (BGB) to apply the resulting models on a set of rental agreements afterwards. The performance of our models varies on the contract set. Some models perform significantly worse, while certain settings reveal a portability. Additionally, we trained and evaluated the same classifiers on a dataset consisting solely of contracts, to be able to observe a reference performance. We could show that the performance of ML models may depend on the document type used for training, while certain setups result in portable models. © 2018 The authors and IOS Press.",2018,Frontiers in Artificial Intelligence and Applications,7,legal contract analysis is @ important research area @ @ classification of clause @ sentence enables valuable insight @ a @ extraction of right and obligation @ however datasets consisting of contract @ quite rare particularly regarding german language @ therefore @ @ experiment @ portability of machine learning @ ml @ model @ regard to different document type @ @ trained different ml classifier on @ tenancy law of @ german civil code @ bgb @ to apply @ resulting model on a set of rental agreement afterwards @ @ performance of @ model varies on @ contract set @ some model perform significantly worse @ certain setting reveal a portability @ additionally @ trained and evaluated @ @ classifier on a dataset consisting solely of contract to @ able to observe a reference performance @ @ could @ @ @ performance of ml model may depend on @ document type used @ training @ certain setup @ in portable model @ @ author and io @ @ 
1474,Interactive attention network for adverse drug reaction classification,"Detection of new adverse drug reactions is intended to both improve the quality of medications and drug reprofiling. Social media and electronic clinical reports are becoming increasingly popular as a source for obtaining the health-related information, such as identification of adverse drug reactions. One of the tasks of extracting adverse drug reactions from social media is the classification of entities that describe the state of health. In this paper, we investigate the applicability of Interactive Attention Network for identification of adverse drug reactions from user reviews. We formulate this problem as a binary classification task. We show the effectiveness of this method on a number of publicly available corpora. © Springer Nature Switzerland AG 2018.",2018,Communications in Computer and Information Science,5,detection of @ adverse drug reaction is intended to @ improve @ quality of medication and drug reprofiling @ social medium and electronic clinical report @ becoming increasingly popular a a source @ obtaining @ health-related information @ a identification of adverse drug reaction @ @ of @ task of extracting adverse drug reaction @ social medium is @ classification of entity @ describe @ state of health @ in @ @ @ investigate @ applicability of interactive attention network @ identification of adverse drug reaction @ user review @ @ formulate @ problem a a binary classification task @ @ @ @ effectiveness of @ method on a number of publicly available corpus @ @ nature switzerland ag @ 
1478,Analysis of social media posts for early detection of mental health conditions,"This paper presents a multipronged approach to predict early risk of mental health issues from user-generated content in social media. Supervised learning and information retrieval methods are used to estimate the risk of depression for a user given the content of its posts in reddit. The approach presented here was evaluated on the CLEF eRisk 2017 pilot task. We describe the details of five systems submitted to the task, and compare their performance. The comparisons show that combining information retrieval and machine learning methods gives the best results. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ @ a multipronged approach to predict early risk of mental health issue @ user-generated content in social medium @ supervised learning and information retrieval method @ used to estimate @ risk of depression @ a user given @ content of @ post in reddit @ @ approach presented @ wa evaluated on @ clef erisk pilot task @ @ describe @ detail of five system submitted to @ task and compare @ performance @ @ comparison @ @ combining information retrieval and machine learning method give @ best @ @ @ international publishing ag part of @ nature @ 
1479,Supervised topic models for diagnosis code assignment to discharge summaries,"Mining medical data has significantly gained interest in the recent years thanks to the advances in data mining and machine learning fields. In this work, we focus on a challenging issue in medical data mining: automatic diagnosis code assignment to discharge summaries, i.e., characterizing patient’s hospital stay (diseases, symptoms, treatments, etc.) with a set of codes usually derived from the International Classification of Diseases (ICD). We cast the problem as a machine learning task and we experiment some recent approaches based on the probabilistic topic models. We demonstrate the efficiency of these models in terms of high predictive scores and ease of result interpretation. As such, we show how topic models enable gaining insights into this field and provide new research opportunities for possible improvements. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,mining medical data ha significantly gained interest in @ recent year thanks to @ advance in data mining and machine learning field @ in @ work @ focus on a challenging issue in medical data mining @ automatic diagnosis code assignment to discharge summary i @ e @ characterizing patient s hospital stay @ disease symptom treatment etc @ @ @ a set of code usually derived @ @ international classification of disease @ icd @ @ @ cast @ problem a a machine learning task and @ experiment some recent approach based on @ probabilistic topic model @ @ demonstrate @ efficiency of @ model in term of high predictive score and ease of @ interpretation @ a @ @ @ @ topic model enable gaining insight @ @ field and provide @ research opportunity @ possible improvement @ @ international publishing ag part of @ nature @ 
1480,Semantic Analysis for Identifying Security Concerns in Software Procurement Edicts,"Brazilian Federal Institutions must acquire software tools by procurement, so their software teams have to develop, verify, and audit the specifications to ensure that the edicts properly include software security risks concerns. This work presents the Automated Analyst of Edicts tool, which aids the analysis of a document by automatic identification of absent relationships between its sentences and concepts related to software security risks or weaknesses. It was compared to software security experts’ performance for multi-label classification into five of the OWASP Top 10 risks. Specificity of over 80% was achieved when analyzing individual sentences for multiple risks, and a 90% negative prediction probability result obtained when applied to specific risk–sentence relationships.. © 2017, Ohmsha, Ltd. and Springer Japan KK.",2018,New Generation Computing,4,brazilian federal institution must acquire software tool by procurement @ @ software team @ to develop verify and audit @ specification to ensure @ @ edict properly include software security risk concern @ @ work @ @ automated analyst of edict tool @ aid @ analysis of a document by automatic identification of absent relationship @ @ sentence and concept related to software security risk @ weakness @ @ wa compared to software security expert performance @ multi-label classification @ five of @ owasp top risk @ specificity of @ wa achieved @ analyzing individual sentence @ multiple risk and a negative prediction probability @ obtained @ applied to specific risk sentence relationship @ @ ohmsha ltd @ and @ japan kk @ 
1481,Trigger words detection by integrating attention mechanism into Bi-LSTM neural network—A case study in PubMED-wide trigger words detection for pancreatic cancer,"A Bi-LSTM based encode/decode mechanism for named entity recognition was studied in this research. In the proposed mechanism, Bi-LSTM was used for encoding, an Attention method was used in the intermediate layers, and an unidirectional LSTM was used as decoder layer. By using element wise product to modify the conventional decoder layers, the proposed model achieved better F-score, compared with other three baseline LSTM-based models. For the purpose of algorithm application, a case study of causal gene discovery in terms of disease pathway enrichment was designed. In addition, the causal gene discovery rate of our proposed method was compared with another baseline methods. The result showed that trigger genes detection effectively increase the performance of a text mining system for causal gene discovery. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,a bi-lstm based encode decode mechanism @ named entity recognition wa studied in @ research @ in @ proposed mechanism bi-lstm wa used @ encoding @ attention method wa used in @ intermediate layer and @ unidirectional lstm wa used a decoder layer @ by @ element wise product to modify @ conventional decoder layer @ proposed model achieved better f-score compared @ @ three baseline lstm-based model @ @ @ purpose of algorithm application a case study of causal gene discovery in term of disease pathway enrichment wa designed @ in addition @ causal gene discovery rate of @ proposed method wa compared @ another baseline method @ @ @ showed @ trigger gene detection effectively increase @ performance of a text mining system @ causal gene discovery @ @ nature switzerland ag @ 
1482,Performance evaluation of text categorization algorithms using an albanian corpus,"Text mining and natural language processing are gaining significant role in our daily life as information volumes increase steadily. Most of the digital information is unstructured in the form of raw text. While for several languages there is extensive research on mining and language processing, much less work has been performed for other languages. In this paper we aim to evaluate the performance of some of the most important text classification algorithms over a corpus composed of Albanian texts. After applying natural language preprocessing steps, we apply several algorithms such as Simple Logistics, Naïve Bayes, k-Nearest Neighbor, Decision Trees, Random Forest, Support Vector Machines and Neural Networks. The experiments show that Naïve Bayes and Support Vector Machines perform best in classifying Albanian corpuses. Furthermore, Simple Logistics algorithm also shows good results. © Springer International Publishing AG 2018.",2018,Lecture Notes on Data Engineering and Communications Technologies,0,text mining and natural language processing @ gaining significant role in @ daily life a information volume increase steadily @ @ of @ digital information is unstructured in @ form of raw text @ @ @ several language @ is extensive research on mining and language processing much le work ha @ performed @ @ language @ in @ @ @ aim to evaluate @ performance of some of @ @ important text classification algorithm @ a corpus composed of albanian text @ @ applying natural language preprocessing step @ apply several algorithm @ a simple logistics naïve bayes k-nearest neighbor decision tree random forest support vector machine and neural network @ @ experiment @ @ naïve bayes and support vector machine perform best in classifying albanian corpus @ furthermore simple logistics algorithm @ @ good @ @ @ international publishing ag @ 
1484,MAMTRA-MED at CLEF eHealth 2018: A combination of information retrieval techniques and neural networks for ICD-10 coding of death certificates,"This paper describes the systems proposed by LSLUNED team in Task 1 of the CLEF eHealth 2018 challenge. The main objective is the automatic coding of death certificates in French, Italian and Hungarian languages according to the ICD-f 0. This task has been tackled through supervised learning methods such as neural networks, and techniques based on Information Retrieval (IR) systems. The first approach has been implemented by training one model for each of the most frequent ICD-fO codes in the corpus. For this purpose, a bag-of-words approach has been applied using the TF-BNS value for terms contained in death certificate statements. As for the IR approach, Lucene has been used as a search engine, indexing dictionaries and the content of the death certificates in the training corpus. Finally, a combination of both methods has been proposed to balance precision and recall, using the IR system for diseases not classified by any learning model. Similar Fi scores are obtained on the test datasets of each language by supervised methods and the combined system giving the latter greater recall values.",2018,CEUR Workshop Proceedings,2,@ @ describes @ system proposed by lsluned team in task of @ clef ehealth challenge @ @ main objective is @ automatic coding of death certificate in french italian and hungarian language according to @ icd-f @ @ task ha @ tackled @ supervised learning method @ a neural network and technique based on information retrieval @ ir @ system @ @ first approach ha @ implemented by training @ model @ @ of @ @ frequent icd-fo code in @ corpus @ @ @ purpose a bag-of-words approach ha @ applied @ @ tf-bns value @ term contained in death certificate statement @ a @ @ ir approach lucene ha @ used a a search engine indexing dictionary and @ content of @ death certificate in @ training corpus @ finally a combination of @ method ha @ proposed to balance precision and recall @ @ ir system @ disease not classified by @ learning model @ similar fi score @ obtained on @ test datasets of @ language by supervised method and @ combined system giving @ latter greater recall value @ 
1485,Generating adversarial text samples,"Adversarial samples are strategically modified samples, which are crafted with the purpose of fooling a trained classifier. In this paper, we propose a new method of crafting adversarial text samples by modification of the original samples. Modifications of the original text samples are done by deleting or replacing the important or salient words in the text or by introducing new words in the text sample. While crafting adversarial samples, one of the key constraint is to generate meaningful sentences which can at pass off as legitimate from the language (English) viewpoint. Experimental results on IMDB movie review dataset for sentiment analysis and Twitter dataset for gender detection show the efficacy of our proposed method. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,adversarial sample @ strategically modified sample @ @ crafted @ @ purpose of fooling a trained classifier @ in @ @ @ propose a @ method of crafting adversarial text sample by modification of @ original sample @ modification of @ original text sample @ done by deleting @ replacing @ important @ salient word in @ text @ by introducing @ word in @ text sample @ @ crafting adversarial sample @ of @ key constraint is to generate meaningful sentence @ @ at pas off a legitimate @ @ language @ english @ viewpoint @ experimental @ on imdb movie review dataset @ sentiment analysis and twitter dataset @ gender detection @ @ efficacy of @ proposed method @ @ international publishing ag part of @ nature @ 
1486,An emotion-driven approach for aspect-based opinion mining,"The remarkable ability to understand the opinion of a user about a specific topic of discussion allows intelligent systems to provide more specific and personalized suggestions especially when no other information is available. The strategies for opinion mining, also known as sentiment analysis, are in last years topic of in-depth studies. In this work, we present an approach of text mining for detecting the topic of discussion for textual contents and the emotion that the writer feels while writing it. Conversely to the classic strategies of sentiment analysis, we enrich the standard polarity prediction task with more fine-grained information about user’s emotion. By using this information, the final behavior of the personalized system could be designed by taking into account the view about the topic of the specific user. For performing this task, we adopted a hybrid approach which uses both lexicons and semantic representation of sentences for the operation of aspect classification. Training data for the aspects detection module have been extracted from already categorized last year world news. The emotional labeling approach is, instead, based on the posts left by users on Facebook, which have been annotated using the emoticon encountered. The evaluation has been conducted on a dataset of tweets opportunely collected using hash-tags which refer both to the topic of discussion and the emotional opinion. Copyright held by the author(s).",2018,CEUR Workshop Proceedings,2,@ remarkable ability to understand @ opinion of a user @ a specific topic of discussion allows intelligent system to provide more specific and personalized suggestion especially @ no @ information is available @ @ strategy @ opinion mining @ known a sentiment analysis @ in last year topic of in-depth study @ in @ work @ @ @ approach of text mining @ detecting @ topic of discussion @ textual content and @ emotion @ @ writer feel @ writing @ @ conversely to @ classic strategy of sentiment analysis @ enrich @ standard polarity prediction task @ more fine-grained information @ user s emotion @ by @ @ information @ final behavior of @ personalized system could @ designed by taking @ account @ view @ @ topic of @ specific user @ @ performing @ task @ adopted a hybrid approach @ us @ lexicon and semantic representation of sentence @ @ operation of aspect classification @ training data @ @ aspect detection module @ @ extracted @ already categorized last year world news @ @ emotional labeling approach is instead based on @ post left by user on facebook @ @ @ annotated @ @ emoticon encountered @ @ evaluation ha @ conducted on a dataset of tweet opportunely collected @ hash-tags @ refer @ to @ topic of discussion and @ emotional opinion @ @ held by @ author @ s @ @ 
1487,Extracting chemical-protein relations using attention-based neural networks,"Relation extraction is an important task in the field of natural language processing. In this paper, we describe our approach for the BioCreative VI Task 5: text mining chemical-protein interactions. We investigate multiple deep neural network (DNN) models, including convolutional neural networks, recurrent neural networks (RNNs) and attention-based (ATT-) RNNs (ATT-RNNs) to extract chemical-protein relations. Our experimental results indicate that ATT-RNN models outperform the same models without using attention and the ATT-gated recurrent unit (ATT-GRU) achieves the best performing micro average F1 score of 0.527 on the test set among the tested DNNs. In addition, the result of word-level attention weights also shows that attention mechanism is effective on selecting the most important trigger words when trained with semantic relation labels without the need of semantic parsing and feature engineering. The source code of this work is available at https://github.com/ohnlp/att-chemprot. © The Author(s) 2018. Published by Oxford University Press.",2018,Database,12,relation extraction is @ important task in @ field of natural language processing @ in @ @ @ describe @ approach @ @ biocreative vi task @ text mining chemical-protein interaction @ @ investigate multiple deep neural network @ dnn @ model including convolutional neural network recurrent neural network @ rnns @ and attention-based @ att @ rnns @ att-rnns @ to extract chemical-protein relation @ @ experimental @ indicate @ att-rnn model outperform @ @ model without @ attention and @ att-gated recurrent unit @ att-gru @ achieves @ best performing micro average f score of @ on @ test set among @ tested dnns @ in addition @ @ of word-level attention weight @ @ @ attention mechanism is effective on selecting @ @ important trigger word @ trained @ semantic relation label without @ need of semantic parsing and feature engineering @ @ source code of @ work is available at http @ github @ com ohnlp att-chemprot @ @ author @ s @ @ published by oxford university @ @ 
1489,Semantic representations in text data,"Automatic text mining processes and other sophisticated natural language processing constructs need realistic representations of text/documents which embed semantics efficiently. All the representations work on the notion that every data contains different explanatory factors (attributes). In this article, we exploit these explanatory factors to study and compare various semantic representation methods for text documents. The article critically reviews recent trends in the area of semi-supervised semantic representations, covering cutting-edge methods in distributed representations such as embeddings. This article gives a broad and synthesized description of various forms of text representations, presented in their chronological order ranging from BoW models to the most recent embeddings learning. Conclusively, various findings taken together provide valuable pointers for researchers looking to work in the field of semantic representations. In addition, the article also shows that one need to develop a model for learning universal embeddings in unsupervised/semi-supervised settings that incorporate contextual as well as word-order information, with language independent features and which would be feasible for large dataset. Copyright ⓒ 2018 SERSC Australia.",2018,International Journal of Grid and Distributed Computing,1,automatic text mining process and @ sophisticated natural language processing construct need realistic representation of text document @ embed semantics efficiently @ @ @ representation work on @ notion @ every data contains different explanatory factor @ attribute @ @ in @ article @ exploit @ explanatory factor to study and compare various semantic representation method @ text document @ @ article critically review recent trend in @ area of semi-supervised semantic representation covering cutting-edge method in distributed representation @ a embeddings @ @ article give a broad and synthesized description of various form of text representation presented in @ chronological order ranging @ bow model to @ @ recent embeddings learning @ conclusively various finding taken together provide valuable pointer @ researcher looking to work in @ field of semantic representation @ in addition @ article @ @ @ @ need to develop a model @ learning universal embeddings in unsupervised semi-supervised setting @ incorporate contextual a well a word-order information @ language independent feature and @ would @ feasible @ @ dataset @ @ sersc australia @ 
1490,A survey of ontology learning techniques and applications,"Ontologies have gained a lot of popularity and recognition in the semantic web because of their extensive use in Internet-based applications. Ontologies are often considered a fine source of semantics and interoperability in all artificially smart systems. Exponential increase in unstructured data on the web has made automated acquisition of ontology from unstructured text a most prominent research area. Several methodologies exploiting numerous techniques of various fields (machine learning, text mining, knowledge representation and reasoning, information retrieval and natural language processing) are being proposed to bring some level of automation in the process of ontology acquisition from unstructured text. This paper describes the process of ontology learning and further classification of ontology learning techniques into three classes (linguistics, statistical and logical) and discusses many algorithms under each category. This paper also explores ontology evaluation techniques by highlighting their pros and cons. Moreover, it describes the scope and use of ontology learning in several industries. Finally, the paper discusses challenges of ontology learning along with their corresponding future directions. © The Author(s) 2018. Published by Oxford University Press.",2018,Database,37,ontology @ gained a lot of popularity and recognition in @ semantic web @ of @ extensive use in internet-based application @ ontology @ often considered a fine source of semantics and interoperability in @ artificially smart system @ exponential increase in unstructured data on @ web ha made automated acquisition of ontology @ unstructured text a @ prominent research area @ several methodology exploiting numerous technique of various field @ machine learning text mining knowledge representation and reasoning information retrieval and natural language processing @ @ @ proposed to bring some level of automation in @ process of ontology acquisition @ unstructured text @ @ @ describes @ process of ontology learning and @ classification of ontology learning technique @ three class @ linguistics statistical and logical @ and discus many algorithm @ @ category @ @ @ @ explores ontology evaluation technique by highlighting @ pro and con @ moreover @ describes @ scope and use of ontology learning in several industry @ finally @ @ discus challenge of ontology learning along @ @ corresponding future direction @ @ author @ s @ @ published by oxford university @ @ 
1491,InstaSent: A novel framework for sentiment analysis based on instagram selfies,"Sentiment analysis and opinion mining is the field of study to analyse opinions, attitudes, and emotions of people. It is the most studied research field. With the evolution of social media, many new terms have been evolved including selfies. A selfie has provided a way for to the users of social media to record their personal memories. People are sharing their images on social media in a massive amount. Keeping in mind the big data nature, it is very difficult to analyse selfies manually. In this research study, we have proposed a framework called InstaSent for sentiment analysis based on Instagram selfies. This framework incorporates both text mining and image mining techniques for sentiment prediction. Support vector machine is used for sentiment classification based on the text associated with selfies like captions, hashtags, comments, and emoticons, while the deep learning method Convolutional neural network is used for processing image data for sentiment analyses. By combining the practices for text mining and image mining we believe that this technique will outperform all other techniques presented in this domain. In a nutshell, we believe that our research study has provided novel opportunities for researchers to explore the use of selfies in other domains. © Springer Nature Switzerland AG 2019.",2018,Advances in Intelligent Systems and Computing,0,sentiment analysis and opinion mining is @ field of study to analyse opinion attitude and emotion of people @ @ is @ @ studied research field @ @ @ evolution of social medium many @ term @ @ evolved including selfies @ a selfie ha provided a way @ to @ user of social medium to record @ personal memory @ people @ sharing @ image on social medium in a massive amount @ keeping in mind @ big data nature @ is @ difficult to analyse selfies manually @ in @ research study @ @ proposed a framework called instasent @ sentiment analysis based on instagram selfies @ @ framework incorporates @ text mining and image mining technique @ sentiment prediction @ support vector machine is used @ sentiment classification based on @ text associated @ selfies like caption hashtags comment and emoticon @ @ deep learning method convolutional neural network is used @ processing image data @ sentiment analysis @ by combining @ practice @ text mining and image mining @ believe @ @ technique @ outperform @ @ technique presented in @ domain @ in a nutshell @ believe @ @ research study ha provided novel opportunity @ researcher to explore @ use of selfies in @ domain @ @ nature switzerland ag @ 
1492,Morphological evaluation and sentiment analysis of Punjabi text using deep learning classification,"Morphological processing of Indian languages is one of the most escalating fields in the era of Natural Language Processing (NLP) since the last decade. The evaluation of Asian languages is a highly relevant field in the times of text mining and information retrieval. The morphological evaluation of a text can be employed for extraction and classification of knowledge. This paper amalgamates morphological evaluation and sentiment prediction of Punjabi language text. The textual data for Punjabi language is concerned with farmer suicide cases reported for Punjab state of India. The pre-processing phase of this study involves morphological evaluation and normalization of Punjabi words to their respective canonical forms. The next phase carries out training and testing of deep neural network model on refined Punjabi tokens obtained from the earlier phase. The proposed model classifies Punjabi tokens into four negatively oriented classes tailored for farmer suicide cases. The average accuracies of sentiment prediction obtained after 10-fold cross validation are 93.85%, 88.53%, 83.3%, and 95.45% for the four respective classes. The proposed framework yields satisfactory results on 275 Punjabi text documents with the overall accuracy of 90.29% for sentiment classification. © 2018 The Authors",2018,Journal of King Saud University - Computer and Information Sciences,9,morphological processing of indian language is @ of @ @ escalating field in @ era of natural language processing @ nlp @ since @ last decade @ @ evaluation of asian language is a highly relevant field in @ time of text mining and information retrieval @ @ morphological evaluation of a text @ @ employed @ extraction and classification of knowledge @ @ @ amalgamates morphological evaluation and sentiment prediction of punjabi language text @ @ textual data @ punjabi language is concerned @ farmer suicide case reported @ punjab state of india @ @ pre-processing phase of @ study involves morphological evaluation and normalization of punjabi word to @ respective canonical form @ @ next phase carry @ training and testing of deep neural network model on refined punjabi token obtained @ @ earlier phase @ @ proposed model classifies punjabi token @ four negatively oriented class tailored @ farmer suicide case @ @ average accuracy of sentiment prediction obtained @ fold cross validation @ @ @ @ and @ @ @ four respective class @ @ proposed framework yield satisfactory @ on punjabi text document @ @ overall accuracy of @ @ sentiment classification @ @ author
1494,Gemedoc: A text similarity annotation platform,"We present Gemedoc, a platform for text similarity annotation based on the spatial and the thematic dimension. To this end, a two-step annotation protocol was designed to assess the similarity between two documents: (1) identification of salient features according to the two analysis dimensions; (2) similarity assessment according to a 4-degree scale. Ultimately, the labeled data retrieved from different corpora could be used as benchmark for text-mining applications. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ gemedoc a platform @ text similarity annotation based on @ spatial and @ thematic dimension @ to @ end a two-step annotation protocol wa designed to ass @ similarity @ @ document @ @ @ identification of salient feature according to @ @ analysis dimension @ @ @ similarity assessment according to a degree scale @ ultimately @ labeled data retrieved @ different corpus could @ used a benchmark @ text-mining application @ @ international publishing ag part of @ nature @ 
1495,On the automatic construction of knowledge-map from handouts for MOOC courses,"Massive open online courses (MOOCs) offer valuable opportunities for freedom in learning; however, many learners face cognitive overload and conceptual and navigational disorientation. In this study, we used handouts to automatically build domain-specific knowledge maps for MOOCs. We considered handouts as conceptual models created by teachers, and we performed text mining to extract keywords from MOOC handouts. Each knowlege map is based on the structure of the handouts, each consisting of an outline, title, and content. The findings suggest that using handouts to build knowledge maps is feasible. © Springer International Publishing AG 2018.",2018,"Smart Innovation, Systems and Technologies",3,massive open online course @ moocs @ offer valuable opportunity @ freedom in learning @ however many learner face cognitive overload and conceptual and navigational disorientation @ in @ study @ used handout to automatically build domain-specific knowledge map @ moocs @ @ considered handout a conceptual model created by teacher and @ performed text mining to extract keywords @ mooc handout @ @ knowlege map is based on @ structure of @ handout @ consisting of @ outline title and content @ @ finding suggest @ @ handout to build knowledge map is feasible @ @ international publishing ag @ 
1497,Evaluation of Named Entity Recognition algorithms using clinical text data,"Named Entity Recognition (NER) is one of the most important research areas in the field of medical. Presently, most of the clinical NER research is based on two approaches as Knowledge Engineering (KE) and Machine Learning (ML). KE is used a word lookup table approach and ML is known as supervised learning approach. The aim of this work is to evaluate a recent algorithm in KE and ML approaches using various clinical text databases. Therefore, the NOBLE Coder and Clinical Named Entity Recognition (CliNER) algorithms are selected, NOBLE Coder is depended on KE approach and CliNER is ML approach. The two algorithms will be described and compared its performance on three openly available datasets that is obtained from Medical Information Mart for Intensive Care II (MIMIC II), Pittsburgh Medical Center, and i2b2 2010 challenge. Among these datasets, the annotated data are included which is used to detect the highest sensitivity and specificity on each algorithm. The randomly distributed patient reports were taken as input data to these algorithms. By executing these algorithms, the information is extracted and which classified into predefined concept types, for example medical problems, treatments and tests. The accuracy of both algorithms is calculated using standard measures. The taken two algorithms are analyzed based on the produced results. Finally, the best among two is suggested for better use in clinical data. © 2018 Authors.",2018,International Journal of Engineering and Technology(UAE),0,named entity recognition @ ner @ is @ of @ @ important research area in @ field of medical @ presently @ of @ clinical ner research is based on @ approach a knowledge engineering @ ke @ and machine learning @ ml @ @ ke is used a word lookup table approach and ml is known a supervised learning approach @ @ aim of @ work is to evaluate a recent algorithm in ke and ml approach @ various clinical text database @ therefore @ noble coder and clinical named entity recognition @ cliner @ algorithm @ selected noble coder is depended on ke approach and cliner is ml approach @ @ @ algorithm @ @ described and compared @ performance on three openly available datasets @ is obtained @ medical information mart @ intensive care ii @ mimic ii @ pittsburgh medical center and i b challenge @ among @ datasets @ annotated data @ included @ is used to detect @ highest sensitivity and specificity on @ algorithm @ @ randomly distributed patient report @ taken a input data to @ algorithm @ by executing @ algorithm @ information is extracted and @ classified @ predefined concept type @ example medical problem treatment and test @ @ accuracy of @ algorithm is calculated @ standard measure @ @ taken @ algorithm @ analyzed based on @ produced @ @ finally @ best among @ is suggested @ better use in clinical data @ author @ 
1500,Named entity recognition in Tatar: Corpus-based algorithm,"Named entities recognition is one of the urgent tasks in the researches of language using electronic language corpuses. This article discusses the main methods for solving this problem, including algorithms based on various machine learning models, regular expressions and dictionaries. Also in the article, the authors proposed their own algorithm, which allows named entities recognition on the basis of search queries using direct and reverse search. The results of the algorithm, presented in the article, suggest what additional functions are necessary to achieve the best results. The proposed algorithm is used in the “Tugan Tel” corpus management system and can be used both with the electronic corpus of the Tatar language and with corpuses of other languages. © 2018 CEUR-WS. All rights reserved.",2018,CEUR Workshop Proceedings,0,named entity recognition is @ of @ urgent task in @ research of language @ electronic language corpus @ @ article discus @ main method @ solving @ problem including algorithm based on various machine learning model regular expression and dictionary @ @ in @ article @ author proposed @ @ algorithm @ allows named entity recognition on @ basis of search query @ direct and reverse search @ @ @ of @ algorithm presented in @ article suggest @ additional function @ necessary to achieve @ best @ @ @ proposed algorithm is used in @ tugan tel corpus management system and @ @ used @ @ @ electronic corpus of @ tatar language and @ corpus of @ language @ ceur-ws @ @ right reserved @ 
1501,Computational classification of phenologs across biological diversity,"Phenotypic diversity analyses are the basis for research discoveries ranging from basic biology to applied research. Phenotypic analyses often benefit from the availability of large quantities of high-quality data in a standardized format. Image and spectral analyses have been shown to enable high-throughput, computational classification of a variety of phenotypes and traits. However, equivalent phenotypes expressed across individuals or groups that are not anatomically similar can pose a problem for such classification methods. In these cases, high-throughput, computational classification is still possible if the phenotypes are documented using standardized, language-based descriptions. Conversion of language-based phenotypes to computer-readable ""EQ"" statements enables such large-scale analyses. EQ statements are composed of entities (e.g., leaf) and qualities (e.g., increased length) drawn from terms in ontologies. In this work, we present a method for automatically converting free-text descriptions of plant phenotypes to EQ statements using a machine learning approach. Random forest classifiers identify potential matches between phenotype descriptions and terms from a set of ontologies including GO (gene ontology), PO (plant ontology), and PATO (phenotype and trait ontology), among others. These candidate ontology terms are combined into candidate EQ statements, which are probabilistically evaluated with respect to a natural language parse of the phenotype description. Models and parameters in this method are trained using a dataset of plant phenotypes and curator-converted EQ statements from the Plant PhenomeNET project (Oellrich, Walls et al., 2015). Preliminary results comparing predicted and curated EQ statements are presented. Potential use across datasets to enable automated phenolog discovery are discussed. © 2018 CEUR-WS.",2018,CEUR Workshop Proceedings,0,phenotypic diversity analysis @ @ basis @ research discovery ranging @ basic biology to applied research @ phenotypic analysis often benefit @ @ availability of @ quantity of high-quality data in a standardized format @ image and spectral analysis @ @ @ to enable high-throughput computational classification of a variety of phenotype and trait @ however equivalent phenotype expressed across individual @ group @ @ not anatomically similar @ pose a problem @ @ classification method @ in @ case high-throughput computational classification is still possible if @ phenotype @ documented @ standardized language-based description @ conversion of language-based phenotype to computer-readable @ eq @ statement enables @ large-scale analysis @ eq statement @ composed of entity @ e @ g @ leaf @ and quality @ e @ g @ increased length @ drawn @ term in ontology @ in @ work @ @ a method @ automatically converting free-text description of plant phenotype to eq statement @ a machine learning approach @ random forest classifier identify potential match @ phenotype description and term @ a set of ontology including go @ gene ontology @ po @ plant ontology @ and pato @ phenotype and trait ontology @ among others @ @ candidate ontology term @ combined @ candidate eq statement @ @ probabilistically evaluated @ respect to a natural language parse of @ phenotype description @ model and parameter in @ method @ trained @ a dataset of plant phenotype and curator-converted eq statement @ @ plant phenomenet project @ oellrich wall et al @ @ @ preliminary @ comparing predicted and curated eq statement @ presented @ potential use across datasets to enable automated phenolog discovery @ discussed @ ceur-ws @ 
1503,SSN_NLp@IECSIL-FIRE-2018: Deep learning approach to named entity recognition and relation extraction for conversational systems in Indian languages,"Named Entity Recognition (NER) focuses on the classification of proper nouns into the generic named entities (NE) such as person_names, organizations, locations, currency and dates. NER has several applications like conversation systems, machine translation, automatic summarization and question answering. Relation Extraction (RE) is an information extraction process used to identify the relationship between NEs. RE is very important in applications like short answer grading, conversation systems, question answering and ontology learning. NER and RE in Indian languages are difficult tasks due to their agglutinative nature and rich morphological structure. Further, developing language independent framework that supports all Indian Languages is a challenging task. In this paper, we present a deep learning methodology for both NER and RE in five Indian languages namely Hindi, Kannada, Malayalam, Tamil and Telugu. We proposed a common approach that works for both NER and RE tasks. We have used neural machine translation architecture to implement our methodology for these tasks. Our approach was evaluated using the data set given by IECSIL@FIRE2018 shared task. We have evaluated on two sets of data for NER task and obtained the accuracies as 94.41%, 95.23%, 95.97% and 96.02% for the four variations on pre-evaluation test set and 95.9%, 95.85% and 95.05% for the three runs on final-evaluation test set. Also, for RE task, we have obtained the accuracies as 56.19%, 60.74%, 60.7%, 75.43% and 79.11% for our five variations on pre-evaluation test set and 79.44%, 76.01% and 61.11% for Run 1, Run 2 and Run 3 respectively on final-evaluation test set. © 2018 CEUR-WS. All Rights Reserved.",2018,CEUR Workshop Proceedings,2,named entity recognition @ ner @ focus on @ classification of proper noun @ @ generic named entity @ ne @ @ a person name organization location currency and date @ ner ha several application like conversation system machine translation automatic summarization and question answering @ relation extraction @ @ @ is @ information extraction process used to identify @ relationship @ ne @ @ is @ important in application like short answer grading conversation system question answering and ontology learning @ ner and @ in indian language @ difficult task due to @ agglutinative nature and rich morphological structure @ @ developing language independent framework @ support @ indian language is a challenging task @ in @ @ @ @ a deep learning methodology @ @ ner and @ in five indian language namely hindi kannada malayalam tamil and telugu @ @ proposed a common approach @ work @ @ ner and @ task @ @ @ used neural machine translation architecture to implement @ methodology @ @ task @ @ approach wa evaluated @ @ data set given by iecsil fire shared task @ @ @ evaluated on @ set of data @ ner task and obtained @ accuracy a @ @ @ and @ @ @ four variation on pre-evaluation test set and @ @ and @ @ @ three run on final-evaluation test set @ @ @ @ task @ @ obtained @ accuracy a @ @ @ @ and @ @ @ five variation on pre-evaluation test set and @ @ and @ @ run run and run respectively on final-evaluation test set @ ceur-ws @ @ right reserved @ 
1504,Deep learning approach to English-Tamil and Hindi-Tamil verb phrase translations,"Verb phrase (VP) translation focuses on translating all forms of verbs that helps in Machine translation (MT) task. This has several applications such as cross lingual information retrieval (CLIR), speech synthesis, natural language understanding and generation. VP translation is a challenging task due to variations of characteristics, structure and families among the languages. Further, developing a language independent methodology for VP translation is an interesting task. In this paper, we present a deep learning methodology for English-Tamil and Hindi-Tamil VP translations. We have adopted neural machine translation model to implement our methodology for VP translation. Our approach was evaluated using the data set given by VPT-IL@FIRE2018 shared task. © 2018 CEUR-WS. All Rights Reserved.",2018,CEUR Workshop Proceedings,0,verb phrase @ vp @ translation focus on translating @ form of verb @ help in machine translation @ mt @ task @ @ ha several application @ a cross lingual information retrieval @ clir @ speech synthesis natural language understanding and generation @ vp translation is a challenging task due to variation of characteristic structure and family among @ language @ @ developing a language independent methodology @ vp translation is @ interesting task @ in @ @ @ @ a deep learning methodology @ english-tamil and hindi-tamil vp translation @ @ @ adopted neural machine translation model to implement @ methodology @ vp translation @ @ approach wa evaluated @ @ data set given by vpt-il fire shared task @ ceur-ws @ @ right reserved @ 
1505,A machine learning approach to indian native language identification,"NLI (Native Language Identification) determines the native language of the non-native users using their writings in a foreign language. It has several applications namely forensic and security, author profiling and identification, and educational applications. English is a most common language used in social media by many non-English people in the world to share their thoughts and ideas. They blend English with their native language for their posts and comments. Identifying the native language from the short text in English is still a challenging task. In this paper, we present a language agnostic approach without any language specific processing and employed machine learning approach with and without feature selection to identify the native language of a Indian speaker using their comments and posts in social network. The bag of word features are extracted from the text posted by the user and the feature vectors are constructed using TF-IDF score for the training data. We have used a statistical feature selection methodology to select the features that are significantly contributing to NLI task. The classifier with highest cross validation accuracy was used for predicting the native language of the user. Our approaches are evaluated using INLI@FIRE2018 shared task data set. © 2018 CEUR-WS. All Rights Reserved.",2018,CEUR Workshop Proceedings,0,nli @ native language identification @ determines @ native language of @ non-native user @ @ writing in a foreign language @ @ ha several application namely forensic and security author profiling and identification and educational application @ english is a @ common language used in social medium by many non-english people in @ world to share @ thought and idea @ @ blend english @ @ native language @ @ post and comment @ identifying @ native language @ @ short text in english is still a challenging task @ in @ @ @ @ a language agnostic approach without @ language specific processing and employed machine learning approach @ and without feature selection to identify @ native language of a indian speaker @ @ comment and post in social network @ @ bag of word feature @ extracted @ @ text posted by @ user and @ feature vector @ constructed @ tf-idf score @ @ training data @ @ @ used a statistical feature selection methodology to select @ feature @ @ significantly contributing to nli task @ @ classifier @ highest cross validation accuracy wa used @ predicting @ native language of @ user @ @ approach @ evaluated @ inli fire shared task data set @ ceur-ws @ @ right reserved @ 
1506,Book recommendation beyond the usual suspects: Embedding book plots together with place and time information,"Content-based recommendation of books and other media is usually based on semantic similarity measures. While metadata can be compared easily, measuring the semantic similarity of narrative literature is challenging. Keyword-based approaches are biased to retrieve books of the same series or do not retrieve any results at all in sparser libraries. We propose to represent plots with dense vectors to foster semantic search for similar plots even if they do not have any words in common. Further, we propose to embed plots, places, and times in the same embedding space. Thereby, we allow arithmetics on these aspects. For example, a book with a similar plot but set in a different, user-specified place can be retrieved. We evaluate our findings on a set of 16,000 book synopses that spans literature from 500 years and 200 genres and compare our approach to a keyword-based baseline. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,content-based recommendation of book and @ medium is usually based on semantic similarity measure @ @ metadata @ @ compared easily measuring @ semantic similarity of narrative literature is challenging @ keyword-based approach @ biased to retrieve book of @ @ series @ @ not retrieve @ @ at @ in sparser library @ @ propose to represent plot @ dense vector to foster semantic search @ similar plot even if @ @ not @ @ word in common @ @ @ propose to embed plot place and time in @ @ embedding space @ thereby @ allow arithmetic on @ aspect @ @ example a book @ a similar plot @ set in a different user-specified place @ @ retrieved @ @ evaluate @ finding on a set of book synopsis @ span literature @ year and genre and compare @ approach to a keyword-based baseline @ @ nature switzerland ag @ 
1507,An Ensemble Neural Network Model for Benefiting Pregnancy Health Stats from Mining Social Media,"Extensive use of social media for communication has made it a desired resource in human behavior intensive tasks like product popularity, public polls and more recently for public health surveillance tasks such as lifestyle associated diseases and mental health. In this paper, we exploited Twitter data for detecting pregnancy cases and used tweets about pregnancy to study trigger terms associated with maternal physical and mental health. Such systems can enable clinicians to offer a more comprehensive health care in real time. Using a Twitter-based corpus, we have developed an ensemble Long-short Term Memory (LSTM) – Recurrent Neural Networks (RNN) and Convolution Neural Networks (CNN) network representation model to learn legitimate pregnancy cases discussed online. These ensemble representations were learned by a SVM classifier, which can achieve F1-score of 95% in predicting pregnancy accounts discussed in tweets. We also further investigate the words most commonly associated with physical disease symptoms ‘Distress’ and negative emotions ‘Annoyed’ sentiment. Results from our sentiment analysis study are quite encouraging, identifying more accurate triggers for pregnancy sentiment classes. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,extensive use of social medium @ communication ha made @ a desired resource in human behavior intensive task like product popularity public poll and more recently @ public health surveillance task @ a lifestyle associated disease and mental health @ in @ @ @ exploited twitter data @ detecting pregnancy case and used tweet @ pregnancy to study trigger term associated @ maternal physical and mental health @ @ system @ enable clinician to offer a more comprehensive health care in real time @ @ a twitter-based corpus @ @ developed @ ensemble long-short term memory @ lstm @ recurrent neural network @ rnn @ and convolution neural network @ cnn @ network representation model to learn legitimate pregnancy case discussed online @ @ ensemble representation @ learned by a svm classifier @ @ achieve f score of in predicting pregnancy account discussed in tweet @ @ @ @ investigate @ word @ commonly associated @ physical disease symptom distress and negative emotion annoyed sentiment @ @ @ @ sentiment analysis study @ quite encouraging identifying more accurate trigger @ pregnancy sentiment class @ @ nature switzerland ag @ 
1509,A metamodel enabled approach for discovery of coherent topics in short text microblogs,"Comprehending social media discussions in short text microblogs is fundamental for knowledge-based applications like recommender systems. Twitter, for example, provides rich real-time information in keeping with its streaming nature. Making sense of such data without automated support is not feasible due to its vast size and nature. The problem becomes more complex when the data in question have a low variance in terms of topical diversity. Therefore, an automatic method for understanding textual patterns in such topically constrained data needs to be developed. A major challenge to building such a system is in its ability to comprehend the nature of the data with regard to diversity of word structure correlations, vocabulary sparsity, and distinguishing factors in the generated topics. In this paper, we present a novel semi-supervised approach called metamodel enabled latent Dirichlet allocation to address this challenge. Compared to state-of-the-art approaches, our model incorporates a domain-specific metamodel. The metamodel is defined as a set of topic label vectors derived from long texts to guide the learning process in shorter texts. © 2013 IEEE.",2018,IEEE Access,2,comprehending social medium discussion in short text microblogs is fundamental @ knowledge-based application like recommender system @ twitter @ example provides rich real-time information in keeping @ @ streaming nature @ making sense of @ data without automated support is not feasible due to @ vast size and nature @ @ problem becomes more complex @ @ data in question @ a low variance in term of topical diversity @ therefore @ automatic method @ understanding textual pattern in @ topically constrained data need to @ developed @ a major challenge to building @ a system is in @ ability to comprehend @ nature of @ data @ regard to diversity of word structure correlation vocabulary sparsity and distinguishing factor in @ generated topic @ in @ @ @ @ a novel semi-supervised approach called metamodel enabled latent dirichlet allocation to address @ challenge @ compared to state-of-the-art approach @ model incorporates a domain-specific metamodel @ @ metamodel is defined a a set of topic label vector derived @ long text to guide @ learning process in shorter text @ @ @ 
1510,WS4ABSA: An NMF-Based Weakly-Supervised Approach for Aspect-Based Sentiment Analysis with Application to Online Reviews,"The goal of Aspect-Based Sentiment Analysis is to identify opinions regarding specific targets and the corresponding sentiment polarity in a document. The proposed approach is designed for real-world scenarios, where the amount of available information and annotated data is often too limited to train supervised models. We focus on the two core tasks of Aspect-Based Sentiment Analysis: aspect and sentiment polarity classification. The first task – which consists in the identification of the opinion targets in a document – is tackled by means of a weakly-supervised technique based on Non-negative Matrix Factorization. This strategy allows users to easily embed some a priori domain knowledge by means of short seed terms lists. Experimental results on publicly available data sets related to online reviews suggest that the proposed approach is very flexible and can be easily adapted to different languages and domains. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ goal of aspect-based sentiment analysis is to identify opinion regarding specific target and @ corresponding sentiment polarity in a document @ @ proposed approach is designed @ real-world scenario @ @ amount of available information and annotated data is often too limited to train supervised model @ @ focus on @ @ core task of aspect-based sentiment analysis @ aspect and sentiment polarity classification @ @ first task @ consists in @ identification of @ opinion target in a document is tackled by mean of a weakly-supervised technique based on non-negative matrix factorization @ @ strategy allows user to easily embed some a priori domain knowledge by mean of short seed term list @ experimental @ on publicly available data set related to online review suggest @ @ proposed approach is @ flexible and @ @ easily adapted to different language and domain @ @ nature switzerland ag @ 
1511,Detecting Constraints and Their Relations from Regulatory Documents Using NLP Techniques,"Extracting constraints and process models from natural language text is an ongoing challenge. While the focus of current research is merely on the extraction itself, this paper presents a three step approach to group constraints as well as to detect and display relations between constraints in order to ease their implementation. For this, the approach uses NLP techniques to extract sentences containing constraints, group them by, e.g., stakeholders or topics, and detect redundant, subsuming, and conflicting pairs of constraints. These relations are displayed using network maps. The approach is prototypically implemented and evaluated based on regulatory documents from the financial sector as well as expert interviews. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7,extracting constraint and process model @ natural language text is @ ongoing challenge @ @ @ focus of current research is merely on @ extraction @ @ @ @ a three step approach to group constraint a well a to detect and display relation @ constraint in order to ease @ implementation @ @ @ @ approach us nlp technique to extract sentence containing constraint group @ by e @ g @ stakeholder @ topic and detect redundant subsuming and conflicting pair of constraint @ @ relation @ displayed @ network map @ @ approach is prototypically implemented and evaluated based on regulatory document @ @ financial sector a well a expert interview @ @ nature switzerland ag @ 
1513,Identifying participant mentions and resolving their coreferences in legal court judgements,"Legal court judgements have multiple participants (e.g. judge, complainant, petitioner, lawyer, etc.). They may be referred to in multiple ways, e.g., the same person may be referred as lawyer, counsel, learned counsel, advocate, as well as his/her proper name. For any analysis of legal texts, it is important to resolve such multiple mentions which are coreferences of the same participant. In this paper, we propose a supervised approach to this challenging task. To avoid human annotation efforts for Legal domain data, we exploit ACE 2005 dataset by mapping its entities to participants in Legal domain. We use basic Transfer Learning paradigm by training classification models on general purpose text (news in ACE 2005 data) and applying them to Legal domain text. We evaluate our approach on a sample annotated test dataset in Legal domain and demonstrate that it outperforms state-of-the-art baselines. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,legal court judgement @ multiple participant @ e @ g @ judge complainant petitioner lawyer etc @ @ @ @ may @ referred to in multiple way e @ g @ @ @ person may @ referred a lawyer counsel learned counsel advocate a well a @ @ proper name @ @ @ analysis of legal text @ is important to resolve @ multiple mention @ @ coreference of @ @ participant @ in @ @ @ propose a supervised approach to @ challenging task @ to avoid human annotation effort @ legal domain data @ exploit ace dataset by mapping @ entity to participant in legal domain @ @ use basic transfer learning paradigm by training classification model on general purpose text @ news in ace data @ and applying @ to legal domain text @ @ evaluate @ approach on a sample annotated test dataset in legal domain and demonstrate @ @ outperforms state-of-the-art baseline @ @ nature switzerland ag @ 
1514,Evaluation of Named Entity Recognition algorithms using clinical text data,"Named Entity Recognition (NER) is one of the most important research areas in the field of medical. Presently, most of the clinical NER research is based on two approaches as Knowledge Engineering (KE) and Machine Learning (ML). KE is used a word lookup table approach and ML is known as supervised learning approach. The aim of this work is to evaluate a recent algorithm in KE and ML approaches using various clinical text databases. Therefore, the NOBLE Coder and Clinical Named Entity Recognition (CliNER) algorithms are selected, NOBLE Coder is depended on KE approach and CliNER is ML approach. The two algorithms will be described and compared its performance on three openly available datasets that is obtained from Medical Information Mart for Intensive Care II (MIMIC II), Pittsburgh Medical Center, and i2b2 2010 challenge. Among these datasets, the annotated data are included which is used to detect the highest sensitivity and specificity on each algorithm. The randomly distributed patient reports were taken as input data to these algorithms. By executing these algorithms, the information is extracted and which classified into predefined concept types, for example medical problems, treatments and tests. The accuracy of both algorithms is calculated using standard measures. The taken two algorithms are analyzed based on the produced results. Finally, the best among two is suggested for better use in clinical data. © 2018 Authors.",2018,International Journal of Engineering and Technology(UAE),0,named entity recognition @ ner @ is @ of @ @ important research area in @ field of medical @ presently @ of @ clinical ner research is based on @ approach a knowledge engineering @ ke @ and machine learning @ ml @ @ ke is used a word lookup table approach and ml is known a supervised learning approach @ @ aim of @ work is to evaluate a recent algorithm in ke and ml approach @ various clinical text database @ therefore @ noble coder and clinical named entity recognition @ cliner @ algorithm @ selected noble coder is depended on ke approach and cliner is ml approach @ @ @ algorithm @ @ described and compared @ performance on three openly available datasets @ is obtained @ medical information mart @ intensive care ii @ mimic ii @ pittsburgh medical center and i b challenge @ among @ datasets @ annotated data @ included @ is used to detect @ highest sensitivity and specificity on @ algorithm @ @ randomly distributed patient report @ taken a input data to @ algorithm @ by executing @ algorithm @ information is extracted and @ classified @ predefined concept type @ example medical problem treatment and test @ @ accuracy of @ algorithm is calculated @ standard measure @ @ taken @ algorithm @ analyzed based on @ produced @ @ finally @ best among @ is suggested @ better use in clinical data @ author @ 
1515,Debate stance classification using word embeddings,"Online debate sites act as a popular platform for users to express and form opinions. In this paper, we propose a novel unsupervised approach to perform stance classification of two-sided online debate posts. We propose the use of word embeddings to address the problem of identifying the preferred target of each aspect. We also use word embeddings to train a supervised classifier for selecting only target related aspects. The aspect-target preference information is used to model the stance classification task as an integer linear programming problem. The classifier gives an average aspect classification accuracy of 84% on multiple datasets. Our word embedding based stance classification approach gives 19.80% higher user stance classification accuracy (F1-score) compared to the existing methods. Our results suggest that the use of word embeddings improves accuracy and enables us to perform stance classification without the need for external domain-specific information. © Springer Nature Switzerland AG 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,online debate site act a a popular platform @ user to express and form opinion @ in @ @ @ propose a novel unsupervised approach to perform stance classification of two-sided online debate post @ @ propose @ use of word embeddings to address @ problem of identifying @ preferred target of @ aspect @ @ @ use word embeddings to train a supervised classifier @ selecting only target related aspect @ @ aspect-target preference information is used to model @ stance classification task a @ integer linear programming problem @ @ classifier give @ average aspect classification accuracy of on multiple datasets @ @ word embedding based stance classification approach give @ higher user stance classification accuracy @ f score @ compared to @ existing method @ @ @ suggest @ @ use of word embeddings improves accuracy and enables u to perform stance classification without @ need @ external domain-specific information @ @ nature switzerland ag @ 
1516,Comprehensive survey on sentiment analysis based on workflow foundation,"In the recent days, data’s plays a vital role in all the aspects related to computer science fields. As all the areas are emerging drastically, several challenges are merging and exists in the real-time applications. However, here we are discussing about the language processing where decision making are merging as the problem related to sentimental analysis. In this paper, we have given a detailed survey regarding the concept of sentiment analysis and its applications. The various real world applications of sentiment analysis along with the workflow foundation are discussed briefly for the execution. The current procedures followed for the utilization of sentiment analysis are investigated elaborately and explains the execution measurements needed to connect them. Hence, the paper illuminates the analysis of assessment of sentimental and its significance and also explains existing tools needed to execute sentiment analysis procedure. © 2018, Institute of Advanced Scientific Research, Inc. All rights reserved.",2018,Journal of Advanced Research in Dynamical and Control Systems,3,in @ recent day data s play a vital role in @ @ aspect related to computer science field @ a @ @ area @ emerging drastically several challenge @ merging and exists in @ real-time application @ however @ @ @ discussing @ @ language processing @ decision making @ merging a @ problem related to sentimental analysis @ in @ @ @ @ given a detailed survey regarding @ concept of sentiment analysis and @ application @ @ various real world application of sentiment analysis along @ @ workflow foundation @ discussed briefly @ @ execution @ @ current procedure followed @ @ utilization of sentiment analysis @ investigated elaborately and explains @ execution measurement needed to connect @ @ hence @ @ illuminates @ analysis of assessment of sentimental and @ significance and @ explains existing tool needed to execute sentiment analysis procedure @ institute of advanced scientific research inc @ @ right reserved @ 
1517,Product Sentiment Trend Prediction,"The prospects of spectrum sentiment analysis are great and is a field that has been given very little research focus. We develop a system that can recognize human recognizable emotions and quantify them, the system can then predict the trend in the spectrum sentiments provided a chronological data. This paper discusses a lexicon-based approach for spectrum sentiment analysis. It further describes a quantification method to factor in the effects of time in trend prediction and a novel idea of using consecutive calculated values for current trend value calculation. The system is designed for e-commerce data but has flexibility to be used for other fields too. The system uses a simple neural network with image and text features as input and the trend values as output. This system can then be used to predict sentiment trend for newer or existing products. The system shows great prospects for multi-modal sentiment analysis of sentiments on spectrum range and can be advanced by using more complex approach. © 2018, Springer International Publishing AG, part of Springer Nature.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ prospect of spectrum sentiment analysis @ great and is a field @ ha @ given @ little research focus @ @ develop a system @ @ recognize human recognizable emotion and quantify @ @ system @ @ predict @ trend in @ spectrum sentiment provided a chronological data @ @ @ discus a lexicon-based approach @ spectrum sentiment analysis @ @ @ describes a quantification method to factor in @ effect of time in trend prediction and a novel idea of @ consecutive calculated value @ current trend value calculation @ @ system is designed @ e-commerce data @ ha flexibility to @ used @ @ field too @ @ system us a simple neural network @ image and text feature a input and @ trend value a output @ @ system @ @ @ used to predict sentiment trend @ newer @ existing product @ @ system @ great prospect @ multi-modal sentiment analysis of sentiment on spectrum range and @ @ advanced by @ more complex approach @ @ international publishing ag part of @ nature @ 
1518,Use of linguistic forms mining in the link analysis of legal documents,"This document employs a statistical approach in exploring language and extracting linguistic forms there contained, so as to identify the linguistic forms which are most frequently used in legal documents. Thus retrieved data, as the second part of this paper shows, can be used to research information, analyze references and links, trace pathways between correlating legal documents and establish the relevance of legal documents on the grounds of their mutual correlation. The retrieved data can further be utilized in various other manners. The methodology of this research and thus attained information form a good basis and act as input data for numerous further analyses. © 2018, ComSIS Consortium. All rights reserved.",2018,Computer Science and Information Systems,1,@ document employ a statistical approach in exploring language and extracting linguistic form @ contained @ a to identify @ linguistic form @ @ @ frequently used in legal document @ thus retrieved data a @ second part of @ @ @ @ @ used to research information analyze reference and link trace pathway @ correlating legal document and establish @ relevance of legal document on @ ground of @ mutual correlation @ @ retrieved data @ @ @ utilized in various @ manner @ @ methodology of @ research and thus attained information form a good basis and act a input data @ numerous @ analysis @ comsis consortium @ @ right reserved @ 
1519,Application marketplace malware detection by user feedback analysis,"Smartphones are becoming increasingly ubiquitous. Like recommended best practices for personal computers, users are encouraged to install antivirus and intrusion detection software on their mobile devices. However, even with such software these devises are far from being fully protected. Given that application stores are the source of most applications, malware detection on these platforms is an important issue. Based on our intuition, which suggests that an application’s suspicious behavior will be noticed by some users and influence their feedback, we present an approach for analyzing user reviews in mobile application stores for the purpose of detecting malicious apps. The proposed method transfers an application’s text reviews to numerical features in two main steps: (1) extract domain-phrases based on external domain-specific textual corpus on computer and network security, and (2) compute three statistical features based on domain-phrases occurrences. We evaluated the proposed methods on 2,506 applications along with their 128,863 reviews collected from “Amazon AppStore”. The results show that proposed method yields an AUC of 86% in the detection of malicious applications. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Communications in Computer and Information Science,0,smartphones @ becoming increasingly ubiquitous @ like recommended best practice @ personal computer user @ encouraged to install antivirus and intrusion detection software on @ mobile device @ however even @ @ software @ devise @ far @ @ fully protected @ given @ application store @ @ source of @ application malware detection on @ platform is @ important issue @ based on @ intuition @ suggests @ @ application s suspicious behavior @ @ noticed by some user and influence @ feedback @ @ @ approach @ analyzing user review in mobile application store @ @ purpose of detecting malicious apps @ @ proposed method transfer @ application s text review to numerical feature in @ main step @ @ @ extract domain-phrases based on external domain-specific textual corpus on computer and network security and @ @ compute three statistical feature based on domain-phrases occurrence @ @ evaluated @ proposed method on application along @ @ review collected @ amazon appstore @ @ @ @ @ proposed method yield @ auc of in @ detection of malicious application @ @ international publishing ag part of @ nature @ 
1520,The predictive power of the sentiment of financial reports,"The present study examines the predictive power of the tone or sentiment of 10-K annual and 10-Q quarterly financial statements for future corporate development. The sentiment indicator was calculated using word lists developed for financial texts by Loughran and McDonalds [23] and Henry [14] and applying a conventional and a tf-idf weighted word count. The results show that the sentiment indicator is of significant incremental prognostic quality both for the next quarter and the quarter following it. Unlike suggested by previous literature, neither the scope and content of the word lists nor the weighting method applied had a significant influence on forecasting quality. © 2011 Springer-Verlag Berlin Heidelberg.",2018,CEUR Workshop Proceedings,0,@ @ study examines @ predictive power of @ tone @ sentiment of k annual and q quarterly financial statement @ future corporate development @ @ sentiment indicator wa calculated @ word list developed @ financial text by loughran and mcdonalds and henry and applying a conventional and a tf-idf weighted word count @ @ @ @ @ @ sentiment indicator is of significant incremental prognostic quality @ @ @ next quarter and @ quarter following @ @ unlike suggested by previous literature neither @ scope and content of @ word list @ @ weighting method applied @ a significant influence on forecasting quality @ springer-verlag @ @ @ 
1523,An empirical analysis of articles on sentiment analysis,"Expression of a thought is not only important for an individual but there is a necessity for an automated system to get an opinion from it. Sentiment analysis (SA) or opinion mining (OM) is used to identify the sentiment/opinion of the speaker. Web 2.0 provides us various platforms such as Twitter, Facebook where we comment or post to express our happiness, anger, disbelief, sadness, etc. For SA of text, computationally it is required to know the concepts and technologies being used in the field of SA. This article gives brief knowledge about the techniques used in SA by categorizing various articles over the past four years. This article also explains the preprocessing steps, various application programmable interface (API), and available datasets for a better understanding of SA. This article is concluded with a future work which needs a separate attention of researchers to improve the performance of sentiment analysis. © Springer Nature Singapore Pte Ltd. 2018.",2018,Advances in Intelligent Systems and Computing,0,expression of a thought is not only important @ @ individual @ @ is a necessity @ @ automated system to get @ opinion @ @ @ sentiment analysis @ sa @ @ opinion mining @ om @ is used to identify @ sentiment opinion of @ speaker @ web @ provides u various platform @ a twitter facebook @ @ comment @ post to express @ happiness anger disbelief sadness etc @ @ sa of text computationally @ is required to know @ concept and technology @ used in @ field of sa @ @ article give brief knowledge @ @ technique used in sa by categorizing various article @ @ past four year @ @ article @ explains @ preprocessing step various application programmable interface @ api @ and available datasets @ a better understanding of sa @ @ article is concluded @ a future work @ need a separate attention of researcher to improve @ performance of sentiment analysis @ @ nature singapore pte ltd @ @ 
1524,YAKE! collection-independent automatic keyword extractor,"In this paper, we present YAKE!, a novel feature-based system for multi-lingual keyword extraction from single documents, which supports texts of different sizes, domains or languages. Unlike most systems, YAKE! does not rely on dictionaries or thesauri, neither it is trained against any corpora. Instead, we follow an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in many different languages without the need for external knowledge. This can be beneficial for a large number of tasks and a plethora of situations where the access to training corpora is either limited or restricted. In this demo, we offer an easy to use, interactive session, where users from both academia and industry can try our system, either by using a sample document or by introducing their own text. As an add-on, we compare our extracted keywords against the output produced by the IBM Natural Language Understanding (IBM NLU) and Rake system. YAKE! demo is available at http://bit.ly/YakeDemoECIR2018. A python implementation of YAKE! is also available at PyPi repository (https://pypi.python.org/pypi/yake/). © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),19,in @ @ @ @ yake @ a novel feature-based system @ multi-lingual keyword extraction @ single document @ support text of different size domain @ language @ unlike @ system yake @ doe not rely on dictionary @ thesaurus neither @ is trained @ @ corpus @ instead @ follow @ unsupervised approach @ build upon feature extracted @ @ text making @ thus applicable to document written in many different language without @ need @ external knowledge @ @ @ @ beneficial @ a @ number of task and a plethora of situation @ @ access to training corpus is either limited @ restricted @ in @ demo @ offer @ easy to use interactive session @ user @ @ academia and industry @ try @ system either by @ a sample document @ by introducing @ @ text @ a @ add-on @ compare @ extracted keywords @ @ output produced by @ ibm natural language understanding @ ibm nlu @ and rake system @ yake @ demo is available at http @ bit @ ly yakedemoecir @ a python implementation of yake @ is @ available at pypi repository @ http @ pypi @ python @ org pypi yake @ @ @ international publishing ag part of @ nature @ 
1525,English-Cebuano parallel language resource for statistical machine translation system,This paper describes the building of an English-Cebuano parallel corpus to be fed into a Statistical Machine Translation System. This parallel corpus is built as a combination of human translation and automatic translation. The automatic translation involves the use of web crawlers that automatically identify bilingual websites that contain English and Cebuano web pages in them. The extracted pages underwent language filtering to exclude those that are not written in English or Cebuano. The extracted and identified pages underwent preprocessing to clean and structure the texts. Then a sentence-level text alignment algorithm was employed to align sentences from an English text to a Cebuano text. The result of the sentence-level alignment and the output of the human translators comprised the parallel English-Cebuano corpus that will eventually be fed into a Statistical Machine Translation System. © Springer International Publishing AG 2018.,2018,Advances in Intelligent Systems and Computing,0,@ @ describes @ building of @ english-cebuano parallel corpus to @ fed @ a statistical machine translation system @ @ parallel corpus is built a a combination of human translation and automatic translation @ @ automatic translation involves @ use of web crawler @ automatically identify bilingual website @ contain english and cebuano web page in @ @ @ extracted page underwent language filtering to exclude @ @ @ not written in english @ cebuano @ @ extracted and identified page underwent preprocessing to clean and structure @ text @ @ a sentence-level text alignment algorithm wa employed to align sentence @ @ english text to a cebuano text @ @ @ of @ sentence-level alignment and @ output of @ human translator comprised @ parallel english-cebuano corpus @ @ eventually @ fed @ a statistical machine translation system @ @ international publishing ag @ 
1526,A simultaneous topic and sentiment classification of tweets,"Social networks have been an emerging technology for communication among billions of users. One of the most popular social networks is Twitter. The popularity of Twitter comes from its simplicity since it allows users to exchange messages of short length that does not exceed 140 characters and takes the form of tweets. In this paper, we propose a model for performing a classification of tweets posted by the Twitter user based on a mixture of the topic and sentiment of those tweets. The proposed approach is new in that it creates a model that combines the processes of topic and sentiment classification of tweets simultaneously. Therefore, with this model, one can categorize tweets according to their topics and simultaneously assign them into different sentiments categories. The topic of the tweets in the basic experiment of the proposed approach is classified into five main different categories including: “political”, “commercials”, “educational”, “religious”, and “sportive”. Meanwhile, the sentiment of those tweets is classified into three main different categories including “positive”, “negative”, “neutral”. The effectiveness of the proposed approach is demonstrated on a real dataset that consists of various extracted tweets with different categories of topics and opinions. The empirical results show that our approach is very powerful in categorizing tweets according to topics and simultaneously assigning them into different sentiments categories. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Advances in Intelligent Systems and Computing,0,social network @ @ @ emerging technology @ communication among billion of user @ @ of @ @ popular social network is twitter @ @ popularity of twitter come @ @ simplicity since @ allows user to exchange message of short length @ doe not exceed character and take @ form of tweet @ in @ @ @ propose a model @ performing a classification of tweet posted by @ twitter user based on a mixture of @ topic and sentiment of @ tweet @ @ proposed approach is @ in @ @ creates a model @ combine @ process of topic and sentiment classification of tweet simultaneously @ therefore @ @ model @ @ categorize tweet according to @ topic and simultaneously assign @ @ different sentiment category @ @ topic of @ tweet in @ basic experiment of @ proposed approach is classified @ five main different category including @ political commercial educational religious and sportive @ meanwhile @ sentiment of @ tweet is classified @ three main different category including positive negative neutral @ @ effectiveness of @ proposed approach is demonstrated on a real dataset @ consists of various extracted tweet @ different category of topic and opinion @ @ empirical @ @ @ @ approach is @ powerful in categorizing tweet according to topic and simultaneously assigning @ @ different sentiment category @ @ international publishing ag part of @ nature @ 
1527,Mining Twitter data for crime trend prediction,"While conventional crime prediction methods rely on historical crime records and geographical information of the location of interest, we pursue the question of whether a social media context can provide socio-behavior ""signals"" for a crime prediction problem. The hypothesis is that crowd publicly available data in Twitter may include predictive variables which can indicate changes in crime rates without being only limited to the availability of historical crime records of specific locations. We developed a prediction model for crime trend prediction, where the objective is to employ Twitter content to predict crime rate directions in a prospective time-frame. The model employs content, sentiment, and topics, as the predictive indicators to infer the changes of crime indexes. Since our problem has a sequential order, we propose a temporal topic detection model to infer predictive topics over time. The main challenge of topic detection over time is information evolution, in which data are more related when they are close in time rather than further apart. Our proposed topic detection model builds a dynamic vocabulary to detect emerging topics rather than considering a vocabulary in bulk. We applied our model on data collected from Chicago for crime trend prediction using historical tweets. The results have revealed the correlation between features extracted from the content as content-based features and the crime trends. Moreover, the results indicate the feasibility of our proposed temporal topic detection model in identifying the most predictive features over time compared to a static model without time consideration. We also studied the contribution of socio-economic indexes and temporal features as auxiliary features. The experiment shows the content-based features improve the prediction performance significantly compared to the auxiliary features. Overall, the study provides a deep insight into the correlation between language and crime trends and the impact of social data as an extra resource in providing predictive indicators. © 2018 - IOS Press and the authors. All rights reserved.",2018,Intelligent Data Analysis,7,@ conventional crime prediction method rely on historical crime record and geographical information of @ location of interest @ pursue @ question of whether a social medium context @ provide socio-behavior @ signal @ @ a crime prediction problem @ @ hypothesis is @ crowd publicly available data in twitter may include predictive variable @ @ indicate change in crime rate without @ only limited to @ availability of historical crime record of specific location @ @ developed a prediction model @ crime trend prediction @ @ objective is to employ twitter content to predict crime rate direction in a prospective time-frame @ @ model employ content sentiment and topic a @ predictive indicator to infer @ change of crime index @ since @ problem ha a sequential order @ propose a temporal topic detection model to infer predictive topic @ time @ @ main challenge of topic detection @ time is information evolution in @ data @ more related @ @ @ close in time rather @ @ apart @ @ proposed topic detection model build a dynamic vocabulary to detect emerging topic rather @ considering a vocabulary in bulk @ @ applied @ model on data collected @ chicago @ crime trend prediction @ historical tweet @ @ @ @ revealed @ correlation @ feature extracted @ @ content a content-based feature and @ crime trend @ moreover @ @ indicate @ feasibility of @ proposed temporal topic detection model in identifying @ @ predictive feature @ time compared to a static model without time consideration @ @ @ studied @ contribution of socio-economic index and temporal feature a auxiliary feature @ @ experiment @ @ content-based feature improve @ prediction performance significantly compared to @ auxiliary feature @ overall @ study provides a deep insight @ @ correlation @ language and crime trend and @ impact of social data a @ extra resource in providing predictive indicator @ io @ and @ author @ @ right reserved @ 
1528,Social Media Mining for Assessing Brand Popularity,"Businesses seek to analyse their customer feedback to compare their brand’s popularity with the popularity of competing brands. The increasing use of social media in recent years is producing large amounts of textual content, which has become rich source of data for brand popularity analysis. In this article, a novel hybrid approach of classification and lexicon based methods is proposed to assess brand popularity based on the sentiments expressed in social media posts. Two different classification models using Naïve Bayes (NB) and SVM are built based on Twitter messages for 9 different brands of 3 cosmetic products. In addition, sentiment quantification have been performed using a lexicon-based approach. Based on the overall comparison of the proposed models, the SVM classifier has the highest performance with 78.85% accuracy and 94.60% AUC, compared to 73.57% and 63.63% accuracy, 80.63% and 69.38% AUC of the NB classifier and the sentiment quantification approach respectively. Specific indices based on classification and lexicon approaches are proposed to assess the brand popularity. Copyright © 2018, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",2018,International Journal of Data Warehousing and Mining,3,@ seek to analyse @ customer feedback to compare @ brand s popularity @ @ popularity of competing brand @ @ increasing use of social medium in recent year is producing @ amount of textual content @ ha become rich source of data @ brand popularity analysis @ in @ article a novel hybrid approach of classification and lexicon based method is proposed to ass brand popularity based on @ sentiment expressed in social medium post @ @ different classification model @ naïve bayes @ nb @ and svm @ built based on twitter message @ different brand of cosmetic product @ in addition sentiment quantification @ @ performed @ a lexicon-based approach @ based on @ overall comparison of @ proposed model @ svm classifier ha @ highest performance @ @ accuracy and @ auc compared to @ and @ accuracy @ and @ auc of @ nb classifier and @ sentiment quantification approach respectively @ specific index based on classification and lexicon approach @ proposed to ass @ brand popularity @ @ igi global @ copying @ distributing in print @ electronic form without written permission of igi global is prohibited @ 
1531,Predicting Contextual Informativeness for Vocabulary Learning,"Vocabulary knowledge is essential to educational progress. High quality vocabulary instruction requires supportive contextual examples to teach word meaning and proper usage. Identifying such contexts by hand for a large number of words can be difficult. In this work, we take a statistical learning approach to engineer a system that predicts informativeness of a context for target words that span the range of difficulty from middle school to college level. Our database (released open source) includes 1,000 hand-selected words associated with approximately 70,000 contextual examples gathered from the Internet. Our training data included each context rated by 10 individuals on a four-point informativeness scale. We process the text of each context into a novel collection of approximately 600 numerical features that captures diverse linguistic information. We then fit a nonparametric regression model using Random Forests and compute out-of-sample prediction performance using cross-validation. Our system performs well enough that it can replace a human judge: for a target word not found in our dataset, we can provide curated contexts to a student learner such that most of the contexts (54 percent) feature rich contextual clues and confusing contexts are rare (< 1 percent). The quality of our curated contexts was validated by an independent panel of high school language arts teachers. © 2018 IEEE.",2018,IEEE Transactions on Learning Technologies,2,vocabulary knowledge is essential to educational progress @ high quality vocabulary instruction requires supportive contextual example to teach word meaning and proper usage @ identifying @ context by hand @ a @ number of word @ @ difficult @ in @ work @ take a statistical learning approach to engineer a system @ predicts informativeness of a context @ target word @ span @ range of difficulty @ middle school to college level @ @ database @ released open source @ includes hand-selected word associated @ approximately contextual example gathered @ @ internet @ @ training data included @ context rated by individual on a four-point informativeness scale @ @ process @ text of @ context @ a novel collection of approximately numerical feature @ capture diverse linguistic information @ @ @ fit a nonparametric regression model @ random forest and compute out-of-sample prediction performance @ cross-validation @ @ system performs well enough @ @ @ replace a human judge @ @ a target word not found in @ dataset @ @ provide curated context to a student learner @ @ @ of @ context @ percent @ feature rich contextual clue and confusing context @ rare @ percent @ @ @ quality of @ curated context wa validated by @ independent panel of high school language art teacher @ @ @ 
1532,Dynamic semantic network analysis of unstructured text corpora,The natural language structure can be viewed as weighted semantic network. Such representation gives an option to investigate the text corpus as the model of the subject domain. In this paper we propose the mechanism of the semantic network identification and construction. We apply the methodological instrument for the social media text analysis and trace the dynamics of the discussions about 1917 year within the internet communities. Network changes illustrate the changes of the interest to different topics. The proposed mechanism can be used for the monitoring of the different social processes and phenomenal in online social networks and media. © Springer International Publishing AG 2018.,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ natural language structure @ @ viewed a weighted semantic network @ @ representation give @ option to investigate @ text corpus a @ model of @ subject domain @ in @ @ @ propose @ mechanism of @ semantic network identification and construction @ @ apply @ methodological instrument @ @ social medium text analysis and trace @ dynamic of @ discussion @ year within @ internet community @ network change illustrate @ change of @ interest to different topic @ @ proposed mechanism @ @ used @ @ monitoring of @ different social process and phenomenal in online social network and medium @ @ international publishing ag @ 
1533,Searching through scientific PDF files supported by bi-clustering of key terms matrices,"We describe an original approach for exploring corpora of pdf format scientific texts in the area of bio-medical research, created over a wide topic of interest, e.g., cancer, thyroid cancer, biological process etc. Our methodology is based on indexing large lists of appropriate key-terms and additionally performing bi-clustering of term occurrence matrices. In our approach the position of phrase inside text (abstract or text) is not considered, but we include statistics based on occurrences frequency. We treat documents as a bags of words and the results are processed toward unique list of values. Bi-clustering is used to achieve separating character of lists of key-terms, characterizing sub-types of the studied category, e.g., different cancers or different sub-classes of a given cancer. We prove usefulness of the algorithm by searching for lists of genes characteristic for cancer types. © 2018, Springer International Publishing AG.",2018,Advances in Intelligent Systems and Computing,0,@ describe @ original approach @ exploring corpus of pdf format scientific text in @ area of bio-medical research created @ a wide topic of interest e @ g @ cancer thyroid cancer biological process etc @ @ methodology is based on indexing @ list of appropriate key-terms and additionally performing bi-clustering of term occurrence matrix @ in @ approach @ position of phrase inside text @ abstract @ text @ is not considered @ @ include statistic based on occurrence frequency @ @ treat document a a bag of word and @ @ @ processed toward unique list of value @ bi-clustering is used to achieve separating character of list of key-terms characterizing sub-types of @ studied category e @ g @ different cancer @ different sub-classes of a given cancer @ @ prove usefulness of @ algorithm by searching @ list of gene characteristic @ cancer type @ @ international publishing ag @ 
1534,Semantic graph based automatic text summarization for hindi documents using particle swarm optimization,"Automatic text summarization can be defined as a process of extracting and describing important information from given document using computer algorithms. A number of techniques have been proposed by researchers in the past for summarization of English text. Automatic summarization of Indian text has received a very little attention so far. In this paper, we propose an approach for summarizing Hindi text based on semantic graph of the document using Particle Swarm Optimization (PSO) algorithm. PSO is one of the most powerful bio-inspired algorithms used to obtain optimal solution. The subject-object-verb (SOV) triples are extracted from the document. These triples are used to construct semantic graph of the document. A classifier is trained using PSO algorithm which is then used to generate semantic sub-graph and to obtain document summary. © Springer International Publishing AG 2018.",2018,"Smart Innovation, Systems and Technologies",1,automatic text summarization @ @ defined a a process of extracting and describing important information @ given document @ computer algorithm @ a number of technique @ @ proposed by researcher in @ past @ summarization of english text @ automatic summarization of indian text ha received a @ little attention @ far @ in @ @ @ propose @ approach @ summarizing hindi text based on semantic graph of @ document @ particle swarm optimization @ pso @ algorithm @ pso is @ of @ @ powerful bio-inspired algorithm used to obtain optimal solution @ @ subject-object-verb @ sov @ triple @ extracted @ @ document @ @ triple @ used to construct semantic graph of @ document @ a classifier is trained @ pso algorithm @ is @ used to generate semantic sub-graph and to obtain document summary @ @ international publishing ag @ 
1535,Sentiment analysis method for tracking touristics reviews in social media network,"The touristic sector in Tunisia has declined after the “Arabic Spring”. Therefore, the number of comments published by tourists to give their opinions about it has increased. Consequently, this resulted in a high volume of data in the different social networks such as Facebook and Twitter. In this case, the opinion mining plays an important role to more understanding and then ameliorating the situation of tourism in Tunisia. In this paper, the main goal is to select the tourists’ viewpoints in Twitter after the revolution. For this reason, we create a sentiment lexicon based on the emoticons and interjections as well as acronyms. We also use a sentiWordnet to build lexical scales for sentiment analysis of different tourist reviews with reference to a travel agency page on Facebook. Then, we propose a method relying on Support Vector Machine (SVM), Maximum entropy and Naive Bayes. Our approach is efficient as it gives encouraging results. © Springer International Publishing AG 2018.",2018,"Smart Innovation, Systems and Technologies",5,@ touristic sector in tunisia ha declined @ @ arabic spring @ therefore @ number of comment published by tourist to give @ opinion @ @ ha increased @ consequently @ resulted in a high volume of data in @ different social network @ a facebook and twitter @ in @ case @ opinion mining play @ important role to more understanding and @ ameliorating @ situation of tourism in tunisia @ in @ @ @ main goal is to select @ tourist viewpoint in twitter @ @ revolution @ @ @ reason @ create a sentiment lexicon based on @ emoticon and interjection a well a acronym @ @ @ use a sentiwordnet to build lexical scale @ sentiment analysis of different tourist review @ reference to a travel agency page on facebook @ @ @ propose a method relying on support vector machine @ svm @ maximum entropy and naive bayes @ @ approach is efficient a @ give encouraging @ @ @ international publishing ag @ 
1536,The Eurolect Observatory Multilingual Corpus: Construction and query tools,"This chapter aims to explain the corpus design of the Eurolect Observatory Multilingual Corpus and the steps required to build all the different monolingual corpora the project needed to accomplish its research objectives. The first two paragraphs after the general introduction will point out the differences and the overlaps that characterize all the corpora that the author of this paper was in charge of producing as a member of the UNINT research team and that were used in the Eurolect Observatory Project for text mining. After accurately defining the data collection and corpus building strategies adopted, this paper will describe the corpus search tool that was developed in order to help scholars look for and save samples of text from the whole corpus in a convenient and easy way. © 2018 John Benjamins Publishing Company.",2018,Studies in Corpus Linguistics,4,@ chapter aim to explain @ corpus design of @ eurolect observatory multilingual corpus and @ step required to build @ @ different monolingual corpus @ project needed to accomplish @ research objective @ @ first @ paragraph @ @ general introduction @ point @ @ difference and @ overlap @ characterize @ @ corpus @ @ author of @ @ wa in charge of producing a a member of @ unint research team and @ @ used in @ eurolect observatory project @ text mining @ @ accurately defining @ data collection and corpus building strategy adopted @ @ @ describe @ corpus search tool @ wa developed in order to help scholar look @ and save sample of text @ @ whole corpus in a convenient and easy way @ john benjamin publishing company @ 
1538,Active learning with adaptive density weighted sampling for information extraction from scientific papers,"The paper addresses the task of information extraction from scientific literature with machine learning methods. In particular, the tasks of definition and result extraction from scientific publications in Russian are considered. We note that annotation of scientific texts for creation of training dataset is very labor insensitive and expensive process. To tackle this problem, we propose methods and tools based on active learning. We describe and evaluate a novel adaptive density-weighted sampling (ADWeS) meta-strategy for active learning. The experiments demonstrate that active learning can be a very efficient technique for scientific text mining, and the proposed meta-strategy can be beneficial for corpus annotation with strongly skewed class distribution. We also investigate informative task-independent features for information extraction from scientific texts and present an openly available tool for corpus annotation, which is equipped with ADWeS and compatible with well-known sampling strategies. © Springer International Publishing AG 2018.",2018,Communications in Computer and Information Science,1,@ @ address @ task of information extraction @ scientific literature @ machine learning method @ in particular @ task of definition and @ extraction @ scientific publication in russian @ considered @ @ note @ annotation of scientific text @ creation of training dataset is @ labor insensitive and expensive process @ to tackle @ problem @ propose method and tool based on active learning @ @ describe and evaluate a novel adaptive density-weighted sampling @ adwes @ meta-strategy @ active learning @ @ experiment demonstrate @ active learning @ @ a @ efficient technique @ scientific text mining and @ proposed meta-strategy @ @ beneficial @ corpus annotation @ strongly skewed class distribution @ @ @ investigate informative task-independent feature @ information extraction @ scientific text and @ @ openly available tool @ corpus annotation @ is equipped @ adwes and compatible @ well-known sampling strategy @ @ international publishing ag @ 
1540,A framework for generating rankings to E-commerce products based on reviews using NLP,"There has been a tremendous change in the way people buy and sell products/goods/services in the past decade, the modern way of shopping is done through E-commerce (Online Shopping) Amazon, Alibaba, flipkart are the current giants in E-Commerce. There has been a huge inclination towards online shopping due to various reasons such as wide-range, availability, various choices, easy to compare, can be brought/sold from anywhere, Unlike offline stores the consumer cannot verify the product before buying, the alternative way for this is relying on the reviews and ratings of the products. Which might be generated by various customers or spammers, as the range of the users cannot be confined there not accurate rating or review that can truly help the consumers. In this, we aim to overcome the trouble of avoiding such inaccurate data or insufficient data in terms of decision making, we aim to provide the users or customers a reliable ranking that help them to analyse the product and evaluate the product based on the ranking generated, We intend to create an algorithm that takes the reviews and generates an overall rank that should be helpful in decision making of the consumer. We plan to achieve this by using NLP (Natural Language Processing) techniques such as sentimental analysis and various NLP kits such as CNTK, KERAS Wrapper etc. for text mining for mining the review texts collected. © IAEME Publication.",2018,International Journal of Mechanical Engineering and Technology,1,@ ha @ a tremendous change in @ way people buy and sell product good service in @ past decade @ modern way of shopping is done @ e-commerce @ online shopping @ amazon alibaba flipkart @ @ current giant in e-commerce @ @ ha @ a huge inclination towards online shopping due to various reason @ a wide-range availability various choice easy to compare @ @ brought sold @ anywhere unlike offline store @ consumer cannot verify @ product @ buying @ alternative way @ @ is relying on @ review and rating of @ product @ @ might @ generated by various customer @ spammer a @ range of @ user cannot @ confined @ not accurate rating @ review @ @ truly help @ consumer @ in @ @ aim to overcome @ trouble of avoiding @ inaccurate data @ insufficient data in term of decision making @ aim to provide @ user @ customer a reliable ranking @ help @ to analyse @ product and evaluate @ product based on @ ranking generated @ intend to create @ algorithm @ take @ review and generates @ overall rank @ @ @ helpful in decision making of @ consumer @ @ plan to achieve @ by @ nlp @ natural language processing @ technique @ a sentimental analysis and various nlp kit @ a cntk kera wrapper etc @ @ text mining @ mining @ review text collected @ iaeme publication @ 
1543,Named entity recognition in text documents using a modified conditional random field,"The Named Entity Recognition in documents is an active and challenging research topic in text mining. The major objective of our work is to extract a phrase from the sentence and classify this phrase to one of the predefined named entities. The proposed system works in two layers, in the first phase each and every word in the phrase is tagged using word feature extraction approaches. In the second phase the model recognizes named entities in the phrase level using Modified Conditional Random Field. This work identifies four classes of entities such as Person, Organization, Location and Other. Our algorithm first parses the text document and identifies the sentence structure. From this sentence structure concepts are extracted. In this work the feature extraction module make use of the yahoo Geoplanet Web service for identifying the location. We have created person ontology of all available Indian names to check whether a word is name or not. Inorder to check whether the word is organization or not we have used a database with company name indicators. Finally, our MCRF assign a label to the tagged phrase. © Springer Nature Singapore Pte Ltd. 2018",2018,Advances in Intelligent Systems and Computing,1,@ named entity recognition in document is @ active and challenging research topic in text mining @ @ major objective of @ work is to extract a phrase @ @ sentence and classify @ phrase to @ of @ predefined named entity @ @ proposed system work in @ layer in @ first phase @ and every word in @ phrase is tagged @ word feature extraction approach @ in @ second phase @ model recognizes named entity in @ phrase level @ modified conditional random field @ @ work identifies four class of entity @ a person organization location and @ @ @ algorithm first par @ text document and identifies @ sentence structure @ @ @ sentence structure concept @ extracted @ in @ work @ feature extraction module make use of @ yahoo geoplanet web service @ identifying @ location @ @ @ created person ontology of @ available indian name to check whether a word is name @ not @ inorder to check whether @ word is organization @ not @ @ used a database @ company name indicator @ finally @ mcrf assign a label to @ tagged phrase @ @ nature singapore pte ltd @ 
1544,Learning BLSTM-CRF with Multi-channel Attribute Embedding for Medical Information Extraction,"In Recent years, medical text mining has been an active research field because of its significant application potential, and information extraction (IE) is an essential step in it. This paper focuses on the medical IE, whose aim is to extract the pivotal contents from the medical texts such as drugs, treatments and so on. In existing works, introducing side information into neural network based Conditional Random Fields (CRFs) models have been verified to be effective and widely used in IE. However, they always neglect the traditional attributes of data, which are important for the IE performance, such as lexical and morphological information. Therefore, starting from the raw data, a novel attribute embedding based MC-BLSTM-CRF model is proposed in this paper. We first exploit a bidirectional LSTM (BLSTM) layer to capture the context semantic information. Meanwhile, a multi-channel convolutional neural network (MC-CNN) layer is constructed to learn the relations between multiple attributes automatically and flexibly. And on top of these two layers, we introduce a CRF layer to predict the output labels. We evaluate our model on a Chinese medical dataset and obtain the state-of-the-art performance with 80.71% F1 score. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,in recent year medical text mining ha @ @ active research field @ of @ significant application potential and information extraction @ ie @ is @ essential step in @ @ @ @ focus on @ medical ie whose aim is to extract @ pivotal content @ @ medical text @ a drug treatment and @ on @ in existing work introducing side information @ neural network based conditional random field @ crfs @ model @ @ verified to @ effective and widely used in ie @ however @ always neglect @ traditional attribute of data @ @ important @ @ ie performance @ a lexical and morphological information @ therefore starting @ @ raw data a novel attribute embedding based mc-blstm-crf model is proposed in @ @ @ @ first exploit a bidirectional lstm @ blstm @ layer to capture @ context semantic information @ meanwhile a multi-channel convolutional neural network @ mc-cnn @ layer is constructed to learn @ relation @ multiple attribute automatically and flexibly @ and on top of @ @ layer @ introduce a crf layer to predict @ output label @ @ evaluate @ model on a chinese medical dataset and obtain @ state-of-the-art performance @ @ f score @ @ nature switzerland ag @ 
1545,Automating the extraction of essential genes from literature,"The construction of repositories with curated information about gene essentiality for organisms of interest in Biotechnology is a very relevant task, mainly in the design of cell factories for the enhanced production of added-value products. However, it requires retrieval and extraction of relevant information from literature, leading to high costs regarding manual curation. Text mining tools implementing methods addressing tasks as information retrieval, named entity recognition and event extraction have been developed to automate and reduce the time required to obtain relevant information from literature in many biomedical fields. However, current tools are not designed or optimized for the purpose of identifying mentions to essential genes in scientific texts. In this work, we propose a pipeline to automatically extract mentions to genes and to classify them accordingly to their essentiality for a specific organism. This pipeline implements a machine learning approach that is trained using a manually curated set of documents related with gene essentiality in yeast. This corpus is provided as a resource for the community, as a benchmark for the development of new methods. Our pipeline was evaluated performing resampling and cross validation over this curated dataset, presenting an accuracy of over 80%, and an f1-score over 75%. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ construction of repository @ curated information @ gene essentiality @ organism of interest in biotechnology is a @ relevant task mainly in @ design of cell factory @ @ enhanced production of added-value product @ however @ requires retrieval and extraction of relevant information @ literature leading to high cost regarding manual curation @ text mining tool implementing method addressing task a information retrieval named entity recognition and event extraction @ @ developed to automate and reduce @ time required to obtain relevant information @ literature in many biomedical field @ however current tool @ not designed @ optimized @ @ purpose of identifying mention to essential gene in scientific text @ in @ work @ propose a pipeline to automatically extract mention to gene and to classify @ accordingly to @ essentiality @ a specific organism @ @ pipeline implement a machine learning approach @ is trained @ a manually curated set of document related @ gene essentiality in yeast @ @ corpus is provided a a resource @ @ community a a benchmark @ @ development of @ method @ @ pipeline wa evaluated performing resampling and cross validation @ @ curated dataset presenting @ accuracy of @ and @ f score @ @ @ international publishing ag part of @ nature @ 
1547,Taking a dive: Experiments in deep learning for automatic ontology-based annotation of scientific literature,"Text mining approaches for automated ontology-based curation of biological and biomedical literature have largely focused on syntactic and lexical analysis along with machine learning. Recent advances in deep learning have shown increased accuracy for textual data annotation. However, the application of deep learning for ontology-based curation is a relatively new area and prior work has focused on a limited set of models. Here, we introduce a new deep learning model/architecture based on combining multiple Gated Recurrent Units (GRU) with a character+word based input. We use data from five ontologies in the CRAFT corpus as a Gold Standard to evaluate our model's performance. We also compare our model to seven models from prior work. We use four metrics-Precision, Recall, F1 score, and a semantic similarity metric (Jaccard similarity) to compare our model's output to the Gold Standard. Our model resulted in a 84% Precision, 84% Recall, 83% F1, and a 84% Jaccard similarity. Results show that our GRU-based model outperforms prior models across all five ontologies. We also observed that character+word inputs result in a higher performance across models as compared to word only inputs. These findings indicate that deep learning algorithms are a promising avenue to be explored for automated ontology-based curation of data. This study also serves as a formal comparison and guideline for building and selecting deep learning models and architectures for ontology-based curation. © 2018 CEUR-WS.",2018,CEUR Workshop Proceedings,0,text mining approach @ automated ontology-based curation of biological and biomedical literature @ largely focused on syntactic and lexical analysis along @ machine learning @ recent advance in deep learning @ @ increased accuracy @ textual data annotation @ however @ application of deep learning @ ontology-based curation is a relatively @ area and prior work ha focused on a limited set of model @ @ @ introduce a @ deep learning model architecture based on combining multiple gated recurrent unit @ gru @ @ a character word based input @ @ use data @ five ontology in @ craft corpus a a gold standard to evaluate @ model @ s performance @ @ @ compare @ model to seven model @ prior work @ @ use four metrics-precision recall f score and a semantic similarity metric @ jaccard similarity @ to compare @ model @ s output to @ gold standard @ @ model resulted in a precision recall f and a jaccard similarity @ @ @ @ @ gru-based model outperforms prior model across @ five ontology @ @ @ observed @ character word input @ in a higher performance across model a compared to word only input @ @ finding indicate @ deep learning algorithm @ a promising avenue to @ explored @ automated ontology-based curation of data @ @ study @ serf a a formal comparison and guideline @ building and selecting deep learning model and architecture @ ontology-based curation @ ceur-ws @ 
1548,Multi-objective topic modeling for exploratory search in tech news,"Exploratory search is a paradigm of information retrieval, in which the user’s intention is to learn the subject domain better. To do this the user repeats “query–browse–refine” interactions with the search engine many times. We consider typical exploratory search tasks formulated by long text queries. People usually solve such a task in about half an hour and find dozens of documents using conventional search facilities iteratively. The goal of this paper is to reduce the time-consuming multi-step process to one step without impairing the quality of the search. Probabilistic topic modeling is a suitable text mining technique to retrieve documents, which are semantically relevant to a long text query. We use the additive regularization of topic models (ARTM) to build a model that meets multiple objectives. The model should have sparse, diverse and interpretable topics. Also, it should incorporate meta-data and multimodal data such as n-grams, authors, tags and categories. Balancing the regularization criteria is an important issue for ARTM. We tackle this problem with coordinate-wise optimization technique, which chooses the regularization trajectory automatically. We use the parallel online implementation of ARTM from the open source library BigARTM. Our evaluation technique is based on crowdsourcing and includes two tasks for assessors: the manual exploratory search and the explicit relevance feedback. Experiments on two popular tech news media show that our topic-based exploratory search outperforms assessors as well as simple baselines, achieving precision and recall of about 85–92%. © Springer International Publishing AG 2018.",2018,Communications in Computer and Information Science,12,exploratory search is a paradigm of information retrieval in @ @ user s intention is to learn @ subject domain better @ to @ @ @ user repeat query browse refine interaction @ @ search engine many time @ @ consider typical exploratory search task formulated by long text query @ people usually solve @ a task in @ half @ hour and find dozen of document @ conventional search facility iteratively @ @ goal of @ @ is to reduce @ time-consuming multi-step process to @ step without impairing @ quality of @ search @ probabilistic topic modeling is a suitable text mining technique to retrieve document @ @ semantically relevant to a long text query @ @ use @ additive regularization of topic model @ artm @ to build a model @ meet multiple objective @ @ model @ @ sparse diverse and interpretable topic @ @ @ @ incorporate meta-data and multimodal data @ a n-grams author tag and category @ balancing @ regularization criterion is @ important issue @ artm @ @ tackle @ problem @ coordinate-wise optimization technique @ chooses @ regularization trajectory automatically @ @ use @ parallel online implementation of artm @ @ open source library bigartm @ @ evaluation technique is based on crowdsourcing and includes @ task @ assessor @ @ manual exploratory search and @ explicit relevance feedback @ experiment on @ popular tech news medium @ @ @ topic-based exploratory search outperforms assessor a well a simple baseline achieving precision and recall of @ @ @ international publishing ag @ 
1549,Feature extraction from social media posts for psychometric typing of participants,"Sentiment analysis is an important tool for assessing the dynamic emotional terrain of social media interactions and behaviors [1]. Underlying the shallow emotional phenomenology are deeper and more stable strata, such as culture and psychology. This work addresses the latter, by applying text mining methods to the assessment of individual psychometrics. A methodology is described for reducing bulk, unstructured text to low-dimensional numeric feature vectors, from which components of the Myers-Briggs Typology Indicator (MBTI) [2] of the text’s author can be reliably inferred. MBTI is a psychometric schema that emerged from the personality theories of Freud and Jung in the early 20th Century, refined and codified by K. C. Briggs and her daughter, I. Briggs-Myers in the 1940’s and 50’s. This schema positions people along four (nominally independent) axes between pairs of polar motivations/preferences: Extroversion vs. Introversion (E-I); Intuition vs. Sensing (N-S); Feeling vs. Thinking (F-T); and, Judging vs Perceiving (J-P). Under this schema, each person falls into one of 16 psychometric groups, each designated by a four-character string (e.g., INTJ) [3]. Empirical results are shown for text generated during the social media interaction of over 8,600 PersonalityCafe users [4], all of whom are of known MBTI type. Blind tests to validate the features were conducted for a population (balanced by MBTI type), with exemplars based upon text samples having several thousand words each. The feature extraction method presented supports partial (1-letter) MBTI psychometric typing: E-I 95%; J-P 76.25%; F-T 91.25%, N-S 90%. Other results are reported. © Springer International Publishing AG, part of Springer Nature 2018.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,sentiment analysis is @ important tool @ assessing @ dynamic emotional terrain of social medium interaction and behavior @ underlying @ shallow emotional phenomenology @ deeper and more stable stratum @ a culture and psychology @ @ work address @ latter by applying text mining method to @ assessment of individual psychometrics @ a methodology is described @ reducing bulk unstructured text to low-dimensional numeric feature vector @ @ component of @ myers-briggs typology indicator @ mbti @ of @ text s author @ @ reliably inferred @ mbti is a psychometric schema @ emerged @ @ personality theory of freud and jung in @ early th century refined and codified by k @ c @ briggs and @ daughter i @ briggs-myers in @ s and s @ @ schema position people along four @ nominally independent @ ax @ pair of polar motivation preference @ extroversion v @ introversion @ e-i @ @ intuition v @ sensing @ n-s @ @ feeling v @ thinking @ f-t @ @ and judging v perceiving @ j-p @ @ @ @ schema @ person fall @ @ of psychometric group @ designated by a four-character string @ e @ g @ intj @ @ empirical @ @ @ @ text generated @ @ social medium interaction of @ personalitycafe user @ of @ @ of known mbti type @ blind test to validate @ feature @ conducted @ a population @ balanced by mbti type @ @ exemplar based upon text sample @ several thousand word @ @ @ feature extraction method presented support partial @ letter @ mbti psychometric typing @ e-i @ j-p @ @ f-t @ n-s @ @ @ @ reported @ @ international publishing ag part of @ nature @ 
1551,An integrated pipeline model for biomedical entity alignment,"Biomedical entity alignment, composed of two sub-tasks: entity identification and entity-concept mapping, is of great research value in biomedical text mining while these techniques are widely used for name entity standardization, information retrieval, knowledge acquisition and ontology construction. Previous works made many efforts on feature engineering to employ feature-based models for entity identification and alignment. However, the models depended on subjective feature selection may suffer error propagation and are not able to utilize the hidden information. With rapid development in health-related research, researchers need an effective method to explore the large amount of available biomedical literatures. Therefore, we propose a two-stage entity alignment process, biomedical entity exploring model, to identify biomedical entities and align them to the knowledge base interactively. The model aims to automatically obtain semantic information for extracting biomedical entities and mining semantic relations through the standard biomedical knowledge base. The experiments show that the proposed method achieves better performance on entity alignment. The proposed model dramatically improves the F1 scores of the task by about 4.5% in entity identification and 2.5% in entity-concept mapping. © 2020, Higher Education Press.",2021,Frontiers of Computer Science,0,biomedical entity alignment composed of @ sub-tasks @ entity identification and entity-concept mapping is of great research value in biomedical text mining @ @ technique @ widely used @ name entity standardization information retrieval knowledge acquisition and ontology construction @ previous work made many effort on feature engineering to employ feature-based model @ entity identification and alignment @ however @ model depended on subjective feature selection may suffer error propagation and @ not able to utilize @ hidden information @ @ rapid development in health-related research researcher need @ effective method to explore @ @ amount of available biomedical literature @ therefore @ propose a two-stage entity alignment process biomedical entity exploring model to identify biomedical entity and align @ to @ knowledge base interactively @ @ model aim to automatically obtain semantic information @ extracting biomedical entity and mining semantic relation @ @ standard biomedical knowledge base @ @ experiment @ @ @ proposed method achieves better performance on entity alignment @ @ proposed model dramatically improves @ f score of @ task by @ @ in entity identification and @ in entity-concept mapping @ higher education @ @ 
1552,Deflated reputation using multiplicative long short-term memory neural networks,"Current reputation systems are facing the inflation problem, which renders reputation systems to lose information and sometimes even cause misunderstandings. To address this problem, we propose a data-driven approach that combines natural language processing techniques with the conditional logit model for reputation deflation. We consider multiplicative long short-term memory neural networks (mLSTM) to predict sentiment scores from the feedback content. The mLSTM was pre-trained on 82.83 million unique reviews. We conduct experiments on one of the largest online labor marketplaces, Freelancer.com. We focus on comparing ratings and predicted sentiment scores in the online labor market. The results show that our proposed model can estimate deflated reputation information effectively. In addition, the estimated sentiment score is a quality disclosure signal, and has a better effect on the market outcome than the inflated reputation rating. © 2021 Elsevier B.V.",2021,Future Generation Computer Systems,0,current reputation system @ facing @ inflation problem @ render reputation system to lose information and sometimes even cause misunderstanding @ to address @ problem @ propose a data-driven approach @ combine natural language processing technique @ @ conditional logit model @ reputation deflation @ @ consider multiplicative long short-term memory neural network @ mlstm @ to predict sentiment score @ @ feedback content @ @ mlstm wa pre-trained on @ million unique review @ @ conduct experiment on @ of @ largest online labor marketplace freelancer @ com @ @ focus on comparing rating and predicted sentiment score in @ online labor market @ @ @ @ @ @ proposed model @ estimate deflated reputation information effectively @ in addition @ estimated sentiment score is a quality disclosure signal and ha a better effect on @ market outcome @ @ inflated reputation rating @ @ b @ v @ 
1553,Extracting salient features from convolutional discriminative filters,"Convolutional neural networks (CNN) have been widely used in various tasks, largely due to their ability to efficiently extract n-gram features for text analysis and document representation. In this paper, we intend to insight the CNN model regarding its capability on text analysis. Vanilla CNNs do have weaknesses when it comes to the representation and feature extraction. Duplicate filters are inevitable with vanilla CNNs, which reduces the discriminative power of the representations. In addition, the current pooling operations either limit the CNN to the local optimum (i.e., max pooling) or they do not consider the importance of all features (i.e., mean pooling). In this paper, we propose two modules for vanilla CNNs to overcome these shortcomings. The first equips the CNN with discriminative filters (distinct filters with maximised divergence) and the second provides the ability to comprehensively extract all salient features. Specifically, our model increases the discriminative power of the model by maximizing the distance between different filters, and a novel global pooling mechanism for feature extraction. Validation tests against state-of-the-art baselines on five benchmark classification datasets achieve the competitive performance of our proposed model. Furthermore, visualization on upgrade filters and pooling features verify our hypothesis that the proposed model can receive discriminative filters and salient features. © 2021 The Author(s)",2021,Information Sciences,0,convolutional neural network @ cnn @ @ @ widely used in various task largely due to @ ability to efficiently extract n-gram feature @ text analysis and document representation @ in @ @ @ intend to insight @ cnn model regarding @ capability on text analysis @ vanilla cnns @ @ weakness @ @ come to @ representation and feature extraction @ duplicate filter @ inevitable @ vanilla cnns @ reduces @ discriminative power of @ representation @ in addition @ current pooling operation either limit @ cnn to @ local optimum @ i @ e @ max pooling @ @ @ @ not consider @ importance of @ feature @ i @ e @ mean pooling @ @ in @ @ @ propose @ module @ vanilla cnns to overcome @ shortcoming @ @ first equips @ cnn @ discriminative filter @ distinct filter @ maximised divergence @ and @ second provides @ ability to comprehensively extract @ salient feature @ specifically @ model increase @ discriminative power of @ model by maximizing @ distance @ different filter and a novel global pooling mechanism @ feature extraction @ validation test @ state-of-the-art baseline on five benchmark classification datasets achieve @ competitive performance of @ proposed model @ furthermore visualization on upgrade filter and pooling feature verify @ hypothesis @ @ proposed model @ receive discriminative filter and salient feature @ @ author @ s @ 
1554,MTVRep: A movie and TV show reputation system based on fine-grained sentiment and semantic analysis,"Customer reviews are a valuable source of information from which we can extract very useful data about different online shopping experiences. For trendy items (products, movies, TV shows, hotels, services...), the number of available users and customers' opinions could easily surpass thousands. Therefore, online reputation systems could aid potential customers in making the right decision (buying, renting, booking...) by automatically mining textual reviews and their ratings. This paper presents MTVRep, a movie and TV show reputation system that incorporates fine-grained opinion mining and semantic analysis to generate and visualize reputation toward movies and TV shows. Differently from previous studies on reputation generation that treat the task of sentiment analysis as a binary classification problem (positive, negative), the proposed system identifies the sentiment strength during the phase of sentiment classification by using fine-grained sentiment analysis to separate movie and TV show reviews into five discrete classes: strongly negative, weakly negative, neutral, weakly positive and strongly positive. Besides, it employs embeddings from language models (ELMo) representations to extract semantic relations between reviews. The contribution of this paper is threefold. First, movie and TV show reviews are separated into five groups based on their sentiment orientation. Second, a custom score is computed for each opinion group. Finally, a numerical reputation value is produced toward the target movie or TV show. The efficacy of the proposed system is illustrated by conducting several experiments on a real-world movie and TV show dataset. © 2021 Institute of Advanced Engineering and Science. All rights reserved.",2021,International Journal of Electrical and Computer Engineering,1,customer review @ a valuable source of information @ @ @ @ extract @ useful data @ different online shopping experience @ @ trendy item @ product movie tv @ hotel service @ @ @ @ @ number of available user and customer @ opinion could easily surpass thousand @ therefore online reputation system could aid potential customer in making @ right decision @ buying renting booking @ @ @ @ by automatically mining textual review and @ rating @ @ @ @ mtvrep a movie and tv @ reputation system @ incorporates fine-grained opinion mining and semantic analysis to generate and visualize reputation toward movie and tv @ @ differently @ previous study on reputation generation @ treat @ task of sentiment analysis a a binary classification problem @ positive negative @ @ proposed system identifies @ sentiment strength @ @ phase of sentiment classification by @ fine-grained sentiment analysis to separate movie and tv @ review @ five discrete class @ strongly negative weakly negative neutral weakly positive and strongly positive @ besides @ employ embeddings @ language model @ elmo @ representation to extract semantic relation @ review @ @ contribution of @ @ is threefold @ first movie and tv @ review @ separated @ five group based on @ sentiment orientation @ second a custom score is computed @ @ opinion group @ finally a numerical reputation value is produced toward @ target movie @ tv @ @ @ efficacy of @ proposed system is illustrated by conducting several experiment on a real-world movie and tv @ dataset @ institute of advanced engineering and science @ @ right reserved @ 
1555,Opportunities and challenges of text mining in aterials research,"Research publications are the major repository of scientific knowledge. However, their unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of the information contained within. Recent progress in natural language processing (NLP) has provided a variety of tools for high-quality information extraction from unstructured text. These tools are primarily trained on non-technical text and struggle to produce accurate results when applied to scientific text, involving specific technical terminology. During the last years, significant efforts in information retrieval have been made for biomedical and biochemical publications. For materials science, text mining (TM) methodology is still at the dawn of its development. In this review, we survey the recent progress in creating and applying TM and NLP approaches to materials science field. This review is directed at the broad class of researchers aiming to learn the fundamentals of TM as applied to the materials science publications. © 2021 The Author(s) Data Analysis; Computing Methodology; Computational Materials Science; Materials Design © 2021 The Author(s)",2021,iScience,0,research publication @ @ major repository of scientific knowledge @ however @ unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of @ information contained within @ recent progress in natural language processing @ nlp @ ha provided a variety of tool @ high-quality information extraction @ unstructured text @ @ tool @ primarily trained on non-technical text and struggle to produce accurate @ @ applied to scientific text involving specific technical terminology @ @ @ last year significant effort in information retrieval @ @ made @ biomedical and biochemical publication @ @ material science text mining @ tm @ methodology is still at @ dawn of @ development @ in @ review @ survey @ recent progress in creating and applying tm and nlp approach to material science field @ @ review is directed at @ broad class of researcher aiming to learn @ fundamental of tm a applied to @ material science publication @ @ author @ s @ data analysis @ computing methodology @ computational material science @ material design @ author @ s @ 
1556,A hybrid model for finding abbreviation–definition pairs from biomedical abstracts using heuristics-based sequence labeling and perceptron linear classifier,"Automatic extraction of abbreviation and its definition from free format text is a constructive task in text mining. The previous work pertinent to automatic abbreviation/definition extraction from text followed either heuristics or machine learning approach. This paper proposes a hybrid model to identify abbreviation definition pairs from biomedical text. The proposed system uses two approaches i) To identify abbreviation-definition pairs, pattern matching is done through sequence labeling based on the heuristics approach. Three mapping strategies such as Predecessor Term Mapping, Word Level Mapping, and Character Level Mapping are used in sequence labeling tasks. ii) To validate the identified abbreviation-definition pair, an ANN-based approach such as a single layer neural network (perceptron) is used in this work. PubMed biomedical abstracts are utilized as a data source to find the abbreviation-definition pairs. The system performance is analyzed across six different entities in biomedical abstracts. The experiment result shows that our model achieves precision of 96.2%, recall of 92.4%, and F1 of 94.6%. To cross-validate the system performance, the proposed model is validated by using two corpuses AB3P and BioADI, the outcomes of which are discussed in the results section. © 2020 Elsevier Ltd",2021,Expert Systems with Applications,0,automatic extraction of abbreviation and @ definition @ free format text is a constructive task in text mining @ @ previous work pertinent to automatic abbreviation definition extraction @ text followed either heuristic @ machine learning approach @ @ @ proposes a hybrid model to identify abbreviation definition pair @ biomedical text @ @ proposed system us @ approach i @ to identify abbreviation-definition pair pattern matching is done @ sequence labeling based on @ heuristic approach @ three mapping strategy @ a predecessor term mapping word level mapping and character level mapping @ used in sequence labeling task @ ii @ to validate @ identified abbreviation-definition pair @ ann-based approach @ a a single layer neural network @ perceptron @ is used in @ work @ pubmed biomedical abstract @ utilized a a data source to find @ abbreviation-definition pair @ @ system performance is analyzed across six different entity in biomedical abstract @ @ experiment @ @ @ @ model achieves precision of @ recall of @ and f of @ @ to cross-validate @ system performance @ proposed model is validated by @ @ corpus ab p and bioadi @ outcome of @ @ discussed in @ @ section @ @ ltd
1558,Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain,"The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge. © 2020 Elsevier B.V.",2021,Future Generation Computer Systems,0,@ continuous growth of scientific literature brings innovation and at @ @ time raise @ challenge @ @ of @ is related to @ fact @ @ analysis ha become difficult due to @ high volume of published @ @ @ manual effort @ annotation and management is required @ novel technological infrastructure @ needed to help researcher research policy maker and company to time-efficiently browse analyse and forecast scientific research @ knowledge graph i @ e @ @ network of entity and relationship @ proved to @ effective solution in @ space @ scientific knowledge graph focus on @ scholarly domain and typically contain metadata describing research publication @ a author venue organization research topic and citation @ however @ current generation of knowledge graph lack of @ explicit representation of @ knowledge presented in @ research @ @ a @ in @ @ @ @ a @ architecture @ take advantage of natural language processing and machine learning method @ extracting entity and relationship @ research publication and integrates @ in a large-scale knowledge graph @ within @ research work @ @ i @ tackle @ challenge of knowledge extraction by employing several state-of-the-art natural language processing and text mining tool @ ii @ describe @ approach @ integrating entity and relationship generated by @ tool @ iii @ @ @ advantage of @ @ hybrid system @ alternative approach and @ vi @ a a chosen use case @ generated a scientific knowledge graph including triple extracted @ abstract of @ within @ semantic web domain @ a @ approach is general and @ @ applied to @ domain @ expect @ @ @ facilitate @ management analysis dissemination and processing of scientific knowledge @ @ b @ v @ 
1564,TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing,"The role of text mining based on technological documents such as patents is important in the research field of technology intelligence for technology R&D planning. In addition, WordNet, an English-based lexical database, is widely used for pre-processing text data such as word lemmatization and synonym search. However, technological vocabulary information is complex and specific, and WordNet's ability to analyze technological information is limited in its reflecting technological features. Thus, to improve the text mining performance of technological information, this study proposes a methodology for designing a TechWord-based lexical database that is based on the lexical characteristics of technological words that are differentiated from general words. To do this, we define TechWord, a technology lexical information, and construct a TechSynset, a synonym set between TechWords. First, through dependency parsing between words, TechWord, a unit word that describes a technology, is structured and identifies nouns and verbs. The importance of connectivity is investigated by a network centrality index analysis based on the dependency relations of words. Subsequently, to search for synonyms suitable for the target technology domain, a TechSynset is constructed through synset information, with an additional analysis that calculates cosine similarity based on a word embedding vector. Applying the proposed methodology to the actual technology-related information analysis, we collect patent data on the technological fields of the automotive field, and present the results of the TechWord and TechSynset. This study improves technological information-based text mining by structuring the word-to-word link information in technological documents based on an automated process. © 2020 Elsevier Ltd",2021,Expert Systems with Applications,1,@ role of text mining based on technological document @ a patent is important in @ research field of technology intelligence @ technology r @ planning @ in addition wordnet @ english-based lexical database is widely used @ pre-processing text data @ a word lemmatization and synonym search @ however technological vocabulary information is complex and specific and wordnet @ s ability to analyze technological information is limited in @ reflecting technological feature @ thus to improve @ text mining performance of technological information @ study proposes a methodology @ designing a techword-based lexical database @ is based on @ lexical characteristic of technological word @ @ differentiated @ general word @ to @ @ @ define techword a technology lexical information and construct a techsynset a synonym set @ techwords @ first @ dependency parsing @ word techword a unit word @ describes a technology is structured and identifies noun and verb @ @ importance of connectivity is investigated by a network centrality index analysis based on @ dependency relation of word @ subsequently to search @ synonym suitable @ @ target technology domain a techsynset is constructed @ synset information @ @ additional analysis @ calculates cosine similarity based on a word embedding vector @ applying @ proposed methodology to @ actual technology-related information analysis @ collect patent data on @ technological field of @ automotive field and @ @ @ of @ techword and techsynset @ @ study improves technological information-based text mining by structuring @ word-to-word link information in technological document based on @ automated process @ @ ltd
1567,UrbangEnCy: An emergency events dataset based on citizen sensors for monitoring urban scenarios in Ecuador,"Recently, the use of the citizen-sensors (people generating and sharing real data by social media) for detecting and disseminating emergency events in real-time have shown a considerable increase because people at the place of the event, as well as elsewhere, can quickly post relevant information on this type of alerts. Here, we present an emergency events dataset called UrbangEnCy. The dataset contains over 25500 texts in Spanish posted on Twitter from January 19th to August 19th, 2020, with emergencies and non-emergencies related content in Ecuador. We obtained, cleaned and, filtered these tweets and, then we selected the location and temporal data as well as tweet content. Besides, the data set includes annotations regarding the type of tweet (emergency / non-emergency) as well as additional nomenclature used to describe emergencies in the Center for immediate response service to emergencies (ECU 911) of Ecuador and international emergency services agencies (ESAs). UrbangEnCy dataset facilitates evaluating data science performance, machine learning, and natural language processing algorithms used with supervised and unsupervised problems re- related to text mining and pattern recognition. The dataset is freely and publicly available at https://doi.org/10.17632/4x37zz82k8. © 2020",2021,Data in Brief,0,recently @ use of @ citizen-sensors @ people generating and sharing real data by social medium @ @ detecting and disseminating emergency event in real-time @ @ a considerable increase @ people at @ place of @ event a well a elsewhere @ quickly post relevant information on @ type of alert @ @ @ @ @ emergency event dataset called urbangency @ @ dataset contains @ text in spanish posted on twitter @ january th to august th @ emergency and non-emergencies related content in ecuador @ @ obtained cleaned and filtered @ tweet and @ @ selected @ location and temporal data a well a tweet content @ besides @ data set includes annotation regarding @ type of tweet @ emergency non-emergency @ a well a additional nomenclature used to describe emergency in @ center @ immediate response service to emergency @ ecu @ of ecuador and international emergency service agency @ esas @ @ urbangency dataset facilitates evaluating data science performance machine learning and natural language processing algorithm used @ supervised and unsupervised problem @ related to text mining and pattern recognition @ @ dataset is freely and publicly available at http @ doi @ org @ x zz k @ 
1571,Topic Modeling Uncovers Shifts in Media Framing of the German Renewable Energy Act,"Renewable energy policies have been recognized as a cornerstone in the transition toward low-emission energy systems. Media reports are an important variable in the policy-making process, interrelating politicians and the public. To understand the changes in media framing of a pioneering renewable energy support act, we collected 6,645 articles from five Germany-wide newspapers between 2000 and 2017 on the German Renewable Energy Act. We developed a structural topic model based on a change-point analysis to assess the temporal patterns of newspaper coverage. We introduced the notion of topic sentiment to elucidate the emotional content of topics. The results show that after its enactment, optimism about renewable energies dominated the media agenda. After 2012, however, the Renewable Energy Act was more associated with its costs. Such shifts in renewable energy policy framing may limit political leverage to reach ambitious climate and energy targets. Worldwide, policymakers push for a faster adoption of renewable energy technologies to mitigate climate change. Although policies that support the adoption of new technologies often have positive effects on innovation and job creation in an industry, they also involve costs borne by society. Media representations often have effects on public opinion on a policy. To understand how media reports on the German Renewable Energy Act developed over time, we developed advanced text mining models. We find that media coverage has shifted from positive accounts of the renewable energy industry toward the costs that the Renewable Energy Act imposes on society. If such patterns generalize, then public support and long-term renewable goals might be endangered. We propose that policies could be designed so that new innovative technologies, such as batteries or power-to-gas, and the optimism created by new technologies rub off onto “old” renewables to maintain broad public support. A structural topic model is developed to assess the temporal dynamics of topic prevalence and sentiment in newspaper coverage of the German Renewable Energy Act. The results show that coverage followed a pattern similar to issue-attention cycles. Newspapers predominantly reported on the renewable energy industry until, in 2012, framing changed, and from then on, costs dominated the agenda. The shift in framing can affect political leverage in reaching more ambitious renewable energy targets. © 2020 The Authors",2021,Patterns,0,renewable energy policy @ @ recognized a a cornerstone in @ transition toward low-emission energy system @ medium report @ @ important variable in @ policy-making process interrelating politician and @ public @ to understand @ change in medium framing of a pioneering renewable energy support act @ collected article @ five germany-wide newspaper @ and on @ german renewable energy act @ @ developed a structural topic model based on a change-point analysis to ass @ temporal pattern of newspaper coverage @ @ introduced @ notion of topic sentiment to elucidate @ emotional content of topic @ @ @ @ @ @ @ enactment optimism @ renewable energy dominated @ medium agenda @ @ however @ renewable energy act wa more associated @ @ cost @ @ shift in renewable energy policy framing may limit political leverage to reach ambitious climate and energy target @ worldwide policymakers push @ a faster adoption of renewable energy technology to mitigate climate change @ although policy @ support @ adoption of @ technology often @ positive effect on innovation and job creation in @ industry @ @ involve cost borne by society @ medium representation often @ effect on public opinion on a policy @ to understand @ medium report on @ german renewable energy act developed @ time @ developed advanced text mining model @ @ find @ medium coverage ha shifted @ positive account of @ renewable energy industry toward @ cost @ @ renewable energy act imposes on society @ if @ pattern generalize @ public support and long-term renewable goal might @ endangered @ @ propose @ policy could @ designed @ @ @ innovative technology @ a battery @ power-to-gas and @ optimism created by @ technology rub off onto old renewables to maintain broad public support @ a structural topic model is developed to ass @ temporal dynamic of topic prevalence and sentiment in newspaper coverage of @ german renewable energy act @ @ @ @ @ coverage followed a pattern similar to issue-attention cycle @ newspaper predominantly reported on @ renewable energy industry @ in framing changed and @ @ on cost dominated @ agenda @ @ shift in framing @ affect political leverage in reaching more ambitious renewable energy target @ @ author
1573,Translating sentimental statements using deep learning techniques,"Natural Language Processing (NLP) allows machines to know nature languages and helps us do tasks, such as retrieving information, answering questions, text summarization, categorizing text, and machine translation. To our understanding, no NLP was used to translate statements from negative sentiment to positive sentiment with resembling semantics, although human communication needs. The developments of translating sentimental statements using deep learning techniques are proposed in this paper. First, for a sentiment translation model, we create negative–positive sentimental statement datasets. Then using deep learning techniques, the sentiment translation model is developed. Perplexity, bilingual evaluation understudy, and human evaluations are used in the experiments to test the model, and the results are satisfactory. Finally, if the trained datasets can be constructed as planned, we believe the techniques used in translating sentimental statements are possible, and more sophisticated models can be developed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Electronics (Switzerland),0,natural language processing @ nlp @ allows machine to know nature language and help u @ task @ a retrieving information answering question text summarization categorizing text and machine translation @ to @ understanding no nlp wa used to translate statement @ negative sentiment to positive sentiment @ resembling semantics although human communication need @ @ development of translating sentimental statement @ deep learning technique @ proposed in @ @ @ first @ a sentiment translation model @ create negative positive sentimental statement datasets @ @ @ deep learning technique @ sentiment translation model is developed @ perplexity bilingual evaluation understudy and human evaluation @ used in @ experiment to test @ model and @ @ @ satisfactory @ finally if @ trained datasets @ @ constructed a planned @ believe @ technique used in translating sentimental statement @ possible and more sophisticated model @ @ developed @ by @ author @ licensee mdpi basel switzerland @ 
1574,"A survey of text mining approaches, techniques, and tools on discharge summaries","The discharge summary contains voluminous information regarding the patient like history, symptoms, investigations, treatment, medication, etc. Though the discharge summary has a general structured way of representation, it is still not structured in a way that clinical systems can process. Different natural language processing (NLP) and machine learning techniques have been explored on the discharge summaries to extract various interesting information. Text mining techniques have been carried out in public and private discharge summaries. This survey discusses different tasks performed on discharge summaries and the existing tools which have been explored. The major dataset which has been used in existing research is also discussed. A common outline of system architectures on discharge summaries across various researches is explored. Major challenges in extracting information from discharge summaries are also detailed. © Springer Nature Singapore Pte Ltd. 2021.",2021,Advances in Intelligent Systems and Computing,0,@ discharge summary contains voluminous information regarding @ patient like history symptom investigation treatment medication etc @ though @ discharge summary ha a general structured way of representation @ is still not structured in a way @ clinical system @ process @ different natural language processing @ nlp @ and machine learning technique @ @ explored on @ discharge summary to extract various interesting information @ text mining technique @ @ carried @ in public and private discharge summary @ @ survey discus different task performed on discharge summary and @ existing tool @ @ @ explored @ @ major dataset @ ha @ used in existing research is @ discussed @ a common outline of system architecture on discharge summary across various research is explored @ major challenge in extracting information @ discharge summary @ @ detailed @ @ nature singapore pte ltd @ @ 
1575,Text summarization using natural language processing,"With the advancements in the technology, most of the things in this world have become automated. The concept of text summarization came into limelight as summarization of text manually has become a tough and time-consuming task. So, the main purpose of text summarization is to overcome the difficulties faced during manual summarization of text documents or other information from various sources. Text summarization is the process of extracting the main idea of the context or the text and briefly explaining about the context. This process is not only to extract key idea and phrases from the text sources but also generating meaningful summary in a concise and crisp way. The demand for text summarization is raising nowadays because of the large amounts of data from multiple sources like Internet, Twitter, Facebook, Instagram, research papers, and other news articles. Text summarization can be efficiently implemented using NLP as it has many packages and methods in Python or R. Text summarization is also related to text mining as summary is generated based on classifying the given input text. There are different approaches for text summarization and some algorithms are identified to implement these approaches. In this paper, unsupervised learning approach is implemented and cosine similarity technique is used to find the similarity between sentences. To generate rank based on similarity, text rank algorithm is used and sentences with top rank are placed in summarized text. © Springer Nature Singapore Pte Ltd 2021.",2021,Advances in Intelligent Systems and Computing,0,@ @ advancement in @ technology @ of @ thing in @ world @ become automated @ @ concept of text summarization came @ limelight a summarization of text manually ha become a tough and time-consuming task @ @ @ main purpose of text summarization is to overcome @ difficulty faced @ manual summarization of text document @ @ information @ various source @ text summarization is @ process of extracting @ main idea of @ context @ @ text and briefly explaining @ @ context @ @ process is not only to extract key idea and phrase @ @ text source @ @ generating meaningful summary in a concise and crisp way @ @ demand @ text summarization is raising nowadays @ of @ @ amount of data @ multiple source like internet twitter facebook instagram research @ and @ news article @ text summarization @ @ efficiently implemented @ nlp a @ ha many package and method in python @ r @ text summarization is @ related to text mining a summary is generated based on classifying @ given input text @ @ @ different approach @ text summarization and some algorithm @ identified to implement @ approach @ in @ @ unsupervised learning approach is implemented and cosine similarity technique is used to find @ similarity @ sentence @ to generate rank based on similarity text rank algorithm is used and sentence @ top rank @ placed in summarized text @ @ nature singapore pte ltd @ 
1576,GapFinder: Finding Inconsistency of Security Information from Unstructured Text,"Textual data mining of open source intelligence on the Web has become an increasingly important topic across a wide range of domains such as business, law enforcement, military, and cybersecurity. Text mining efforts utilize natural language processing to transform unstructured web content into structured forms that can drive various machine learning applications and data indexing services. For example, applications for text mining in cybersecurity have produced a range of threat intelligence services that serve the IT industry. However, a less studied problem is that of automating the identification of semantic inconsistencies among various text input sources. In this paper, we introduce GapFinder, a new inconsistency checking system for identifying semantic inconsistencies within the cybersecurity domain. Specifically, we examine the problem of identifying technical inconsistencies that arise in the functional descriptions of open source malware threat reporting information. Our evaluation, using tens of thousands of relations derived from web-based malware threat reports, demonstrates the ability of GapFinder to identify the presence of inconsistencies. © 2005-2012 IEEE.",2021,IEEE Transactions on Information Forensics and Security,0,textual data mining of open source intelligence on @ web ha become @ increasingly important topic across a wide range of domain @ a @ law enforcement military and cybersecurity @ text mining effort utilize natural language processing to transform unstructured web content @ structured form @ @ drive various machine learning application and data indexing service @ @ example application @ text mining in cybersecurity @ produced a range of threat intelligence service @ serve @ @ industry @ however a le studied problem is @ of automating @ identification of semantic inconsistency among various text input source @ in @ @ @ introduce gapfinder a @ inconsistency checking system @ identifying semantic inconsistency within @ cybersecurity domain @ specifically @ examine @ problem of identifying technical inconsistency @ arise in @ functional description of open source malware threat reporting information @ @ evaluation @ ten of thousand of relation derived @ web-based malware threat report demonstrates @ ability of gapfinder to identify @ presence of inconsistency @ @ @ 
1577,Research on Public Sentiment Data Center Based on Key Technology of Big Data,"Through the construction of public opinion data center as the research object, a series of data acquisition, monitoring and analysis and mining tasks based on public opinion information can provide natural language processing functions such as web data crawling and monitoring, web data parsing, text data preprocessing, text analysis and mining. Research shows that automatic summarization technology, event recognition technology, event context combing and emotion analysis technology can be based on text mining algorithm to achieve automatic keyword extraction, content simplification, similarity calculation, tracking and monitoring, and subjective text with emotional color analysis has a very good application value and efficiency. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",2021,Advances in Intelligent Systems and Computing,0,@ @ construction of public opinion data center a @ research object a series of data acquisition monitoring and analysis and mining task based on public opinion information @ provide natural language processing function @ a web data crawling and monitoring web data parsing text data preprocessing text analysis and mining @ research @ @ automatic summarization technology event recognition technology event context combing and emotion analysis technology @ @ based on text mining algorithm to achieve automatic keyword extraction content simplification similarity calculation tracking and monitoring and subjective text @ emotional color analysis ha a @ good application value and efficiency @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1578,A comparative empirical evaluation of topic modeling techniques,"This paper presents empirical evaluation of three most important topic modeling techniques—latent semantic analysis, latent Dirichlet allocation and correlated topic modeling, The novelty of work exists in application or domain-independent comparative evaluation of techniques, that is not presented before in the literature. Most of the work on topic modeling is very application specific, so it becomes difficult to conclude a generalization solution. In this research, a detailed comparative evaluation of topic modeling techniques is presented, and it is found that latent Dirichlet allocation technique is the true synonym for topic modeling. So latent Dirichlet allocation is a truly unsupervised kind of machine learning technique, correlated topic modeling is another Dirichlet free probabilistic topic modelling to find correlation among topics. Latent semantic analysis presents semantic gist of corpus at the diagonal in such an effective way, that it can be considered as a very promising technique in natural language processing and text mining. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2021.",2021,Advances in Intelligent Systems and Computing,0,@ @ @ empirical evaluation of three @ important topic modeling technique latent semantic analysis latent dirichlet allocation and correlated topic modeling @ novelty of work exists in application @ domain-independent comparative evaluation of technique @ is not presented @ in @ literature @ @ of @ work on topic modeling is @ application specific @ @ becomes difficult to conclude a generalization solution @ in @ research a detailed comparative evaluation of topic modeling technique is presented and @ is found @ latent dirichlet allocation technique is @ true synonym @ topic modeling @ @ latent dirichlet allocation is a truly unsupervised kind of machine learning technique correlated topic modeling is another dirichlet free probabilistic topic modelling to find correlation among topic @ latent semantic analysis @ semantic gist of corpus at @ diagonal in @ @ effective way @ @ @ @ considered a a @ promising technique in natural language processing and text mining @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1580,Understanding the temporal evolution of COVID-19 research through machine learning and natural language processing,"The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been continuously affecting human lives and communities around the world in many ways, from cities under lockdown to new social experiences. Although in most cases COVID-19 results in mild illness, it has drawn global attention due to the extremely contagious nature of SARS-CoV-2. Governments and healthcare professionals, along with people and society as a whole, have taken any measures to break the chain of transition and flatten the epidemic curve. In this study, we used multiple data sources, i.e., PubMed and ArXiv, and built several machine learning models to characterize the landscape of current COVID-19 research by identifying the latent topics and analyzing the temporal evolution of the extracted research themes, publications similarity, and sentiments, within the time-frame of January–May 2020. Our findings confirm the types of research available in PubMed and ArXiv differ significantly, with the former exhibiting greater diversity in terms of COVID-19 related issues and the latter focusing more on intelligent systems/tools to predict/diagnose COVID-19. The special attention of the research community to the high-risk groups and people with complications was also confirmed. © 2020, Crown.",2021,Scientometrics,0,@ outbreak of @ novel coronavirus disease @ covid @ caused by @ severe acute respiratory syndrome coronavirus @ sars-cov @ ha @ continuously affecting human life and community around @ world in many way @ city @ lockdown to @ social experience @ although in @ case covid @ in mild illness @ ha drawn global attention due to @ extremely contagious nature of sars-cov @ government and healthcare professional along @ people and society a a whole @ taken @ measure to break @ chain of transition and flatten @ epidemic curve @ in @ study @ used multiple data source i @ e @ pubmed and arxiv and built several machine learning model to characterize @ landscape of current covid research by identifying @ latent topic and analyzing @ temporal evolution of @ extracted research theme publication similarity and sentiment within @ time-frame of january may @ @ finding confirm @ type of research available in pubmed and arxiv differ significantly @ @ former exhibiting greater diversity in term of covid related issue and @ latter focusing more on intelligent system tool to predict diagnose covid @ @ special attention of @ research community to @ high-risk group and people @ complication wa @ confirmed @ crown @ 
1581,Computational Media Intelligence: Human-Centered Machine Analysis of Media,"Media is created by humans for humans to tell stories. There exists a natural and imminent need for creating human-centered media analytics to illuminate the stories being told and to understand their impact on individuals and society at large. An objective understanding of media content has numerous applications for different stakeholders, from creators to decision-/policy-makers to consumers. Advances in multimodal signal processing and machine learning (ML) can enable detailed and nuanced characterization of media content (of who, what, how, where, and why) at scale. They can also aid our understanding of the impact of media on a range of issues, including individual experiences, behavioral, cultural, and societal trends, and commercial outcomes. Modern deep learning models combined with audiovisual signal processing can analyze entertainment media, such as Film &amp; TV content to quantify gender, age, and race representations. This creates awareness in an objective way that was hitherto impossible. On the other hand, text mining and natural language processing allow nuanced understanding of language use and spoken interactions in media, such as News to track patterns and trends across different contexts. Moreover, advances in human sensing have enabled us to directly measure the influence of media on an individual's physiology (and brain), while social media analysis enables tracking the societal impact of media content on different cross sections of the society. This article reviews representative methodologies and algorithms, tools, and systems advancing human-centered media understanding through ML in the pursuit of developing computational media intelligence. IEEE",2021,Proceedings of the IEEE,0,medium is created by human @ human to tell story @ @ exists a natural and imminent need @ creating human-centered medium analytics to illuminate @ story @ told and to understand @ impact on individual and society at @ @ @ objective understanding of medium content ha numerous application @ different stakeholder @ creator to decision policy-makers to consumer @ advance in multimodal signal processing and machine learning @ ml @ @ enable detailed and nuanced characterization of medium content @ of @ @ @ @ and @ @ at scale @ @ @ @ aid @ understanding of @ impact of medium on a range of issue including individual experience behavioral cultural and societal trend and commercial outcome @ modern deep learning model combined @ audiovisual signal processing @ analyze entertainment medium @ a film amp @ tv content to quantify gender age and race representation @ @ creates awareness in @ objective way @ wa hitherto impossible @ on @ @ hand text mining and natural language processing allow nuanced understanding of language use and spoken interaction in medium @ a news to track pattern and trend across different context @ moreover advance in human sensing @ enabled u to directly measure @ influence of medium on @ individual @ s physiology @ and brain @ @ social medium analysis enables tracking @ societal impact of medium content on different cross section of @ society @ @ article review representative methodology and algorithm tool and system advancing human-centered medium understanding @ ml in @ pursuit of developing computational medium intelligence @ @
1582,An abstractive summarization technique with variable length keywords as per document diversity,"Text Summarization is an essential area in text mining, which has procedures for text extraction. In natural language processing, text summarization maps the documents to a representative set of descriptive words. Therefore, the objective of text extraction is to attain reduced expressive contents from the text documents. Text summarization has two main areas such as abstractive, and extractive summarization. Extractive text summarization has further two approaches, in which the first approach applies the sentence score algorithm, and the second approach follows the word embedding principles. All such text extractions have limitations in providing the basic theme of the underlying documents. In this paper, we have employed text summarization by TF-IDF with PageRank keywords, sentence score algorithm, and Word2Vec word embedding. The study compared these forms of the text summarizations with the actual text, by calculating cosine similarities. Furthermore, TF-IDF based PageRank keywords are extracted from the other two extractive summarizations. An intersection over these three types of TD-IDF keywords to generate the more representative set of keywords for each text document is performed. This technique generates variable-length keywords as per document diversity instead of selecting fixed-length keywords for each document. This form of abstractive summarization improves metadata similarity to the original text compared to all other forms of summarized text. It also solves the issue of deciding the number of representative keywords for a specific text document. To evaluate the technique, the study used a sample of more than eighteen hundred text documents. The abstractive summarization follows the principles of deep learning to create uniform similarity of extracted words with actual text and all other forms of text summarization. The proposed technique provides a stable measure of similarity as compared to existing forms of text summarization. © 2021 Tech Science Press. All rights reserved.",2021,"Computers, Materials and Continua",0,text summarization is @ essential area in text mining @ ha procedure @ text extraction @ in natural language processing text summarization map @ document to a representative set of descriptive word @ therefore @ objective of text extraction is to attain reduced expressive content @ @ text document @ text summarization ha @ main area @ a abstractive and extractive summarization @ extractive text summarization ha @ @ approach in @ @ first approach applies @ sentence score algorithm and @ second approach follows @ word embedding principle @ @ @ text extraction @ limitation in providing @ basic theme of @ underlying document @ in @ @ @ @ employed text summarization by tf-idf @ pagerank keywords sentence score algorithm and word vec word embedding @ @ study compared @ form of @ text summarization @ @ actual text by calculating cosine similarity @ furthermore tf-idf based pagerank keywords @ extracted @ @ @ @ extractive summarization @ @ intersection @ @ three type of td-idf keywords to generate @ more representative set of keywords @ @ text document is performed @ @ technique generates variable-length keywords a per document diversity instead of selecting fixed-length keywords @ @ document @ @ form of abstractive summarization improves metadata similarity to @ original text compared to @ @ form of summarized text @ @ @ solves @ issue of deciding @ number of representative keywords @ a specific text document @ to evaluate @ technique @ study used a sample of more @ eighteen hundred text document @ @ abstractive summarization follows @ principle of deep learning to create uniform similarity of extracted word @ actual text and @ @ form of text summarization @ @ proposed technique provides a stable measure of similarity a compared to existing form of text summarization @ tech science @ @ @ right reserved @ 
1583,Algorithm for detecting polarity of opinions in laptop and restaurant domains,"The easy access to the Internet and the large amounts of information produced on the Web, Artificial Intelligence and more specifically the Natural Language Processing (NLP) provide information extraction mechanisms. The information found on the Internet is presented in most cases in an unstructured way, and examples of this are the social networks, source of access to opinions, products or services that society generates daily in these sites. This information can be a source for the application of the NLP, which is responsible for the automatic detection of feelings expressed in the texts and its classification according to the polarity they have; it is the area of analysis of feelings, also called opinion mining. This paper presents a study for the detection of polarity in a set of user opinions issued to Restaurants in Spanish and English. © 2021, Springer Nature Singapore Pte Ltd.",2021,Advances in Intelligent Systems and Computing,0,@ easy access to @ internet and @ @ amount of information produced on @ web artificial intelligence and more specifically @ natural language processing @ nlp @ provide information extraction mechanism @ @ information found on @ internet is presented in @ case in @ unstructured way and example of @ @ @ social network source of access to opinion product @ service @ society generates daily in @ site @ @ information @ @ a source @ @ application of @ nlp @ is responsible @ @ automatic detection of feeling expressed in @ text and @ classification according to @ polarity @ @ @ @ is @ area of analysis of feeling @ called opinion mining @ @ @ @ a study @ @ detection of polarity in a set of user opinion issued to restaurant in spanish and english @ @ nature singapore pte ltd @ 
1584,Short text similarity measurement methods: a review,"Short text similarity measurement methods play an important role in many applications within natural language processing. This paper reviews the research literature on short text similarity (STS) measurement method with the aim to (i) classify and give a broad overview of existing techniques; (ii) find out its strengths and weaknesses in terms of the domain the independence, language independence, requirement of semantic knowledge, corpus and training data, ability to identify semantic meaning, word order similarity and polysemy; and (iii) identify semantic knowledge and corpus resource that can be utilized for the STS measurement methods. Furthermore, our study also considers various issues such as the difference between the various text similarity methods and the difference between semantic knowledge sources and corpora for text similarity. Although there are a few review papers in this area, they focus mostly only on one/two existing techniques. Furthermore, existing review papers do not cover recent research. To the best of our knowledge, this is a comprehensive systematic literature review on this topic. The findings of this research can be as follows: It identified four semantic knowledge and eight corpus resources as external resources that can be classified into general-purpose and domain-specific. Furthermore, the existing techniques can be classified into string-based, corpus-based, knowledge-based and hybrid-based. Moreover, expert researchers can utilize this review as a benchmark as well as reference to the limitations of current techniques. The paper also identifies the open issues that can be considered as feasible opportunities for future research directions. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.",2021,Soft Computing,0,short text similarity measurement method play @ important role in many application within natural language processing @ @ @ review @ research literature on short text similarity @ sts @ measurement method @ @ aim to @ i @ classify and give a broad overview of existing technique @ @ ii @ find @ @ strength and weakness in term of @ domain @ independence language independence requirement of semantic knowledge corpus and training data ability to identify semantic meaning word order similarity and polysemy @ and @ iii @ identify semantic knowledge and corpus resource @ @ @ utilized @ @ sts measurement method @ furthermore @ study @ considers various issue @ a @ difference @ @ various text similarity method and @ difference @ semantic knowledge source and corpus @ text similarity @ although @ @ a @ review @ in @ area @ focus mostly only on @ @ existing technique @ furthermore existing review @ @ not cover recent research @ to @ best of @ knowledge @ is a comprehensive systematic literature review on @ topic @ @ finding of @ research @ @ a follows @ @ identified four semantic knowledge and eight corpus resource a external resource @ @ @ classified @ general-purpose and domain-specific @ furthermore @ existing technique @ @ classified @ string-based corpus-based knowledge-based and hybrid-based @ moreover expert researcher @ utilize @ review a a benchmark a well a reference to @ limitation of current technique @ @ @ @ identifies @ open issue @ @ @ considered a feasible opportunity @ future research direction @ springer-verlag gmbh germany part of @ nature @ 
1585,A new ensemble technique for recognize the long and shortest text,"In data mining, shorter-text analysis is performed more widely for many applications. Based on the syntax of the language, it is very difficult to analyze the short text with several traditional tools of natural language processing, and this is not applied correctly either. In short text, it is known that there are rare and insufficient data available with this text, and it is difficult to identify semantic knowledge. With the great noise and ambiguity of short texts, it is very difficult to find semantic knowledge. In this paper, it was proposed to replace the coefficient of similarity of cosine with the measure of similarity of Jaro–Winkler to obtain the coincidence of similarity between pairs of text (source text and target text). Jaro–Winkler does a much better job of determining the similarity of the strings because it takes order into account when using positional indices to estimate relevance. It is presumed that the performance of CACT driven by Jaro–Winkler with respect to one-to-many data links offers optimized performance compared to the operation of CACT driven by cosine. An evaluation of our proposed concept is sufficient as validation. © Springer Nature Singapore Pte Ltd 2021.",2021,Advances in Intelligent Systems and Computing,0,in data mining shorter-text analysis is performed more widely @ many application @ based on @ syntax of @ language @ is @ difficult to analyze @ short text @ several traditional tool of natural language processing and @ is not applied correctly either @ in short text @ is known @ @ @ rare and insufficient data available @ @ text and @ is difficult to identify semantic knowledge @ @ @ great noise and ambiguity of short text @ is @ difficult to find semantic knowledge @ in @ @ @ wa proposed to replace @ coefficient of similarity of cosine @ @ measure of similarity of jaro winkler to obtain @ coincidence of similarity @ pair of text @ source text and target text @ @ jaro winkler doe a much better job of determining @ similarity of @ string @ @ take order @ account @ @ positional index to estimate relevance @ @ is presumed @ @ performance of cact driven by jaro winkler @ respect to one-to-many data link offer optimized performance compared to @ operation of cact driven by cosine @ @ evaluation of @ proposed concept is sufficient a validation @ @ nature singapore pte ltd @ 
1587,Feedback seminar analysis - An introductory approach from an intelligent perspective,"This paper presents a mixture of Artificial Intelligence Topics: Automated Learning with Deep Learning and Natural Language Processing, as an advantage provided by intelligent and automated approaches in opinion mining. The main objective of this paper is to determine if a writing review can contain a positive or a negative opinion and to discover which are the best intelligent approach to detect this issue’s objective. Data set used in our experiments contains 180 collected opinions/reviews from students related to a particular seminar. Methods used in our experiments are Multinomial Naive Bayes, Support Vector Machine with the RBF kernel, linear Support Vector Classification and Recurrent Neural Network with Long-Short Term Memory Unit. Computed metrics used to evaluate methods performance are accuracy, precision, recall and F1-score. © Springer Nature Switzerland AG 2021.",2021,Advances in Intelligent Systems and Computing,0,@ @ @ a mixture of artificial intelligence topic @ automated learning @ deep learning and natural language processing a @ advantage provided by intelligent and automated approach in opinion mining @ @ main objective of @ @ is to determine if a writing review @ contain a positive @ a negative opinion and to discover @ @ @ best intelligent approach to detect @ issue s objective @ data set used in @ experiment contains collected opinion review @ student related to a particular seminar @ method used in @ experiment @ multinomial naive bayes support vector machine @ @ rbf kernel linear support vector classification and recurrent neural network @ long-short term memory unit @ computed metric used to evaluate method performance @ accuracy precision recall and f score @ @ nature switzerland ag @ 
1588,Text mining techniques for cyberbullying detection: State of the art,"The dramatic growth of social media during the last years has been associated with the emergence of a new bullying types. Platforms such as Facebook, Twitter, YouTube, and others are now privileged ways to disseminate all kinds of information. Indeed, communicating through social media without revealing the real identity has emerged an ideal atmosphere for cyberbullying, where people can pour out their hatred. Therefore, become very urgent to find automated methods to detect cyberbullying through text mining techniques. So, many researchers have recently investigated various approaches, and the number of scientific studies about this topic is growing very rapidly. Nonetheless, the methods are used to classify the phenomenon and evaluation methods are still under discussion. Subsequently, comparing the results between the studies and identifying their performance is still difficult. Therefore, the current systematic review has been conducted with the aim of survey the researches and studies that have been conducted so far by the research community in the topic of cyberbullying classification based on text language. In order to direct future studies on the topic to a more consistent and compatible perspective on recent works, we undertook a deep review of evaluation methods, features, dataset size, language, and dataset source of the latest research in this field. We made a choice to focus more on techniques that adopted neural networks and machine learning algorithms. After conducting systematic searches and applying the inclusion criteria, 16 different studies were included. It was found that the best accuracy was achieved when a deep learning approach is used particularly CNN approach. It was found also that, SVM is the most common classifier in both Arabic and Latin languages and outperformed the other classifiers. Also, the most widely used feature is N-Gram especially bigram and trigram. Furthermore, results show that Twitter is the main source for the collected datasets, and there are no unified datasets. There is also a shortage of studies in Arabic texts for cyberbullying identification in contrast with English texts. © 2021 ASTES Publishers. All rights reserved.",2021,"Advances in Science, Technology and Engineering Systems",0,@ dramatic growth of social medium @ @ last year ha @ associated @ @ emergence of a @ bullying type @ platform @ a facebook twitter youtube and others @ now privileged way to disseminate @ kind of information @ indeed communicating @ social medium without revealing @ real identity ha emerged @ ideal atmosphere @ cyberbullying @ people @ pour @ @ hatred @ therefore become @ urgent to find automated method to detect cyberbullying @ text mining technique @ @ many researcher @ recently investigated various approach and @ number of scientific study @ @ topic is growing @ rapidly @ nonetheless @ method @ used to classify @ phenomenon and evaluation method @ still @ discussion @ subsequently comparing @ @ @ @ study and identifying @ performance is still difficult @ therefore @ current systematic review ha @ conducted @ @ aim of survey @ research and study @ @ @ conducted @ far by @ research community in @ topic of cyberbullying classification based on text language @ in order to direct future study on @ topic to a more consistent and compatible perspective on recent work @ undertook a deep review of evaluation method feature dataset size language and dataset source of @ latest research in @ field @ @ made a choice to focus more on technique @ adopted neural network and machine learning algorithm @ @ conducting systematic search and applying @ inclusion criterion different study @ included @ @ wa found @ @ best accuracy wa achieved @ a deep learning approach is used particularly cnn approach @ @ wa found @ @ svm is @ @ common classifier in @ arabic and latin language and outperformed @ @ classifier @ @ @ @ widely used feature is n-gram especially bigram and trigram @ furthermore @ @ @ twitter is @ main source @ @ collected datasets and @ @ no unified datasets @ @ is @ a shortage of study in arabic text @ cyberbullying identification in contrast @ english text @ astes publisher @ @ right reserved @ 
1590,Suggestion Mining for Generating New Services and New Product Ideas from YELP Reviews,"YELP reviews strongly influence the business’s reputation and revenue as well as customers and new-users attitudes toward the acquisition of products and services. Due to the overwhelming number of customer reviews available on Web sites, usually, the decision-making process of customers, new-users, and business owners, is not associated with the daunting task of reading reviews online. Text mining can address the issue of summarizing huge volumes of unstructured data; furthermore, text mining can extract emotions, sentiments, and insights from texts written by customers. This study proposes a text mining approach to unravel suggestions from YELP reviews. This study shows that suggestion mining differentiates reviews with suggestions from reviews without suggestions and therefore can identify new services and new product ideas from customer reviews. © 2021, Springer Nature Switzerland AG.",2021,"Smart Innovation, Systems and Technologies",0,yelp review strongly influence @ @ s reputation and revenue a well a customer and new-users attitude toward @ acquisition of product and service @ due to @ overwhelming number of customer review available on web site usually @ decision-making process of customer new-users and @ owner is not associated @ @ daunting task of reading review online @ text mining @ address @ issue of summarizing huge volume of unstructured data @ furthermore text mining @ extract emotion sentiment and insight @ text written by customer @ @ study proposes a text mining approach to unravel suggestion @ yelp review @ @ study @ @ suggestion mining differentiates review @ suggestion @ review without suggestion and therefore @ identify @ service and @ product idea @ customer review @ @ nature switzerland ag @ 
1592,Big data augmentated business trend identification: the case of mobile commerce,"Identifying and monitoring business and technological trends are crucial for innovation and competitiveness of businesses. Exponential growth of data across the world is invaluable for identifying emerging and evolving trends. On the other hand, the vast amount of data leads to information overload and can no longer be adequately processed without the use of automated methods of extraction, processing, and generation of knowledge. There is a growing need for information systems that would monitor and analyse data from heterogeneous and unstructured sources in order to enable timely and evidence-based decision-making. Recent advancements in computing and big data provide enormous opportunities for gathering evidence on future developments and emerging opportunities. The present study demonstrates the use of text-mining and semantic analysis of large amount of documents for investigating in business trends in mobile commerce (m-commerce). Particularly with the on-going COVID-19 pandemic and resultant social isolation, m-commerce has become a large technology and business domain with ever growing market potentials. Thus, our study begins with a review of global challenges, opportunities and trends in the development of m-commerce in the world. Next, the study identifies critical technologies and instruments for the full utilization of the potentials in the sector by using the intelligent big data analytics system based on in-depth natural language processing utilizing text-mining, machine learning, science bibliometry and technology analysis. The results generated by the system can be used to produce a comprehensive and objective web of interconnected technologies, trends, drivers and barriers to give an overview of the whole landscape of m-commerce in one business intelligence (BI) data mart diagram. © 2021, Akadémiai Kiadó, Budapest, Hungary.",2021,Scientometrics,0,identifying and monitoring @ and technological trend @ crucial @ innovation and competitiveness of @ @ exponential growth of data across @ world is invaluable @ identifying emerging and evolving trend @ on @ @ hand @ vast amount of data lead to information overload and @ no longer @ adequately processed without @ use of automated method of extraction processing and generation of knowledge @ @ is a growing need @ information system @ would monitor and analyse data @ heterogeneous and unstructured source in order to enable timely and evidence-based decision-making @ recent advancement in computing and big data provide enormous opportunity @ gathering evidence on future development and emerging opportunity @ @ @ study demonstrates @ use of text-mining and semantic analysis of @ amount of document @ investigating in @ trend in mobile commerce @ m-commerce @ @ particularly @ @ on-going covid pandemic and resultant social isolation m-commerce ha become a @ technology and @ domain @ ever growing market potential @ thus @ study begin @ a review of global challenge opportunity and trend in @ development of m-commerce in @ world @ next @ study identifies critical technology and instrument @ @ full utilization of @ potential in @ sector by @ @ intelligent big data analytics system based on in-depth natural language processing utilizing text-mining machine learning science bibliometry and technology analysis @ @ @ generated by @ system @ @ used to produce a comprehensive and objective web of interconnected technology trend driver and barrier to give @ overview of @ whole landscape of m-commerce in @ @ intelligence @ bi @ data mart diagram @ akadémiai kiadó budapest hungary @ 
1593,Opinion Mining to Aid User Acceptance Testing for Open Beta Versions,"Social media enables the sharing of opinions, ideas, interests, thoughts, hobbies, etc., by creating social platforms such as Twitter, Facebook, TripAdvisor, Yelp, Rooter, Goodreads, etc. The above sharing generates a lot of information over various social media platforms, which is used in different research areas. Among the many research areas, one is opinion mining, which identifies and extracts subjective information from source materials by using text analysis, natural language processing, and computational linguistics. The application of opinion mining is abundantly seen in the areas of marketing, governance, tourism industry, etc. However, the application of opinion mining is not much explored in the area of user acceptance testing. Therefore, in this research article, we propose a model for opinion mining and use it as a supporting tool for user acceptance testing. Specifically, we use the opinion mining on “Twitter” tweets to analyze the response of an Open Beta, i.e., Public Beta versions of the software to quantify acceptance criteria qualities. Hence, we try to inculcate opinion mining in earlier stages of software development to aid user acceptance testing in order to get the user feedback as accurately as possible to make change in the concerned software before its final version is released. The model is implemented and evaluated for Open Beta versions of IOS 13 and AndroidQ using machine learning techniques such as decision tree, K-nearest neighbor, multilayer perceptron, and logistic regression. Our evaluation shows that we achieved the best result with K-nearest neighbor having an accuracy of 97.12%, which is 14.37% higher as compared to the highest accuracy ever obtained in this area. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",2021,Advances in Intelligent Systems and Computing,0,social medium enables @ sharing of opinion idea interest thought hobby etc @ by creating social platform @ a twitter facebook tripadvisor yelp rooter goodreads etc @ @ @ sharing generates a lot of information @ various social medium platform @ is used in different research area @ among @ many research area @ is opinion mining @ identifies and extract subjective information @ source material by @ text analysis natural language processing and computational linguistics @ @ application of opinion mining is abundantly seen in @ area of marketing governance tourism industry etc @ however @ application of opinion mining is not much explored in @ area of user acceptance testing @ therefore in @ research article @ propose a model @ opinion mining and use @ a a supporting tool @ user acceptance testing @ specifically @ use @ opinion mining on twitter tweet to analyze @ response of @ open beta i @ e @ public beta version of @ software to quantify acceptance criterion quality @ hence @ try to inculcate opinion mining in earlier stage of software development to aid user acceptance testing in order to get @ user feedback a accurately a possible to make change in @ concerned software @ @ final version is released @ @ model is implemented and evaluated @ open beta version of io and androidq @ machine learning technique @ a decision tree k-nearest neighbor multilayer perceptron and logistic regression @ @ evaluation @ @ @ achieved @ best @ @ k-nearest neighbor @ @ accuracy of @ @ is @ higher a compared to @ highest accuracy ever obtained in @ area @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1594,A Health-Related Study from Food Online Reviews. The Case of Gluten-Free Foods,"Concerning the field of Health Sciences, one of the trends with the greatest impact today is related to the feeding and consumption of foods intended for chronic patients with a nutritional association. According to this, the gluten-free diet is one of the diets with a today’s fastest-growing due to their health therapy associations for different diseases and because of more and more people are freely choosing to follow a gluten-free diet. On this subject, e-commerce platforms are destined to be a key point for the sale of this kind of products due to: (i) the change in the business model caused by the electronic platforms and (ii) the expected increase consumption of disease designed special foods. For this reason, the experiences written by chronic disease patients and consumers in these platforms have become increasingly relevant. In this sense, the present work applies biomedical text mining methods and demographic knowledge inference techniques to a large number of gluten-free foods consumer’s experiences to discover a wide range of health-related topics associated to gluten-free products, alimentary social trends and demographics differences. The results of these analyses show significant trends over time; differences between males and females consumption and a wide range of health-related conclusions, such as: why consumers start a free gluten diet or what are the most discussed diseases and adverse reactions related to these products. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.",2021,Advances in Intelligent Systems and Computing,0,concerning @ field of health science @ of @ trend @ @ greatest impact today is related to @ feeding and consumption of food intended @ chronic patient @ a nutritional association @ according to @ @ gluten-free diet is @ of @ diet @ a today s fastest-growing due to @ health therapy association @ different disease and @ of more and more people @ freely choosing to follow a gluten-free diet @ on @ subject e-commerce platform @ destined to @ a key point @ @ sale of @ kind of product due to @ @ i @ @ change in @ @ model caused by @ electronic platform and @ ii @ @ expected increase consumption of disease designed special food @ @ @ reason @ experience written by chronic disease patient and consumer in @ platform @ become increasingly relevant @ in @ sense @ @ work applies biomedical text mining method and demographic knowledge inference technique to a @ number of gluten-free food consumer s experience to discover a wide range of health-related topic associated to gluten-free product alimentary social trend and demographic difference @ @ @ of @ analysis @ significant trend @ time @ difference @ male and female consumption and a wide range of health-related conclusion @ a @ @ consumer start a free gluten diet @ @ @ @ @ discussed disease and adverse reaction related to @ product @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature switzerland ag @ 
1596,An Arabic Multi-source News Corpus: Experimenting on Single-document Extractive Summarization,"Automatic text summarization is considered as an important task in various fields in natural language processing such as information retrieval. It is a process of automatically generating a text representation. Text summarization can be a solution to the problem of information overload. Hence, with the large amount of information available on the Internet, the presentation of a document by a summary helps to get the most relevant result of a search. We propose in this paper a new free Arabic structured corpus in the standard XML TREC format. ANT corpus v2.1 is collected using RSS feeds from different news sources. This corpus is useful for multiple text mining purposes such as generic text summarization, clustering or classification. We test this corpus for an unsupervised single-document extractive summarization using statistical and graph-based language-independent summarizers such as LexRank, TextRank, Luhn and LSA. We investigate the sensitivity of the summarization process to the stemming and stop words removal steps. We evaluate these summarizers performance by comparing the extracted texts fragments to the abstracts existing in ANT corpus v2.1 using ROUGE and BLEU metrics. Experimental results show that LexRank summarizer has achieved the best scores for the ROUGE metric using the stop words removal scenario. © 2021, King Fahd University of Petroleum & Minerals.",2021,Arabian Journal for Science and Engineering,0,automatic text summarization is considered a @ important task in various field in natural language processing @ a information retrieval @ @ is a process of automatically generating a text representation @ text summarization @ @ a solution to @ problem of information overload @ hence @ @ @ amount of information available on @ internet @ presentation of a document by a summary help to get @ @ relevant @ of a search @ @ propose in @ @ a @ free arabic structured corpus in @ standard xml trec format @ ant corpus v @ is collected @ r feed @ different news source @ @ corpus is useful @ multiple text mining purpose @ a generic text summarization clustering @ classification @ @ test @ corpus @ @ unsupervised single-document extractive summarization @ statistical and graph-based language-independent summarizers @ a lexrank textrank luhn and lsa @ @ investigate @ sensitivity of @ summarization process to @ stemming and stop word removal step @ @ evaluate @ summarizers performance by comparing @ extracted text fragment to @ abstract existing in ant corpus v @ @ rouge and bleu metric @ experimental @ @ @ lexrank summarizer ha achieved @ best score @ @ rouge metric @ @ stop word removal scenario @ king fahd university of petroleum mineral @ 
1598,Multiscale Laplacian graph kernel combined with lexico-syntactic patterns for biomedical event extraction from literature,"Bio-event extraction is an extensive research area in the field of biomedical text mining, this focuses on elaborating relationships between biomolecules and can provide various aspects of their nature. Bio-event extraction plays a vital role in biomedical literature mining applications such as biological network construction, pathway curation, and drug repurposing. Extracting biological events automatically is a difficult task because of the uncertainty and assortment of natural language processing such as negations and speculations, which provides further room for the development of feasible methodologies. This paper presents a hybrid approach that integrates an ensemble-learning framework by combining a Multiscale Laplacian Graph kernel and a feature-based linear kernel, using a pattern-matching engine to identify biomedical events with arguments. This graph-based kernel not only captures the topological relationships between the individual event nodes but also identifies the associations among the subgraphs for complex events. In addition, the lexico-syntactic patterns were used to automatically discover the semantic role of each word in the sentence. For performance evaluation, we used the gold standard corpora, namely BioNLP-ST (2009, 2011, and 2013) and GENIA-MK. Experimental results show that our approach achieved better performance than other state-of-the-art systems. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",2021,Knowledge and Information Systems,0,bio-event extraction is @ extensive research area in @ field of biomedical text mining @ focus on elaborating relationship @ biomolecules and @ provide various aspect of @ nature @ bio-event extraction play a vital role in biomedical literature mining application @ a biological network construction pathway curation and drug repurposing @ extracting biological event automatically is a difficult task @ of @ uncertainty and assortment of natural language processing @ a negation and speculation @ provides @ room @ @ development of feasible methodology @ @ @ @ a hybrid approach @ integrates @ ensemble-learning framework by combining a multiscale laplacian graph kernel and a feature-based linear kernel @ a pattern-matching engine to identify biomedical event @ argument @ @ graph-based kernel not only capture @ topological relationship @ @ individual event node @ @ identifies @ association among @ subgraphs @ complex event @ in addition @ lexico-syntactic pattern @ used to automatically discover @ semantic role of @ word in @ sentence @ @ performance evaluation @ used @ gold standard corpus namely bionlp-st @ and @ and genia-mk @ experimental @ @ @ @ approach achieved better performance @ @ state-of-the-art system @ springer-verlag london ltd @ part of @ nature @ 
1600,Brave Men and Emotional Women: Analyzing Gender Bias in Bollywood Songs,"Stereotypes exist in several sections of society including various means of popular entertainment. We believe that Bollywood songs are no exception as there has been a certain change in the characteristics of the songs’ lyrics over the past few years. Hence, to computationally study Bollywood’s songs lyrics from the Hindi movie industry, in this paper, we examine their style of writing and the presence of any biases. We analyze the changes in the songs’ lyrics over time and quantitatively show the change in the pattern by evaluating the rank of certain sensitive words in the songs. We calculate embeddings for the lyrical vocabulary using Word2Vec, FastText, and GloVe algorithms and use the WEAT similarity score to show that Hindi songs indeed suffer from racial and gender bias. The metrics we obtain can be further used for more formal problems of music recommendation, lyrics generations and popularity prediction. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",2021,Lecture Notes on Data Engineering and Communications Technologies,0,stereotype exist in several section of society including various mean of popular entertainment @ @ believe @ bollywood song @ no exception a @ ha @ a certain change in @ characteristic of @ song lyric @ @ past @ year @ hence to computationally study bollywood s song lyric @ @ hindi movie industry in @ @ @ examine @ style of writing and @ presence of @ bias @ @ analyze @ change in @ song lyric @ time and quantitatively @ @ change in @ pattern by evaluating @ rank of certain sensitive word in @ song @ @ calculate embeddings @ @ lyrical vocabulary @ word vec fasttext and glove algorithm and use @ weat similarity score to @ @ hindi song indeed suffer @ racial and gender bias @ @ metric @ obtain @ @ @ used @ more formal problem of music recommendation lyric generation and popularity prediction @ @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1601,Arabic Sexist Comments Detection in Youtube: A Context-Aware Opinion Analysis Approach,"In this chapter, we present an approach to automatize the assessment of attitudes toward violence against women and women’s rights, by analyzing Youtube comments, written in Arabic. More specifically, we propose a context-aware approach to opinion analysis in comments by taking into account the polarity of videos to which comments are associated. We build a training set and use it to train a classifier that predicts videos’ polarity. The accuracy and precision of the produced video classifier are 98 % and 94 %, respectively. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",2021,Advances in Intelligent Systems and Computing,0,in @ chapter @ @ @ approach to automatize @ assessment of attitude toward violence @ woman and woman s right by analyzing youtube comment written in arabic @ more specifically @ propose a context-aware approach to opinion analysis in comment by taking @ account @ polarity of video to @ comment @ associated @ @ build a training set and use @ to train a classifier @ predicts video polarity @ @ accuracy and precision of @ produced video classifier @ and respectively @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1602,Sentiment analysis on movie review using deep learning rnn method,"The usage of social media grows rapidly because of the functionality like easy to use and it will also allow user to connect with all around the globe to share the ideas. It is desired to automatically use the information which is user’s interest. One of the meaningful information that is derived from the social media sites are sentiments. Sentiment analysis is used for finding relevant documents, overall sentiment, and relevant sections; quantifying the sentiment; and aggregating all sentiments to form an overview. Sentiment analysis for movie review classification is useful to analyze the information in the form of number of reviews where opinions are either positive or negative. In this paper we had applied the deep learning-based classification algorithm RNN, measured the performance of the classifier based on the pre-process of data, and obtained 94.61% accuracy. Here we had used RNN algorithm instead of machine learning algorithm because machine learning algorithm works only in single layer while RNN algorithm works on multilayer that gives you better output as compared to machine learning. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2021.",2021,Advances in Intelligent Systems and Computing,0,@ usage of social medium grows rapidly @ of @ functionality like easy to use and @ @ @ allow user to connect @ @ around @ globe to share @ idea @ @ is desired to automatically use @ information @ is user s interest @ @ of @ meaningful information @ is derived @ @ social medium site @ sentiment @ sentiment analysis is used @ finding relevant document overall sentiment and relevant section @ quantifying @ sentiment @ and aggregating @ sentiment to form @ overview @ sentiment analysis @ movie review classification is useful to analyze @ information in @ form of number of review @ opinion @ either positive @ negative @ in @ @ @ @ applied @ deep learning-based classification algorithm rnn measured @ performance of @ classifier based on @ pre-process of data and obtained @ accuracy @ @ @ @ used rnn algorithm instead of machine learning algorithm @ machine learning algorithm work only in single layer @ rnn algorithm work on multilayer @ give @ better output a compared to machine learning @ @ editor @ s @ @ if applicable @ and @ author @ s @ @ exclusive license to @ nature singapore pte ltd @ 
1604,"AHFE Virtual Conferences on Software and Systems Engineering, and Artificial Intelligence and Social Computing, 2020","The proceedings contain 82 papers. The special focus in this conference is on Software and Systems Engineering. The topics include: Top-Level Design of Intelligent Video Surveillance System for Abandoned Objects; detection of Human Trafficking Ads in Twitter Using Natural Language Processing and Image Processing; text Mining in Smart Cities to Identify Urban Events and Public Service Problems; deep Learning-Based Creative Intention Understanding and Color Suggestions for Illustration; comparison of Probability Distributions for Evolving Artificial Neural Networks Using Bat Algorithm; deep Neural Networks for Grid-Based Elusive Crime Prediction Using a Private Dataset Obtained from Japanese Municipalities; Remote Sensing, Heat Island Effect and Housing Price Prediction via AutoML; A Framework for Selecting Machine Learning Models Using TOPSIS; application-Oriented Approach for Detecting Cyberaggression in Social Media; fighting Cyberbullying: An Analysis of Algorithms Used to Detect Harassing Text Found on YouTube; a Study of Social Media Behaviors and Mental Health Wellbeing from a Privacy Perspective; risk Analysis for Ethical, Legal and Social Implications of Information and Communication Technologies in the Forestry Sector; Traffic Scene Detection Based on YOLOv3; social Networks’ Factors Driving Consumer Restaurant Choice: An Exploratory Analysis; conversational Advisors – Are These Really What Users Prefer? User Preferences, Lessons Learned and Design Recommended Practices; Investigating Users’ Perceived Credibility of Real and Fake News Posts in Facebook’s News Feed: UK Case Study; future Trends in Voice User Interfaces; artificial Intelligence Enabled User Experience Research; product Sampling Based on Remarks of Customs in Online Shopping Websites for Quality Evaluation; dynamic Instructions for Lock-Out Tag-Out.",2021,Advances in Intelligent Systems and Computing,0,@ proceeding contain @ @ @ special focus in @ conference is on software and system engineering @ @ topic include @ top-level design of intelligent video surveillance system @ abandoned object @ detection of human trafficking ad in twitter @ natural language processing and image processing @ text mining in smart city to identify urban event and public service problem @ deep learning-based creative intention understanding and color suggestion @ illustration @ comparison of probability distribution @ evolving artificial neural network @ bat algorithm @ deep neural network @ grid-based elusive crime prediction @ a private dataset obtained @ japanese municipality @ remote sensing heat island effect and housing price prediction via automl @ a framework @ selecting machine learning model @ topsis @ application-oriented approach @ detecting cyberaggression in social medium @ fighting cyberbullying @ @ analysis of algorithm used to detect harassing text found on youtube @ a study of social medium behavior and mental health wellbeing @ a privacy perspective @ risk analysis @ ethical legal and social implication of information and communication technology in @ forestry sector @ traffic scene detection based on yolov @ social network factor driving consumer restaurant choice @ @ exploratory analysis @ conversational advisor @ @ really @ user prefer @ user preference lesson learned and design recommended practice @ investigating user perceived credibility of real and fake news post in facebook s news feed @ uk case study @ future trend in voice user interface @ artificial intelligence enabled user experience research @ product sampling based on remark of custom in online shopping website @ quality evaluation @ dynamic instruction @ lock-out tag-out @ 
1605,Content Tree Word Embedding for document representation,"Only humans can understand and comprehend the actual meaning that underlies natural written language, whereas machines can form semantic relationships only after humans have provided the parameters that are necessary to model the meaning. To enable computer models to access the underlying meaning in written language, accurate and sufficient document representation is crucial. Recently, word embedding approaches have drawn much attention in text mining research. One of the main benefits of such approaches is the use of global corpuses with the generation of pre-trained word vectors. Although very effective, these approaches have their disadvantages. Relying only on pre-trained word vectors may neglect the local context and increase word ambiguity. In this study, a new approach, Content Tree Word Embedding (CTWE), is introduced to mitigate the risk of word ambiguity and inject a local context into globally pre-trained word vectors. CTWE is basically a framework for document representation while using word embedding feature learning. The CTWE structure is locally learned from training data and ultimately represents the local context. While CTWE is constructed, each word vector is updated based on its location in the content tree. For the task of classification, the results show an improvement in F-score and accuracy measures when using two deep learning-based word embedding approaches, namely GloVe and Word2Vec. © 2017 Elsevier Ltd",2017,Expert Systems with Applications,21,only human @ understand and comprehend @ actual meaning @ underlies natural written language whereas machine @ form semantic relationship only @ human @ provided @ parameter @ @ necessary to model @ meaning @ to enable computer model to access @ underlying meaning in written language accurate and sufficient document representation is crucial @ recently word embedding approach @ drawn much attention in text mining research @ @ of @ main benefit of @ approach is @ use of global corpus @ @ generation of pre-trained word vector @ although @ effective @ approach @ @ disadvantage @ relying only on pre-trained word vector may neglect @ local context and increase word ambiguity @ in @ study a @ approach content tree word embedding @ ctwe @ is introduced to mitigate @ risk of word ambiguity and inject a local context @ globally pre-trained word vector @ ctwe is basically a framework @ document representation @ @ word embedding feature learning @ @ ctwe structure is locally learned @ training data and ultimately represents @ local context @ @ ctwe is constructed @ word vector is updated based on @ location in @ content tree @ @ @ task of classification @ @ @ @ improvement in f-score and accuracy measure @ @ @ deep learning-based word embedding approach namely glove and word vec @ @ ltd
1606,Pattern Based Comprehensive Urdu Stemmer and Short Text Classification,"Urdu language is used by approximately 200 million people for spoken and written communications. The bulk of unstructured Urdu textual data is available in the world. We can employ data mining techniques to extract useful information from such a large, potentially informative base data. There are many text processing systems available to process unstructured textual data. However, these systems are mostly language specific with the large proportion of systems applicable to English text. This is primarily due to language-dependent preprocessing systems, mainly the stemming requirement. Stemming is a vital preprocessing step in the text mining process and its primary aim is to reduce grammatical words form, e.g., parts of speech, gender, tense, and so on, to their root form. In the proposed work, we have developed a rule-based comprehensive stemming method for Urdu text. This proposed Urdu stemmer has the ability to generate the stem of Urdu words as well as loan words that belong to borrowed languages, such as Arabic, Persian, and Turkish, by removing prefix, infix, and suffix from the words. In the proposed stemming technique, we introduced six novel Urdu infix words classes and a minimum word length rule to generate the stem of Urdu text. In order to cope with the challenge of Urdu infix stemming, we have developed infix stripping rules for introduced infix words classes and generic stemming rules for prefix and suffix stemming. We also present a probabilistic classification approach to classify Urdu short text. Different experiments are performed to demonstrate the effectiveness and efficacy of the proposed approach. Comparison with existing state-of-the art approaches is also made. Stemming accuracy results demonstrate the adoptability of the proposed stemming approach for a variety text processing applications. © 2013 IEEE.",2017,IEEE Access,11,urdu language is used by approximately million people @ spoken and written communication @ @ bulk of unstructured urdu textual data is available in @ world @ @ @ employ data mining technique to extract useful information @ @ a @ potentially informative base data @ @ @ many text processing system available to process unstructured textual data @ however @ system @ mostly language specific @ @ @ proportion of system applicable to english text @ @ is primarily due to language-dependent preprocessing system mainly @ stemming requirement @ stemming is a vital preprocessing step in @ text mining process and @ primary aim is to reduce grammatical word form e @ g @ part of speech gender tense and @ on to @ root form @ in @ proposed work @ @ developed a rule-based comprehensive stemming method @ urdu text @ @ proposed urdu stemmer ha @ ability to generate @ stem of urdu word a well a loan word @ belong to borrowed language @ a arabic persian and turkish by removing prefix infix and suffix @ @ word @ in @ proposed stemming technique @ introduced six novel urdu infix word class and a minimum word length rule to generate @ stem of urdu text @ in order to cope @ @ challenge of urdu infix stemming @ @ developed infix stripping rule @ introduced infix word class and generic stemming rule @ prefix and suffix stemming @ @ @ @ a probabilistic classification approach to classify urdu short text @ different experiment @ performed to demonstrate @ effectiveness and efficacy of @ proposed approach @ comparison @ existing state-of-the art approach is @ made @ stemming accuracy @ demonstrate @ adoptability of @ proposed stemming approach @ a variety text processing application @ @ @ 
1623,Knowledge guided short-text classification for healthcare applications,"The need for short-text classification arises in many text mining applications particularly health care applications. In such applications shorter texts mean linguistic ambiguity limits the semantic expression, which in turns would make typical methods fail to capture the exact semantics of the scarce words. This is particularly true in health care domains when the text contains domain-specific or infrequently appearing words, whose embedding can not be easily learned due to the lack of training data. Deep neural network has shown great potentials in boost the performance of such problems according to its strength on representation capacity. In this paper, we propose a bidirectional long short-term memory (BI-LSTM) recurrent network to address the short-text classification problem that can be used in two settings. Firstly when a knowledge dictionary is available we adopt the well-known attention mechanism to guide the training of network using the domain knowledge in the dictionary. Secondly, to address the cases when domain knowledge dictionary is not available, we present a multi-task model to jointly learn the domain knowledge dictionary and do the text classification task simultaneously. We apply our method to a real-world interactive healthcare system and an extensively public available ATIS dataset. The results show that our model can positively grasp the key point of the text and significantly outperforms many state-of-the-art baselines. © 2017 IEEE.",2017,"Proceedings - IEEE International Conference on Data Mining, ICDM",3,@ need @ short-text classification arises in many text mining application particularly health care application @ in @ application shorter text mean linguistic ambiguity limit @ semantic expression @ in turn would make typical method fail to capture @ exact semantics of @ scarce word @ @ is particularly true in health care domain @ @ text contains domain-specific @ infrequently appearing word whose embedding @ not @ easily learned due to @ lack of training data @ deep neural network ha @ great potential in boost @ performance of @ problem according to @ strength on representation capacity @ in @ @ @ propose a bidirectional long short-term memory @ bi-lstm @ recurrent network to address @ short-text classification problem @ @ @ used in @ setting @ firstly @ a knowledge dictionary is available @ adopt @ well-known attention mechanism to guide @ training of network @ @ domain knowledge in @ dictionary @ secondly to address @ case @ domain knowledge dictionary is not available @ @ a multi-task model to jointly learn @ domain knowledge dictionary and @ @ text classification task simultaneously @ @ apply @ method to a real-world interactive healthcare system and @ extensively public available atis dataset @ @ @ @ @ @ model @ positively grasp @ key point of @ text and significantly outperforms many state-of-the-art baseline @ @ @ 
1640,TMAP: Discovering relevant API methods through text mining of API documentation,"Developers often migrate their applications to support various platform/programming-language application programming interfaces (APIs) to retain existing users and to attract new users. To migrate an application written using 1 API (source) to another API (target), a developer must know how the methods in the source API map to the methods in the target API. Given that a typical platform or language exposes a large number of API methods, manually discovering API mappings is prohibitively resource-intensive and may be error prone. The goal of this research is to support software developers in migrating an application from a source API to a target API by automatically discovering relevant method mappings across APIs using text mining on the natural language API method descriptions. This paper proposes text mining based approach (TMAP) to discover relevant API mappings. To evaluate our approach, we used TMAP to discover API mappings for 15 classes across (1) Java and C# API; and (2) Java ME and Android API. We compared the discovered mappings with state-of-the-art source code analysis-based approaches: Rosetta and StaMiner. Our results indicate that TMAP on average found relevant mappings for 56% and 57% more methods compared to the Rosetta and the StaMiner approaches, respectively. Copyright © 2017 John Wiley & Sons, Ltd.",2017,Journal of Software: Evolution and Process,5,developer often migrate @ application to support various platform programming-language application programming interface @ apis @ to retain existing user and to attract @ user @ to migrate @ application written @ api @ source @ to another api @ target @ a developer must know @ @ method in @ source api map to @ method in @ target api @ given @ a typical platform @ language expose a @ number of api method manually discovering api mapping is prohibitively resource-intensive and may @ error prone @ @ goal of @ research is to support software developer in migrating @ application @ a source api to a target api by automatically discovering relevant method mapping across apis @ text mining on @ natural language api method description @ @ @ proposes text mining based approach @ tmap @ to discover relevant api mapping @ to evaluate @ approach @ used tmap to discover api mapping @ class across @ @ java and c api @ and @ @ java me and android api @ @ compared @ discovered mapping @ state-of-the-art source code analysis-based approach @ rosetta and staminer @ @ @ indicate @ tmap on average found relevant mapping @ and more method compared to @ rosetta and @ staminer approach respectively @ @ john wiley son ltd @ 
1641,A customizable pipeline for social media text normalization,"Social networks are persistently generating text-based data that encapsulate vast amounts of knowledge. However, the presence of non-standard terms and misspellings in texts originating from social networks poses a crucial challenge for natural language processing and machine learning systems that attempt to mine this knowledge. To address this problem, we propose a sequential, modular, and hybrid pipeline for social media text normalization. In the first phase, text preprocessing techniques and social media-specific vocabularies gathered from publicly available sources are used to transform, with high precision, out-of-vocabulary terms into in-vocabulary terms. A sequential language model, generated using the partially normalized texts from the first phase, is then utilized to normalize short, high-frequency, ambiguous terms. A supervised learning module is employed to normalize terms based on a manually annotated training corpus. Finally, a tunable, distributed language model-based backoff module at the end of the pipeline enables further customization of the system to specific domains of text. We performed intrinsic evaluations of the system on a publicly available domain-independent dataset from Twitter, and our system obtained an F-score of 0.836, outperforming other benchmark systems for the task. We further performed brief, task-oriented evaluations of the system to illustrate the customizability of the system to domain-specific tasks and the effects of normalization on downstream applications. The modular design enables the easy customization of the system to distinct types domain-specific social media text, in addition to its off-the-shelf application to generic social media text. © 2017, Springer-Verlag GmbH Austria.",2017,Social Network Analysis and Mining,11,social network @ persistently generating text-based data @ encapsulate vast amount of knowledge @ however @ presence of non-standard term and misspelling in text originating @ social network pose a crucial challenge @ natural language processing and machine learning system @ attempt to mine @ knowledge @ to address @ problem @ propose a sequential modular and hybrid pipeline @ social medium text normalization @ in @ first phase text preprocessing technique and social media-specific vocabulary gathered @ publicly available source @ used to transform @ high precision out-of-vocabulary term @ in-vocabulary term @ a sequential language model generated @ @ partially normalized text @ @ first phase is @ utilized to normalize short high-frequency ambiguous term @ a supervised learning module is employed to normalize term based on a manually annotated training corpus @ finally a tunable distributed language model-based backoff module at @ end of @ pipeline enables @ customization of @ system to specific domain of text @ @ performed intrinsic evaluation of @ system on a publicly available domain-independent dataset @ twitter and @ system obtained @ f-score of @ outperforming @ benchmark system @ @ task @ @ @ performed brief task-oriented evaluation of @ system to illustrate @ customizability of @ system to domain-specific task and @ effect of normalization on downstream application @ @ modular design enables @ easy customization of @ system to distinct type domain-specific social medium text in addition to @ off-the-shelf application to generic social medium text @ springer-verlag gmbh austria @ 
1644,Liberation of public data: Exploring central themes in open government data and freedom of information research,"This paper conducts a comparative literature survey of Open Government Data (OGD) and Freedom of Information (FOI), with a view to tracking the central themes in the two civil society campaigns. With seeming similarities and a growing popularity in research, the major themes framing research on the two movements have not clearly emerged. Topic modelling, text mining and document analysis methods are used to extract the themes as well as key named entities. The topics are subsequently labeled and with expert guidance, their semantic meaning are provided. The results indicate that the major theme in FOI research borders on issues relating to disclosure, publishing, access and cost of requests. On the other hand, themes in OGD research have largely centered on technology and related concepts. The approach also helped in determining key similarities and differences in the two campaigns as reported in research. © 2017 Elsevier Ltd",2017,International Journal of Information Management,19,@ @ conduct a comparative literature survey of open government data @ ogd @ and freedom of information @ foi @ @ a view to tracking @ central theme in @ @ civil society campaign @ @ seeming similarity and a growing popularity in research @ major theme framing research on @ @ movement @ not clearly emerged @ topic modelling text mining and document analysis method @ used to extract @ theme a well a key named entity @ @ topic @ subsequently labeled and @ expert guidance @ semantic meaning @ provided @ @ @ indicate @ @ major theme in foi research border on issue relating to disclosure publishing access and cost of request @ on @ @ hand theme in ogd research @ largely centered on technology and related concept @ @ approach @ helped in determining key similarity and difference in @ @ campaign a reported in research @ @ ltd
1645,Decision support from financial disclosures with deep neural networks and transfer learning,"Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives. © 2017 Elsevier B.V.",2017,Decision Support Systems,79,company disclosure greatly aid in @ process of financial decision-making @ therefore @ @ consulted by financial investor and automated trader @ exercising ownership in stock @ @ human @ usually able to correctly interpret @ content @ @ is rarely true of computerized decision support system @ struggle @ @ complexity and ambiguity of natural language @ a possible remedy is represented by deep learning @ overcomes several shortcoming of traditional method of text mining @ @ instance recurrent neural network @ a long short-term memory employ hierarchical structure together @ a @ number of hidden layer to automatically extract feature @ ordered sequence of word and capture highly non-linear relationship @ a context-dependent meaning @ however deep learning ha only recently started to receive traction possibly @ @ performance is largely untested @ hence @ @ study @ use of deep neural network @ financial decision support @ @ additionally experiment @ transfer learning in @ @ pre-train @ network on a different corpus @ a length of @ million word @ @ @ reveal a higher directional accuracy a compared to traditional machine learning @ predicting stock price movement in response to financial disclosure @ @ work thereby help to highlight @ @ value of deep learning and provides recommendation to practitioner and executive @ @ b @ v @ 
1647,Characterizing evolving behavior of context vectors for context based clustering,"Characterizing evolving behavior of document vectors helps in identifying similarity between text documents. As document vectors contain terms and their importances in documents, discovering association and disassociation between terms is very important. This paper introduces characterization of evolving behavior of document vectors to identify similar and dissimilar segments in document vectors. This approach is particularly suitable where document vectors contain similar patterns of term occurrences but the patterns could be away from each other with regard to distance. The main objective of this paper is to capture evolving structure of context vector, document vector of contextually related terms, for discovering similarity between them. Context vector reduces the size of document vector from 6 to 12.57%. Evaluation is done by clustering the documents using Unweighted Pair Group Method with Arithmetic Mean with standard datasets. This results in formation of clusters with better entropy and purity. Mann–Whitney–Wilcoxon U test demonstrates statistically significant quality enhancement. © 2016, Springer-Verlag Berlin Heidelberg.",2017,Evolving Systems,0,characterizing evolving behavior of document vector help in identifying similarity @ text document @ a document vector contain term and @ importance in document discovering association and disassociation @ term is @ important @ @ @ introduces characterization of evolving behavior of document vector to identify similar and dissimilar segment in document vector @ @ approach is particularly suitable @ document vector contain similar pattern of term occurrence @ @ pattern could @ away @ @ @ @ regard to distance @ @ main objective of @ @ is to capture evolving structure of context vector document vector of contextually related term @ discovering similarity @ @ @ context vector reduces @ size of document vector @ to @ @ evaluation is done by clustering @ document @ unweighted pair group method @ arithmetic mean @ standard datasets @ @ @ in formation of cluster @ better entropy and purity @ mann whitney wilcoxon u test demonstrates statistically significant quality enhancement @ springer-verlag @ @ @ 
1648,Explicitly and implicitly exploiting the hierarchical structure for mining website interests on news events,"After a news event, many different websites publish coverage of that event, each expressing their own unique commentary, perspectives, and viewpoints. Websites form around a specific set of interests to cater to different audiences, and discovering these interests can help audiences C especially people and organizations that are interested in news C select the most appropriate websites to use as their sources of information. This paper presents three methods for formally defining and mining a websites interests, each of which is explicitly or implicitly based on a hierarchial structure: website-webpage-keyword. The first, and most straightforward, method explicitly uses keyword-layer network communities and the mapping relations between websites and keywords. The second method expands upon the first method with an iterative algorithm that combines both the mapping relations and the network relations from the website-webpage-keyword structure to further refine the keyword-layer network communities. In the third method, a website topic model implicitly captures the mapping relations among the websites, webpages, and keywords. The performance of three proposed methods in website interest mining is compared using a bespoke evaluation metric. The experimental results show that the iterative procedure designed in the second method is able to improve website interest mining performance, and the website topic model in the third method achieves the best performance among the three methods. © 2017 Elsevier Inc.",2017,Information Sciences,5,@ a news event many different website publish coverage of @ event @ expressing @ @ unique commentary perspective and viewpoint @ website form around a specific set of interest to cater to different audience and discovering @ interest @ help audience c especially people and organization @ @ interested in news c select @ @ appropriate website to use a @ source of information @ @ @ @ three method @ formally defining and mining a website interest @ of @ is explicitly @ implicitly based on a hierarchial structure @ website-webpage-keyword @ @ first and @ straightforward method explicitly us keyword-layer network community and @ mapping relation @ website and keywords @ @ second method expands upon @ first method @ @ iterative algorithm @ combine @ @ mapping relation and @ network relation @ @ website-webpage-keyword structure to @ refine @ keyword-layer network community @ in @ third method a website topic model implicitly capture @ mapping relation among @ website webpage and keywords @ @ performance of three proposed method in website interest mining is compared @ a bespoke evaluation metric @ @ experimental @ @ @ @ iterative procedure designed in @ second method is able to improve website interest mining performance and @ website topic model in @ third method achieves @ best performance among @ three method @ @ inc @ 
1655,Bag-of-concepts: Comprehending document representation through clustering words in distributed representation,"Two document representation methods are mainly used in solving text mining problems. Known for its intuitive and simple interpretability, the bag-of-words method represents a document vector by its word frequencies. However, this method suffers from the curse of dimensionality, and fails to preserve accurate proximity information when the number of unique words increases. Furthermore, this method assumes every word to be independent, disregarding the impact of semantically similar words on preserving document proximity. On the other hand, doc2vec, a basic neural network model, creates low dimensional vectors that successfully preserve the proximity information. However, it loses the interpretability as meanings behind each feature are indescribable. This paper proposes the bag-of-concepts method as an alternative document representation method that overcomes the weaknesses of these two methods. This proposed method creates concepts through clustering word vectors generated from word2vec, and uses the frequencies of these concept clusters to represent document vectors. Through these data-driven concepts, the proposed method incorporates the impact of semantically similar words on preserving document proximity effectively. With appropriate weighting scheme such as concept frequency-inverse document frequency, the proposed method provides better document representation than previously suggested methods, and also offers intuitive interpretability behind the generated document vectors. Based on the proposed method, subsequently constructed text mining models, such as decision tree, can also provide interpretable and intuitive reasons on why certain collections of documents are different from others. © 2017 Elsevier B.V.",2017,Neurocomputing,64,@ document representation method @ mainly used in solving text mining problem @ known @ @ intuitive and simple interpretability @ bag-of-words method represents a document vector by @ word frequency @ however @ method suffers @ @ curse of dimensionality and fails to preserve accurate proximity information @ @ number of unique word increase @ furthermore @ method assumes every word to @ independent disregarding @ impact of semantically similar word on preserving document proximity @ on @ @ hand doc vec a basic neural network model creates low dimensional vector @ successfully preserve @ proximity information @ however @ loses @ interpretability a meaning behind @ feature @ indescribable @ @ @ proposes @ bag-of-concepts method a @ alternative document representation method @ overcomes @ weakness of @ @ method @ @ proposed method creates concept @ clustering word vector generated @ word vec and us @ frequency of @ concept cluster to represent document vector @ @ @ data-driven concept @ proposed method incorporates @ impact of semantically similar word on preserving document proximity effectively @ @ appropriate weighting scheme @ a concept frequency-inverse document frequency @ proposed method provides better document representation @ @ suggested method and @ offer intuitive interpretability behind @ generated document vector @ based on @ proposed method subsequently constructed text mining model @ a decision tree @ @ provide interpretable and intuitive reason on @ certain collection of document @ different @ others @ @ b @ v @ 
1663,Bank distress in the news: Describing events through deep learning,"While many models are purposed for detecting the occurrence of significant events in financial systems, the task of providing qualitative detail on the developments is not usually as well automated. We present a deep learning approach for detecting relevant discussion in text and extracting natural language descriptions of events. Supervised by only a small set of event information, comprising entity names and dates, the model is leveraged by unsupervised learning of semantic vector representations on extensive text data. We demonstrate applicability to the study of financial risk based on news (6.6M articles), particularly bank distress and government interventions (243 events), where indices can signal the level of bank-stress-related reporting at the entity level, or aggregated at national or European level, while being coupled with explanations. Thus, we exemplify how text, as timely, widely available and descriptive data, can serve as a useful complementary source of information for financial and systemic risk analytics. © 2017 Elsevier B.V.",2017,Neurocomputing,18,@ many model @ purposed @ detecting @ occurrence of significant event in financial system @ task of providing qualitative detail on @ development is not usually a well automated @ @ @ a deep learning approach @ detecting relevant discussion in text and extracting natural language description of event @ supervised by only a small set of event information comprising entity name and date @ model is leveraged by unsupervised learning of semantic vector representation on extensive text data @ @ demonstrate applicability to @ study of financial risk based on news @ @ @ article @ particularly bank distress and government intervention @ event @ @ index @ signal @ level of bank-stress-related reporting at @ entity level @ aggregated at national @ european level @ @ coupled @ explanation @ thus @ exemplify @ text a timely widely available and descriptive data @ serve a a useful complementary source of information @ financial and systemic risk analytics @ @ b @ v @ 
1672,Generating Information Relation Matrix Using Semantic Patent Mining for Technology Planning: A Case of Nano-Sensor,"For the purposes of technology planning and research and development strategy development, we present a semi-automated method that extracts text information from patent data, uses natural language processing to extract the key technical information of the patent, and then visualizes this information in a matrix form. We tried to support qualitative analysis of patent contents by extracting functions, components, and contexts, which are the most important information about inventions. We validated the method by applying it to patent data related to nanosensors. The matrix can emphasize technical information that have not been exploited in patents, and thereby identify development opportunities. © 2013 IEEE.",2017,IEEE Access,8,@ @ purpose of technology planning and research and development strategy development @ @ a semi-automated method @ extract text information @ patent data us natural language processing to extract @ key technical information of @ patent and @ visualizes @ information in a matrix form @ @ tried to support qualitative analysis of patent content by extracting function component and context @ @ @ @ important information @ invention @ @ validated @ method by applying @ to patent data related to nanosensors @ @ matrix @ emphasize technical information @ @ not @ exploited in patent and thereby identify development opportunity @ @ @ 
1676,A time-sensitive historical thesaurus-based semantic tagger for deep semantic annotation,"Automatic extraction and analysis of meaning-related information from natural language data has been an important issue in a number of research areas, such as natural language processing (NLP), text mining, corpus linguistics, and data science. An important aspect of such information extraction and analysis is the semantic annotation of language data using a semantic tagger. In practice, various semantic annotation tools have been designed to carry out different levels of semantic annotation, such as topics of documents, semantic role labeling, named entities or events. Currently, the majority of existing semantic annotation tools identify and tag partial core semantic information in language data, but they tend to be applicable only for modern language corpora. While such semantic analyzers have proven useful for various purposes, a semantic annotation tool that is capable of annotating deep semantic senses of all lexical units, or all-words tagging, is still desirable for a deep, comprehensive semantic analysis of language data. With large-scale digitization efforts underway, delivering historical corpora with texts dating from the last 400 years, a particularly challenging aspect is the need to adapt the annotation in the face of significant word meaning change over time. In this paper, we report on the development of a new semantic tagger (the Historical Thesaurus Semantic Tagger), and discuss challenging issues we faced in this work. This new semantic tagger is built on existing NLP tools and incorporates a large-scale historical English thesaurus linked to the Oxford English Dictionary. Employing contextual disambiguation algorithms, this tool is capable of annotating lexical units with a historically-valid highly fine-grained semantic categorization scheme that contains about 225,000 semantic concepts and 4,033 thematic semantic categories. In terms of novelty, it is adapted for processing historical English data, with rich information about historical usage of words and a spelling variant normalizer for historical forms of English. Furthermore, it is able to make use of knowledge about the publication date of a text to adapt its output. In our evaluation, the system achieved encouraging accuracies ranging from 77.12% to 91.08% on individual test texts. Applying time-sensitive methods improved results by as much as 3.54% and by 1.72% on average. © 2017",2017,Computer Speech and Language,9,automatic extraction and analysis of meaning-related information @ natural language data ha @ @ important issue in a number of research area @ a natural language processing @ nlp @ text mining corpus linguistics and data science @ @ important aspect of @ information extraction and analysis is @ semantic annotation of language data @ a semantic tagger @ in practice various semantic annotation tool @ @ designed to carry @ different level of semantic annotation @ a topic of document semantic role labeling named entity @ event @ currently @ majority of existing semantic annotation tool identify and tag partial core semantic information in language data @ @ tend to @ applicable only @ modern language corpus @ @ @ semantic analyzer @ proven useful @ various purpose a semantic annotation tool @ is capable of annotating deep semantic sens of @ lexical unit @ all-words tagging is still desirable @ a deep comprehensive semantic analysis of language data @ @ large-scale digitization effort underway delivering historical corpus @ text dating @ @ last year a particularly challenging aspect is @ need to adapt @ annotation in @ face of significant word meaning change @ time @ in @ @ @ report on @ development of a @ semantic tagger @ @ historical thesaurus semantic tagger @ and discus challenging issue @ faced in @ work @ @ @ semantic tagger is built on existing nlp tool and incorporates a large-scale historical english thesaurus linked to @ oxford english dictionary @ employing contextual disambiguation algorithm @ tool is capable of annotating lexical unit @ a historically-valid highly fine-grained semantic categorization scheme @ contains @ semantic concept and thematic semantic category @ in term of novelty @ is adapted @ processing historical english data @ rich information @ historical usage of word and a spelling variant normalizer @ historical form of english @ furthermore @ is able to make use of knowledge @ @ publication date of a text to adapt @ output @ in @ evaluation @ system achieved encouraging accuracy ranging @ @ to @ on individual test text @ applying time-sensitive method improved @ by a much a @ and by @ on average @ 
1679,Constructing a lexicon of Arabic-English named entity using SMT and semantic linked data,"Named Entity Recognition (NER) is the problem of locating and categorizing atomic entities in a given text. In this work, we used DBpedia Linked datasets and combined existing open source tools to generate from a parallel corpus a bilingual lexicon of Named Entities (NE). To annotate NE in the monolingual English corpus, we used linked data entities by mapping them to Gate Gazetteers. In order to translate entities identified by the gate tool from the English corpus, we used moses, a Statistical Machine Translation (SMT) system. The construction of the Arabic-English NE lexicon is based on the results of moses translation. Our method is fully automatic and aims to help Natural Language Processing (NLP) tasks such as, Machine Translation (MT) information retrieval, text mining and question answering. Our lexicon contains 48753 pairs of Arabic-English NE, it is freely available for use by other researchers. © 2017, Zarka Private University. All rights reserved.",2017,International Arab Journal of Information Technology,6,named entity recognition @ ner @ is @ problem of locating and categorizing atomic entity in a given text @ in @ work @ used dbpedia linked datasets and combined existing open source tool to generate @ a parallel corpus a bilingual lexicon of named entity @ ne @ @ to annotate ne in @ monolingual english corpus @ used linked data entity by mapping @ to gate gazetteer @ in order to translate entity identified by @ gate tool @ @ english corpus @ used moses a statistical machine translation @ smt @ system @ @ construction of @ arabic-english ne lexicon is based on @ @ of moses translation @ @ method is fully automatic and aim to help natural language processing @ nlp @ task @ a machine translation @ mt @ information retrieval text mining and question answering @ @ lexicon contains pair of arabic-english ne @ is freely available @ use by @ researcher @ zarka private university @ @ right reserved @ 
1680,An improved multi-objective based feature selection and document ranking model on high dimension TREC data,"Context identification is one of the major problems in feature extraction on text clustering applications. Recommended systems are the systems that extract user recommended keywords that might match their browsing query. Automatic context identification is an essential research concept in natural language processing, text mining and big data processing. As the size of the TREC data increases exponentially, the number of dimensions and outliers also increases in the context identification and key phrase extraction process. Hence, features extraction and context identification are the most relevant factors for mining interesting patterns on unstructured datasets. In this proposed approach, a multi-objective based ensemble model is used for feature selection and ranking approach. This model analyzes sentence to feature extraction, sentence to pattern analysis and phrase to sentence relationships to select optimal meaningful phrases/sentences from the given large text documents and reduce the duplication/outliers in the topic selection process. Experimental results prove that the proposed context based feature selection and text pattern discovery approach has got improvement compared to traditional graph and probability based approaches. © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.",2017,Journal of Advanced Research in Dynamical and Control Systems,0,context identification is @ of @ major problem in feature extraction on text clustering application @ recommended system @ @ system @ extract user recommended keywords @ might match @ browsing query @ automatic context identification is @ essential research concept in natural language processing text mining and big data processing @ a @ size of @ trec data increase exponentially @ number of dimension and outlier @ increase in @ context identification and key phrase extraction process @ hence feature extraction and context identification @ @ @ relevant factor @ mining interesting pattern on unstructured datasets @ in @ proposed approach a multi-objective based ensemble model is used @ feature selection and ranking approach @ @ model analyzes sentence to feature extraction sentence to pattern analysis and phrase to sentence relationship to select optimal meaningful phrase sentence @ @ given @ text document and reduce @ duplication outlier in @ topic selection process @ experimental @ prove @ @ proposed context based feature selection and text pattern discovery approach ha got improvement compared to traditional graph and probability based approach @ institute of advanced scientific research inc @ @ right reserved @ 
1681,The influence of negative emotions on customer innovation activities: An examination using sentiment analysis,"With the advent of Web 2.0, increased user participation in diverse e-communities resulted in an abundance of information, including emotional information. We examined the influence of negative emotion in an online brand community, MyStarbuckIdea.com, developed to collect diverse customer ideas for the firm's innovation, with the purpose of investigating how such emotion affects customer activities for innovation in the community. We first established several hypotheses on the relationships between discrete negative emotions and innovation activities. Then, having collected 84,918 customer ideas, we conducted POS tagging and term-based matching to calculate the inclusion and intensity of negative emotion using the negative emotion lexicon which we developed. As a result of testing our hypotheses with regression models, we show that 1) negative emotion significantly affects innovation activities in the brand community, and frustration is the most influential among the discrete negative emotions; and 2) as the intensity level of negative emotions increases, so does their influence.",2017,Data Base for Advances in Information Systems,2,@ @ advent of web @ increased user participation in diverse e-communities resulted in @ abundance of information including emotional information @ @ examined @ influence of negative emotion in @ online brand community mystarbuckidea @ com developed to collect diverse customer idea @ @ firm @ s innovation @ @ purpose of investigating @ @ emotion affect customer activity @ innovation in @ community @ @ first established several hypothesis on @ relationship @ discrete negative emotion and innovation activity @ @ @ collected customer idea @ conducted po tagging and term-based matching to calculate @ inclusion and intensity of negative emotion @ @ negative emotion lexicon @ @ developed @ a a @ of testing @ hypothesis @ regression model @ @ @ @ negative emotion significantly affect innovation activity in @ brand community and frustration is @ @ influential among @ discrete negative emotion @ and @ a @ intensity level of negative emotion increase @ doe @ influence @ 
1682,Fuzzy formal concept analysis based opinion mining for CRM in financial services,"Owing to the easy access to social media, consumers or customers are increasingly turning to social media to express their grievances and feedback on various products and services offered by the Banking, Financial, Services and Insurance industry. Because non-redressal of complaints eventually leads to customer churn, there is an urgent need to analyze the complaints. In this regard, we propose a novel descriptive analytics model that performs complaints/grievances analytics and summarizes the lengthy and verbose complaints concisely in a form that resembles association rules. The proposed hybrid model comprises fuzzy formal concept analysis and concept-level sentiment analysis (FFCA + SA) in tandem, which in turn is compared against formal concept analysis and concept-level sentiment analysis (FCA + SA). Because of the immediate fallout of the negative sentiments, a financial company is interested in studying them in more detail than the positive ones. Therefore, the model generates a list of ‘association rules’ the corresponding negative sentiment score along with the list of associated documents. Association rules are rank ordered according to the negative sentiment score, which in turn reflects severity affected services/products. The proposed model also provides interactive visualization that enables business analysts and managers to access a specific set of complaints without having to go through the entire set thoroughly. This saves a lot of time that would have otherwise been spent on cumbersome manual operations. Moreover, partial evaluation of the proposed methodology by human annotators yielded 64.06% matching score in terms of the opinions determination of aspects. © 2017 Elsevier B.V.",2017,Applied Soft Computing Journal,25,owing to @ easy access to social medium consumer @ customer @ increasingly turning to social medium to express @ grievance and feedback on various product and service offered by @ banking financial service and insurance industry @ @ non-redressal of complaint eventually lead to customer churn @ is @ urgent need to analyze @ complaint @ in @ regard @ propose a novel descriptive analytics model @ performs complaint grievance analytics and summarizes @ lengthy and verbose complaint concisely in a form @ resembles association rule @ @ proposed hybrid model comprises fuzzy formal concept analysis and concept-level sentiment analysis @ ffca sa @ in tandem @ in turn is compared @ formal concept analysis and concept-level sentiment analysis @ fca sa @ @ @ of @ immediate fallout of @ negative sentiment a financial company is interested in studying @ in more detail @ @ positive @ @ therefore @ model generates a list of association rule @ corresponding negative sentiment score along @ @ list of associated document @ association rule @ rank ordered according to @ negative sentiment score @ in turn reflects severity affected service product @ @ proposed model @ provides interactive visualization @ enables @ analyst and manager to access a specific set of complaint without @ to go @ @ entire set thoroughly @ @ save a lot of time @ would @ otherwise @ spent on cumbersome manual operation @ moreover partial evaluation of @ proposed methodology by human annotator yielded @ matching score in term of @ opinion determination of aspect @ @ b @ v @ 
1683,Recent advances in document summarization,"The task of automatic document summarization aims at generating short summaries for originally long documents. A good summary should cover the most important information of the original document or a cluster of documents, while being coherent, non-redundant and grammatically readable. Numerous approaches for automatic summarization have been developed to date. In this paper we give a self-contained, broad overview of recent progress made for document summarization within the last 5 years. Specifically, we emphasize on significant contributions made in recent years that represent the state-of-the-art of document summarization, including progress on modern sentence extraction approaches that improve concept coverage, information diversity and content coherence, as well as attempts from summarization frameworks that integrate sentence compression, and more abstractive systems that are able to produce completely new sentences. In addition, we review progress made for document summarization in domains, genres and applications that are different from traditional settings. We also point out some of the latest trends and highlight a few possible future directions. © 2017, Springer-Verlag London.",2017,Knowledge and Information Systems,54,@ task of automatic document summarization aim at generating short summary @ originally long document @ a good summary @ cover @ @ important information of @ original document @ a cluster of document @ @ coherent non-redundant and grammatically readable @ numerous approach @ automatic summarization @ @ developed to date @ in @ @ @ give a self-contained broad overview of recent progress made @ document summarization within @ last year @ specifically @ emphasize on significant contribution made in recent year @ represent @ state-of-the-art of document summarization including progress on modern sentence extraction approach @ improve concept coverage information diversity and content coherence a well a attempt @ summarization framework @ integrate sentence compression and more abstractive system @ @ able to produce completely @ sentence @ in addition @ review progress made @ document summarization in domain genre and application @ @ different @ traditional setting @ @ @ point @ some of @ latest trend and highlight a @ possible future direction @ springer-verlag london @ 
1685,Clustering short text using a centroid-based lexical clustering algorithm,"Traditional lexical clustering methods process text as a bag of words, with similarity between two text-fragments measured on the basis of word co-occurrence. While this approach is suitable for clustering large fragments of text (e.g., documents), it performs poorly when clustering smaller text fragments such as sentences (e.g., short text or quotes). This is because two sentences may be semantically similar while containing no common words. This paper proposes a new variant of the standard k-means algorithm for short text clustering that is based on the notion of synonym expansion semantic vectors. These vectors represent short text using semantic information derived from a lexical database constructed to identify the correct meaning to a word, based on the context in which it appears. Thus, whereas conventional k-means algorithm application is based on measuring the distance between patterns, the proposed approach is based on measuring semantic similarity between patterns (e, g., sentences). This enables it to utilise a higher degree of semantic information available within the clustered sentences. Empirical results show that the proposed variant method performs favorably against other clustering technique on two specially constructed datasets of famous quotations, benchmark datasets in several other domains, and that its incorporation as a short text similarity using synonym expansion leads to a significant improvement in the centroid-based clustering performance. Therefore, it is potential use in a variety of knowledge discovery processing tasks including text summarisation and text mining.",2017,IAENG International Journal of Computer Science,4,traditional lexical clustering method process text a a bag of word @ similarity @ @ text-fragments measured on @ basis of word co-occurrence @ @ @ approach is suitable @ clustering @ fragment of text @ e @ g @ document @ @ performs poorly @ clustering smaller text fragment @ a sentence @ e @ g @ short text @ quote @ @ @ is @ @ sentence may @ semantically similar @ containing no common word @ @ @ proposes a @ variant of @ standard k-means algorithm @ short text clustering @ is based on @ notion of synonym expansion semantic vector @ @ vector represent short text @ semantic information derived @ a lexical database constructed to identify @ correct meaning to a word based on @ context in @ @ appears @ thus whereas conventional k-means algorithm application is based on measuring @ distance @ pattern @ proposed approach is based on measuring semantic similarity @ pattern @ e g @ sentence @ @ @ enables @ to utilise a higher degree of semantic information available within @ clustered sentence @ empirical @ @ @ @ proposed variant method performs favorably @ @ clustering technique on @ specially constructed datasets of famous quotation benchmark datasets in several @ domain and @ @ incorporation a a short text similarity @ synonym expansion lead to a significant improvement in @ centroid-based clustering performance @ therefore @ is potential use in a variety of knowledge discovery processing task including text summarisation and text mining @ 
1715,Fraud analysis and detection for real-time messaging communications on social networks,"With the successful development and rapid advancement of social networking technology, people tend to exchange and share information via online social networks, such as Facebook and LINE.Massive amounts of information are aggregated promptly and circulated quickly among people. However, with the enormous volume of human-interactions, various types of swindles via online social networks have been launched in recent years. Effectively detecting fraudulent activities on social networks has taken on increased importance, and is a topic of ongoing interest. In this paper, we develop a fraud analysis and detection system based on realtime messaging communications, which constitute one of the most common human-interacted services of online social networks. An integrated platform consisting of various text-mining techniques, such as natural language processing, matrix processing and content analysis via a latent semantic model, is proposed. In the system implementation, we first collect a series of fraud events, all of which happened in Taiwan, to construct analysis modules for detecting such fraud events. An Android-based application is then built for alert notification when dubious logs and fraud events happen. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.",2017,IEICE Transactions on Information and Systems,3,@ @ successful development and rapid advancement of social networking technology people tend to exchange and share information via online social network @ a facebook and line @ massive amount of information @ aggregated promptly and circulated quickly among people @ however @ @ enormous volume of human-interactions various type of swindle via online social network @ @ launched in recent year @ effectively detecting fraudulent activity on social network ha taken on increased importance and is a topic of ongoing interest @ in @ @ @ develop a fraud analysis and detection system based on realtime messaging communication @ constitute @ of @ @ common human-interacted service of online social network @ @ integrated platform consisting of various text-mining technique @ a natural language processing matrix processing and content analysis via a latent semantic model is proposed @ in @ system implementation @ first collect a series of fraud event @ of @ happened in taiwan to construct analysis module @ detecting @ fraud event @ @ android-based application is @ built @ alert notification @ dubious log and fraud event happen @ @ @ institute of electronics information and communication engineer @ 
1716,TexRep: A Text Mining Framework for Online Reputation Monitoring,"This work aims to understand, formalize and explore the scientific challenges of using unstructured text data from different Web sources for Online Reputation Monitoring. We here present TexRep, an adaptable text mining framework specifically tailored for Online Reputation Monitoring that can be reused in multiple application scenarios, from politics to finance. This framework is able to collect texts from online media, such as Twitter, and identify entities of interest and classify sentiment polarity and intensity. The framework supports multiple data aggregation methods, as well as visualization and modeling techniques that can be used for both descriptive analytics, such as analyze how political polls evolve over time, and predictive analytics, such as predict elections. We here present case studies that illustrate and validate TexRep for Online Reputation Monitoring. In particular, we provide an evaluation of TexRep Entity Filtering and Sentiment Analysis modules using well known external benchmarks. We also present an illustrative example of TexRep application in the political domain. © 2017, Ohmsha, Ltd. and Springer Japan KK.",2017,New Generation Computing,5,@ work aim to understand formalize and explore @ scientific challenge of @ unstructured text data @ different web source @ online reputation monitoring @ @ @ @ texrep @ adaptable text mining framework specifically tailored @ online reputation monitoring @ @ @ reused in multiple application scenario @ politics to finance @ @ framework is able to collect text @ online medium @ a twitter and identify entity of interest and classify sentiment polarity and intensity @ @ framework support multiple data aggregation method a well a visualization and modeling technique @ @ @ used @ @ descriptive analytics @ a analyze @ political poll evolve @ time and predictive analytics @ a predict election @ @ @ @ case study @ illustrate and validate texrep @ online reputation monitoring @ in particular @ provide @ evaluation of texrep entity filtering and sentiment analysis module @ well known external benchmark @ @ @ @ @ illustrative example of texrep application in @ political domain @ ohmsha ltd @ and @ japan kk @ 
1718,Ranking and tagging bursty features in text streams with context language models,"Detecting and using bursty patterns to analyze text streams has been one of the fundamental approaches in many temporal text mining applications. So far, most existing studies have focused on developing methods to detect bursty features based purely on term frequency changes. Few have taken the semantic contexts of bursty features into consideration, and as a result the detected bursty features may not always be interesting and can be hard to interpret. In this article, we propose to model the contexts of bursty features using a language modeling approach. We propose two methods to estimate the context language models based on sentence-level context and document-level context.We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features. We also use two example text mining applications to qualitatively demonstrate the usefulness of bursty feature ranking and tagging. © 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg.",2017,Frontiers of Computer Science,0,detecting and @ bursty pattern to analyze text stream ha @ @ of @ fundamental approach in many temporal text mining application @ @ far @ existing study @ focused on developing method to detect bursty feature based purely on term frequency change @ @ @ taken @ semantic context of bursty feature @ consideration and a a @ @ detected bursty feature may not always @ interesting and @ @ hard to interpret @ in @ article @ propose to model @ context of bursty feature @ a language modeling approach @ @ propose @ method to estimate @ context language model based on sentence-level context and document-level context @ @ @ propose a novel topic diversity-based metric @ @ context model to find newsworthy bursty feature @ @ @ propose to use @ context model to automatically assign meaningful tag to bursty feature @ @ a @ corpus of news article @ quantitatively @ @ @ proposed context language model @ bursty feature @ effectively help rank bursty feature based on @ newsworthiness and to assign meaningful tag to annotate bursty feature @ @ @ use @ example text mining application to qualitatively demonstrate @ usefulness of bursty feature ranking and tagging @ higher education @ and springer-verlag @ @ @ 
1720,Location detection and disambiguation from twitter messages,"A remarkable amount of Twitter messages are generated every second. Detecting the location entities mentioned in these messages is useful in text mining applications. Therefore, techniques for extracting the location entities from the Twitter textual content are needed. In this work, we approach this task in a similar manner to the Named Entity Recognition (NER) task, but we focus only on locations, while NER systems detect names of persons, organizations, locations, and sometimes more (e.g., dates, times). But, unlike NER systems, we address a deeper task: classifying the detected locations into names of cities, provinces/states, and countries in order to map them into physical locations. We approach the task in a novel way, consisting in two stages. In the first stage, we train Conditional Random Fields (CRF) models that are able to detect the locations mentioned in the messages. We train three classifiers: one for cities, one for provinces/states, and one for countries, with various sets of features. Since a dataset annotated with this kind of information was not available, we collected and annotated our own dataset to use for training and testing. In the second stage, we resolve the remaining ambiguities, namely, cases when there exists more than one place with the same name. We proposed a set of heuristics able to choose the correct physical location in these cases. Our two-stage model will allow a social media monitoring system to visualize the places mentioned in Twitter messages on a map of the world or to compute statistics about locations. This kind of information can be of interest to business or marketing applications. © 2017, Springer Science+Business Media New York.",2017,Journal of Intelligent Information Systems,15,a remarkable amount of twitter message @ generated every second @ detecting @ location entity mentioned in @ message is useful in text mining application @ therefore technique @ extracting @ location entity @ @ twitter textual content @ needed @ in @ work @ approach @ task in a similar manner to @ named entity recognition @ ner @ task @ @ focus only on location @ ner system detect name of person organization location and sometimes more @ e @ g @ date time @ @ @ unlike ner system @ address a deeper task @ classifying @ detected location @ name of city province state and country in order to map @ @ physical location @ @ approach @ task in a novel way consisting in @ stage @ in @ first stage @ train conditional random field @ crf @ model @ @ able to detect @ location mentioned in @ message @ @ train three classifier @ @ @ city @ @ province state and @ @ country @ various set of feature @ since a dataset annotated @ @ kind of information wa not available @ collected and annotated @ @ dataset to use @ training and testing @ in @ second stage @ resolve @ remaining ambiguity namely case @ @ exists more @ @ place @ @ @ name @ @ proposed a set of heuristic able to choose @ correct physical location in @ case @ @ two-stage model @ allow a social medium monitoring system to visualize @ place mentioned in twitter message on a map of @ world @ to compute statistic @ location @ @ kind of information @ @ of interest to @ @ marketing application @ @ science @ medium @ york @ 
1723,Feature network-driven quadrant mapping for summarizing customer reviews,"With the rapid growth of e-commerce, customers increasingly write online reviews of the product they purchase. These customer reviews are one of the most valuable sources of information affecting selection of products or services. Summarizing these customer reviews is becoming an interesting area of research, inspiring researchers to develop a more condensed, concise summarization for users. However, most of the current efforts at summarization are based on general product features without feature’s relationship. As a result, these summaries either ignore feedback from customers or do a poor job of reflecting the opinions expressed in customer reviews. To remedy this summarization shortcoming, we propose a feature network-driven quadrant mapping that captures and incorporates opinions from customer reviews. Our focus is on construction of a feature network, which is based on co-occurrence and sematic similarities, and a quadrant display showing the opinions polarity of feature groups. Moreover, the proposed approach involves clustering similar product features, and thus, it is different from standard text summarization based on abstraction and extraction. The summarized results can help customers better understand the overall opinions about a product. © 2017, Systems Engineering Society of China and Springer-Verlag Berlin Heidelberg.",2017,Journal of Systems Science and Systems Engineering,4,@ @ rapid growth of e-commerce customer increasingly write online review of @ product @ purchase @ @ customer review @ @ of @ @ valuable source of information affecting selection of product @ service @ summarizing @ customer review is becoming @ interesting area of research inspiring researcher to develop a more condensed concise summarization @ user @ however @ of @ current effort at summarization @ based on general product feature without feature s relationship @ a a @ @ summary either ignore feedback @ customer @ @ a poor job of reflecting @ opinion expressed in customer review @ to remedy @ summarization shortcoming @ propose a feature network-driven quadrant mapping @ capture and incorporates opinion @ customer review @ @ focus is on construction of a feature network @ is based on co-occurrence and sematic similarity and a quadrant display showing @ opinion polarity of feature group @ moreover @ proposed approach involves clustering similar product feature and thus @ is different @ standard text summarization based on abstraction and extraction @ @ summarized @ @ help customer better understand @ overall opinion @ a product @ system engineering society of china and springer-verlag @ @ @ 
1729,Noisy-free Length Discriminant Analysis with cosine hyperbolic framework for dimensionality reduction,"Dimensionality Reduction (DR) is very useful and popular in many application areas of expert and intelligent systems, such as machine learning, finance, data and text mining, multimedia mining, image processing, anomaly detection, defense applications, bioinformatics and natural language processing. DR is widely applied for better data visualization and improving learning in all the above fields. In this manuscript, we propose a novel DR approach namely, Noisy-free Length Discriminant Analysis (NLDA) by developing Noisy-free Relevant Pattern Selection (NRPS). Traditional pattern selection methods discriminate boundary and non-boundary patterns with the help of class information and nearest neighbors. And these methods completely ignore noisy patterns which may degrade the performance of subsequent subspace learning. To overcome this, we develop Noisy-free Relevant Pattern Selection (NRPS), in which data instances are partitioned into boundary, non-boundary and noisy patterns. With the help of noisy-free boundary and non-boundary patterns, Noisy-free Length Discriminant Analysis (NLDA) has been proposed by developing new within and between-class scatters. These scatters model discriminations between lengths (L2-norms) of different class instances by considering only boundary and non-boundary patterns, while ignoring noisy patterns. A cosine hyperbolic frame work has been developed to formulate the objective of NLDA. Moreover, NLDA can also model the discrimination of multimodal data as different class data may consist of different lengths. Experimental study conducted on the synthesized data, UCI, and leeds butterfly databases. Moreover, an experimental study over human and computer interaction, i.e., face recognition (one of the application areas of expert and intelligent systems), has been performed. And, these studies prove that the proposed method can produce better discriminated subspace compare to the state-of-the-art methods. © 2017 Elsevier Ltd",2017,Expert Systems with Applications,0,dimensionality reduction @ dr @ is @ useful and popular in many application area of expert and intelligent system @ a machine learning finance data and text mining multimedia mining image processing anomaly detection defense application bioinformatics and natural language processing @ dr is widely applied @ better data visualization and improving learning in @ @ @ field @ in @ manuscript @ propose a novel dr approach namely noisy-free length discriminant analysis @ nlda @ by developing noisy-free relevant pattern selection @ nrps @ @ traditional pattern selection method discriminate boundary and non-boundary pattern @ @ help of class information and nearest neighbor @ and @ method completely ignore noisy pattern @ may degrade @ performance of subsequent subspace learning @ to overcome @ @ develop noisy-free relevant pattern selection @ nrps @ in @ data instance @ partitioned @ boundary non-boundary and noisy pattern @ @ @ help of noisy-free boundary and non-boundary pattern noisy-free length discriminant analysis @ nlda @ ha @ proposed by developing @ within and between-class scatter @ @ scatter model discrimination @ length @ l norm @ of different class instance by considering only boundary and non-boundary pattern @ ignoring noisy pattern @ a cosine hyperbolic frame work ha @ developed to formulate @ objective of nlda @ moreover nlda @ @ model @ discrimination of multimodal data a different class data may consist of different length @ experimental study conducted on @ synthesized data uci and leeds butterfly database @ moreover @ experimental study @ human and computer interaction i @ e @ face recognition @ @ of @ application area of expert and intelligent system @ ha @ performed @ and @ study prove @ @ proposed method @ produce better discriminated subspace compare to @ state-of-the-art method @ @ ltd
1730,Opinion classification of online reviews using the probabilistic neural network and principal component analysis,"Sentiment analysis of product reviews is an attracting and increasing interest in the area of natural language processing and web text mining. Objective is to analyze the effect of ANN based method for opinion classification. In the research that has done so far on sentiment analysis, ANNs have been considered rarely. In this work, the probabilistic neural network (PNN) has been examined in sentiment classification. This work also examines neural network based sentiment classification methods for feature level sentiment classification on various levels of word granularity are used as features. Product reviews collected from the Amazon reviews website are used as dataset for evaluation. Our objective is to classify the product reviews into three classes: positive, negative and neutral. The results are empirically compared with SVM using various quality measures. The superiority of PNN with Principal Component Analysis (PCA) is also shown in terms of training time. PNN is found to perform better and yields higher accuracy in prediction. In general, statistical based approaches do not perform well as that of neural network based approaches. Compared with traditional techniques, the ANN based approach shows the performance improvement in quality measures and in training time. Through the experimental results it will be show that shortening of training time and increasing the classification accuracy can be achieved by hybrid combination of PNN with PCA. © 2005 - Ongoing JATIT & LLS.",2017,Journal of Theoretical and Applied Information Technology,0,sentiment analysis of product review is @ attracting and increasing interest in @ area of natural language processing and web text mining @ objective is to analyze @ effect of ann based method @ opinion classification @ in @ research @ ha done @ far on sentiment analysis anns @ @ considered rarely @ in @ work @ probabilistic neural network @ pnn @ ha @ examined in sentiment classification @ @ work @ examines neural network based sentiment classification method @ feature level sentiment classification on various level of word granularity @ used a feature @ product review collected @ @ amazon review website @ used a dataset @ evaluation @ @ objective is to classify @ product review @ three class @ positive negative and neutral @ @ @ @ empirically compared @ svm @ various quality measure @ @ superiority of pnn @ principal component analysis @ pca @ is @ @ in term of training time @ pnn is found to perform better and yield higher accuracy in prediction @ in general statistical based approach @ not perform well a @ of neural network based approach @ compared @ traditional technique @ ann based approach @ @ performance improvement in quality measure and in training time @ @ @ experimental @ @ @ @ @ @ shortening of training time and increasing @ classification accuracy @ @ achieved by hybrid combination of pnn @ pca @ ongoing jatit lls @ 
1731,A case study of Spanish text transformations for twitter sentiment analysis,"Sentiment analysis is a text mining task that determines the polarity of a given text, i.e., its positiveness or negativeness. Recently, it has received a lot of attention given the interest in opinion mining in micro-blogging platforms. These new forms of textual expressions present new challenges to analyze text because of the use of slang, orthographic and grammatical errors, among others. Along with these challenges, a practical sentiment classifier should be able to handle efficiently large workloads. The aim of this research is to identify in a large set of combinations which text transformations (lemmatization, stemming, entity removal, among others), tokenizers (e.g., word n-grams), and token-weighting schemes make the most impact on the accuracy of a classifier (Support Vector Machine) trained on two Spanish datasets. The methodology used is to exhaustively analyze all combinations of text transformations and their respective parameters to find out what common characteristics the best performing classifiers have. Furthermore, we introduce a novel approach based on the combination of word-based n-grams and character-based q-grams. The results show that this novel combination of words and characters produces a classifier that outperforms the traditional word-based combination by 11.17% and 5.62% on the INEGI and TASS’15 dataset, respectively. © 2017 Elsevier Ltd",2017,Expert Systems with Applications,25,sentiment analysis is a text mining task @ determines @ polarity of a given text i @ e @ @ positiveness @ negativeness @ recently @ ha received a lot of attention given @ interest in opinion mining in micro-blogging platform @ @ @ form of textual expression @ @ challenge to analyze text @ of @ use of slang orthographic and grammatical error among others @ along @ @ challenge a practical sentiment classifier @ @ able to handle efficiently @ workload @ @ aim of @ research is to identify in a @ set of combination @ text transformation @ lemmatization stemming entity removal among others @ tokenizers @ e @ g @ word n-grams @ and token-weighting scheme make @ @ impact on @ accuracy of a classifier @ support vector machine @ trained on @ spanish datasets @ @ methodology used is to exhaustively analyze @ combination of text transformation and @ respective parameter to find @ @ common characteristic @ best performing classifier @ @ furthermore @ introduce a novel approach based on @ combination of word-based n-grams and character-based q-grams @ @ @ @ @ @ novel combination of word and character produce a classifier @ outperforms @ traditional word-based combination by @ and @ on @ inegi and ta dataset respectively @ @ ltd
1736,Combining lexical-syntactic patterns and topic analysis for automatic keyphrase extraction from texts,"The automatic keyphrases extraction is a useful task for many computational solutions in the natural language processing and text mining areas. In this paper, a new unsupervised method for keyphrase extraction from texts is proposed, in which the use of lexical-syntactic patterns is combined with a graph-based topic analysis strategy. The method was evaluated with the SemEval-2010 and INSPEC corpus, and compared with other state-of-the-art proposals, obtaining promising results. © 2017 Sociedad Espanola para el Procesamiento del Lenguaje Natural.",2017,Procesamiento de Lenguaje Natural,4,@ automatic keyphrases extraction is a useful task @ many computational solution in @ natural language processing and text mining area @ in @ @ a @ unsupervised method @ keyphrase extraction @ text is proposed in @ @ use of lexical-syntactic pattern is combined @ a graph-based topic analysis strategy @ @ method wa evaluated @ @ semeval and inspec corpus and compared @ @ state-of-the-art proposal obtaining promising @ @ sociedad espanola para el procesamiento del lenguaje natural @ 
1737,A Dictionary-Based Approach for Identifying Biomedical Concepts,"In this research, we provided a dictionary-based approach for identifying biomedical concepts from the literature. The approach first crawled experimental corpus by E-utilities and built a concept dictionary. Then, we developed an algorithm called Variable-step Window Identification Algorithm (VWIA) for matching biomedical concepts based on preprocessing, POS tagging and the formation of phrase block. The approach could identify embedded biomedical concepts and new concepts, which could identify concepts more completely. The proposed approach obtain 95.0% F-measure overall for the test dataset. Thus, it is promising for the method of biomedical text mining. © 2017 World Scientific Publishing Company.",2017,International Journal of Pattern Recognition and Artificial Intelligence,0,in @ research @ provided a dictionary-based approach @ identifying biomedical concept @ @ literature @ @ approach first crawled experimental corpus by e-utilities and built a concept dictionary @ @ @ developed @ algorithm called variable-step window identification algorithm @ vwia @ @ matching biomedical concept based on preprocessing po tagging and @ formation of phrase block @ @ approach could identify embedded biomedical concept and @ concept @ could identify concept more completely @ @ proposed approach obtain @ f-measure overall @ @ test dataset @ thus @ is promising @ @ method of biomedical text mining @ world scientific publishing company @ 
1738,Modeling content structures of domain-specific texts with RUP-HDP-HSMM and its applications,"We propose a novel method, built upon the hierarchical Dirichlet process hidden semi-Markov model, to reveal the content structures of unstructured domain-specific texts. The content structures of texts consisting of sequential local contexts are useful for tasks, such as text retrieval, classification, and text mining. The prominent feature of our model is the use of the recursive uniform partitioning, a stochastic process taking a view different from existing HSMMs in modeling state duration. We show that the recursive uniform partitioning plays an important role in avoiding the rapid switching between hidden states. Remarkably, our method greatly outperforms others in terms of ranking performance in our text retrieval experiments, and provides more accurate features for SVM to achieve higher F1 scores in our text classification experiments. These experiment results suggest that our method can yield improved representations of domainspecific texts. Furthermore, we present a method of automatically discovering the local contexts that serve to account for why a text is classified as a positive instance, in the supervised learning settings. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.",2017,IEICE Transactions on Information and Systems,0,@ propose a novel method built upon @ hierarchical dirichlet process hidden semi-markov model to reveal @ content structure of unstructured domain-specific text @ @ content structure of text consisting of sequential local context @ useful @ task @ a text retrieval classification and text mining @ @ prominent feature of @ model is @ use of @ recursive uniform partitioning a stochastic process taking a view different @ existing hsmms in modeling state duration @ @ @ @ @ recursive uniform partitioning play @ important role in avoiding @ rapid switching @ hidden state @ remarkably @ method greatly outperforms others in term of ranking performance in @ text retrieval experiment and provides more accurate feature @ svm to achieve higher f score in @ text classification experiment @ @ experiment @ suggest @ @ method @ yield improved representation of domainspecific text @ furthermore @ @ a method of automatically discovering @ local context @ serve to account @ @ a text is classified a a positive instance in @ supervised learning setting @ @ @ institute of electronics information and communication engineer @ 
1739,Xtroad: The tweet extraction method for profiling road and traffic conditions,"Twitter contains many natural language text elements that can be used for many purposes. One purpose is to produce the information about road and traffic conditions. The goal of this research is to propose a new extraction method for Indonesia Twitter text called xTRoad. By applying the xTRoad method, the national roads at Jakarta city could be identified by the other roadways nearby those which are connected in the particular region. The identification of the road and traffic conditions is completed in less than four minutes. The obtained information has the attributes of time, date, day, road names, incidents, weather and traffic conditions. All elements can be used to form the profile of road and traffic conditions. The results of this study showed that the obtained profiles have many dimensions such as the congestion conditions per thirty minutes, the types of barriers that occur on national roads, the trend of traffic jam in 24 hours, etc. The similarity level for comparison between the traffic conditions from the extracted text and video data has an accuracy of 62.8%. © 2006-2017 Asian Research Publishing Network (ARPN).",2017,ARPN Journal of Engineering and Applied Sciences,1,twitter contains many natural language text element @ @ @ used @ many purpose @ @ purpose is to produce @ information @ road and traffic condition @ @ goal of @ research is to propose a @ extraction method @ indonesia twitter text called xtroad @ by applying @ xtroad method @ national road at jakarta city could @ identified by @ @ roadway nearby @ @ @ connected in @ particular region @ @ identification of @ road and traffic condition is completed in le @ four minute @ @ obtained information ha @ attribute of time date day road name incident weather and traffic condition @ @ element @ @ used to form @ profile of road and traffic condition @ @ @ of @ study showed @ @ obtained profile @ many dimension @ a @ congestion condition per thirty minute @ type of barrier @ occur on national road @ trend of traffic jam in hour etc @ @ similarity level @ comparison @ @ traffic condition @ @ extracted text and video data ha @ accuracy of @ @ asian research publishing network @ arpn @ @ 
1756,Using of natural language processing techniques in suicide research,"It is estimated that each year many people, most of whom are teenagers and young adults die by suicide worldwide. Suicide receives special attention with many countries developing national strategies for prevention. Since, more medical information is available in text, Preventing the growing trend of suicide in communities requires analyzing various textual resources, such as patient records, information on the web or questionnaires. For this purpose, this study systematically reviews recent studies related to the use of natural language processing techniques in the area of people’s health who have completed suicide or are at risk. After electronically searching for the PubMed and ScienceDirect databases and studying articles by two reviewers, 21 articles matched the inclusion criteria. This study revealed that, if a suitable data set is available, natural language processing techniques are well suited for various types of suicide related research. © This is an open access article under the CC-BY license.",2017,Emerging Science Journal,1,@ is estimated @ @ year many people @ of @ @ teenager and young adult die by suicide worldwide @ suicide receives special attention @ many country developing national strategy @ prevention @ since more medical information is available in text preventing @ growing trend of suicide in community requires analyzing various textual resource @ a patient record information on @ web @ questionnaire @ @ @ purpose @ study systematically review recent study related to @ use of natural language processing technique in @ area of people s health @ @ completed suicide @ @ at risk @ @ electronically searching @ @ pubmed and sciencedirect database and studying article by @ reviewer article matched @ inclusion criterion @ @ study revealed @ if a suitable data set is available natural language processing technique @ well suited @ various type of suicide related research @ @ is @ open access article @ @ cc-by license @ 
1757,A state of art approaches on sentimental analysis techniques,"In the recent years, sentiment analysis (SA) becomes more and more popular in the domain of decision making. SA is the process of automatic extraction or classification of sentiments from the user reviews or opinions using Natural Language Processing (NLP), text mining and computational techniques. The aim of SA is to locate the opinions, identify the sentiments and classify the polarity of the sentiment. It is useful in various ways such as marketing, business, government and individuals. In this survey, a comprehensive review of various SA techniques is done with their objectives, underlying techniques, advantages, datasets used and performance results. A detailed comparison is also made to analyze the different characteristics of various techniques in terms of task, domain oriented, polarity, scope, data set and languages. The applications of SA in different domains are also discussed. This paper gives the overall picture of SA techniques. At the end of the paper, open issues and research challenges in the field of SA are explained in detail. © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.",2017,Journal of Advanced Research in Dynamical and Control Systems,0,in @ recent year sentiment analysis @ sa @ becomes more and more popular in @ domain of decision making @ sa is @ process of automatic extraction @ classification of sentiment @ @ user review @ opinion @ natural language processing @ nlp @ text mining and computational technique @ @ aim of sa is to locate @ opinion identify @ sentiment and classify @ polarity of @ sentiment @ @ is useful in various way @ a marketing @ government and individual @ in @ survey a comprehensive review of various sa technique is done @ @ objective underlying technique advantage datasets used and performance @ @ a detailed comparison is @ made to analyze @ different characteristic of various technique in term of task domain oriented polarity scope data set and language @ @ application of sa in different domain @ @ discussed @ @ @ give @ overall picture of sa technique @ at @ end of @ @ open issue and research challenge in @ field of sa @ explained in detail @ institute of advanced scientific research inc @ @ right reserved @ 
1760,Sentimental analysis on online product reviews using LS-SVM method,"SA is the process of automatic extraction or classification of sentiments from the user reviews or opinions using Natural Language Processing (NLP), text mining and computational techniques. The aim of SA is to locate the expressions, determine the sentiments and classify the nature of the sentiment. It is useful in various ways such as marketing, business, government and individuals. Online product reviews play a major role in customer purchase decision. A new Least Square-Support Vector Machine (LS-SVM) to classify the sentiment and rank the online product using the customer reviews is proposed. The online product reviews of 5 products namely canon digital camera, DVD scan, iPod, Nokia 6601 mobile and Nokia Coolpix camera are gathered from Amazon shopping website to perform this study. The proposed method provides the rating of the online products using the customer reviews. The extensive experimentation results show the proposed LS-SVM method produces effective results than the existing methods such as Naïve Bayes, Random Forest and Support Vector Machine (SVM). © 2017, Institute of Advanced Scientific Research, Inc. All rights reserved.",2017,Journal of Advanced Research in Dynamical and Control Systems,2,sa is @ process of automatic extraction @ classification of sentiment @ @ user review @ opinion @ natural language processing @ nlp @ text mining and computational technique @ @ aim of sa is to locate @ expression determine @ sentiment and classify @ nature of @ sentiment @ @ is useful in various way @ a marketing @ government and individual @ online product review play a major role in customer purchase decision @ a @ least square-support vector machine @ ls-svm @ to classify @ sentiment and rank @ online product @ @ customer review is proposed @ @ online product review of product namely canon digital camera dvd scan ipod nokia mobile and nokia coolpix camera @ gathered @ amazon shopping website to perform @ study @ @ proposed method provides @ rating of @ online product @ @ customer review @ @ extensive experimentation @ @ @ proposed ls-svm method produce effective @ @ @ existing method @ a naïve bayes random forest and support vector machine @ svm @ @ institute of advanced scientific research inc @ @ right reserved @ 
1761,"Digital technologies and artificial intelligence’s present and foreseeable impact on lawyering, judging, policing and law enforcement","‘AI & Law’ research has been around since the 1970s, even though with shifting emphasis. This is an overview of the contributions of digital technologies, both artificial intelligence and non-AI smart tools, to both the legal professions and the police. For example, we briefly consider text mining and case-automated summarization, tools supporting argumentation, tools concerning sentencing based on the technique of case-based reasoning, the role of abductive reasoning, research into applying AI to legal evidence, tools for fighting crime and tools for identification. © 2015, Springer-Verlag London.",2017,AI and Society,15, ai law research ha @ around since @ s even though @ shifting emphasis @ @ is @ overview of @ contribution of digital technology @ artificial intelligence and non-ai smart tool to @ @ legal profession and @ police @ @ example @ briefly consider text mining and case-automated summarization tool supporting argumentation tool concerning sentencing based on @ technique of case-based reasoning @ role of abductive reasoning research @ applying ai to legal evidence tool @ fighting crime and tool @ identification @ springer-verlag london @ 
1772,Sentiment mining approaches for big data classification and clustering,"Sentiment analysis is one of the most important applications in the field of text mining. It computes people's opinions, comments, posts, reviews, evaluations, and emotions which are expressed on products, sales, services, individuals, organizations, etc. Nowadays, large amounts of structured and unstructured data are being produced on the web. The categorizing and grouping of these data become a real-world problem. In this chapter, the authors address the current research in this field, issues and the problem of sentiment analysis on Big Data for classification and clustering. It suggests new methods, applications, algorithm extensions of classification and clustering and software tools in the field of sentiment analysis. © 2018 IGI Global. All rights reserved.",2017,Modern Technologies for Big Data Classification and Clustering,3,sentiment analysis is @ of @ @ important application in @ field of text mining @ @ computes people @ s opinion comment post review evaluation and emotion @ @ expressed on product sale service individual organization etc @ nowadays @ amount of structured and unstructured data @ @ produced on @ web @ @ categorizing and grouping of @ data become a real-world problem @ in @ chapter @ author address @ current research in @ field issue and @ problem of sentiment analysis on big data @ classification and clustering @ @ suggests @ method application algorithm extension of classification and clustering and software tool in @ field of sentiment analysis @ igi global @ @ right reserved @ 
1776,Validating curriculum development using text mining,"Interdisciplinarity requires the collaboration of two or more disciplines to combine their expertise to jointly develop and deliver learning and teaching outcomes appropriate for a subject area. Curricula and assessment mapping are critical components to foster and enhance interdisciplinary learning environments. Emerging careers in data science and machine learning coupled with the necessary graduate outcomes mandate the need for a truly interdisciplinary pedagogical approach. The challenges for emerging academic disciplines such as data science and machine learning center on the need for multiple fields to coherently develop university-level curricula. Using text mining, we empirically analyze the breadth and depth of existing tertiary-level curricula to quantify patterns in curricula through the use of surface and deep cluster analysis. This approach helps educators validate the breadth and depth of a proposed curriculum relative to the broad evolution of data science as a discipline. © 2016 British Educational Research Association.",2017,Curriculum Journal,2,interdisciplinarity requires @ collaboration of @ @ more discipline to combine @ expertise to jointly develop and deliver learning and teaching outcome appropriate @ a subject area @ curriculum and assessment mapping @ critical component to foster and enhance interdisciplinary learning environment @ emerging career in data science and machine learning coupled @ @ necessary graduate outcome mandate @ need @ a truly interdisciplinary pedagogical approach @ @ challenge @ emerging @ discipline @ a data science and machine learning center on @ need @ multiple field to coherently develop university-level curriculum @ @ text mining @ empirically analyze @ breadth and depth of existing tertiary-level curriculum to quantify pattern in curriculum @ @ use of surface and deep cluster analysis @ @ approach help educator validate @ breadth and depth of a proposed curriculum relative to @ broad evolution of data science a a discipline @ british educational research association @ 
1781,Sentiment analysis with text mining in contexts of big data,"The evolution of technology, along with the common use of different devices connected to the Internet, provides a vast growth in the volume and variety of data that are daily generated at high velocity, phenomenon commonly denominated as Big Data. Related with this, several Text Mining techniques make possible the extraction of useful insights from that data, benefiting the decision-making process across multiple areas, using the information, models, patterns or tendencies that these techniques are able to identify. With Sentiment Analysis, it is possible to understand which sentiments and opinions are implicit in this data. This paper proposes an architecture for Sentiment Analysis that uses data from the Twitter, which is able to collect, store, process and analyse data on a real-time fashion. To demonstrate its utility, practical applications are developed using real world examples where Sentiment Analysis brings benefits when applied. With the presented demonstration case, it is possible to verify the role of each used technology and the techniques adopted for Sentiment Analysis. Copyright © 2017, IGI Global.",2017,International Journal of Technology and Human Interaction,5,@ evolution of technology along @ @ common use of different device connected to @ internet provides a vast growth in @ volume and variety of data @ @ daily generated at high velocity phenomenon commonly denominated a big data @ related @ @ several text mining technique make possible @ extraction of useful insight @ @ data benefiting @ decision-making process across multiple area @ @ information model pattern @ tendency @ @ technique @ able to identify @ @ sentiment analysis @ is possible to understand @ sentiment and opinion @ implicit in @ data @ @ @ proposes @ architecture @ sentiment analysis @ us data @ @ twitter @ is able to collect store process and analyse data on a real-time fashion @ to demonstrate @ utility practical application @ developed @ real world example @ sentiment analysis brings benefit @ applied @ @ @ presented demonstration case @ is possible to verify @ role of @ used technology and @ technique adopted @ sentiment analysis @ @ igi global @ 
1786,BrAgriNews: A temporal-causal Brazilian-Portuguese corpus for agriculture,"There has been a recent sharp increase in interest in academia and industry in applying machine learning and artificial intelligence to agricultural problems. Text mining and related natural language processing techniques, have been rarely used to tackle agricultural problems, and at the time of writing there was a single project in the Portuguese language. It is possible that the failure of researchers to use text mining techniques to analyze Portuguese texts to resolve agricultural problems may be due to a lack of freely available corpora. To correct the lack of a Portuguese language agriculture centric corpus we are releasing a Brazilian-Portuguese agricultural language resource, which is described by this paper. The corpus is partially non-contiguous and spans a time period from 1996 to 2016. It consists of news stories that have been scraped from Brazilian News sites that have been annotated with the following information types: causal, sentiment, named entities that include temporal expressions. The corpus has additional resources such as a: treebank, lists of frequent: unigrams, bigrams and trigrams, as well words or phrases that have been identified by journalists as either: ""important"" or domain specific. It is hoped that the release of this corpus will stimulate the adoption of text mining in agriculture in the Lusophonic research community.",2017,Linguamatica,2,@ ha @ a recent sharp increase in interest in academia and industry in applying machine learning and artificial intelligence to agricultural problem @ text mining and related natural language processing technique @ @ rarely used to tackle agricultural problem and at @ time of writing @ wa a single project in @ portuguese language @ @ is possible @ @ failure of researcher to use text mining technique to analyze portuguese text to resolve agricultural problem may @ due to a lack of freely available corpus @ to correct @ lack of a portuguese language agriculture centric corpus @ @ releasing a brazilian-portuguese agricultural language resource @ is described by @ @ @ @ corpus is partially non-contiguous and span a time period @ to @ @ consists of news story @ @ @ scraped @ brazilian news site @ @ @ annotated @ @ following information type @ causal sentiment named entity @ include temporal expression @ @ corpus ha additional resource @ a a @ treebank list of frequent @ unigrams bigram and trigram a well word @ phrase @ @ @ identified by journalist a either @ @ important @ @ domain specific @ @ is hoped @ @ release of @ corpus @ stimulate @ adoption of text mining in agriculture in @ lusophonic research community @ 
1790,Extraction of emotions from multilingual text using intelligent text processing and computational linguistics,"Extraction of Emotions from Multilingual Text posted on social media by different categories of users is one of the crucial tasks in the field of opining mining and sentiment analysis. Every major event in the world has an online presence and social media. Users use social media platforms to express their sentiments and opinions towards it. In this paper, an advanced framework for detection of emotions of users in Multilanguage text data using emotion theories has been presented, which deals with linguistics and psychology. The emotion extraction system is developed based on multiple features groups for the better understanding of emotion lexicons. Empirical studies of three real-time events in domains like a Political election, healthcare, and sports are performed using proposed framework. The technique used for dynamic keywords collection is based on RSS (Rich Site Summary) feeds of headlines of news articles and trending hashtags from Twitter. An intelligent data collection model has been developed using dynamic keywords. Every word of emotion contained in a tweet is important in decision making and hence to retain the importance of multilingual emotional words, effective pre-processing technique has been used. Naive Bayes algorithm and Support Vector Machine (SVM) are used for fine-grained emotions classification of tweets. Experiments conducted on collected data sets, show that the proposed method performs better in comparison to corpus-driven approach which assign affective orientation or scores to words. The proposed emotion extraction framework performs better on the collected dataset by combining feature sets consisting of words from publicly available lexical resources. Furthermore, the presented work for extraction of emotion from tweets performs better in comparisons of other popular sentiment analysis techniques which are dependent of specific existing affect lexicons. © 2017 Elsevier B.V.",2017,Journal of Computational Science,58,extraction of emotion @ multilingual text posted on social medium by different category of user is @ of @ crucial task in @ field of opining mining and sentiment analysis @ every major event in @ world ha @ online presence and social medium @ user use social medium platform to express @ sentiment and opinion towards @ @ in @ @ @ advanced framework @ detection of emotion of user in multilanguage text data @ emotion theory ha @ presented @ deal @ linguistics and psychology @ @ emotion extraction system is developed based on multiple feature group @ @ better understanding of emotion lexicon @ empirical study of three real-time event in domain like a political election healthcare and sport @ performed @ proposed framework @ @ technique used @ dynamic keywords collection is based on r @ rich site summary @ feed of headline of news article and trending hashtags @ twitter @ @ intelligent data collection model ha @ developed @ dynamic keywords @ every word of emotion contained in a tweet is important in decision making and hence to retain @ importance of multilingual emotional word effective pre-processing technique ha @ used @ naive bayes algorithm and support vector machine @ svm @ @ used @ fine-grained emotion classification of tweet @ experiment conducted on collected data set @ @ @ proposed method performs better in comparison to corpus-driven approach @ assign affective orientation @ score to word @ @ proposed emotion extraction framework performs better on @ collected dataset by combining feature set consisting of word @ publicly available lexical resource @ furthermore @ presented work @ extraction of emotion @ tweet performs better in comparison of @ popular sentiment analysis technique @ @ dependent of specific existing affect lexicon @ @ b @ v @ 
1799,Challenges with sentiment analysis of on-line micro-texts,"With the evolution of World Wide Web (WWW) 2.0 and the emergence of many micro-blogging and social networking sites like Twitter, the internet has become a massive source of short textual messages called on-line micro-texts, which are limited to a few number of characters (e.g. 140 characters on Twitter). These on-line micro-texts are considered as real-time text streams. Online micro-texts are extremely subjective; they contain opinions about various events, social issues, personalities, and products. However, despite being so voluminous in quantity, the qualitative nature of these micro-texts is very inconsistent. These qualitative inconsistencies of raw on-line micro-texts impose many challenges in sentiment analysis of on-line micro-texts by using the established methods of sentiment analysis of unstructured reviews. This paper presents many challenges and issues observed during sentiment analysis of On-line Microtexts.",2017,International Journal of Intelligent Systems and Applications,9,@ @ evolution of world wide web @ www @ @ and @ emergence of many micro-blogging and social networking site like twitter @ internet ha become a massive source of short textual message called on-line micro-texts @ @ limited to a @ number of character @ e @ g @ character on twitter @ @ @ on-line micro-texts @ considered a real-time text stream @ online micro-texts @ extremely subjective @ @ contain opinion @ various event social issue personality and product @ however despite @ @ voluminous in quantity @ qualitative nature of @ micro-texts is @ inconsistent @ @ qualitative inconsistency of raw on-line micro-texts impose many challenge in sentiment analysis of on-line micro-texts by @ @ established method of sentiment analysis of unstructured review @ @ @ @ many challenge and issue observed @ sentiment analysis of on-line microtexts @ 
1822,Modeling of fuzzy-based voice of customer for business decision analytics,"Identification, interpretation and response to customer requirements are the key success factors for companies, regardless of their industry. Failing to satisfy customer requirements can damage a company's reputation and cause heavy losses. In this study, we have developed a new approach for properly interpreting and analyzing the fuzzy voice of the customer using association rule learning and text mining. This unique methodology converts textual and qualitative data into a common quantitative format which is then used to develop a mapped Integrated Customer Satisfaction Index (ICSI). ICSI is a framework for measuring customer satisfaction. Previous measures of customer satisfaction ratio failed to incorporate the cost implications of resolving customer complaints/issues and the fuzzy impact of those complaints/issues on the system. In addition to including these important and unique factors in the present study, we have also introduced a dynamic Critical to Quality (CTQ) concept, a novel method that provides a real-time system to monitor the CTQ list through an updated CTQ library. Finally, a procedure for customer feedback mining and sentiment analysis is proposed that handles typographical errors, which are unavoidable in every real database. The results of this study suggest that incorporating the fuzzy level of negativity and positivity of comments into the model instead of treating negative and positive comments as binary variables, leads to more reasonable outcomes. In addition, this study provides a more structured framework for understanding customer requirements. © 2017",2017,Knowledge-Based Systems,19,identification interpretation and response to customer requirement @ @ key success factor @ company regardless of @ industry @ failing to satisfy customer requirement @ damage a company @ s reputation and cause heavy loss @ in @ study @ @ developed a @ approach @ properly interpreting and analyzing @ fuzzy voice of @ customer @ association rule learning and text mining @ @ unique methodology convert textual and qualitative data @ a common quantitative format @ is @ used to develop a mapped integrated customer satisfaction index @ icsi @ @ icsi is a framework @ measuring customer satisfaction @ previous measure of customer satisfaction ratio failed to incorporate @ cost implication of resolving customer complaint issue and @ fuzzy impact of @ complaint issue on @ system @ in addition to including @ important and unique factor in @ @ study @ @ @ introduced a dynamic critical to quality @ ctq @ concept a novel method @ provides a real-time system to monitor @ ctq list @ @ updated ctq library @ finally a procedure @ customer feedback mining and sentiment analysis is proposed @ handle typographical error @ @ unavoidable in every real database @ @ @ of @ study suggest @ incorporating @ fuzzy level of negativity and positivity of comment @ @ model instead of treating negative and positive comment a binary variable lead to more reasonable outcome @ in addition @ study provides a more structured framework @ understanding customer requirement @ 
1823,Modeling and learning distributed word representation with metadata for question retrieval,"Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the Web. This paper focuses on addressing the lexical gap problem in question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings a new challenge for question retrieval in cQA. In this paper, we propose to model and learn distributed word representations with metadata of category information within cQA pages for question retrieval using two novel category powered models. One is a basic category powered model called MB-NET and the other one is an enhanced category powered model called ME-NET which can better learn the distributed word representations and alleviate the lexical gap problem. To deal with the variable size of word representation vectors, we employ the framework of fisher kernel to transform them into the fixed-length vectors. Experimental results on large-scale English and Chinese cQA data sets show that our proposed approaches can significantly outperform state-of-the-art retrieval models for question retrieval in cQA. Moreover, we further conduct our approaches on large-scale automatic evaluation experiments. The evaluation results show that promising and significant performance improvements can be achieved. © 1989-2012 IEEE.",2017,IEEE Transactions on Knowledge and Data Engineering,15,community question answering @ cqa @ ha become @ important issue due to @ popularity of cqa archive on @ web @ @ @ focus on addressing @ lexical gap problem in question retrieval @ question retrieval in cqa archive aim to find @ existing question @ @ semantically equivalent @ relevant to @ queried question @ however @ lexical gap problem brings a @ challenge @ question retrieval in cqa @ in @ @ @ propose to model and learn distributed word representation @ metadata of category information within cqa page @ question retrieval @ @ novel category powered model @ @ is a basic category powered model called mb-net and @ @ @ is @ enhanced category powered model called me-net @ @ better learn @ distributed word representation and alleviate @ lexical gap problem @ to deal @ @ variable size of word representation vector @ employ @ framework of fisher kernel to transform @ @ @ fixed-length vector @ experimental @ on large-scale english and chinese cqa data set @ @ @ proposed approach @ significantly outperform state-of-the-art retrieval model @ question retrieval in cqa @ moreover @ @ conduct @ approach on large-scale automatic evaluation experiment @ @ evaluation @ @ @ promising and significant performance improvement @ @ achieved @ @ @ 
1824,An approach to managing and organizing text documents using intelligent text analysis,"Regarding the fact that stored data occupies a large space in organizations and retention systems and information management that has been resulted in gigantic data warehouses, the need for extracting an appropriate model is felt increasingly. Text mining is one of the most significant methods for extracting a useful and appropriate model that helps organizations in achieving their goals through extraction and adaption of knowledge out of data sets. Those methods allow for a new horizon for trading and protecting intellectual property of authors' works. In this paper, a new approach is needed to decipher the text patterns to organize an intelligent text analysis. The main purpose of the paper is applying a proper method of preserving the works of writers, scholars and text documents. Regarding the number of those works and documentary management systems the size of available data has been increased considerably. In order to uncover the implicit knowledge out of this data with considerable usefulness for users a specific method is required that has been practiced in the data mining field. Much of this available data is unstructured or semi-structured text which one can use it in addition to data mining methods, technologies such as natural language processing, intelligent analysis and Science Statistics used.",2017,Iranian Journal of Information Processing Management,4,regarding @ fact @ stored data occupies a @ space in organization and retention system and information management @ ha @ resulted in gigantic data warehouse @ need @ extracting @ appropriate model is felt increasingly @ text mining is @ of @ @ significant method @ extracting a useful and appropriate model @ help organization in achieving @ goal @ extraction and adaption of knowledge @ of data set @ @ method allow @ a @ horizon @ trading and protecting intellectual property of author @ work @ in @ @ a @ approach is needed to decipher @ text pattern to organize @ intelligent text analysis @ @ main purpose of @ @ is applying a proper method of preserving @ work of writer scholar and text document @ regarding @ number of @ work and documentary management system @ size of available data ha @ increased considerably @ in order to uncover @ implicit knowledge @ of @ data @ considerable usefulness @ user a specific method is required @ ha @ practiced in @ data mining field @ much of @ available data is unstructured @ semi-structured text @ @ @ use @ in addition to data mining method technology @ a natural language processing intelligent analysis and science statistic used @ 
1825,Resolving ambiguity in sentiment classification: The role of dependency features,"Sentiment analysis has become popular in business intelligence and analytics applications due to the great need for learning insights from the vast amounts of user generated content on the Internet. One major challenge of sentiment analysis, like most text classification tasks, is finding structures from unstructured texts. Existing sentiment analysis techniques employ the supervised learning approach and the lexicon scoring approach, both of which largely rely on the representation of a document as a collection of words and phrases. The semantic ambiguity (i.e., polysemy) of single words and the sparsity of phrases negatively affect the robustness of sentiment analysis, especially in the context of short social media texts. In this study, we propose to represent texts using dependency features. We test the effectiveness of dependency features in supervised sentiment classification. We compare our method with the current standard practice using a labeled data set containing 170,874 microblogging messages. The combination of unigram features and dependency features significantly outperformed other popular types of features. © 2017 ACM.",2017,ACM Transactions on Management Information Systems,3,sentiment analysis ha become popular in @ intelligence and analytics application due to @ great need @ learning insight @ @ vast amount of user generated content on @ internet @ @ major challenge of sentiment analysis like @ text classification task is finding structure @ unstructured text @ existing sentiment analysis technique employ @ supervised learning approach and @ lexicon scoring approach @ of @ largely rely on @ representation of a document a a collection of word and phrase @ @ semantic ambiguity @ i @ e @ polysemy @ of single word and @ sparsity of phrase negatively affect @ robustness of sentiment analysis especially in @ context of short social medium text @ in @ study @ propose to represent text @ dependency feature @ @ test @ effectiveness of dependency feature in supervised sentiment classification @ @ compare @ method @ @ current standard practice @ a labeled data set containing microblogging message @ @ combination of unigram feature and dependency feature significantly outperformed @ popular type of feature @ acm @ 
1828,Instance labeling in semi-supervised learning with meaning values of words,"In supervised learning systems; only labeled samples are used for building a classifier that is then used to predict the class labels of the unlabeled samples. However, obtaining labeled data is very expensive, time consuming and difficult in real-life practical situations as labeling a data set requires the effort of a human expert. On the other side, unlabeled data are often plentiful which makes it relatively inexpensive and easier to obtain. Semi-Supervised Learning methods strive to utilize this plentiful source of unlabeled examples to increase the learning capacity of the classifier particularly when amount of labeled examples are restricted. Since SSL techniques usually reach higher accuracy and require less human effort, they attract a substantial amount of attention both in practical applications and theoretical research. A novel semi-supervised methodology is offered in this study. This algorithm utilizes a new method to predict the class labels of unlabeled examples in a corpus and incorporate them into the training set to build a better classifier. The approach presented here depends on a meaning calculation, which computes the words’ meaning scores in the scope of classes. Meaning computation is constructed on the Helmholtz principle and utilized to various applications in the field of text mining like feature extraction, information retrieval and document summarization. Nevertheless, according to the literature, ILBOM is the first work which uses meaning calculation in a semi-supervised way to construct a semantic smoothing kernel for Support Vector Machines (SVM). Evaluation of the proposed methodology is done by performing various experiments on standard textual datasets. ILBOM's experimental results are compared with three baseline algorithms including SVM using linear kernel which is one of the most frequently used algorithms in text classification field. Experimental results show that labeling unlabeled instances based on meaning scores of words to augment the training set is valuable, and increases the classification accuracy on previously unseen test instances significantly. © 2017 Elsevier Ltd",2017,Engineering Applications of Artificial Intelligence,6,in supervised learning system @ only labeled sample @ used @ building a classifier @ is @ used to predict @ class label of @ unlabeled sample @ however obtaining labeled data is @ expensive time consuming and difficult in real-life practical situation a labeling a data set requires @ effort of a human expert @ on @ @ side unlabeled data @ often plentiful @ make @ relatively inexpensive and easier to obtain @ semi-supervised learning method strive to utilize @ plentiful source of unlabeled example to increase @ learning capacity of @ classifier particularly @ amount of labeled example @ restricted @ since ssl technique usually reach higher accuracy and require le human effort @ attract a substantial amount of attention @ in practical application and theoretical research @ a novel semi-supervised methodology is offered in @ study @ @ algorithm utilizes a @ method to predict @ class label of unlabeled example in a corpus and incorporate @ @ @ training set to build a better classifier @ @ approach presented @ depends on a meaning calculation @ computes @ word meaning score in @ scope of class @ meaning computation is constructed on @ helmholtz principle and utilized to various application in @ field of text mining like feature extraction information retrieval and document summarization @ nevertheless according to @ literature ilbom is @ first work @ us meaning calculation in a semi-supervised way to construct a semantic smoothing kernel @ support vector machine @ svm @ @ evaluation of @ proposed methodology is done by performing various experiment on standard textual datasets @ ilbom @ s experimental @ @ compared @ three baseline algorithm including svm @ linear kernel @ is @ of @ @ frequently used algorithm in text classification field @ experimental @ @ @ labeling unlabeled instance based on meaning score of word to augment @ training set is valuable and increase @ classification accuracy on @ unseen test instance significantly @ @ ltd
1833,Graph coloring and ACO based summarization for social networks,"Due to the increasing popularity of contents of social media platforms, the number of posts and messages is steadily increasing. A huge amount of data is generated daily as an outcome of the interactions between fans of the networking platforms. It becomes extremely troublesome to find the most relevant, interactive information for the subscribers. The aim of this work is to enable the users to get a powerful brief of comments without reading the entire list. This paper opens up a new field of short text summarization (STS) predicated on a hybrid ant colony optimization coming with a mechanism of local search, called ACO-LS-STS, to produce an optimal or near-optimal summary. Initially, the graph coloring algorithm, called GC-ISTS, was employed before to shrink the solution area of ants to small sets. Evidently, the main purpose of using the GC algorithm is to make the search process more facilitated, faster and prevents the ants from falling into the local optimum. First, the dissimilar comments are assembled together into the same color, at the same time preserving the information ratio as for an original list of comment. Subsequently, activating the ACO-LS-STS algorithm, which is a novel technique concerning the extraction of the most interactive comments from each color in a parallel form. At the end, the best summary is picked from the best color. This problem is formalized as an optimization problem utilizing GC and ACO-LS to generate the optimal solution. Eventually, the proposed algorithm was evaluated and tested over a collection of Facebook messages with their associated comments. Indeed, it was found that the proposed algorithm has an ability to capture a good solution that is guaranteed to be near optimal and had realized notable performance in comparison with traditional document summarization algorithms. © 2017 Elsevier Ltd",2017,Expert Systems with Applications,24,due to @ increasing popularity of content of social medium platform @ number of post and message is steadily increasing @ a huge amount of data is generated daily a @ outcome of @ interaction @ fan of @ networking platform @ @ becomes extremely troublesome to find @ @ relevant interactive information @ @ subscriber @ @ aim of @ work is to enable @ user to get a powerful brief of comment without reading @ entire list @ @ @ open up a @ field of short text summarization @ sts @ predicated on a hybrid ant colony optimization coming @ a mechanism of local search called aco-ls-sts to produce @ optimal @ near-optimal summary @ initially @ graph coloring algorithm called gc-ists wa employed @ to shrink @ solution area of ant to small set @ evidently @ main purpose of @ @ gc algorithm is to make @ search process more facilitated faster and prevents @ ant @ falling @ @ local optimum @ first @ dissimilar comment @ assembled together @ @ @ color at @ @ time preserving @ information ratio a @ @ original list of comment @ subsequently activating @ aco-ls-sts algorithm @ is a novel technique concerning @ extraction of @ @ interactive comment @ @ color in a parallel form @ at @ end @ best summary is picked @ @ best color @ @ problem is formalized a @ optimization problem utilizing gc and aco-ls to generate @ optimal solution @ eventually @ proposed algorithm wa evaluated and tested @ a collection of facebook message @ @ associated comment @ indeed @ wa found @ @ proposed algorithm ha @ ability to capture a good solution @ is guaranteed to @ near optimal and @ realized notable performance in comparison @ traditional document summarization algorithm @ @ ltd
1838,Current state of text sentiment analysis from opinion to emotion mining,"Sentiment analysis from text consists of extracting information about opinions, sentiments, and even emotions conveyed by writers towards topics of interest. It is often equated to opinion mining, but it should also encompass emotion mining. Opinion mining involves the use of natural language processing and machine learning to determine the attitude of a writer towards a subject. Emotion mining is also using similar technologies but is concerned with detecting and classifying writers emotions toward events or topics. Textual emotion-mining methods have various applications, including gaining information about customer satisfaction, helping in selecting teaching materials in e-learning, recommending products based on users emotions, and even predicting mental-health disorders. In surveys on sentiment analysis, which are often old or incomplete, the strong link between opinion mining and emotion mining is understated. This motivates the need for a different and new perspective on the literature on sentiment analysis, with a focus on emotion mining. We present the state-of-the-art methods and propose the following contributions: (1) a taxonomy of sentiment analysis; (2) a survey on polarity classification methods and resources, especially those related to emotion mining; (3) a complete survey on emotion theories and emotion-mining research; and (4) some useful resources, including lexicons and datasets. © 2017 ACM.",2017,ACM Computing Surveys,123,sentiment analysis @ text consists of extracting information @ opinion sentiment and even emotion conveyed by writer towards topic of interest @ @ is often equated to opinion mining @ @ @ @ encompass emotion mining @ opinion mining involves @ use of natural language processing and machine learning to determine @ attitude of a writer towards a subject @ emotion mining is @ @ similar technology @ is concerned @ detecting and classifying writer emotion toward event @ topic @ textual emotion-mining method @ various application including gaining information @ customer satisfaction helping in selecting teaching material in e-learning recommending product based on user emotion and even predicting mental-health disorder @ in survey on sentiment analysis @ @ often old @ incomplete @ strong link @ opinion mining and emotion mining is understated @ @ motivates @ need @ a different and @ perspective on @ literature on sentiment analysis @ a focus on emotion mining @ @ @ @ state-of-the-art method and propose @ following contribution @ @ @ a taxonomy of sentiment analysis @ @ @ a survey on polarity classification method and resource especially @ related to emotion mining @ @ @ a complete survey on emotion theory and emotion-mining research @ and @ @ some useful resource including lexicon and datasets @ acm @ 
1844,Subjective text mining for Arabic social media,"The need for designing Arabic text mining systems for the use on social media posts is increasingly becoming a significant and attractive research area. It serves and enhances the knowledge needed in various domains. The main focus of this paper is to propose a novel framework combining sentiment analysis with subjective analysis on Arabic social media posts to determine whether people are interested or not interested in a defined subject. For those purposes, text classification methods- including preprocessing and machine learning mechanisms-are applied. Essentially, the performance of the framework is tested using Twitter as a data source, where possible volunteers on a certain subject are identified based on their posted tweets along with their subject-related information. Twitter is considered because of its popularity and its rich content from online microblogging services. The results obtained are very promising with an accuracy of 89%, thereby encouraging further research. © 2017, IGI Global.",2017,International Journal on Semantic Web and Information Systems,5,@ need @ designing arabic text mining system @ @ use on social medium post is increasingly becoming a significant and attractive research area @ @ serf and enhances @ knowledge needed in various domain @ @ main focus of @ @ is to propose a novel framework combining sentiment analysis @ subjective analysis on arabic social medium post to determine whether people @ interested @ not interested in a defined subject @ @ @ purpose text classification method including preprocessing and machine learning mechanisms-are applied @ essentially @ performance of @ framework is tested @ twitter a a data source @ possible volunteer on a certain subject @ identified based on @ posted tweet along @ @ subject-related information @ twitter is considered @ of @ popularity and @ rich content @ online microblogging service @ @ @ obtained @ @ promising @ @ accuracy of thereby encouraging @ research @ igi global @ 
1845,Forecasting Oil Price Trends with Sentiment of Online News Articles,"With the rapid development of the Internet and big data technologies, a rich of online data (including news releases) can helpfully facilitate forecasting oil price trends. Accordingly, this study introduces sentiment analysis, a useful big data analysis tool, to understand the relevant information of online news articles and formulate an oil price trend prediction method with sentiment. Three main steps are included in the proposed method, i.e., sentiment analysis, relationship investigation and trend prediction. In sentiment analysis, the sentiment (or tone) is extracted based on a dictionary-based approach to capture the relevant online information concerning oil markets and the driving factors. In relationship investigation, the Granger causality analysis is conducted to explore whether and how the sentiment impacts oil price. In trend prediction, the sentiment is used as an important independent variable, and some popular forecasting models, e.g., logistic regression, support vector machine, decision tree and back propagation neural network, are performed. With crude oil futures prices of the West Texas Intermediate (WTI) and news articles of the Thomson Reuters as studying samples, the empirical results statistically support the powerful predictive power of sentiment for oil price trends and hence the effectiveness of the proposed method. © 2017 World Scientific Publishing Co.",2017,Asia-Pacific Journal of Operational Research,6,@ @ rapid development of @ internet and big data technology a rich of online data @ including news release @ @ helpfully facilitate forecasting oil price trend @ accordingly @ study introduces sentiment analysis a useful big data analysis tool to understand @ relevant information of online news article and formulate @ oil price trend prediction method @ sentiment @ three main step @ included in @ proposed method i @ e @ sentiment analysis relationship investigation and trend prediction @ in sentiment analysis @ sentiment @ @ tone @ is extracted based on a dictionary-based approach to capture @ relevant online information concerning oil market and @ driving factor @ in relationship investigation @ granger causality analysis is conducted to explore whether and @ @ sentiment impact oil price @ in trend prediction @ sentiment is used a @ important independent variable and some popular forecasting model e @ g @ logistic regression support vector machine decision tree and back propagation neural network @ performed @ @ crude oil future price of @ west texas intermediate @ wti @ and news article of @ thomson reuters a studying sample @ empirical @ statistically support @ powerful predictive power of sentiment @ oil price trend and hence @ effectiveness of @ proposed method @ world scientific publishing co @ 
1848,A novel approach for automatic text analysis and generation for the cultural heritage domain,"Knowledge is information that has been contextualised in a certain domain, to be used or applied. It represents the basic core of our Cultural Heritage and Natural Language provides us with prime versatile means of construing experience at multiple levels of organization. The natural language generation field consists in the creation of texts providing information contained in other kind of sources (numerical data, graphics, taxonomies and ontologies or even other texts), with the aim of making such texts indistinguishable, as far as possible, from those created by humans. On the other hand, the knowledge extraction, basing on text mining and text analysis tasks, as examples of the many applications born from computational linguistic, provides summarization, categorization, topics extractions from textual resources using linguistic concepts, which deal with the imprecision and ambiguity of human language. This paper presents a research activity focused on exploring and scientifically describing knowledge structure and organization involved in textual resources’ generation. Thus, a novel multidimensional model for the representation of conceptual knowledge, is proposed. Furthermore, a real case study in the Cultural Heritage domain is described to demonstrate the effectiveness and the feasibility of the proposed model and approach. © 2016, Springer Science+Business Media New York.",2017,Multimedia Tools and Applications,4,knowledge is information @ ha @ contextualised in a certain domain to @ used @ applied @ @ represents @ basic core of @ cultural heritage and natural language provides u @ prime versatile mean of construing experience at multiple level of organization @ @ natural language generation field consists in @ creation of text providing information contained in @ kind of source @ numerical data graphic taxonomy and ontology @ even @ text @ @ @ aim of making @ text indistinguishable a far a possible @ @ created by human @ on @ @ hand @ knowledge extraction basing on text mining and text analysis task a example of @ many application born @ computational linguistic provides summarization categorization topic extraction @ textual resource @ linguistic concept @ deal @ @ imprecision and ambiguity of human language @ @ @ @ a research activity focused on exploring and scientifically describing knowledge structure and organization involved in textual resource generation @ thus a novel multidimensional model @ @ representation of conceptual knowledge is proposed @ furthermore a real case study in @ cultural heritage domain is described to demonstrate @ effectiveness and @ feasibility of @ proposed model and approach @ @ science @ medium @ york @ 
1869,Understand Short Texts by Harvesting and Analyzing Semantic Knowledge,"Understanding short texts is crucial to many applications, but challenges abound. First, short texts do not always observe the syntax of a written language. As a result, traditional natural language processing tools, ranging from part-of-speech tagging to dependency parsing, cannot be easily applied. Second, short texts usually do not contain sufficient statistical signals to support many state-of-the-art approaches for text mining such as topic modeling. Third, short texts are more ambiguous and noisy, and are generated in an enormous volume, which further increases the difficulty to handle them. We argue that semantic knowledge is required in order to better understand short texts. In this work, we build a prototype system for short text understanding which exploits semantic knowledge provided by a well-known knowledgebase and automatically harvested from a web corpus. Our knowledge-intensive approaches disrupt traditional methods for tasks such as text segmentation, part-of-speech tagging, and concept labeling, in the sense that we focus on semantics in all these tasks. We conduct a comprehensive performance evaluation on real-life data. The results show that semantic knowledge is indispensable for short text understanding, and our knowledge-intensive approaches are both effective and efficient in discovering semantics of short texts. © 2016 IEEE.",2017,IEEE Transactions on Knowledge and Data Engineering,34,understanding short text is crucial to many application @ challenge abound @ first short text @ not always observe @ syntax of a written language @ a a @ traditional natural language processing tool ranging @ part-of-speech tagging to dependency parsing cannot @ easily applied @ second short text usually @ not contain sufficient statistical signal to support many state-of-the-art approach @ text mining @ a topic modeling @ third short text @ more ambiguous and noisy and @ generated in @ enormous volume @ @ increase @ difficulty to handle @ @ @ argue @ semantic knowledge is required in order to better understand short text @ in @ work @ build a prototype system @ short text understanding @ exploit semantic knowledge provided by a well-known knowledgebase and automatically harvested @ a web corpus @ @ knowledge-intensive approach disrupt traditional method @ task @ a text segmentation part-of-speech tagging and concept labeling in @ sense @ @ focus on semantics in @ @ task @ @ conduct a comprehensive performance evaluation on real-life data @ @ @ @ @ semantic knowledge is indispensable @ short text understanding and @ knowledge-intensive approach @ @ effective and efficient in discovering semantics of short text @ @ @ 
1870,Three data analytics party tricks,"Making little software tools might seem trivial, but ""party tricks"" are a good way to explore a new field, find useful code libraries, and help build skills. In this spirit, it is fun and instructive to apply machine learning methods to text-mining tasks. It is especially interesting to use bibliographic data from the journal Geophysics because the results actually might be useful to those conducting geophysical research. For example, by vectorizing abstracts-using free and open-source natural-language processing tools in Python-it is possible to use the vector space to find nearby abstracts and interpret those as being similar in content. This forms the basis of a recommendation engine for geophysical papers. If not outright useful, then the party trick still might be interesting. For example, the collaboration network from the journal reveals the most prolific collaborators as George McMechan, Alan Green, and Jerry Harris, and it lets us calculate the collaboration distance between Brian Russell and Sergey Fomel (it is 4). Other party tricks are less useful and strictly silly, for example a recurrent neural network that generates random articles and authors from a parallel universe (e.g., Like-wave beam by D. J. Laniert; one imagines Like waves are a sort of attenuated Love wave). © 2017 by The Society of Exploration Geophysicists.",2017,Leading Edge,0,making little software tool might seem trivial @ @ party trick @ @ a good way to explore a @ field find useful code library and help build skill @ in @ spirit @ is fun and instructive to apply machine learning method to text-mining task @ @ is especially interesting to use bibliographic data @ @ journal geophysics @ @ @ actually might @ useful to @ conducting geophysical research @ @ example by vectorizing abstracts-using free and open-source natural-language processing tool in python-it is possible to use @ vector space to find nearby abstract and interpret @ a @ similar in content @ @ form @ basis of a recommendation engine @ geophysical @ @ if not outright useful @ @ party trick still might @ interesting @ @ example @ collaboration network @ @ journal reveals @ @ prolific collaborator a george mcmechan alan green and jerry harris and @ let u calculate @ collaboration distance @ brian russell and sergey fomel @ @ is @ @ @ party trick @ le useful and strictly silly @ example a recurrent neural network @ generates random article and author @ a parallel universe @ e @ g @ like-wave beam by @ @ j @ laniert @ @ imago like wave @ a sort of attenuated love wave @ @ by @ society of exploration geophysicist @ 
1871,Sampling strategies for information extraction over the deep web,"Information extraction systems discover structured information in natural language text. Having information in structured form enables much richer querying and data mining than possible over the natural language text. However, information extraction is a computationally expensive task, and hence improving the efficiency of the extraction process over large text collections is of critical interest. In this paper, we focus on an especially valuable family of text collections, namely, the so-called deep-web text collections, whose contents are not crawlable and are only available via querying. Important steps for efficient information extraction over deep-web text collections (e.g., selecting the collections on which to focus the extraction effort, based on their contents; or learning which documents within these collections—and in which order—to process, based on their words and phrases) require having a representative document sample from each collection. These document samples have to be collected by querying the deep-web text collections, an expensive process that renders impractical the existing sampling approaches developed for other data scenarios. In this paper, we systematically study the space of query-based document sampling techniques for information extraction over the deep web. Specifically, we consider (i) alternative query execution schedules, which vary on how they account for the query effectiveness, and (ii) alternative document retrieval and processing schedules, which vary on how they distribute the extraction effort over documents. We report the results of the first large-scale experimental evaluation of sampling techniques for information extraction over the deep web. Our results show the merits and limitations of the alternative query execution and document retrieval and processing strategies, and provide a roadmap for addressing this critically important building block for efficient, scalable information extraction. © 2016 Elsevier Ltd",2017,Information Processing and Management,8,information extraction system discover structured information in natural language text @ @ information in structured form enables much richer querying and data mining @ possible @ @ natural language text @ however information extraction is a computationally expensive task and hence improving @ efficiency of @ extraction process @ @ text collection is of critical interest @ in @ @ @ focus on @ especially valuable family of text collection namely @ so-called deep-web text collection whose content @ not crawlable and @ only available via querying @ important step @ efficient information extraction @ deep-web text collection @ e @ g @ selecting @ collection on @ to focus @ extraction effort based on @ content @ @ learning @ document within @ collection and in @ order to process based on @ word and phrase @ require @ a representative document sample @ @ collection @ @ document sample @ to @ collected by querying @ deep-web text collection @ expensive process @ render impractical @ existing sampling approach developed @ @ data scenario @ in @ @ @ systematically study @ space of query-based document sampling technique @ information extraction @ @ deep web @ specifically @ consider @ i @ alternative query execution schedule @ vary on @ @ account @ @ query effectiveness and @ ii @ alternative document retrieval and processing schedule @ vary on @ @ distribute @ extraction effort @ document @ @ report @ @ of @ first large-scale experimental evaluation of sampling technique @ information extraction @ @ deep web @ @ @ @ @ merit and limitation of @ alternative query execution and document retrieval and processing strategy and provide a roadmap @ addressing @ critically important building block @ efficient scalable information extraction @ @ ltd
1873,On the risk prediction and analysis of soft information in finance reports,"We attempt in this paper to utilize soft information in financial reports to analyze financial risk among companies. Specifically, on the basis of the text information in financial reports, which is the so-called soft information, we apply analytical techniques to study relations between texts and financial risk. Furthermore, we conduct a study on financial sentiment analysis by using a finance-specific sentiment lexicon to examine the relations between financial sentiment words and financial risk. A large collection of financial reports published annually by publicly-traded companies is employed to conduct our experiments; moreover, two analytical techniques – regression and ranking methods – are applied to conduct these analyses. The experimental results show that, based on a bag-of-words model, using only financial sentiment words results in performance comparable to using the whole texts; this confirms the importance of financial sentiment words with respect to risk prediction. In addition to this performance comparison, via the learned models, we draw attention to some strong and interesting correlations between texts and financial risk. These valuable findings yield greater insight and understanding into the usefulness of soft information in financial reports and can be applied to a broad range of financial and accounting applications. © 2016 Elsevier B.V.",2017,European Journal of Operational Research,18,@ attempt in @ @ to utilize soft information in financial report to analyze financial risk among company @ specifically on @ basis of @ text information in financial report @ is @ so-called soft information @ apply analytical technique to study relation @ text and financial risk @ furthermore @ conduct a study on financial sentiment analysis by @ a finance-specific sentiment lexicon to examine @ relation @ financial sentiment word and financial risk @ a @ collection of financial report published annually by publicly-traded company is employed to conduct @ experiment @ moreover @ analytical technique regression and ranking method @ applied to conduct @ analysis @ @ experimental @ @ @ based on a bag-of-words model @ only financial sentiment word @ in performance comparable to @ @ whole text @ @ confirms @ importance of financial sentiment word @ respect to risk prediction @ in addition to @ performance comparison via @ learned model @ draw attention to some strong and interesting correlation @ text and financial risk @ @ valuable finding yield greater insight and understanding @ @ usefulness of soft information in financial report and @ @ applied to a broad range of financial and accounting application @ @ b @ v @ 
1874,Ant colony heuristic for user-contributed comments summarization,"User-contributed comments (UCC) are one of the signs of the social media. Due to the high popularity of social media, it becomes already exceedingly difficult to find the most relevant, interactive information for the users. The motivation behind this work is the fact that users may interest to get an efficacious brief understanding of comments without reading the entire comments. This paper opens up an unconventional field of comment's summarization predicated on Ant colony optimization mixed with Jensen–Shannon divergence (ACO-JSD). ACO-JSD is a proposed novel technique concerning the extraction the most interactive comments from the huge amount of concise comment's perspectives. This problem is unfastened utilizing ACO to generate the optimal solution. Moreover, the JSD model is employed to ensure a summary could capture the essence of the original comments. First, an acyclic semi-graph has been constructed under two constraints: (1) the longest comments will be isolated from the graph, (2) The more similarity between two comments, the greater the chance that mutual connectivity is eliminated. Next, a feasible solution is constructed to select the high-quality summarization. Finally, the proposed algorithm has been evaluated over a collection of Facebook posts with their associated comments and an excellent performance in comparison with traditional document summarization algorithms was obtained. Accordingly, the computational results show the efficiency of the proposed algorithm, as well as its ability to find a good summary that is guaranteed to be near-optimal. © 2016 Elsevier B.V.",2017,Knowledge-Based Systems,13,user-contributed comment @ ucc @ @ @ of @ sign of @ social medium @ due to @ high popularity of social medium @ becomes already exceedingly difficult to find @ @ relevant interactive information @ @ user @ @ motivation behind @ work is @ fact @ user may interest to get @ efficacious brief understanding of comment without reading @ entire comment @ @ @ open up @ unconventional field of comment @ s summarization predicated on ant colony optimization mixed @ jensen shannon divergence @ aco-jsd @ @ aco-jsd is a proposed novel technique concerning @ extraction @ @ interactive comment @ @ huge amount of concise comment @ s perspective @ @ problem is unfastened utilizing aco to generate @ optimal solution @ moreover @ jsd model is employed to ensure a summary could capture @ essence of @ original comment @ first @ acyclic semi-graph ha @ constructed @ @ constraint @ @ @ @ longest comment @ @ isolated @ @ graph @ @ @ more similarity @ @ comment @ greater @ chance @ mutual connectivity is eliminated @ next a feasible solution is constructed to select @ high-quality summarization @ finally @ proposed algorithm ha @ evaluated @ a collection of facebook post @ @ associated comment and @ excellent performance in comparison @ traditional document summarization algorithm wa obtained @ accordingly @ computational @ @ @ efficiency of @ proposed algorithm a well a @ ability to find a good summary @ is guaranteed to @ near-optimal @ @ b @ v @ 
1880,A feature selection model based on genetic rank aggregation for text sentiment classification,"Sentiment analysis is an important research direction of natural language processing, text mining and web mining which aims to extract subjective information in source materials. The main challenge encountered in machine learning method-based sentiment classification is the abundant amount of data available. This amount makes it difficult to train the learning algorithms in a feasible time and degrades the classification accuracy of the built model. Hence, feature selection becomes an essential task in developing robust and efficient classification models whilst reducing the training time. In text mining applications, individual filter-based feature selection methods have been widely utilized owing to their simplicity and relatively high performance. This paper presents an ensemble approach for feature selection, which aggregates the several individual feature lists obtained by the different feature selection methods so that a more robust and efficient feature subset can be obtained. In order to aggregate the individual feature lists, a genetic algorithm has been utilized. Experimental evaluations indicated that the proposed aggregation model is an efficient method and it outperforms individual filter-based feature selection methods on sentiment classification. © The Author(s) 2015.",2017,Journal of Information Science,56,sentiment analysis is @ important research direction of natural language processing text mining and web mining @ aim to extract subjective information in source material @ @ main challenge encountered in machine learning method-based sentiment classification is @ abundant amount of data available @ @ amount make @ difficult to train @ learning algorithm in a feasible time and degrades @ classification accuracy of @ built model @ hence feature selection becomes @ essential task in developing robust and efficient classification model whilst reducing @ training time @ in text mining application individual filter-based feature selection method @ @ widely utilized owing to @ simplicity and relatively high performance @ @ @ @ @ ensemble approach @ feature selection @ aggregate @ several individual feature list obtained by @ different feature selection method @ @ a more robust and efficient feature subset @ @ obtained @ in order to aggregate @ individual feature list a genetic algorithm ha @ utilized @ experimental evaluation indicated @ @ proposed aggregation model is @ efficient method and @ outperforms individual filter-based feature selection method on sentiment classification @ @ author @ s @ @ 
1881,Using text mining techniques for identifying research gaps and priorities: a case study of the environmental science in Iran,"This study aims to observe the researchers’ behavior in Iranian scientific databases to determine the research gaps and priorities in their field of research. Text mining and natural language processing techniques were used to identify what researchers are looking for and to analyze existing research works. In this paper, the information about the behavior of researchers who work in the field of environmental science and existing research works in the Iranian scientific database are processed. The search trends in all areas are evaluated by analyzing the users’ search data. The trend analysis indicates that in the period of February 2013 to July 2015, the growth of the researchers’ requests in some domains of the environment such as Industry, Training, Assessment, Material, Water and Pollution was 1.5 up to 2 times more than the overall requests. A Combination of the trend analysis and clustering of queries led to shaping four priority zones. Then, the research priorities for each environmental research area were determined. The results show that Training, Pollution, Rangeland, Management and Law are those domains in the environmental research which have the most research gaps in Iran, but there are enough research in Forest, Soil and Industry domains. At the end, we describe the steps for the implementation of a decision support system in environmental research management. Researchers, managers and policy makers can use this proposed “research demand and supply monitoring” system or RDSM to make appropriate decisions and allocate their resources more efficiently. © 2016, Akadémiai Kiadó, Budapest, Hungary.",2017,Scientometrics,6,@ study aim to observe @ researcher behavior in iranian scientific database to determine @ research gap and priority in @ field of research @ text mining and natural language processing technique @ used to identify @ researcher @ looking @ and to analyze existing research work @ in @ @ @ information @ @ behavior of researcher @ work in @ field of environmental science and existing research work in @ iranian scientific database @ processed @ @ search trend in @ area @ evaluated by analyzing @ user search data @ @ trend analysis indicates @ in @ period of february to july @ growth of @ researcher request in some domain of @ environment @ a industry training assessment material water and pollution wa @ up to time more @ @ overall request @ a combination of @ trend analysis and clustering of query led to shaping four priority zone @ @ @ research priority @ @ environmental research area @ determined @ @ @ @ @ training pollution rangeland management and law @ @ domain in @ environmental research @ @ @ @ research gap in iran @ @ @ enough research in forest soil and industry domain @ at @ end @ describe @ step @ @ implementation of a decision support system in environmental research management @ researcher manager and policy maker @ use @ proposed research demand and supply monitoring system @ rdsm to make appropriate decision and allocate @ resource more efficiently @ akadémiai kiadó budapest hungary @ 
1897,Toward an automatic classification of negotiation styles using natural language processing,"We present a natural language processing model that allows automatic classification and prediction of the user’s negotiation style during the interaction with virtual humans in a 3D game. We collected the sentences used in the interactions of the users with virtual artificial agents and their associated negotiation style as measured by ROCI-II test. We analyzed the documents containing the sentences for each style applying text mining techniques and found statistical differences among the styles in agreement with their theoretical definitions. Finally, we trained two machine learning classifiers on the two datasets using pre-trained Word2Vec embeddings. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ @ a natural language processing model @ allows automatic classification and prediction of @ user s negotiation style @ @ interaction @ virtual human in a @ game @ @ collected @ sentence used in @ interaction of @ user @ virtual artificial agent and @ associated negotiation style a measured by roci-ii test @ @ analyzed @ document containing @ sentence @ @ style applying text mining technique and found statistical difference among @ style in agreement @ @ theoretical definition @ finally @ trained @ machine learning classifier on @ @ datasets @ pre-trained word vec embeddings @ @ international publishing ag @ 
1898,Natural Language Processing for the Semantic Web,"This book introduces core natural language processing (NLP) technologies to non-experts in an easily accessible way, as a series of building blocks that lead the user to understand key technologies, why they are required, and how to integrate them into Semantic Web applications. Natural language processing and Semantic Web technologies have different, but complementary roles in data management. Combining these two technologies enables structured and unstructured data to merge seamlessly. Semantic Web technologies aim to convert unstructured data to meaningful representations, which benefit enormously from the use of NLP technologies, thereby enabling applications such as connecting text to Linked Open Data, connecting texts to each other, semantic searching, information visualization, and modeling of user behavior in online networks. The first half of this book describes the basic NLP processing tools: tokenization, part-of-speech tagging, and morphological analysis, in addition to the main tools required for an information extraction system (named entity recognition and relation extraction) which build on these components. The second half of the book explains how Semantic Web and NLP technologies can enhance each other, for example via semantic annotation, ontology linking, and population. These chapters also discuss sentiment analysis, a key component in making sense of textual data, and the difficulties of performing NLP on social media, as well as some proposed solutions. The book finishes by investigating some applications of these tools, focusing on semantic search and visualization, modeling user behavior, and an outlook on the future. © Copyright 2017 by Morgan & Claypool.",2017,Synthesis Lectures on the Semantic Web: Theory and Technology,8,@ book introduces core natural language processing @ nlp @ technology to non-experts in @ easily accessible way a a series of building block @ lead @ user to understand key technology @ @ @ required and @ to integrate @ @ semantic web application @ natural language processing and semantic web technology @ different @ complementary role in data management @ combining @ @ technology enables structured and unstructured data to merge seamlessly @ semantic web technology aim to convert unstructured data to meaningful representation @ benefit enormously @ @ use of nlp technology thereby enabling application @ a connecting text to linked open data connecting text to @ @ semantic searching information visualization and modeling of user behavior in online network @ @ first half of @ book describes @ basic nlp processing tool @ tokenization part-of-speech tagging and morphological analysis in addition to @ main tool required @ @ information extraction system @ named entity recognition and relation extraction @ @ build on @ component @ @ second half of @ book explains @ semantic web and nlp technology @ enhance @ @ @ example via semantic annotation ontology linking and population @ @ chapter @ discus sentiment analysis a key component in making sense of textual data and @ difficulty of performing nlp on social medium a well a some proposed solution @ @ book finish by investigating some application of @ tool focusing on semantic search and visualization modeling user behavior and @ outlook on @ future @ @ by morgan claypool @ 
1902,Interactive text mining suite: Data visualization for literary studies,"In recent years, there has been growing interest in visualization methods for literary text analysis. While text mining and visualization tools have evolved into mainstream research methods in many fields (e.g. social sciences, machine learning), their application to literary studies still remains infrequent. In addition to technological challenges, the use of these tools requires a methodological shift from traditional close reading to distant reading approaches. This transition also aligns digital humanities with corpus linguistics, which still ""remains obscure"" and not fully embraced by digital humanists [16]. To address some of these challenges, we introduce Interactive Text Mining Suite, a user-friendly toolkit developed both for digital humanists and corpus linguists. We further demonstrate that the integration of visual analytics and corpus linguistics methods helps unveil language patterns, otherwise hidden from a human eye. Making use of both linguistically annotated data and natural language processing techniques, we are able to discern patterns of part-of-speech uses in Medieval Occitan manuscript Romance de Flamenca and its English translation. Furthermore, visual analysis not only detects stylistic differences at a word level, but also at sentential and document levels. While preserving traditional close reading techniques, this toolkit makes it possible to apply an interactive control over documents, thus allowing for a ""synthesis of computational and humanistic modes of inquiry"" [18]. © 2017, CEUR-WS. All rights reserved.",2017,CEUR Workshop Proceedings,3,in recent year @ ha @ growing interest in visualization method @ literary text analysis @ @ text mining and visualization tool @ evolved @ mainstream research method in many field @ e @ g @ social science machine learning @ @ application to literary study still remains infrequent @ in addition to technological challenge @ use of @ tool requires a methodological shift @ traditional close reading to distant reading approach @ @ transition @ aligns digital humanity @ corpus linguistics @ still @ remains obscure @ and not fully embraced by digital humanist @ to address some of @ challenge @ introduce interactive text mining suite a user-friendly toolkit developed @ @ digital humanist and corpus linguist @ @ @ demonstrate @ @ integration of visual analytics and corpus linguistics method help unveil language pattern otherwise hidden @ a human eye @ making use of @ linguistically annotated data and natural language processing technique @ @ able to discern pattern of part-of-speech us in medieval occitan manuscript romance de flamenca and @ english translation @ furthermore visual analysis not only detects stylistic difference at a word level @ @ at sentential and document level @ @ preserving traditional close reading technique @ toolkit make @ possible to apply @ interactive control @ document thus allowing @ a @ synthesis of computational and humanistic mode of inquiry @ @ ceur-ws @ @ right reserved @ 
1905,Expert-based text mining with Delphi method for crude oil price prediction,"As crude is one of the most important commodities, the crude price forecasting has continuously been a centre of interest. The traditional techniques are focusing on econometric models which could not cope with the short term abnormality. Text data mining from news articles could be an effective method to predict the crude oil price variation caused by irregularities, but, the main issues in text mining originate from the particularities of natural language. In this research, the expert-based Delphi text mining (EDTM) is proposed to predict the movement of crude prices when the irregularity occurs. We employ the hierarchical clustering algorithm to reveal implicit knowledge hidden in news streams. Next, the Delphi method is introduced to give weighted ratings for different corresponding events extracted from the news. Finally, a comprehensive experiment is illustrated to show the effectiveness. ©2017 Inderscience Enterprises Ltd.",2017,International Journal of Industrial and Systems Engineering,3,a crude is @ of @ @ important commodity @ crude price forecasting ha continuously @ a centre of interest @ @ traditional technique @ focusing on econometric model @ could not cope @ @ short term abnormality @ text data mining @ news article could @ @ effective method to predict @ crude oil price variation caused by irregularity @ @ main issue in text mining originate @ @ particularity of natural language @ in @ research @ expert-based delphi text mining @ edtm @ is proposed to predict @ movement of crude price @ @ irregularity occurs @ @ employ @ hierarchical clustering algorithm to reveal implicit knowledge hidden in news stream @ next @ delphi method is introduced to give weighted rating @ different corresponding event extracted @ @ news @ finally a comprehensive experiment is illustrated to @ @ effectiveness @ inderscience enterprise ltd @ 
1906,A survey of text mining in social media: Facebook and Twitter perspectives,"Text mining has become one of the trendy fields that has been incorporated in several research fields such as computational linguistics, Information Retrieval (IR) and data mining. Natural Language Processing (NLP) techniques were used to extract knowledge from the textual text that is written by human beings. Text mining reads an unstructured form of data to provide meaningful information patterns in a shortest time period. Social networking sites are a great source of communication as most of the people in today’s world use these sites in their daily lives to keep connected to each other. It becomes a common practice to not write a sentence with correct grammar and spelling. This practice may lead to different kinds of ambiguities like lexical, syntactic, and semantic and due to this type of unclear data, it is hard to find out the actual data order. Accordingly, we are conducting an investigation with the aim of looking for different text mining methods to get various textual orders on social media websites. This survey aims to describe how studies in social media have used text analytics and text mining techniques for the purpose of identifying the key themes in the data. This survey focused on analyzing the text mining studies related to Facebook and Twitter; the two dominant social media in the world. Results of this survey can serve as the baselines for future text mining research. © 2017 ASTES Publishers. All rights reserved.",2017,"Advances in Science, Technology and Engineering Systems",69,text mining ha become @ of @ trendy field @ ha @ incorporated in several research field @ a computational linguistics information retrieval @ ir @ and data mining @ natural language processing @ nlp @ technique @ used to extract knowledge @ @ textual text @ is written by human @ @ text mining read @ unstructured form of data to provide meaningful information pattern in a shortest time period @ social networking site @ a great source of communication a @ of @ people in today s world use @ site in @ daily life to keep connected to @ @ @ @ becomes a common practice to not write a sentence @ correct grammar and spelling @ @ practice may lead to different kind of ambiguity like lexical syntactic and semantic and due to @ type of unclear data @ is hard to find @ @ actual data order @ accordingly @ @ conducting @ investigation @ @ aim of looking @ different text mining method to get various textual order on social medium website @ @ survey aim to describe @ study in social medium @ used text analytics and text mining technique @ @ purpose of identifying @ key theme in @ data @ @ survey focused on analyzing @ text mining study related to facebook and twitter @ @ @ dominant social medium in @ world @ @ of @ survey @ serve a @ baseline @ future text mining research @ astes publisher @ @ right reserved @ 
1907,Characterizing regulatory documents and guidelines based on text mining,"Implementing rules, constraints, and requirements contained in regulatory documents such as standards or guidelines constitutes a mandatory task for organizations and institutions across several domains. Due to the amount of domain-specific information and actions encoded in these documents, organizations often need to establish cooperations between several departments and consulting experts to guide managers and employees in eliciting compliance requirements. Providing computer-based guidance and support for this often costly and tedious compliance task is the aim of this paper. The presented methodology utilizes well-known text mining techniques and clustering algorithms to classify (families) of documents according to topics and to derive significant sentences which support users in understanding and implementing compliance-related documents. Applying the approach to collections of documents from the security and the medical domain demonstrates that text mining is a promising domain-independent mean to provide support to the understanding, extraction, and analysis of regulatory documents. © 2017, Springer International Publishing AG.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,implementing rule constraint and requirement contained in regulatory document @ a standard @ guideline constitutes a mandatory task @ organization and institution across several domain @ due to @ amount of domain-specific information and action encoded in @ document organization often need to establish cooperation @ several department and consulting expert to guide manager and employee in eliciting compliance requirement @ providing computer-based guidance and support @ @ often costly and tedious compliance task is @ aim of @ @ @ @ presented methodology utilizes well-known text mining technique and clustering algorithm to classify @ family @ of document according to topic and to derive significant sentence @ support user in understanding and implementing compliance-related document @ applying @ approach to collection of document @ @ security and @ medical domain demonstrates @ text mining is a promising domain-independent mean to provide support to @ understanding extraction and analysis of regulatory document @ @ international publishing ag @ 
1908,Development of text mining tools for information retrieval from patents,"Biomedical literature is composed of an ever increasing number of publications in natural language. Patents are a relevant fraction of those, being important sources of information due to all the curated data from the granting process. However, their unstructured data turns the search of information a challenging task. To surpass that, Biomedical text mining (BioTM) creates methodologies to search and structure that data. Several BioTM techniques can be applied to patents. From those, Information Retrieval is the process where relevant data is obtained from collections of documents. In this work, a patent pipeline was developed and integrated into @Note2, an open-source computational framework for BioTM. This integration allows to run further BioTM tools over the patent documents, including Information Extraction processes as Named Entity Recognition or Relation Extraction. © Springer International Publishing AG 2017.",2017,Advances in Intelligent Systems and Computing,2,biomedical literature is composed of @ ever increasing number of publication in natural language @ patent @ a relevant fraction of @ @ important source of information due to @ @ curated data @ @ granting process @ however @ unstructured data turn @ search of information a challenging task @ to surpass @ biomedical text mining @ biotm @ creates methodology to search and structure @ data @ several biotm technique @ @ applied to patent @ @ @ information retrieval is @ process @ relevant data is obtained @ collection of document @ in @ work a patent pipeline wa developed and integrated @ note @ open-source computational framework @ biotm @ @ integration allows to run @ biotm tool @ @ patent document including information extraction process a named entity recognition @ relation extraction @ @ international publishing ag @ 
1909,A text mining-based framework for constructing an RDF-compliant biodiversity knowledge repository,"In our aim to make the information encapsulated by biodiversity literature more accessible and searchable, we have developed a text mining-based framework for automatically transforming text into a structured knowledge repository. A text mining workflow employing information extraction techniques, i.e., named entity recognition and relation extraction, was implemented in the Argo platform and was subsequently applied on biodiversity literature to extract structured information. The resulting annotations were stored in a repository following the emerging Open Annotation standard, thus promoting interoperability with external applications. Accessible as a SPARQL endpoint, the repository facilitates knowledge discovery over a huge amount of biodiversity literature by retrieving annotations matching user-specified queries. We present some use cases to illustrate the types of queries that the knowledge repository currently accommodates. © Springer International Publishing AG 2017.",2017,Communications in Computer and Information Science,1,in @ aim to make @ information encapsulated by biodiversity literature more accessible and searchable @ @ developed a text mining-based framework @ automatically transforming text @ a structured knowledge repository @ a text mining workflow employing information extraction technique i @ e @ named entity recognition and relation extraction wa implemented in @ argo platform and wa subsequently applied on biodiversity literature to extract structured information @ @ resulting annotation @ stored in a repository following @ emerging open annotation standard thus promoting interoperability @ external application @ accessible a a sparql endpoint @ repository facilitates knowledge discovery @ a huge amount of biodiversity literature by retrieving annotation matching user-specified query @ @ @ some use case to illustrate @ type of query @ @ knowledge repository currently accommodates @ @ international publishing ag @ 
1911,Supporting biological pathway curation through text mining,"Text mining technology performs automated analysis of large document collections, in order to detect various aspects of information about their structure and meaning. This information can be used to develop systems that make it much easier for researchers to locate information of relevance to their needs in huge volumes of text, compared to standard search mechanisms. With a focus on the challenging task of constructing biological pathway models, which typically involves gathering, interpreting and combining complex information from a large number of publications, we show how text mining applications can provide various levels of support to ease the burden placed on pathway curators. Such support ranges from applications that provide help in searching and exploring the literature for evidence relevant to pathway reactions, to those which are able to make automated suggestions about how to construct and update pathway models. © Springer International Publishing AG 2017.",2017,Communications in Computer and Information Science,1,text mining technology performs automated analysis of @ document collection in order to detect various aspect of information @ @ structure and meaning @ @ information @ @ used to develop system @ make @ much easier @ researcher to locate information of relevance to @ need in huge volume of text compared to standard search mechanism @ @ a focus on @ challenging task of constructing biological pathway model @ typically involves gathering interpreting and combining complex information @ a @ number of publication @ @ @ text mining application @ provide various level of support to ease @ burden placed on pathway curator @ @ support range @ application @ provide help in searching and exploring @ literature @ evidence relevant to pathway reaction to @ @ @ able to make automated suggestion @ @ to construct and update pathway model @ @ international publishing ag @ 
1912,Performance of multiple string matching algorithms in text mining,"Ever since the evolution of Internet Information retrieval is being made by surfers in large amount. The data gets increased everyday as the thirst of acquiring knowledge by the users gets increased day-by-day. The data which is raw needs to be processed for usage which increases the potential value in all major areas like Education, Business etc. Therefore Text Mining is an emerging area where unstructured information were made as relevant information. Text mining process can be divided into Information Extraction, Topic Tracking, Summarization, Categorization, Clustering, concept Linkage and Information visualization. Even though all other things can be applied to text only properly it is extracted from the web. Using Pattern matching or String matching algorithms to retrieve proper results from the Sea of information. In this paper we discuss the three types of algorithms Aho Corasick, Wu Manber and Commentz Walter. The performance of the algorithms are identified by implementing it in Python language. Finally the suitable algorithm for extracting information is found. © Springer Nature Singapore Pte Ltd. 2017.",2017,Advances in Intelligent Systems and Computing,0,ever since @ evolution of internet information retrieval is @ made by surfer in @ amount @ @ data get increased everyday a @ thirst of acquiring knowledge by @ user get increased day-by-day @ @ data @ is raw need to @ processed @ usage @ increase @ potential value in @ major area like education @ etc @ therefore text mining is @ emerging area @ unstructured information @ made a relevant information @ text mining process @ @ divided @ information extraction topic tracking summarization categorization clustering concept linkage and information visualization @ even though @ @ thing @ @ applied to text only properly @ is extracted @ @ web @ @ pattern matching @ string matching algorithm to retrieve proper @ @ @ sea of information @ in @ @ @ discus @ three type of algorithm aho corasick wu manber and commentz walter @ @ performance of @ algorithm @ identified by implementing @ in python language @ finally @ suitable algorithm @ extracting information is found @ @ nature singapore pte ltd @ @ 
1913,Semantic association rule mining in text using domain ontology,"This paper reports a procedure for ontology-based association rule mining for knowledge extraction from text. Association rule mining (ARM) algorithms have the limitations of generating many non-interesting rules, huge number of discovered rules, and low algorithm performance. This research demonstrates a procedure for improving the performance of ARM in text mining by using domain ontology. A study context of Nigerian politics using news text from a Nigerian online newspaper was selected, and a methodology that combined natural language processing, ontology-based keywords extraction, and the modified Generating Association Rules based on Weighting (GARW) scheme was applied. The result revealed significant rule reduction in the number of generated rules, and produced rules, which are more semantically related to the problem context when compared to when ARM approaches that are not ontology-based is used. The study shows that domain ontology can improve the performance of ARM algorithms when dealing with unstructured textual data. Copyright © 2017 Inderscience Enterprises Ltd.",2017,"International Journal of Metadata, Semantics and Ontologies",2,@ @ report a procedure @ ontology-based association rule mining @ knowledge extraction @ text @ association rule mining @ arm @ algorithm @ @ limitation of generating many non-interesting rule huge number of discovered rule and low algorithm performance @ @ research demonstrates a procedure @ improving @ performance of arm in text mining by @ domain ontology @ a study context of nigerian politics @ news text @ a nigerian online newspaper wa selected and a methodology @ combined natural language processing ontology-based keywords extraction and @ modified generating association rule based on weighting @ garw @ scheme wa applied @ @ @ revealed significant rule reduction in @ number of generated rule and produced rule @ @ more semantically related to @ problem context @ compared to @ arm approach @ @ not ontology-based is used @ @ study @ @ domain ontology @ improve @ performance of arm algorithm @ dealing @ unstructured textual data @ @ inderscience enterprise ltd @ 
1914,Call center text mining framework,"In these days, the ability to convert call records from voice to text allows the application of text mining techniques on the call center text data. This study proposes a reusable software framework that automatically extracts subject and vocabulary from the conversation records that have been translated into sentences and whose contents are evaluated emotionally (positive/negative), customer satisfaction and customer representative performance. In consequence of this study, it is aimed to use the developed software framework for call center applications within Türk Telekom.",2017,CEUR Workshop Proceedings,0,in @ day @ ability to convert call record @ voice to text allows @ application of text mining technique on @ call center text data @ @ study proposes a reusable software framework @ automatically extract subject and vocabulary @ @ conversation record @ @ @ translated @ sentence and whose content @ evaluated emotionally @ positive negative @ customer satisfaction and customer representative performance @ in consequence of @ study @ is aimed to use @ developed software framework @ call center application within türk telekom @ 
1915,Active learning for text mining from crowds,"The benefits of crowdsourcing have been widely recognized in active learning for text mining. Due to the lack of golden ground-truth, it is crucial to evaluate how trustworthy of “noisy” labelers when labeling informative instances. Despite recent achievements made on active learning with crowdsourcing, most of the research works are involved in tuning a considerable amount of parameters, and also sensitive to noise. In this paper, a novel framework to select both the best-fitting labeler and the most informative instance is proposed, with the help of the minimum description length principle which is acknowledged as noise-tolerant and parameter-free. The algorithm is proved to be effective through extensive experiments on texts. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ benefit of crowdsourcing @ @ widely recognized in active learning @ text mining @ due to @ lack of golden ground-truth @ is crucial to evaluate @ trustworthy of noisy labelers @ labeling informative instance @ despite recent achievement made on active learning @ crowdsourcing @ of @ research work @ involved in tuning a considerable amount of parameter and @ sensitive to noise @ in @ @ a novel framework to select @ @ best-fitting labeler and @ @ informative instance is proposed @ @ help of @ minimum description length principle @ is acknowledged a noise-tolerant and parameter-free @ @ algorithm is proved to @ effective @ extensive experiment on text @ @ international publishing ag @ 
1917,Harnessing Online News for Sarcasm Detection in Hindi Tweets,"Detection of sarcasm in Indian languages is one of the most challenging tasks of Natural Language Processing (NLP) because Indian languages are ambiguous in nature and rich in morphology. Though Hindi is the fourth popular language in the world, sarcasm detection in it remains unexplored. One of the reasons is the lack of annotated resources. In the absence of sufficient resources, processing the NLP tasks such as POS tagging, sentiment analysis, text mining, sarcasm detection, etc., becomes tough for researchers. Here, we proposed a framework for sarcasm detection in Hindi tweets using online news. In this article, the online news is considered as the context of a given tweet during the detection of sarcasm. The proposed framework attains an accuracy of 79.4%. © 2017, Springer International Publishing AG.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,detection of sarcasm in indian language is @ of @ @ challenging task of natural language processing @ nlp @ @ indian language @ ambiguous in nature and rich in morphology @ though hindi is @ fourth popular language in @ world sarcasm detection in @ remains unexplored @ @ of @ reason is @ lack of annotated resource @ in @ absence of sufficient resource processing @ nlp task @ a po tagging sentiment analysis text mining sarcasm detection etc @ becomes tough @ researcher @ @ @ proposed a framework @ sarcasm detection in hindi tweet @ online news @ in @ article @ online news is considered a @ context of a given tweet @ @ detection of sarcasm @ @ proposed framework attains @ accuracy of @ @ @ international publishing ag @ 
1918,Personal research agents on the web of linked open data,"We introduce the concept of Personal Research Agents as semantics-based entities, capable of helping researchers who have to deal with the overwhelming amount of scientific literature to carry out their daily tasks. We demonstrate how a confluence of state-of-the-art techniques from the Semantic Web and Natural Language Processing domains can realize a proactive agent that can offer personalized services to researchers in retrieval and understanding of scientific literature, based on their background knowledge, interests and tasks. The agent’s knowledge base is populated with knowledge automatically extracted from scientific literature of a given domain using text mining techniques and represented in Linked Open Data (LOD) compliant format. Personalization is achieved through automated user profiling, based on a user’s publications. We implemented these ideas in an open source framework and demonstrate its applicability based on a corpus of open access computer science articles. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ introduce @ concept of personal research agent a semantics-based entity capable of helping researcher @ @ to deal @ @ overwhelming amount of scientific literature to carry @ @ daily task @ @ demonstrate @ a confluence of state-of-the-art technique @ @ semantic web and natural language processing domain @ realize a proactive agent @ @ offer personalized service to researcher in retrieval and understanding of scientific literature based on @ background knowledge interest and task @ @ agent s knowledge base is populated @ knowledge automatically extracted @ scientific literature of a given domain @ text mining technique and represented in linked open data @ lod @ compliant format @ personalization is achieved @ automated user profiling based on a user s publication @ @ implemented @ idea in @ open source framework and demonstrate @ applicability based on a corpus of open access computer science article @ @ international publishing ag @ 
1919,Max-cosine matching based neural models for recognizing textual entailment,"Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,recognizing textual entailment is a fundamental task in a variety of text mining @ natural language processing application @ @ @ proposes a simple neural model @ rte problem @ @ first match @ word in @ hypothesis @ @ most-similar word in @ premise producing @ augmented representation of @ hypothesis conditioned on @ premise a a sequence of word pair @ @ lstm model is @ used to model @ augmented sequence and @ final output @ @ lstm is fed @ a softmax layer to make @ prediction @ besides @ base model in order to enhance @ performance @ @ proposed three technique @ @ integration of multiple word-embedding library bi-way integration and ensemble based on model averaging @ experimental @ on @ snli dataset @ @ @ @ three technique @ effective in boosting @ predicative accuracy and @ @ method outperforms several state-of-the-state @ @ @ international publishing ag @ 
1921,Approaches to Cross-Domain Sentiment Analysis: A Systematic Literature Review,"A sentiment analysis has received a lot of attention from researchers working in the fields of natural language processing and text mining. However, there is a lack of annotated data sets that can be used to train a model for all domains, which is hampering the accuracy of sentiment analysis. Many research studies have attempted to tackle this issue and to improve cross-domain sentiment classification. In this paper, we present the results of a comprehensive systematic literature review of the methods and techniques employed in a cross-domain sentiment analysis. We focus on studies published during the period of 2010-2016. From our analysis of those works, it is clear that there is no perfect solution. Hence, one of the aims of this review is to create a resource in the form of an overview of the techniques, methods, and approaches that have been used to attempt to solve the problem of cross-domain sentiment analysis in order to assist researchers in developing new and more accurate techniques in the future. © 2013 IEEE.",2017,IEEE Access,48,a sentiment analysis ha received a lot of attention @ researcher working in @ field of natural language processing and text mining @ however @ is a lack of annotated data set @ @ @ used to train a model @ @ domain @ is hampering @ accuracy of sentiment analysis @ many research study @ attempted to tackle @ issue and to improve cross-domain sentiment classification @ in @ @ @ @ @ @ of a comprehensive systematic literature review of @ method and technique employed in a cross-domain sentiment analysis @ @ focus on study published @ @ period of @ @ @ analysis of @ work @ is clear @ @ is no perfect solution @ hence @ of @ aim of @ review is to create a resource in @ form of @ overview of @ technique method and approach @ @ @ used to attempt to solve @ problem of cross-domain sentiment analysis in order to assist researcher in developing @ and more accurate technique in @ future @ @ @ 
1923,Domain independent approach for aspect oriented sentiment analysis for product reviews,"The Sentiment analysis from text documents is emerging field for the research in Natural Language Processing (NLP) and text mining. Feature specific opinion matters more than the overall opinion. Given a collection of review texts, the goal is to detect the individual product aspects comments by reviewers and to decide whether the comments are rather positive or negative. In this research paper unsupervised approach for domain independent feature specific sentiment analysis has been proposed. SentiWordNet lexical resource has been used to determine the polarity of identified features. Research work has shown the promising results over the previously used approaches using SentiWordNet. Newly introduced Senti-WordNet 3.0 has been proved to be an important lexical resource. © Springer Nature Singapore Pte Ltd. 2017.",2017,Advances in Intelligent Systems and Computing,14,@ sentiment analysis @ text document is emerging field @ @ research in natural language processing @ nlp @ and text mining @ feature specific opinion matter more @ @ overall opinion @ given a collection of review text @ goal is to detect @ individual product aspect comment by reviewer and to decide whether @ comment @ rather positive @ negative @ in @ research @ unsupervised approach @ domain independent feature specific sentiment analysis ha @ proposed @ sentiwordnet lexical resource ha @ used to determine @ polarity of identified feature @ research work ha @ @ promising @ @ @ @ used approach @ sentiwordnet @ newly introduced senti-wordnet @ ha @ proved to @ @ important lexical resource @ @ nature singapore pte ltd @ @ 
1924,Dictionary-based sentiment analysis applied to a specific domain,"The web and social media have been growing exponentially in recent years. We now have access to documents bearing opinions expressed on a broad range of topics. This constitutes a rich resource for natural language processing tasks, particularly for sentiment analysis. Nevertheless, sentiment analysis is usually difficult because expressed sentiments are usually topic-oriented. In this paper, we propose to automatically construct a sentiment dictionary using relevant terms obtained from web pages for a specific domain. This dictionary is initially built by querying the web with a combination of opinion terms, as well as terms of the domain. In order to select only relevant terms we apply two measures AcroDefMI3 and TrueSkill. Experiments conducted on different domains highlight that our automatic approach performs better for specific cases. © Springer International Publishing AG 2017.",2017,Communications in Computer and Information Science,3,@ web and social medium @ @ growing exponentially in recent year @ @ now @ access to document bearing opinion expressed on a broad range of topic @ @ constitutes a rich resource @ natural language processing task particularly @ sentiment analysis @ nevertheless sentiment analysis is usually difficult @ expressed sentiment @ usually topic-oriented @ in @ @ @ propose to automatically construct a sentiment dictionary @ relevant term obtained @ web page @ a specific domain @ @ dictionary is initially built by querying @ web @ a combination of opinion term a well a term of @ domain @ in order to select only relevant term @ apply @ measure acrodefmi and trueskill @ experiment conducted on different domain highlight @ @ automatic approach performs better @ specific case @ @ international publishing ag @ 
1925,Topics discovery in text mining,"Text data has been increasingly growing in the last years, due to the advances of web based technologies that enable the publishing of an overwhelming amount of data. One can say that, many knowledge about the world in text data, besides being stored in articles and books, is also available on blogs, tweets, web pages. This paper overviews some general techniques for text data mining, based on text retrieval models, that can be applicable to any text in natural language. The techniques are targeted to problems requiring minimum or no human effort. These techniques, which can be used in many applications, allow the discovery of main topics of a document in text data with different levels of granularity. © Springer International Publishing AG 2017.",2017,Advances in Intelligent Systems and Computing,2,text data ha @ increasingly growing in @ last year due to @ advance of web based technology @ enable @ publishing of @ overwhelming amount of data @ @ @ say @ many knowledge @ @ world in text data besides @ stored in article and book is @ available on blog tweet web page @ @ @ overview some general technique @ text data mining based on text retrieval model @ @ @ applicable to @ text in natural language @ @ technique @ targeted to problem requiring minimum @ no human effort @ @ technique @ @ @ used in many application allow @ discovery of main topic of a document in text data @ different level of granularity @ @ international publishing ag @ 
1927,Detecting sarcasm in customer tweets: An NLP based approach,"Purpose - The purpose of this paper is to study sarcasm in online text - specifically on twitter - to better understand customer opinions about social issues, products, services, etc. This can be immensely helpful in reducing incorrect classification of consumer sentiment toward issues, products and services. Design/methodology/approach - In this study, 5,000 tweets were downloaded and analyzed. Relevant features were extracted and supervised learning algorithms were applied to identify the best differentiating features between a sarcastic and non-sarcastic sentence. Findings - The results using two different classification algorithms, namely, Naïve Bayes and maximum entropy show that function words and content words together are most effective in identifying sarcasm in tweets. The most differentiating features between a sarcastic and a non-sarcastic tweet were identified. Practical implications - Understanding the use of sarcasm in tweets let companies do better sentiment analysis and product recommendations for users. This could help businesses attract new customers and retain the old ones resulting in better customer management. Originality/value - This paper uses novel features to identify sarcasm in online text which is one of the most challenging problems in natural language processing. To the authors' knowledge, this is the first study on sarcasm detection from a customer management perspective. © Emerald Publishing Limited.",2017,Industrial Management and Data Systems,11,purpose @ purpose of @ @ is to study sarcasm in online text specifically on twitter to better understand customer opinion @ social issue product service etc @ @ @ @ immensely helpful in reducing incorrect classification of consumer sentiment toward issue product and service @ design methodology approach in @ study tweet @ downloaded and analyzed @ relevant feature @ extracted and supervised learning algorithm @ applied to identify @ best differentiating feature @ a sarcastic and non-sarcastic sentence @ finding @ @ @ @ different classification algorithm namely naïve bayes and maximum entropy @ @ function word and content word together @ @ effective in identifying sarcasm in tweet @ @ @ differentiating feature @ a sarcastic and a non-sarcastic tweet @ identified @ practical implication understanding @ use of sarcasm in tweet let company @ better sentiment analysis and product recommendation @ user @ @ could help @ attract @ customer and retain @ old @ resulting in better customer management @ originality value @ @ us novel feature to identify sarcasm in online text @ is @ of @ @ challenging problem in natural language processing @ to @ author @ knowledge @ is @ first study on sarcasm detection @ a customer management perspective @ emerald publishing limited @ 
1932,Tag me a label with multi-arm: Active learning for Telugu sentiment analysis,"Sentiment Analysis is one of the most active research areas in natural language processing and an extensively studied problem in data mining, web mining and text mining for English language. With the proliferation of social media these days, data is widely increasing in regional languages along with English. Telugu is one such regional language with abundant data available in social media, but it’s hard to find a labeled training set as human annotation is time-consuming and cost-ineffective. To address this issue, in this paper the practicality of active learning for Telugu sentiment analysis is investigated. We built a hybrid approach by combining different query selection strategy frameworks to increase more accurate training data instances with limited labeled data. Using a set of classifiers like SVM, XGBoost, and Gradient Boosted Trees (GBT), we achieved promising results with minimal error rate. © 2017, Springer International Publishing AG.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,sentiment analysis is @ of @ @ active research area in natural language processing and @ extensively studied problem in data mining web mining and text mining @ english language @ @ @ proliferation of social medium @ day data is widely increasing in regional language along @ english @ telugu is @ @ regional language @ abundant data available in social medium @ @ s hard to find a labeled training set a human annotation is time-consuming and cost-ineffective @ to address @ issue in @ @ @ practicality of active learning @ telugu sentiment analysis is investigated @ @ built a hybrid approach by combining different query selection strategy framework to increase more accurate training data instance @ limited labeled data @ @ a set of classifier like svm xgboost and gradient boosted tree @ gbt @ @ achieved promising @ @ minimal error rate @ @ international publishing ag @ 
1934,Unify framework for crime data summarization using RSS feed service,"This research presents online crime news analysis using text mining, Natural Language Processing framework (General Framework for Text Mining: GATE), and data warehouse (DW) technologies. The proposed framework aims at extracting key features of crime data available on newspaper website and classifies them into crime categories which are later transformed into a star schema for speedy retrieving and online analytical processing (OLAP). This system can present data in multidimensional structure to perform data analytics to support police officers for determining the security policies to protect locals and tourists who live in the risk areas. The main novelty of this framework is the demonstration of using information available through RSS feed service to generate reports to support decision making. The experimental results show that the extracted data from the Internet can effectively represent the actual crime data occurred in the study areas (low error rate) and allow data analysts to get an insight of the information represented through OLAP. © 2017, Walailak University. All rights reserved.",2017,Walailak Journal of Science and Technology,3,@ research @ online crime news analysis @ text mining natural language processing framework @ general framework @ text mining @ gate @ and data warehouse @ dw @ technology @ @ proposed framework aim at extracting key feature of crime data available on newspaper website and classifies @ @ crime category @ @ later transformed @ a star schema @ speedy retrieving and online analytical processing @ olap @ @ @ system @ @ data in multidimensional structure to perform data analytics to support police officer @ determining @ security policy to protect local and tourist @ live in @ risk area @ @ main novelty of @ framework is @ demonstration of @ information available @ r feed service to generate report to support decision making @ @ experimental @ @ @ @ extracted data @ @ internet @ effectively represent @ actual crime data occurred in @ study area @ low error rate @ and allow data analyst to get @ insight of @ information represented @ olap @ walailak university @ @ right reserved @ 
1936,Sentiment classification from multi-class imbalanced twitter data using binarization,"Twitter became one of the most dynamically developing areas of social media. Due to concise nature of messages, rapid publication and high outreach, people share more and more of their opinions, thoughts and commentaries using this medium. Sentiment analysis is a specific subsection of natural language processing that concentrates on automatically categorizing opinions and attitudes expressed in a given portion of textual information. This requires dedicated machine learning solutions that are able to handle various difficulties embedded in the nature of data. In this paper, we present an efficient framework for automatic sentiment analysis from high-dimensional and sparse datasets that suffer from multi-class imbalance. We propose to approach it by applying a one-vs-one binary decomposition and reducing the dimensionality of each pairwise class set using Multiple Correspondence Analysis. Then we apply preprocessing to alleviate the skewed distributions in reduced number of dimensions. After that, on each pair of classes we train a binary classifier and combined them using a weighted multi-class reconstruction that promotes minority classes. The proposal is evaluated on a large Twitter dataset and obtained results are in favor of the proposed solution. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13,twitter became @ of @ @ dynamically developing area of social medium @ due to concise nature of message rapid publication and high outreach people share more and more of @ opinion thought and commentary @ @ medium @ sentiment analysis is a specific subsection of natural language processing @ concentrate on automatically categorizing opinion and attitude expressed in a given portion of textual information @ @ requires dedicated machine learning solution @ @ able to handle various difficulty embedded in @ nature of data @ in @ @ @ @ @ efficient framework @ automatic sentiment analysis @ high-dimensional and sparse datasets @ suffer @ multi-class imbalance @ @ propose to approach @ by applying a one-vs-one binary decomposition and reducing @ dimensionality of @ pairwise class set @ multiple correspondence analysis @ @ @ apply preprocessing to alleviate @ skewed distribution in reduced number of dimension @ @ @ on @ pair of class @ train a binary classifier and combined @ @ a weighted multi-class reconstruction @ promotes minority class @ @ proposal is evaluated on a @ twitter dataset and obtained @ @ in favor of @ proposed solution @ @ international publishing ag @ 
1937,Towards patent text analysis based on semantic role labelling,"Mining patent texts can obtain valuable technical information and competitive intelligence which is important for the development of technology and business. The current patent text mining approaches suffer from lack of effective, automatic, accurate and wide-coverage techniques that can annotate natural language texts with semantic argument structure. It is helpful for text mining to derive more meaningful semantic relationship from semantic role labelling (SRL) results of patents. This paper uses Word2Vec to learn word real-valued vector and design features related to word vector to train SRL parser. Based on the SRL parser, two patent text mining methods are then given: patent topic extraction and automatic construction of patent technical effect matrix (PTEM). Experiments show that semantic role labelling help achieve satisfactory results and saves manpower. © 2017 Inderscience Enterprises Ltd.",2017,International Journal of Computational Science and Engineering,3,mining patent text @ obtain valuable technical information and competitive intelligence @ is important @ @ development of technology and @ @ @ current patent text mining approach suffer @ lack of effective automatic accurate and wide-coverage technique @ @ annotate natural language text @ semantic argument structure @ @ is helpful @ text mining to derive more meaningful semantic relationship @ semantic role labelling @ srl @ @ of patent @ @ @ us word vec to learn word real-valued vector and design feature related to word vector to train srl parser @ based on @ srl parser @ patent text mining method @ @ given @ patent topic extraction and automatic construction of patent technical effect matrix @ ptem @ @ experiment @ @ semantic role labelling help achieve satisfactory @ and save manpower @ inderscience enterprise ltd @ 
1940,Medical entity recognition and negation extraction: Assessment of NegEx on health records in Spanish,"This work focuses on biomedical text mining. The core of this work is to make a step ahead in the negation detection of biomedical entities on Electronic Health Records (EHRs), where the detection of non-negated entities is as important as the identification of negated entities. For instance, the identification of a negated entity as factual, can produce diagnostic errors in decision support systems. Negated entity recognition tackles two tasks: (1) entity recognition; (2) entity classification as negated or not. To identify negations, in the literature rule-based and machine-learning techniques have been used. This paper presents an adaptation of the rule-based system NegEx, which uses exact-matching for the aforementioned tasks. Our contribution consist in assessing the aforementioned two tasks and explored alternatives for each of them, in such a way that the negation detection improves when the entity recognition is able to detect more entities correctly. The evaluation was carried out within a real domain of 75 EHRs written in Spanish obtaining an f-measure of 76.2 for entity recognition and 73.8 for negation detection. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ work focus on biomedical text mining @ @ core of @ work is to make a step ahead in @ negation detection of biomedical entity on electronic health record @ ehrs @ @ @ detection of non-negated entity is a important a @ identification of negated entity @ @ instance @ identification of a negated entity a factual @ produce diagnostic error in decision support system @ negated entity recognition tackle @ task @ @ @ entity recognition @ @ @ entity classification a negated @ not @ to identify negation in @ literature rule-based and machine-learning technique @ @ used @ @ @ @ @ adaptation of @ rule-based system negex @ us exact-matching @ @ aforementioned task @ @ contribution consist in assessing @ aforementioned @ task and explored alternative @ @ of @ in @ a way @ @ negation detection improves @ @ entity recognition is able to detect more entity correctly @ @ evaluation wa carried @ within a real domain of ehrs written in spanish obtaining @ f-measure of @ @ entity recognition and @ @ negation detection @ @ international publishing ag @ 
1942,Perspectives of the performance metrics in lexicon and hybrid based approaches: A review,"Online social media and social networking services experience a drastic development in the present scenario. Contents generated by hun-dreds of millions of users are used for communication in general. Users mark their opinion and review in various applications such as Twitter, Facebook, YouTube, Weibo, Flicker, LinkedIn, Online-e commerce sites, Microblogging sites, etc. User generated text is spread rapidly on the web, and it has become tedious to analyze the opinionated text in order to arrive at a decision. Sentiment analysis, a sub-category of text mining is the major active research domain in current era due to greater quantity of opinionated text present in the Internet. Semantic detection is the sub-class in the sentiment analysis which is used for measuring the sentiment orientation in any text. Opinionated text is used for analyzing and making the decision simple. This interdisciplinary field draws various techniques from data mining, machine learning, natural language processing, lexicon based and hybrid based approaches. This paper provides a broad perspective with the high-light of the current state-of art techniques emphasizing the various research challenges and gaps present. The performance metrics in terms of detection rate, precision, recall, f-measure/score, average mean, auto-Pearson correlation, cosine similarity and ratio of time on various algorithms is discussed in detail. An analysis of the text mining approaches in different domains is presented. © 2017 Meesala Shobha Rani, Sumathy.S.",2017,International Journal of Engineering and Technology(UAE),4,online social medium and social networking service experience a drastic development in @ @ scenario @ content generated by hun-dreds of million of user @ used @ communication in general @ user mark @ opinion and review in various application @ a twitter facebook youtube weibo flicker linkedin online-e commerce site microblogging site etc @ user generated text is spread rapidly on @ web and @ ha become tedious to analyze @ opinionated text in order to arrive at a decision @ sentiment analysis a sub-category of text mining is @ major active research domain in current era due to greater quantity of opinionated text @ in @ internet @ semantic detection is @ sub-class in @ sentiment analysis @ is used @ measuring @ sentiment orientation in @ text @ opinionated text is used @ analyzing and making @ decision simple @ @ interdisciplinary field draw various technique @ data mining machine learning natural language processing lexicon based and hybrid based approach @ @ @ provides a broad perspective @ @ high-light of @ current state-of art technique emphasizing @ various research challenge and gap @ @ @ performance metric in term of detection rate precision recall f-measure score average mean auto-pearson correlation cosine similarity and ratio of time on various algorithm is discussed in detail @ @ analysis of @ text mining approach in different domain is presented @ meesala shobha rani sumathy @ s @ 
1943,Graphics on demand: The automatic data visualization on the WEB,"Data visualization is an effective tool for communicating the results of opinion surveys, epidemiological studies, statistics on consumer habits, etc. The graphical representation of data usually assists human information processing by reducing demands on attention, working memory, and long-term memory. It allows, among other things, a faster reading of the information (by acting on the forms, directions, colors...), the independence of the language (or culture), a better capture the attention of the audience, etc. Data that could be graphically represented may be structured or unstructured. The unstructured data, whose volume grows exponentially, often hide important and even vital information for society and companies. It, therefore, takes a lot of work to extract valuable information from unstructured data. If it is easier to understand a message through structured data, such as a table, than through a long narrative text, it is even easier to convey a message through a graphic than a table. In our opinion, it is often very useful to synthesize the unstructured data in the form of graphical representations. In this paper, we present an approach for processing unstructured data containing statistics in order to represent them graphically. This approach allows transforming the unstructured data into structured one which globally conveys the same countable information. The graphical representation of such a structured data is then obvious. This approach deals with both quantitative and qualitative data. It is based on Natural Language Processing Techniques and Text Mining. An application that implements this process is also presented in this paper. © 2017 ASTES Publishers. All rights reserved.",2017,"Advances in Science, Technology and Engineering Systems",1,data visualization is @ effective tool @ communicating @ @ of opinion survey epidemiological study statistic on consumer habit etc @ @ graphical representation of data usually assist human information processing by reducing demand on attention working memory and long-term memory @ @ allows among @ thing a faster reading of @ information @ by acting on @ form direction color @ @ @ @ @ independence of @ language @ @ culture @ a better capture @ attention of @ audience etc @ data @ could @ graphically represented may @ structured @ unstructured @ @ unstructured data whose volume grows exponentially often hide important and even vital information @ society and company @ @ therefore take a lot of work to extract valuable information @ unstructured data @ if @ is easier to understand a message @ structured data @ a a table @ @ a long narrative text @ is even easier to convey a message @ a graphic @ a table @ in @ opinion @ is often @ useful to synthesize @ unstructured data in @ form of graphical representation @ in @ @ @ @ @ approach @ processing unstructured data containing statistic in order to represent @ graphically @ @ approach allows transforming @ unstructured data @ structured @ @ globally conveys @ @ countable information @ @ graphical representation of @ a structured data is @ obvious @ @ approach deal @ @ quantitative and qualitative data @ @ is based on natural language processing technique and text mining @ @ application @ implement @ process is @ presented in @ @ @ astes publisher @ @ right reserved @ 
1944,Constructing document vectors using kernel density estimates,"Document vector embeddings are numeric fixed length representations of text documents that can be used for machine learning and text mining purposes. We describe in this paper a new technique for generating document vectors. Our novel idea builds on the recently popular notion of neural word vector embeddings and combines this concept with the statistics of kernel density estimation. We show that robust document vectors can be produced using our new algorithm, and perform an experiment involving several challenging text classification datasets to demonstrate its effectiveness. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,document vector embeddings @ numeric fixed length representation of text document @ @ @ used @ machine learning and text mining purpose @ @ describe in @ @ a @ technique @ generating document vector @ @ novel idea build on @ recently popular notion of neural word vector embeddings and combine @ concept @ @ statistic of kernel density estimation @ @ @ @ robust document vector @ @ produced @ @ @ algorithm and perform @ experiment involving several challenging text classification datasets to demonstrate @ effectiveness @ @ international publishing ag @ 
1945,Application of social media analytics: A case of analyzing online hotel reviews,"Purpose-Online customer reviews could shed light into their experience, opinions, feelings, and concerns. To gain valuable knowledge about customers, it becomes increasingly important for businesses to collect, monitor, analyze, summarize, and visualize online customer reviews posted on social media platforms such as online forums. However, analyzing social media data is challenging due to the vast increase of social media data. The purpose of this paper is to present an approach of using natural language preprocessing, text mining and sentiment analysis techniques to analyze online customer reviews related to various hotels through a case study. Design/methodology/approach-This paper presents a tested approach of using natural language preprocessing, text mining, and sentiment analysis techniques to analyze online textual content. The value of the proposed approach was demonstrated through a case study using online hotel reviews. Findings-The study found that the overall review star rating correlates pretty well with the sentiment scores for both the title and the full content of the online customer review. The case study also revealed that both extremely satisfied and extremely dissatisfied hotel customers share a common interest in the five categories: Food, location, rooms, service, and staff. Originality/value-This study analyzed the online reviews from English-speaking hotel customers in China to understand their preferred hotel attributes, main concerns or demands. This study also provides a feasible approach and a case study as an example to help enterprises more effectively apply social media analytics in practice. © 2017 Emerald Publishing Limited.",2017,Online Information Review,31,purpose-online customer review could shed light @ @ experience opinion feeling and concern @ to gain valuable knowledge @ customer @ becomes increasingly important @ @ to collect monitor analyze summarize and visualize online customer review posted on social medium platform @ a online forum @ however analyzing social medium data is challenging due to @ vast increase of social medium data @ @ purpose of @ @ is to @ @ approach of @ natural language preprocessing text mining and sentiment analysis technique to analyze online customer review related to various hotel @ a case study @ design methodology approach-this @ @ a tested approach of @ natural language preprocessing text mining and sentiment analysis technique to analyze online textual content @ @ value of @ proposed approach wa demonstrated @ a case study @ online hotel review @ findings-the study found @ @ overall review star rating correlate pretty well @ @ sentiment score @ @ @ title and @ full content of @ online customer review @ @ case study @ revealed @ @ extremely satisfied and extremely dissatisfied hotel customer share a common interest in @ five category @ food location room service and staff @ originality value-this study analyzed @ online review @ english-speaking hotel customer in china to understand @ preferred hotel attribute main concern @ demand @ @ study @ provides a feasible approach and a case study a @ example to help enterprise more effectively apply social medium analytics in practice @ emerald publishing limited @ 
1947,Associations between diagnostic patterns and stages in ovarian cancer,"Ovarian cancer (OvCa) is the fifth leading cause of cancer deaths in women and remains the deadliest gynecological cancer. Our study goal is to examine associations between diagnostic patterns and OvCa stages. We used the data from a web-based survey in which more than 500 women diagnosed with OvCa provided both free text responses and staging information. We employed text mining and natural language processing (NPL) to extract information on clinical diagnostic characteristics, together with 21 dichotomous symptomatic variables, patient-centered advocacy, and polytomous disease severity, with internal validation. We conducted multivariate analyses and developed tree-based classification models with the confirmation of Random Forest to determine important factors in the relationships of the clinical diagnostic characteristics with OvCa stages. Models including the symptoms, patient advocacy tendency, disease severity and doctors' responses as predictors, had a much better predictive power than those limited to doctors' responses alone, indicating that OvCa stage at diagnosis depends on more than just doctors' responses. Although effective early stage diagnosis and treatment remains a challenge, our analysis of patient-centered clinical diagnostic characteristics and symptoms shows that self-advocacy is essential for all women. The frontline physician is critically important in ensuring effective follow-up and timely treatment before diagnosis. © 2017 IOS Press and the authors.",2017,Model Assisted Statistics and Applications,1,ovarian cancer @ ovca @ is @ fifth leading cause of cancer death in woman and remains @ deadliest gynecological cancer @ @ study goal is to examine association @ diagnostic pattern and ovca stage @ @ used @ data @ a web-based survey in @ more @ woman diagnosed @ ovca provided @ free text response and staging information @ @ employed text mining and natural language processing @ npl @ to extract information on clinical diagnostic characteristic together @ dichotomous symptomatic variable patient-centered advocacy and polytomous disease severity @ internal validation @ @ conducted multivariate analysis and developed tree-based classification model @ @ confirmation of random forest to determine important factor in @ relationship of @ clinical diagnostic characteristic @ ovca stage @ model including @ symptom patient advocacy tendency disease severity and doctor @ response a predictor @ a much better predictive power @ @ limited to doctor @ response alone indicating @ ovca stage at diagnosis depends on more @ @ doctor @ response @ although effective early stage diagnosis and treatment remains a challenge @ analysis of patient-centered clinical diagnostic characteristic and symptom @ @ self-advocacy is essential @ @ woman @ @ frontline physician is critically important in ensuring effective follow-up and timely treatment @ diagnosis @ io @ and @ author @ 
1952,Opinion mining on non-english short text,"As the type and the number of such venues increase, automated analysis of sentiment on textual resources has become an essential data mining task. In this paper, we investigate the problem of mining opinions on the collection of informal short texts. Both positive and negative sentiment strength of texts are detected. We focus on a non-English language that has few resources for text mining. This approach would help enhance the sentiment analysis in languages where a list of opinionated words does not exist. We present a new method to automatically construct a list of words with their sentiment strengths. Then, we propose a new method projects the text into dense and low dimensional feature vectors according to the sentiment strength of the words. We detect the mixture of positive and negative sentiments on a multi-variant scale. Empirical evaluation of the proposed framework on Turkish tweets shows that our approach gets good results for opinion mining. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,a @ type and @ number of @ venue increase automated analysis of sentiment on textual resource ha become @ essential data mining task @ in @ @ @ investigate @ problem of mining opinion on @ collection of informal short text @ @ positive and negative sentiment strength of text @ detected @ @ focus on a non-english language @ ha @ resource @ text mining @ @ approach would help enhance @ sentiment analysis in language @ a list of opinionated word doe not exist @ @ @ a @ method to automatically construct a list of word @ @ sentiment strength @ @ @ propose a @ method project @ text @ dense and low dimensional feature vector according to @ sentiment strength of @ word @ @ detect @ mixture of positive and negative sentiment on a multi-variant scale @ empirical evaluation of @ proposed framework on turkish tweet @ @ @ approach get good @ @ opinion mining @ @ international publishing ag @ 
1953,Rsentiment: A tool to extract meaningful insights from textual reviews,"Every system needs continuous improvement. Feedback from different stakeholders plays a crucial role here. From literature study, the need of textual feedback analysis for an academic institute is well established. In fact, it has been perceived that often a textual feedback is more informative, more open ended and more effective in producing actionable insights to decision makers as compared to more common score based (on a scale from 1: n) feedback. However, getting this information from textual feedback is not possible through the traditional means of data analysis. Here we have conceptualized a tool, which can apply text mining techniques to elicit insights from textual data and has been published as an open source package for a broader use by practitioners. Appropriate visualization techniques are applied for intuitive understanding of the insights. For this, we have used a real dataset consisting of alumni feedback from a top engineering college in Kolkata. © Springer Nature Singapore Pte Ltd. 2017.",2017,Advances in Intelligent Systems and Computing,11,every system need continuous improvement @ feedback @ different stakeholder play a crucial role @ @ @ literature study @ need of textual feedback analysis @ @ @ institute is well established @ in fact @ ha @ perceived @ often a textual feedback is more informative more open ended and more effective in producing actionable insight to decision maker a compared to more common score based @ on a scale @ @ n @ feedback @ however getting @ information @ textual feedback is not possible @ @ traditional mean of data analysis @ @ @ @ conceptualized a tool @ @ apply text mining technique to elicit insight @ textual data and ha @ published a @ open source package @ a broader use by practitioner @ appropriate visualization technique @ applied @ intuitive understanding of @ insight @ @ @ @ @ used a real dataset consisting of alumnus feedback @ a top engineering college in kolkata @ @ nature singapore pte ltd @ @ 
1955,Design and analysis of a weight-lda model to extract implicit topic of database in social networks,"In the era of big data, the volumes of data are in increasingly rapid growth in social networks. Social networks are a theoretical construct, which is useful in the social sciences to study relationships and interactions between individuals, group, organizations. Massive data processing is essential for providing social network services. In this paper, we focus on the extraction of the implicit aspect and opinion words in social networks. The Latent Dirichlet Allocation (LDA) model is a generative probabilistic model to automatically extract implicit topic in the document set, which has been widely used in natural language processing, text mining and text categorization. However, a large number of non-taxonomy high-frequency content words in the Chinese patent documents will affect the implicit topic generation, and for the more, affect Chinese patent classification. The study finds that the probability distribution of the words in the expert database has an impact on the extraction of the feature words for patent document. This paper proposes a weight-LDA model for the problem of the LDA topic model in Chinese patent classification. The weight-LDA model, which combines the probability distribution of feature words in the expert database with Gibbs sampling, reduces the impact of nontaxonomy high-frequency content words on the distribution of topic and enhances that of low-frequency content words with strong classification effects on the distribution of topic. Six different types of patent data sets extracted from State Intellectual Property Office of the P.R.C are tested. The average F value of the weight-LDA model is 6% higher than that of the traditional LDA model. In addition, the weight-LDA model is compared with wordfrequency-based feature selection methods such as the TFIDF algorithm, and the average F value of the weight-LDA model is 11.4% higher than that of the TF-IDF algorithm. Through the analysis of the experimental results, the weight-LDA for the Chinese patent has better classification effects.",2017,Journal of Internet Technology,1,in @ era of big data @ volume of data @ in increasingly rapid growth in social network @ social network @ a theoretical construct @ is useful in @ social science to study relationship and interaction @ individual group organization @ massive data processing is essential @ providing social network service @ in @ @ @ focus on @ extraction of @ implicit aspect and opinion word in social network @ @ latent dirichlet allocation @ lda @ model is a generative probabilistic model to automatically extract implicit topic in @ document set @ ha @ widely used in natural language processing text mining and text categorization @ however a @ number of non-taxonomy high-frequency content word in @ chinese patent document @ affect @ implicit topic generation and @ @ more affect chinese patent classification @ @ study find @ @ probability distribution of @ word in @ expert database ha @ impact on @ extraction of @ feature word @ patent document @ @ @ proposes a weight-lda model @ @ problem of @ lda topic model in chinese patent classification @ @ weight-lda model @ combine @ probability distribution of feature word in @ expert database @ gibbs sampling reduces @ impact of nontaxonomy high-frequency content word on @ distribution of topic and enhances @ of low-frequency content word @ strong classification effect on @ distribution of topic @ six different type of patent data set extracted @ state intellectual property office of @ p @ r @ c @ tested @ @ average f value of @ weight-lda model is higher @ @ of @ traditional lda model @ in addition @ weight-lda model is compared @ wordfrequency-based feature selection method @ a @ tfidf algorithm and @ average f value of @ weight-lda model is @ higher @ @ of @ tf-idf algorithm @ @ @ analysis of @ experimental @ @ weight-lda @ @ chinese patent ha better classification effect @ 
1956,Design and evaluation of text pre-processor: A tool for text pre-processing,"This paper introduces the Text Pre-processor, a tool that integrates several text preprocessing tasks such as tokenization, parts-of-speech tagging, and elimination of stop words. These pre-processing tasks are prerequisite for any text processing tasks such as sentiment analysis or text summarization. However, there does not exist any one-stop solution to perform multiple text pre-processing tasks. The Text Pre-processor serves to cover this gap. The tool includes five modules. These include text editor, single file processing, file to file processing, multiple file processing, as well as split and merge files. Informed by the technological acceptance model, a qualitative user study was conducted to evaluate the efficacy of the tool. Participants generally found the tool efficacious. © 2017 AMSE Press. All rights reserved.",2017,Advances in Modelling and Analysis A,1,@ @ introduces @ text pre-processor a tool @ integrates several text preprocessing task @ a tokenization parts-of-speech tagging and elimination of stop word @ @ pre-processing task @ prerequisite @ @ text processing task @ a sentiment analysis @ text summarization @ however @ doe not exist @ one-stop solution to perform multiple text pre-processing task @ @ text pre-processor serf to cover @ gap @ @ tool includes five module @ @ include text editor single file processing file to file processing multiple file processing a well a split and merge file @ informed by @ technological acceptance model a qualitative user study wa conducted to evaluate @ efficacy of @ tool @ participant generally found @ tool efficacious @ amse @ @ @ right reserved @ 
1958,Integrating data analysis tools for better treatment of diabetic patients,"This paper presents the construction and usage of an anonymous Diabetes Register for patients in Bulgaria. The Register is generated automatically from outpatient records submitted to the Bulgarian National Health Insurance Fund in 2010-2014 and continuously updated using outpatient records for 2015-2016. The construction relies on advanced automatic analysis of free text information as well as on Business Analytics technologies for storing, maintaining, searching, querying and analyzing data. Original frequent pattern mining algorithms enable to find patterns and sequences taking into account temporal information. The paper discussed the software environment as well as experiments in frequent pattern mining that enable knowledge discovery in the very large repository underlying the Register (currently 262 million pseudonymized outpatient records submitted to the Bulgarian National Health Insurance Fund in 2010-2016 for more than 5 mln citizens yearly). The claim is that the synergy of modern analytics tools transforms a static archive of clinical patient records to a sophisticated software environment for knowledge discovery and prediction.",2017,CEUR Workshop Proceedings,5,@ @ @ @ construction and usage of @ anonymous diabetes register @ patient in bulgaria @ @ register is generated automatically @ outpatient record submitted to @ bulgarian national health insurance fund in and continuously updated @ outpatient record @ @ @ construction relies on advanced automatic analysis of free text information a well a on @ analytics technology @ storing maintaining searching querying and analyzing data @ original frequent pattern mining algorithm enable to find pattern and sequence taking @ account temporal information @ @ @ discussed @ software environment a well a experiment in frequent pattern mining @ enable knowledge discovery in @ @ @ repository underlying @ register @ currently million pseudonymized outpatient record submitted to @ bulgarian national health insurance fund in @ more @ mln citizen yearly @ @ @ claim is @ @ synergy of modern analytics tool transforms a static archive of clinical patient record to a sophisticated software environment @ knowledge discovery and prediction @ 
1960,A deep learning method for ICD-10 coding of free-text death certificates,"The assignment of disease codes to clinical texts has a wide range of applications, including epidemiological studies or disease surveillance. We address the task of automatically assigning the ICD-10 codes for the underlying cause of death, from the free-text descriptions included in death certificates obtained from the Portuguese Ministry of Health. We specifically propose to leverage a deep neural network based on a two-level hierarchy of recurrent nodes together with attention mechanisms. The first level uses recurrent nodes for modeling the sequences of words given in individual fields of the death certificates, together with attention to weight the contribution of each word, producing intermediate representations for the contents of each field. The second level uses recurrent nodes to model a sequence of fields, using the representations produced by the first level and also leveraging attention in order to weight the contributions of the different fields. The paper reports on experiments with a dataset of 115,406 death certificates, presenting the results of an evaluation of the predictive accuracy of the proposed method, for different ICD-10 levels (i.e., chapter, block, or full code) and for particular causes of death. We also discuss how the neural attention mechanisms can help in interpreting the classification results. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,@ assignment of disease code to clinical text ha a wide range of application including epidemiological study @ disease surveillance @ @ address @ task of automatically assigning @ icd code @ @ underlying cause of death @ @ free-text description included in death certificate obtained @ @ portuguese ministry of health @ @ specifically propose to leverage a deep neural network based on a two-level hierarchy of recurrent node together @ attention mechanism @ @ first level us recurrent node @ modeling @ sequence of word given in individual field of @ death certificate together @ attention to weight @ contribution of @ word producing intermediate representation @ @ content of @ field @ @ second level us recurrent node to model a sequence of field @ @ representation produced by @ first level and @ leveraging attention in order to weight @ contribution of @ different field @ @ @ report on experiment @ a dataset of death certificate presenting @ @ of @ evaluation of @ predictive accuracy of @ proposed method @ different icd level @ i @ e @ chapter block @ full code @ and @ particular cause of death @ @ @ discus @ @ neural attention mechanism @ help in interpreting @ classification @ @ @ international publishing ag @ 
1961,Visualizing incongruity and resolution: Visual data mining strategies for modeling sequential humor containing shifts of interpretation,"The goal of this paper is to investigate the use of visualization as an approach to modeling humor within text. In particular, we developed algorithmic and automated approaches to visualizing and detecting shifts in interpretation as intelligent agents parse meaning from garden path jokes. Garden path jokes can occur when a reader’s initial interpretation of an ambiguous text turns out to be incorrect, leading them down the wrong path to a semantic dead end. Given new information, semantic incongruities arise that require resolution, often triggering a humorous response. This is a work of visual text mining, that is visualizing texts in order to detect patterns and features associated with various text based phenomena such as humor. In this paper we describe three successful approaches to text visualization conducive to identifying distinguishing features given humorous and non humorous texts. These are the use of paired collocated coordinates, heat maps, and two-dimensional Boolean plots. The proposed methodology and tools offer a new approach to testing and generating hypotheses related to theories of humor as well as other phenomena involving incongruity-resolution and shifts in interpretation including non-verbal humor. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ goal of @ @ is to investigate @ use of visualization a @ approach to modeling humor within text @ in particular @ developed algorithmic and automated approach to visualizing and detecting shift in interpretation a intelligent agent parse meaning @ garden path joke @ garden path joke @ occur @ a reader s initial interpretation of @ ambiguous text turn @ to @ incorrect leading @ down @ wrong path to a semantic dead end @ given @ information semantic incongruity arise @ require resolution often triggering a humorous response @ @ is a work of visual text mining @ is visualizing text in order to detect pattern and feature associated @ various text based phenomenon @ a humor @ in @ @ @ describe three successful approach to text visualization conducive to identifying distinguishing feature given humorous and non humorous text @ @ @ @ use of paired collocated coordinate heat map and two-dimensional boolean plot @ @ proposed methodology and tool offer a @ approach to testing and generating hypothesis related to theory of humor a well a @ phenomenon involving incongruity-resolution and shift in interpretation including non-verbal humor @ @ international publishing ag @ 
1962,PURE: A novel tripartite model for review sentiment analysis and recommendation,"Nowadays, more and more users like to leave online reviews. These reviews, which are based on their experiences on a set of service or products, often express different opinions and sentiments. Correlated topic model (CTM), an effective text mining model, can reduce the dimension without losing important information. However, traditional analyses based on CTM still have some problems. In this paper, we propose the Product-User-Review tripartite sEntiment model (PURE), which is based on content-based clustering to optimize CTM, to select topic number, extract feature, estimate the reviews’ utility. Moreover, our model analyzes the reviews from the user’s preferences, review content and product properties in three dimensions. Based on the five indexes, such as informative attributes and sentiment attributes, the feature vector of the review data is constructed. We found that after adding user’s preference feature in sentiment analysis and utility estimation, PURE achieves high accuracy and classification speed in the review-mixing Chinese and English processing, and the quality of selection is improved significantly by 21%. To the best of our knowledge, this is the first work to incorporate users’ preference feature in optimized CTM to do the study of sentiment analysis, review selection and recommendation. © 2017, Springer International Publishing AG.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,nowadays more and more user like to leave online review @ @ review @ @ based on @ experience on a set of service @ product often express different opinion and sentiment @ correlated topic model @ ctm @ @ effective text mining model @ reduce @ dimension without losing important information @ however traditional analysis based on ctm still @ some problem @ in @ @ @ propose @ product-user-review tripartite sentiment model @ pure @ @ is based on content-based clustering to optimize ctm to select topic number extract feature estimate @ review utility @ moreover @ model analyzes @ review @ @ user s preference review content and product property in three dimension @ based on @ five index @ a informative attribute and sentiment attribute @ feature vector of @ review data is constructed @ @ found @ @ adding user s preference feature in sentiment analysis and utility estimation pure achieves high accuracy and classification speed in @ review-mixing chinese and english processing and @ quality of selection is improved significantly by @ to @ best of @ knowledge @ is @ first work to incorporate user preference feature in optimized ctm to @ @ study of sentiment analysis review selection and recommendation @ @ international publishing ag @ 
1963,An ontology for mapping cerebral death,"Brain death is one of the most serious diagnoses that can be diagnosed in a patient. The possibility to detect it before its happening is one of the possible steps for the prevention of this event. The x-rays – Computed Tomography scans, are a very important test for the detection of this diagnosis. This paper proposes the use of an ontology on the registration of x-rays made to patients. This work was performed through the data provided by the Centro Hospitalar do Porto - Hospital de Santo António. The ontology was used based on an analysis made to the data and with the use of a dictionary developed in the same analysis. Finally, we added to the ontology the types of patients with brain death that were discov ered in a previous work that used the dictionary that is present in this ontology. © Springer International Publishing AG 2017.",2017,Advances in Intelligent Systems and Computing,1,brain death is @ of @ @ serious diagnosis @ @ @ diagnosed in a patient @ @ possibility to detect @ @ @ happening is @ of @ possible step @ @ prevention of @ event @ @ x-ray computed tomography scan @ a @ important test @ @ detection of @ diagnosis @ @ @ proposes @ use of @ ontology on @ registration of x-ray made to patient @ @ work wa performed @ @ data provided by @ centro hospitalar @ porto hospital de santo antónio @ @ ontology wa used based on @ analysis made to @ data and @ @ use of a dictionary developed in @ @ analysis @ finally @ added to @ ontology @ type of patient @ brain death @ @ discov ered in a previous work @ used @ dictionary @ is @ in @ ontology @ @ international publishing ag @ 
1965,A simple and efficient algorithm for authorship verification,"This paper describes and evaluates an unsupervised and effective authorship verification model called Spatium-L1. As features, we suggest using the 200 most frequent terms of the disputed text (isolated words and punctuation symbols). Applying a simple distance measure and a set of impostors, we can determine whether or not the disputed text was written by the proposed author. Moreover, based on a simple rule we can define when there is enough evidence to propose an answer or when the attribution scheme is unable to make a decision with a high degree of certainty. Evaluations based on 6 test collections (PAN CLEF 2014 evaluation campaign) indicate that Spatium-L1 usually appears in the top 3 best verification systems, and on an aggregate measure, presents the best performance. The suggested strategy can be adapted without any problem to different Indo-European languages (such as English, Dutch, Spanish, and Greek) or genres (essay, novel, review, and newspaper article). © 2016 ASIS&T",2017,Journal of the Association for Information Science and Technology,26,@ @ describes and evaluates @ unsupervised and effective authorship verification model called spatium-l @ a feature @ suggest @ @ @ frequent term of @ disputed text @ isolated word and punctuation symbol @ @ applying a simple distance measure and a set of impostor @ @ determine whether @ not @ disputed text wa written by @ proposed author @ moreover based on a simple rule @ @ define @ @ is enough evidence to propose @ answer @ @ @ attribution scheme is unable to make a decision @ a high degree of certainty @ evaluation based on test collection @ pan clef evaluation campaign @ indicate @ spatium-l usually appears in @ top best verification system and on @ aggregate measure @ @ best performance @ @ suggested strategy @ @ adapted without @ problem to different indo-european language @ @ a english dutch spanish and greek @ @ genre @ essay novel review and newspaper article @ @ asis t
1966,Creating groups for marketing purposes from website usage data,"Customer grouping and knowledge extraction for these groups are important to online businesses because it allows purposeful application of marketing techniques. Individuals can be personally served with the groups, depending on the identified interests and preferences. In this article, we suggest a way to identify and create user groups by processing website usage data. We use the logs stored in the server log data for the visit to a selected website and then retrieve and process the text content of the visited web pages. The approach is based on the technology for natural language processing and uses the methods for clustering of text documents. The experimental testing of this method is done with the software product RapidMiner and data from visits to a Bulgarian e-shop.",2017,"Vestnik Udmurtskogo Universiteta: Matematika, Mekhanika, Komp'yuternye Nauki",1,customer grouping and knowledge extraction @ @ group @ important to online @ @ @ allows purposeful application of marketing technique @ individual @ @ personally served @ @ group depending on @ identified interest and preference @ in @ article @ suggest a way to identify and create user group by processing website usage data @ @ use @ log stored in @ server log data @ @ visit to a selected website and @ retrieve and process @ text content of @ visited web page @ @ approach is based on @ technology @ natural language processing and us @ method @ clustering of text document @ @ experimental testing of @ method is done @ @ software product rapidminer and data @ visit to a bulgarian e-shop @ 
1968,Detection of catchphrases and precedence in legal documents,"""Common Law System"" practiced in India refers to statute as well as precedent to form judgments. As number of cases are increasing rapidly, automation becomes highly desirable. This paper presents two such systems viz. Automatic Catchphrase Detection and Automatic Precedence Detection. Automatic Catchphrase Detection: One of the key requirements of such information retrieval system is to pre-populate database of prior cases with catchphrases for better indexing and faster, relevant retrieval. This paper proposes an automatic catchphrases prediction for cases for the same. The problem catchphrase detection has been modeled as ""custom named entity recognition (NER) using conditional random fields (CRF)"". CRF is trained with pairs of prior cases and their respective catchphrases, the gold standards. The model is, then used to predict catch-phases of unseen legal texts. End of the first section demonstrates efficacy of the proposed system using practical data-set. Automatic Precedence Detection: Due to thousands of past cases it becomes tedious and error-prone to find relevant precedent, manually. An automatic precedent retrieval system is the need of the hour. One of the key requirements of such information system is to find cases which could be ""similar"" to the case in hand. The ""similarity"" used in this paper is about citations. The problem is of predicting prior cases which could potentially be cited by a particular case text. This paper proposes such association system using mixed approaches. It employs rule-based Regular Expressions based on references to statute and Articles. It finds cosine similar ity between case susing vector sgenerated by popular word embedding called doc2vec. It also leverages topic modeling by finding matches between cases based on the number of common topic words. End of the second section demonstrates efficacy of the proposed system by generating cite-able documents from test data-set.",2017,CEUR Workshop Proceedings,0, @ common law system @ practiced in india refers to statute a well a precedent to form judgment @ a number of case @ increasing rapidly automation becomes highly desirable @ @ @ @ @ @ system viz @ automatic catchphrase detection and automatic precedence detection @ automatic catchphrase detection @ @ of @ key requirement of @ information retrieval system is to pre-populate database of prior case @ catchphrase @ better indexing and faster relevant retrieval @ @ @ proposes @ automatic catchphrase prediction @ case @ @ @ @ @ problem catchphrase detection ha @ modeled a @ custom named entity recognition @ ner @ @ conditional random field @ crf @ @ @ crf is trained @ pair of prior case and @ respective catchphrase @ gold standard @ @ model is @ used to predict catch-phases of unseen legal text @ end of @ first section demonstrates efficacy of @ proposed system @ practical data-set @ automatic precedence detection @ due to thousand of past case @ becomes tedious and error-prone to find relevant precedent manually @ @ automatic precedent retrieval system is @ need of @ hour @ @ of @ key requirement of @ information system is to find case @ could @ @ similar @ to @ case in hand @ @ @ similarity @ used in @ @ is @ citation @ @ problem is of predicting prior case @ could potentially @ cited by a particular case text @ @ @ proposes @ association system @ mixed approach @ @ employ rule-based regular expression based on reference to statute and article @ @ find cosine similar ity @ case susing vector sgenerated by popular word embedding called doc vec @ @ @ leverage topic modeling by finding match @ case based on @ number of common topic word @ end of @ second section demonstrates efficacy of @ proposed system by generating cite-able document @ test data-set @ 
1969,Detecting early risk of depression from social media user-generated content,"This paper presents the systems developed by the UQAM team for the CLEF eRisk Pilot Task 2017. The goal was to predict as early as possible the risk of mental health issues from user-generated content in social media. Several approaches based on supervised learning and information retrieval methods were used to estimate the risk of depression for a user given the content of its posts in reddit. Among the five systems evaluated, the experiments show that combining information retrieval and machine learning approaches gives the best results.",2017,CEUR Workshop Proceedings,3,@ @ @ @ system developed by @ uqam team @ @ clef erisk pilot task @ @ goal wa to predict a early a possible @ risk of mental health issue @ user-generated content in social medium @ several approach based on supervised learning and information retrieval method @ used to estimate @ risk of depression @ a user given @ content of @ post in reddit @ among @ five system evaluated @ experiment @ @ combining information retrieval and machine learning approach give @ best @ @ 
1971,Information retrieval and development of conceptual schemas in e-documents for serbian criminal code,"In the process of developing e-Government, Serbian government has implemented a lot of e-Government services which produce a large amount of data and text documents, and whose citizens use more and more these services in their everyday lives. Text documents are in Serbian language and commonly in HTML, PDF and Microsoft Word format. Considering an increased amount of the text documents, Serbian e-Government has indicated the need for certain data and information extraction from the variety of existing text documents which are usually in a format prepared for print. In order to offer technical solution for a case, the authors have developed a dedicated application that includes Lucene library. Lucene is a specialized library for an implementation of the indexing and searching over a large amount of data. The procedure of quick search within unstructured text documents in Serbian language leads to an efficient detection and processing of criminal offenses and contributes to an increased level of security in the Republic of Serbia. In this paper, the authors deal with the possibilities of Lucene indexing and Lucene searching of data and documents within unstructured crime text documents in Serbian language aiming to find elements of crime in cyberspace. © 2017 Nova Science Publishers, Inc.",2017,Knowledge Discovery in Cyberspace: Statistical Analysis and Predictive Modeling,0,in @ process of developing e-government serbian government ha implemented a lot of e-government service @ produce a @ amount of data and text document and whose citizen use more and more @ service in @ everyday life @ text document @ in serbian language and commonly in html pdf and microsoft word format @ considering @ increased amount of @ text document serbian e-government ha indicated @ need @ certain data and information extraction @ @ variety of existing text document @ @ usually in a format prepared @ print @ in order to offer technical solution @ a case @ author @ developed a dedicated application @ includes lucene library @ lucene is a specialized library @ @ implementation of @ indexing and searching @ a @ amount of data @ @ procedure of quick search within unstructured text document in serbian language lead to @ efficient detection and processing of criminal offense and contributes to @ increased level of security in @ republic of serbia @ in @ @ @ author deal @ @ possibility of lucene indexing and lucene searching of data and document within unstructured crime text document in serbian language aiming to find element of crime in cyberspace @ nova science publisher inc @ 
1972,Web queries classification based on the syntactical patterns of search types,"Nowadays, people make frequent use of search engines in order to find the information they need on the web. The abundance of available data has rendered the process of obtaining relevant information challenging in terms of processing and analyzing it. A broad range of web queries classification techniques have been proposed with the aim of helping in understanding the actual intent behind a web search. In this research, we have categorized search queries through introducing Search Type Syntactical Patterns for automatically identifying and classifying search engine user queries. Experiments show that our approach has a good level of accuracy in identifying different search types. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,nowadays people make frequent use of search engine in order to find @ information @ need on @ web @ @ abundance of available data ha rendered @ process of obtaining relevant information challenging in term of processing and analyzing @ @ a broad range of web query classification technique @ @ proposed @ @ aim of helping in understanding @ actual intent behind a web search @ in @ research @ @ categorized search query @ introducing search type syntactical pattern @ automatically identifying and classifying search engine user query @ experiment @ @ @ approach ha a good level of accuracy in identifying different search type @ @ international publishing ag @ 
1973,Analyzing perceived intentions of public health-related communication on Twitter,"The increasing population with chronic diseases and highly engaged in online communication has triggered an urge in healthcare to understand this phenomenon. We propose an automatic approach to analyze the perceived intentions behind public tweets. Our long-term goal is to create high-level, behavioral models of the health information consumers and disseminators, relevant to studies in narrative medicine and health information dissemination. The contributions of this paper are: (1) a validated intention taxonomy, derived from pragmatics and empirically adjusted to Twitter public communication; (2) a tagged healthrelated corpus of 1100 tweets; (3) an effective approach to automatically discover intentions from text, using supervised machine learning with discourse features only, independent of domain vocabulary. Reasoning on the results, we claim the transferability of our solution to other healthcare corpora, enabling thus more extensive studies in the concerned domains. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ increasing population @ chronic disease and highly engaged in online communication ha triggered @ urge in healthcare to understand @ phenomenon @ @ propose @ automatic approach to analyze @ perceived intention behind public tweet @ @ long-term goal is to create high-level behavioral model of @ health information consumer and disseminator relevant to study in narrative medicine and health information dissemination @ @ contribution of @ @ @ @ @ @ a validated intention taxonomy derived @ pragmatic and empirically adjusted to twitter public communication @ @ @ a tagged healthrelated corpus of tweet @ @ @ @ effective approach to automatically discover intention @ text @ supervised machine learning @ discourse feature only independent of domain vocabulary @ reasoning on @ @ @ claim @ transferability of @ solution to @ healthcare corpus enabling thus more extensive study in @ concerned domain @ @ international publishing ag @ 
1975,The complementary nature of different NLP toolkits for named entity recognition in social media,"In this paper we study the combined use of four different NLP toolkits—Stanford CoreNLP, GATE, OpenNLP and Twitter NLP tools—in the context of social media posts. Previous studies have shown performance comparisons between these tools, both on news and social media corporas. In this paper, we go further by trying to understand how differently these toolkits predict Named Entities, in terms of their precision and recall for three different entity types, and how they can complement each other in this task in order to achieve a combined performance superior to each individual one. Experiments on two publicly available datasets from the workshops WNUT-2015 and #MSM2013 show that using an ensemble of toolkits can improve the recognition of specific entity types - up to 10.62% for the entity type Person, 1.97% for the type Location and 1.31% for the type Organization, depending on the dataset and the criteria used for the voting. Our results also showed improvements of 3.76% and 1.69%, in each dataset respectively, on the average performance of the three entity types. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,in @ @ @ study @ combined use of four different nlp toolkits stanford corenlp gate opennlp and twitter nlp tool in @ context of social medium post @ previous study @ @ performance comparison @ @ tool @ on news and social medium corporas @ in @ @ @ go @ by trying to understand @ differently @ toolkits predict named entity in term of @ precision and recall @ three different entity type and @ @ @ complement @ @ in @ task in order to achieve a combined performance superior to @ individual @ @ experiment on @ publicly available datasets @ @ workshop wnut and msm @ @ @ @ ensemble of toolkits @ improve @ recognition of specific entity type up to @ @ @ entity type person @ @ @ type location and @ @ @ type organization depending on @ dataset and @ criterion used @ @ voting @ @ @ @ showed improvement of @ and @ in @ dataset respectively on @ average performance of @ three entity type @ @ international publishing ag @ 
1977,Semantic similarity based web document classification using support vector machine,"With the rapid growth of information on the World Wide Web (WWW), classification of web documents has become important for efficient information retrieval. Relevancy of information retrieved can also be improved by considering semantic relatedness between words which is a basic research area in fields of natural language processing, intelligent retrieval, document clustering and classification, word sense disambiguation etc. The web search engine based semantic relationship from huge web corpus can improve classification of documents. This paper proposes an approach for web document classification that exploits information, including both page count and snippets. To identify the semantic relations between the query words, a lexical pattern extraction algorithm is applied on snippets. A sequential pattern clustering algorithm is used to form clusters of different patterns. The page count based measures are combined with the clustered patterns to define the features extracted from the word-pairs. These features are used to train the Support Vector Machine (SVM), in order to classify the web documents. Experimental results demonstrate 5% and 9% improvement in F1 measure for Reuters 21578 and 20 Newsgroup datasets in the classifier performance. © 2017, Zarka Private University. All rights reserved.",2017,International Arab Journal of Information Technology,7,@ @ rapid growth of information on @ world wide web @ www @ classification of web document ha become important @ efficient information retrieval @ relevancy of information retrieved @ @ @ improved by considering semantic relatedness @ word @ is a basic research area in field of natural language processing intelligent retrieval document clustering and classification word sense disambiguation etc @ @ web search engine based semantic relationship @ huge web corpus @ improve classification of document @ @ @ proposes @ approach @ web document classification @ exploit information including @ page count and snippet @ to identify @ semantic relation @ @ query word a lexical pattern extraction algorithm is applied on snippet @ a sequential pattern clustering algorithm is used to form cluster of different pattern @ @ page count based measure @ combined @ @ clustered pattern to define @ feature extracted @ @ word-pairs @ @ feature @ used to train @ support vector machine @ svm @ in order to classify @ web document @ experimental @ demonstrate and improvement in f measure @ reuters and newsgroup datasets in @ classifier performance @ zarka private university @ @ right reserved @ 
1981,Sentiments and Opinions From Twitter About Peruvian Touristic Places Using Correspondence Analysis,"Tourism in Perú has become very important, since there is a growing number of tourists arriving each year. This paper focus in understand what do speaking-english tourists have in consideration when they visit Perú. We obtained all the tweets published in english during the year 2016, filtered by touristic places visited. In total, more than 192 thousand tweets were collected. We performed different analysis to describe the data, including correspondence analysis, a statistical technique which is normally applied to categorical data. The goal was to understand the sentiments and opinions expressed in those tweets.",2017,CEUR Workshop Proceedings,3,tourism in perú ha become @ important since @ is a growing number of tourist arriving @ year @ @ @ focus in understand @ @ speaking-english tourist @ in consideration @ @ visit perú @ @ obtained @ @ tweet published in english @ @ year filtered by touristic place visited @ in total more @ thousand tweet @ collected @ @ performed different analysis to describe @ data including correspondence analysis a statistical technique @ is normally applied to categorical data @ @ goal wa to understand @ sentiment and opinion expressed in @ tweet @ 
1983,Mining textual terms for stock market prediction analysis using financial news,"This study focuses on the use of machine learning algorithms to construct a model that can predict the movements of Bursa Malaysia stock prices. In this research, we concentrate on linguistics terms from financial news that can contribute movements of the prices. Our aim is to develop a prototype that can classify sentiments towards financial news for investment decision. We experimented with five blue-chip companies from different industries of the top market constituents in Bursa Malaysia KLCI. A total of 14,992 finance articles were crawled and used as the dataset. Support Vector Machine algorithm was employed and the accuracy recorded was at 56%. The findings of this research can be used to assist investors in investment decision making. © Springer Nature Singapore Pte Ltd. 2017.",2017,Communications in Computer and Information Science,3,@ study focus on @ use of machine learning algorithm to construct a model @ @ predict @ movement of bursa malaysia stock price @ in @ research @ concentrate on linguistics term @ financial news @ @ contribute movement of @ price @ @ aim is to develop a prototype @ @ classify sentiment towards financial news @ investment decision @ @ experimented @ five blue-chip company @ different industry of @ top market constituent in bursa malaysia klci @ a total of finance article @ crawled and used a @ dataset @ support vector machine algorithm wa employed and @ accuracy recorded wa at @ @ finding of @ research @ @ used to assist investor in investment decision making @ @ nature singapore pte ltd @ @ 
1984,Recurrent neural conditional random fields for target identification of tweets,"Target-dependent sentiment analysis on Twitter is the problem of identifying the sentiment polarity towards a certain target in a given tweet. All the existing studies of this task assume that the target is known. However, in such tasks, extracting the targets from the text is one of the most important subtasks. In this paper, we propose a model based on Bidirectional Gated Recurrent Units and Conditional Random Fields to identify automatically the targets from the tweets. The model has been evaluated on two benchmarks of tweets, obtaining results which show its superiority over several baseline methods. © 2017 The authors and IOS Press. All rights reserved.",2017,Frontiers in Artificial Intelligence and Applications,1,target-dependent sentiment analysis on twitter is @ problem of identifying @ sentiment polarity towards a certain target in a given tweet @ @ @ existing study of @ task assume @ @ target is known @ however in @ task extracting @ target @ @ text is @ of @ @ important subtasks @ in @ @ @ propose a model based on bidirectional gated recurrent unit and conditional random field to identify automatically @ target @ @ tweet @ @ model ha @ evaluated on @ benchmark of tweet obtaining @ @ @ @ superiority @ several baseline method @ @ author and io @ @ @ right reserved @ 
1985,Convolutional neural networks for unsupervised anomaly detection in text data,"In this paper, we discuss the problem of anomaly detection in text data using convolutional neural network (CNN). Recently CNNs have become one of the most popular and powerful tools for various machine learning tasks. CNN’s main advantage is an ability to extract complicated hidden features from high dimensional data with complex structure. Usually CNNs are applied in supervised learning mode. On the other hand, unsupervised anomaly detection is an important problem in many applications, including computer security, behavioral analytics, etc. Since there is no specified target in unsupervised mode, traditional CNN’s objective functions cannot be used. In this paper, we develop a specific CNN architecture. It consists of one convolutional layer and one subsampling layer, we use RBF activation function and logarithmic loss function on the final layer. Minimization of the corresponding objective function helps us to calculate the location parameter of the features’ weights discovered on the last network layer. We use l2 -regularization to avoid degenerate solution. Proposed CNN has been tested on anomalies discovering in a stream of text documents modeled with well-known Enron dataset, where proposed method demonstrates better results in comparison with the traditional outlier detection methods based on one-class SVM and NMF. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,in @ @ @ discus @ problem of anomaly detection in text data @ convolutional neural network @ cnn @ @ recently cnns @ become @ of @ @ popular and powerful tool @ various machine learning task @ cnn s main advantage is @ ability to extract complicated hidden feature @ high dimensional data @ complex structure @ usually cnns @ applied in supervised learning mode @ on @ @ hand unsupervised anomaly detection is @ important problem in many application including computer security behavioral analytics etc @ since @ is no specified target in unsupervised mode traditional cnn s objective function cannot @ used @ in @ @ @ develop a specific cnn architecture @ @ consists of @ convolutional layer and @ subsampling layer @ use rbf activation function and logarithmic loss function on @ final layer @ minimization of @ corresponding objective function help u to calculate @ location parameter of @ feature weight discovered on @ last network layer @ @ use l regularization to avoid degenerate solution @ proposed cnn ha @ tested on anomaly discovering in a stream of text document modeled @ well-known enron dataset @ proposed method demonstrates better @ in comparison @ @ traditional outlier detection method based on one-class svm and nmf @ @ international publishing ag @ 
1986,Bangla News Summarization,"Document similarity calculation and summarization is a challenging task. Not many works have been done in this field for Bangla Language. Similarity calculation and summarization is more challenging for Bangla Language as Bangla grammar works differently than that of English. This paper proposes a way to calculate similarity between Bangla news and apply summarization on Bangla news documents taken from popular news portals by applying various data mining techniques as accurately as possible. © 2017, Springer International Publishing AG.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,document similarity calculation and summarization is a challenging task @ not many work @ @ done in @ field @ bangla language @ similarity calculation and summarization is more challenging @ bangla language a bangla grammar work differently @ @ of english @ @ @ proposes a way to calculate similarity @ bangla news and apply summarization on bangla news document taken @ popular news portal by applying various data mining technique a accurately a possible @ @ international publishing ag @ 
1987,Extracting core claims from scientific articles,"The number of scientific articles has grown rapidly over the years and there are no signs that this growth will slow down in the near future. Because of this, it becomes increasingly difficult to keep up with the latest developments in a scientific field. To address this problem, we present here an approach to help researchers learn about the latest developments and findings by extracting in a normalized form core claims from scientific articles. This normalized representation is a controlled natural language of English sentences called AIDA, which has been proposed in previous work as a method to formally structure and organize scientific findings and discourse. We show how such AIDA sentences can be automatically extracted by detecting the core claim of an article, checking for AIDA compliance, and – if necessary – transforming it into a compliant sentence. While our algorithm is still far from perfect, our results indicate that the different steps are feasible and they support the claim that AIDA sentences might be a promising approach to improve scientific communication in the future. © 2017, Springer International Publishing AG.",2017,Communications in Computer and Information Science,0,@ number of scientific article ha grown rapidly @ @ year and @ @ no sign @ @ growth @ slow down in @ near future @ @ of @ @ becomes increasingly difficult to keep up @ @ latest development in a scientific field @ to address @ problem @ @ @ @ approach to help researcher learn @ @ latest development and finding by extracting in a normalized form core claim @ scientific article @ @ normalized representation is a controlled natural language of english sentence called aida @ ha @ proposed in previous work a a method to formally structure and organize scientific finding and discourse @ @ @ @ @ aida sentence @ @ automatically extracted by detecting @ core claim of @ article checking @ aida compliance and if necessary transforming @ @ a compliant sentence @ @ @ algorithm is still far @ perfect @ @ indicate @ @ different step @ feasible and @ support @ claim @ aida sentence might @ a promising approach to improve scientific communication in @ future @ @ international publishing ag @ 
1988,What should i cite? cross-collection reference recommendation of patents and papers,"Research results manifest in large corpora of patents and scientific papers. However, both corpora lack a consistent taxonomy and references across different document types are sparse. Therefore, and because of contrastive, domain-specific language, recommending similar papers for a given patent (or vice versa) is challenging. We propose a recommender system that leverages topic distributions and keywords to recommend related work despite these challenges. As a case study, we evaluate our approach on patents and papers of two fields: medical and computer science. We find that topic-based recommenders complement word-based recommenders for documents with collection-specific language and increase mean average precision by up to 27%. As a result of our work, publications from both corpora form a joint digital library, which connects academia and industry. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,research @ manifest in @ corpus of patent and scientific @ @ however @ corpus lack a consistent taxonomy and reference across different document type @ sparse @ therefore and @ of contrastive domain-specific language recommending similar @ @ a given patent @ @ vice versa @ is challenging @ @ propose a recommender system @ leverage topic distribution and keywords to recommend related work despite @ challenge @ a a case study @ evaluate @ approach on patent and @ of @ field @ medical and computer science @ @ find @ topic-based recommenders complement word-based recommenders @ document @ collection-specific language and increase mean average precision by up to @ a a @ of @ work publication @ @ corpus form a joint digital library @ connects academia and industry @ @ international publishing ag @ 
1989,"A comparative study of language modeling to instance-based methods, and feature combinations for authorship attribution","We present a comparative study of language modeling to traditional instance-based methods for authorship attribution, using several different basic units as features, such as characters, words, and other simple lexical measurements, as well as we propose the use of part-of-speech (POS) tags as features for language modeling. In contrast to many other studies which focus on small sets of documents written by major writers regarding several topics, we consider a relatively large corpus with documents edited by non-professional writers regarding the same topic. We find that language models based on either characters or POS tags are the most effective, while the latter provide additional efficiency benefits and robustness against data sparsity. Moreover, we experiment with linearly combining several language models, as well as employing unions of several different feature types in instance-based methods. We find that both such combinations constitute viable strategies which generally improve effectiveness. By linearly combining three language models, based respectively on character, word, and POS trigrams, we achieve the best generalization accuracy of 96%. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ a comparative study of language modeling to traditional instance-based method @ authorship attribution @ several different basic unit a feature @ a character word and @ simple lexical measurement a well a @ propose @ use of part-of-speech @ po @ tag a feature @ language modeling @ in contrast to many @ study @ focus on small set of document written by major writer regarding several topic @ consider a relatively @ corpus @ document edited by non-professional writer regarding @ @ topic @ @ find @ language model based on either character @ po tag @ @ @ effective @ @ latter provide additional efficiency benefit and robustness @ data sparsity @ moreover @ experiment @ linearly combining several language model a well a employing union of several different feature type in instance-based method @ @ find @ @ @ combination constitute viable strategy @ generally improve effectiveness @ by linearly combining three language model based respectively on character word and po trigram @ achieve @ best generalization accuracy of @ @ international publishing ag @ 
1990,Examining the impact of feature selection on sentiment analysis for the greek language,"Sentiment analysis identifies the attitude that a person has towards a service, a topic or an event and it is very useful for companies which receive many written opinions. Research studies have shown that the determination of sentiment in written text can be accurately determined through text and part of speech features. In this paper, we present an approach to recognize opinions in Greek language and we examine the impact of feature selection on the analysis of opinions and the performance of the classifiers. We analyze a large number of feedback and comments from teachers towards e-learning, life-long courses that have attended with the aim to specify their opinions. A number of text-based and part of speech based features from textual data are extracted and a generic approach to analyze text and determine opinion is presented. Evaluation results indicate that the approach illustrated is accurate in specifying opinions in Greek text and also sheds light on the effect that various features have on the classification performance. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,sentiment analysis identifies @ attitude @ a person ha towards a service a topic @ @ event and @ is @ useful @ company @ receive many written opinion @ research study @ @ @ @ determination of sentiment in written text @ @ accurately determined @ text and part of speech feature @ in @ @ @ @ @ approach to recognize opinion in greek language and @ examine @ impact of feature selection on @ analysis of opinion and @ performance of @ classifier @ @ analyze a @ number of feedback and comment @ teacher towards e-learning life-long course @ @ attended @ @ aim to specify @ opinion @ a number of text-based and part of speech based feature @ textual data @ extracted and a generic approach to analyze text and determine opinion is presented @ evaluation @ indicate @ @ approach illustrated is accurate in specifying opinion in greek text and @ shed light on @ effect @ various feature @ on @ classification performance @ @ international publishing ag @ 
1991,K-means and hierarchical clustering method to improve our understanding of citation contexts,"In this paper we focus of the clustering of citation contexts in scientific papers. We use two methods, k-means and hierarchical clustering to better understand the phenomenon and types of citations and to explore the multidimensional nature of the elements composing the contexts of citations in different sections of the papers. We have analyzed a data set of seven peer-reviewed academic journals published by PLOS. The obtained clusters show that the Methods section is specific in nature, regardless of the journal. A proximity between some of the journals can be observed.",2017,CEUR Workshop Proceedings,3,in @ @ @ focus of @ clustering of citation context in scientific @ @ @ use @ method k-means and hierarchical clustering to better understand @ phenomenon and type of citation and to explore @ multidimensional nature of @ element composing @ context of citation in different section of @ @ @ @ @ analyzed a data set of seven peer-reviewed @ journal published by plo @ @ obtained cluster @ @ @ method section is specific in nature regardless of @ journal @ a proximity @ some of @ journal @ @ observed @ 
1992,One-class text document classification with OCSVM and LSI,"In this paper, we propose a novel one-class classification approach for text document classification using One-Class Support Vector Machine (OCSVM) and Latent Semantic Indexing (LSI) in tandem. We first apply t-statistic-based feature selection on the text corpus. Then, we apply OCSVM on the rows corresponding to the negative class of the document-term matrix of a collection of text documents and extract the Support Vectors (SV). Then, in the test phase, we employ LSI on the query documents from the positive class to compare them with the SVs extracted from the negative class and match score is computed using the cosine similarity measure. Then, based on a prespecified threshold for the match score, we classify the positive category of the text corpus. Use of SV for comparison reduces the computational load, which is the main contribution of the paper. We demonstrated the effectiveness of our approach on the datasets pertaining to Phishing, and sentiment analysis in a bank. © Springer Nature Singapore Pte Ltd. 2017.",2017,Advances in Intelligent Systems and Computing,0,in @ @ @ propose a novel one-class classification approach @ text document classification @ one-class support vector machine @ ocsvm @ and latent semantic indexing @ lsi @ in tandem @ @ first apply t-statistic-based feature selection on @ text corpus @ @ @ apply ocsvm on @ row corresponding to @ negative class of @ document-term matrix of a collection of text document and extract @ support vector @ sv @ @ @ in @ test phase @ employ lsi on @ query document @ @ positive class to compare @ @ @ svs extracted @ @ negative class and match score is computed @ @ cosine similarity measure @ @ based on a prespecified threshold @ @ match score @ classify @ positive category of @ text corpus @ use of sv @ comparison reduces @ computational load @ is @ main contribution of @ @ @ @ demonstrated @ effectiveness of @ approach on @ datasets pertaining to phishing and sentiment analysis in a bank @ @ nature singapore pte ltd @ @ 
1993,A process for exploring employees’ relationships via social network and sentiment analysis,"The proposed study is to analyze and visualize properties of the social network constructed from a dataset based on Enron mail dataset, and utilize sentiment analysis as an additional source of information to study employees’ relationships in a company. We concluded that when social network analysis is used in conjunction with emotion detection, it is possible to see the positive or negative areas where the company must work to promote a healthy organizational culture and uncover possible organizational issues in a timely manner. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proposed study is to analyze and visualize property of @ social network constructed @ a dataset based on enron mail dataset and utilize sentiment analysis a @ additional source of information to study employee relationship in a company @ @ concluded @ @ social network analysis is used in conjunction @ emotion detection @ is possible to see @ positive @ negative area @ @ company must work to promote a healthy organizational culture and uncover possible organizational issue in a timely manner @ @ international publishing ag @ 
1994,Toward sentiment analysis in elderly care facility,"Hand-over notes are extremely important to share information about irregular incidents at elderly care facilities and provide high-quality services. However, taking notes is a time-consuming task. Moreover, handwritten hand-over notes make it difficult to pass on experience and related know-how to other workers. To curate that field community intelligence, a handover support system for elderly care facilities was installed into a facility and evaluated. The system is now in actual operation at the care facility. The authors aim to use handover support system as a communication tool that sense the feelings of the care workers and supports them to maintain motivation and cultivate self-directedness. To realize this aim, this paper reports the results of hand-over data analysis and comparison with traditional paper-based hand-over notes. Furthermore, the authors explored the possibility of applying sentiment analysis technology to hand-over messages to sense the atmosphere of their working environment. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,hand-over note @ extremely important to share information @ irregular incident at elderly care facility and provide high-quality service @ however taking note is a time-consuming task @ moreover handwritten hand-over note make @ difficult to pas on experience and related know-how to @ worker @ to curate @ field community intelligence a handover support system @ elderly care facility wa installed @ a facility and evaluated @ @ system is now in actual operation at @ care facility @ @ author aim to use handover support system a a communication tool @ sense @ feeling of @ care worker and support @ to maintain motivation and cultivate self-directedness @ to realize @ aim @ @ report @ @ of hand-over data analysis and comparison @ traditional paper-based hand-over note @ furthermore @ author explored @ possibility of applying sentiment analysis technology to hand-over message to sense @ atmosphere of @ working environment @ @ international publishing ag @ 
1996,"What you use, not what you do: Automatic classification of recipes","Social media data is notoriously noisy and unclean. Recipe collections built by users are no exception, particularly when it comes to cataloging them. However, consistent and transparent categorization is vital to users who search for a specific entry. Similarly, curators are faced with the same challenge given a large collection of existing recipes: They first need to understand the data to be able to build a clean system of categories. This paper presents an empirical study on the automatic classification of recipes on the German cooking website Chefkoch. The central question we aim at answering is: Which information is necessary to perform well at this task? In particular, we compare features extracted from the free text instructions of the recipe to those taken from the list of ingredients. On a sample of 5,000 recipes with 87 classes, our feature analysis shows that a combination of nouns from the textual description of the recipe with ingredient features performs best (48% F1). Nouns alone achieve 45% F1 and ingredients alone 46% F1. However, other word classes do not complement the information from nouns. On a bigger training set of 50,000 instances, the best configuration shows an improvement to 57% highlighting the importance of a sizeable data set. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,social medium data is notoriously noisy and unclean @ recipe collection built by user @ no exception particularly @ @ come to cataloging @ @ however consistent and transparent categorization is vital to user @ search @ a specific entry @ similarly curator @ faced @ @ @ challenge given a @ collection of existing recipe @ @ first need to understand @ data to @ able to build a clean system of category @ @ @ @ @ empirical study on @ automatic classification of recipe on @ german cooking website chefkoch @ @ central question @ aim at answering is @ @ information is necessary to perform well at @ task @ in particular @ compare feature extracted @ @ free text instruction of @ recipe to @ taken @ @ list of ingredient @ on a sample of recipe @ class @ feature analysis @ @ a combination of noun @ @ textual description of @ recipe @ ingredient feature performs best @ f @ @ noun alone achieve f and ingredient alone f @ however @ word class @ not complement @ information @ noun @ on a bigger training set of instance @ best configuration @ @ improvement to highlighting @ importance of a sizeable data set @ @ international publishing ag @ 
1997,Supporting experts to handle tweet collections about significant events,"We introduce Relevancer that processes a tweet set and enables generating an automatic classifier from it. Relevancer satisfies information needs of experts during significant events. Enabling experts to combine automatic procedures with expertise is the main contribution of our approach and the added value of the tool. Even a small amount of feedback enables the tool to distinguish between relevant and irrelevant information effectively. Thus, Relevancer facilitates the quick understanding of and proper reaction to events presented on Twitter. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ introduce relevancer @ process a tweet set and enables generating @ automatic classifier @ @ @ relevancer satisfies information need of expert @ significant event @ enabling expert to combine automatic procedure @ expertise is @ main contribution of @ approach and @ added value of @ tool @ even a small amount of feedback enables @ tool to distinguish @ relevant and irrelevant information effectively @ thus relevancer facilitates @ quick understanding of and proper reaction to event presented on twitter @ @ international publishing ag @ 
1999,Detect online review spammers based on comprehensive trustiness propagation model,"Review spammers detection is an important task in social media sentiment analysis. Previous works employ reviewer behaviors such as text similarities, duplications and rating patterns to indentify suspicious spammers. However, there are still other kinds of abnormal spamming activities which could not be detected by the available techniques. This paper proposes a review spammer detection approach combining both TrustRank and Anti-TrustRank propagation algorithm to identify review spammers. Firstly, a twolayer heterogeneous review relation graph is constructed to capture the relationships among reviewers and products. Secondly, a TrustRank based propagation model and an Anti-TrustRank based propagation model are established to calculate the reviewers' trustiness value and the reviewers' anti-trustiness value respectively. Finally, review spammers are detected according to the comprehensive trustiness value which combines both reviewers' trustiness value and anti-trustiness value. Experimental results show that according to two datasets, our presented method significantly outperforms the existing baselines, and is able to find more abnormal spamming activities.",2017,Journal of Internet Technology,4,review spammer detection is @ important task in social medium sentiment analysis @ previous work employ reviewer behavior @ a text similarity duplication and rating pattern to indentify suspicious spammer @ however @ @ still @ kind of abnormal spamming activity @ could not @ detected by @ available technique @ @ @ proposes a review spammer detection approach combining @ trustrank and anti-trustrank propagation algorithm to identify review spammer @ firstly a twolayer heterogeneous review relation graph is constructed to capture @ relationship among reviewer and product @ secondly a trustrank based propagation model and @ anti-trustrank based propagation model @ established to calculate @ reviewer @ trustiness value and @ reviewer @ anti-trustiness value respectively @ finally review spammer @ detected according to @ comprehensive trustiness value @ combine @ reviewer @ trustiness value and anti-trustiness value @ experimental @ @ @ according to @ datasets @ presented method significantly outperforms @ existing baseline and is able to find more abnormal spamming activity @ 
2000,Application of a new set of pseudo-distances in documents categorization,"Automatic text classification is a very important task that consists in assigning labels (categories, groups, classes) to a given text based on a set of previously labeled texts called training set. The work presented in this paper treats the problem of automatic topical text categorization. It is a supervised classification because it works on a predefined set of classes and topical because it uses topics or subjects of texts as classes. In this context, we used a new approach based on k-NN algorithm, as well as a new set of pseudo-distances (distance metrics) known in the field of language identification. We also proposed a simple and effective method to improve the quality of performed categorization. © 2017 CTU FTS.",2017,Neural Network World,0,automatic text classification is a @ important task @ consists in assigning label @ category group class @ to a given text based on a set of @ labeled text called training set @ @ work presented in @ @ treat @ problem of automatic topical text categorization @ @ is a supervised classification @ @ work on a predefined set of class and topical @ @ us topic @ subject of text a class @ in @ context @ used a @ approach based on k-nn algorithm a well a a @ set of pseudo-distances @ distance metric @ known in @ field of language identification @ @ @ proposed a simple and effective method to improve @ quality of performed categorization @ ctu ft @ 
2001,Financial sentiment orientation of word combinations,This paper presents an ongoing work on sentiment analysis in the financial domain and explores an approach to identifying sentiment orientations of words for a given financial index. The proposed approach takes advantage of the movement of the given financial index and employs an information theoretic measure for estimating sentiment orientation of word combinations in an efficient way. Results on preliminary experiments are reported. © Springer International Publishing AG 2017.,2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ @ ongoing work on sentiment analysis in @ financial domain and explores @ approach to identifying sentiment orientation of word @ a given financial index @ @ proposed approach take advantage of @ movement of @ given financial index and employ @ information theoretic measure @ estimating sentiment orientation of word combination in @ efficient way @ @ on preliminary experiment @ reported @ @ international publishing ag @ 
2002,Aggregating and analyzing articles and comments on a news website,"In the top news stories, the commenting activity is rising and falling until it stops. In some ongoing news stories such as disasters like the disappearance of flight MH370, global warming or climate change, political turmoil or economic crisis, this commenting activity cycle can repeat and last many years. To our knowledge, a study and analysis of those data does not exist up to now. There is a need to separate facts, opinions and junk within those comments data. In this paper, we present our framework for supporting readers in analyzing and visualizing facts, opinions and topics in the comments and its extension with comments aggregation and summarization for comments within several news articles for the same event. We added a time-series analysis and comments features such as surprising comments and a preferential threads attachment model. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ top news story @ commenting activity is rising and falling @ @ stop @ in some ongoing news story @ a disaster like @ disappearance of flight mh global warming @ climate change political turmoil @ economic crisis @ commenting activity cycle @ repeat and last many year @ to @ knowledge a study and analysis of @ data doe not exist up to now @ @ is a need to separate fact opinion and junk within @ comment data @ in @ @ @ @ @ framework @ supporting reader in analyzing and visualizing fact opinion and topic in @ comment and @ extension @ comment aggregation and summarization @ comment within several news article @ @ @ event @ @ added a time-series analysis and comment feature @ a surprising comment and a preferential thread attachment model @ @ international publishing ag @ 
2003,Sentiment analysis for older people in cross-platform instant messaging service,"The population of older people increases in many developed and developing countries, so that the overall structures of the populations has been changing. However, older people are one of the most disadvantaged and vulnerable groups for digital exclusion in this technocratic society. Therefore, in this article, we aims to predict the sentiments for older people when they use the cross-platform instance messaging service such as WeChat or WhatsApp. Specifically, we adopt semi-annotation approaches to obtaining their sentimental labels from the textual data in the cross-platform instance messaging service. Furthermore, we propose a lexical-based framework for predicting the sentimental labels. The findings give us insight to develop applications for the inclusion of older people in digital world. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ population of older people increase in many developed and developing country @ @ @ overall structure of @ population ha @ changing @ however older people @ @ of @ @ disadvantaged and vulnerable group @ digital exclusion in @ technocratic society @ therefore in @ article @ aim to predict @ sentiment @ older people @ @ use @ cross-platform instance messaging service @ a wechat @ whatsapp @ specifically @ adopt semi-annotation approach to obtaining @ sentimental label @ @ textual data in @ cross-platform instance messaging service @ furthermore @ propose a lexical-based framework @ predicting @ sentimental label @ @ finding give u insight to develop application @ @ inclusion of older people in digital world @ @ international publishing ag @ 
2004,Automated defect discovery for dishwasher appliances from online consumer reviews,"Product defects can have a devastating impact on a firm's sales and reputation, especially in the era of social media. The early detection of defects could not only protect consumers from financial losses, but could also mitigate financial damage to the manufacturer. Previous work in automated defect discovery has had success in the automotive, consumer electronics, and toy industries, but so far there has been no application to home appliances. In this study, we extend the text analytic framework conceived in earlier work to the discovery of underperformance in large home appliances, specifically dishwashers. We find that generic cross-domain sentiment techniques can be strongly complemented by domain-specific “smoke” and “sparkle” term lists that are highly correlated with potential defects. These findings can be highly beneficial to improving dishwasher appliance quality management methods. © 2016 Elsevier Ltd",2017,Expert Systems with Applications,34,product defect @ @ a devastating impact on a firm @ s sale and reputation especially in @ era of social medium @ @ early detection of defect could not only protect consumer @ financial loss @ could @ mitigate financial damage to @ manufacturer @ previous work in automated defect discovery ha @ success in @ automotive consumer electronics and toy industry @ @ far @ ha @ no application to home appliance @ in @ study @ extend @ text analytic framework conceived in earlier work to @ discovery of underperformance in @ home appliance specifically dishwasher @ @ find @ generic cross-domain sentiment technique @ @ strongly complemented by domain-specific smoke and sparkle term list @ @ highly correlated @ potential defect @ @ finding @ @ highly beneficial to improving dishwasher appliance quality management method @ @ ltd
2008,Analysis of online discussions in support of requirements discovery,"Feedback about software applications and services that end-users express through web-based communication platforms represents an invaluable knowledge source for diverse software engineering tasks, including requirements elicitation. Research work on automated analysis of textual messages in app store reviews, open source software (OSS) mailing-lists and user forums has been rapidly increasing in the last five years. NLP techniques are applied to filter out irrelevant data, text mining and automated classification techniques are then used to classify messages into different categories, such as bug report and feature request. Our research focuses on online discussions that take place in user forums and OSS mailing-lists, and aims at providing automated analysis techniques to discover contained requirements. In this paper, we present a speech-acts based analysis technique, and experimentally evaluate it on a dataset taken from a widely used OSS project. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13,feedback @ software application and service @ end-users express @ web-based communication platform represents @ invaluable knowledge source @ diverse software engineering task including requirement elicitation @ research work on automated analysis of textual message in app store review open source software @ os @ mailing-lists and user forum ha @ rapidly increasing in @ last five year @ nlp technique @ applied to filter @ irrelevant data text mining and automated classification technique @ @ used to classify message @ different category @ a bug report and feature request @ @ research focus on online discussion @ take place in user forum and os mailing-lists and aim at providing automated analysis technique to discover contained requirement @ in @ @ @ @ a speech-acts based analysis technique and experimentally evaluate @ on a dataset taken @ a widely used os project @ @ international publishing ag @ 
2011,Mining Structures from Massive Text Data: A Data-Driven Approach,"The real-world big data are largely unstructured, interconnected, and in the form of natural language text. One of the grand challenges is to mine structures from such massive unstructured data, and transform such big data into structured networks and actionable knowledge. We propose a text mining approach that requires only distant supervision or minimal supervision but relies on massive data. We show that quality phrases can be mined from such massive text data, types can be extracted from massive text data with distant supervision, and entity-attribute-value triples can be extracted from meta-patterns discovered from such data. Finally, we propose a data-to-network-to-knowledge paradigm, that is, first turn data into relatively structured information networks, and then mine such text-rich and structure-rich networks to generate useful knowledge. We show such a paradigm represents a promising direction at turning massive text data into structured networks and useful knowledge.",2017,CEUR Workshop Proceedings,0,@ real-world big data @ largely unstructured interconnected and in @ form of natural language text @ @ of @ grand challenge is to mine structure @ @ massive unstructured data and transform @ big data @ structured network and actionable knowledge @ @ propose a text mining approach @ requires only distant supervision @ minimal supervision @ relies on massive data @ @ @ @ quality phrase @ @ mined @ @ massive text data type @ @ extracted @ massive text data @ distant supervision and entity-attribute-value triple @ @ extracted @ meta-patterns discovered @ @ data @ finally @ propose a data-to-network-to-knowledge paradigm @ is first turn data @ relatively structured information network and @ mine @ text-rich and structure-rich network to generate useful knowledge @ @ @ @ a paradigm represents a promising direction at turning massive text data @ structured network and useful knowledge @ 
2012,Sentiment classification Method for identification of influential learners in Social Networks Communities,"The growth of social networking has gained much interest from the research community in recent years. Social networking technology as an e-learning tool seems promising for education instructors to combine distance education. Several analysis researches of social media were conducted for detection opinion leaders. While most of the existing algorithms proposed for communities determination are destined to commercial use, in this work, we present a new approach for detecting opinion leaders based on analyzing online learning community interactions. In fact, we aim to identify learners behaviors and attitudes in social network sites as productive online tools for learning. To achieve this purpose, we describe a method of performing detecting opinion leaders by using machine learning techniques. We focus on the application of text mining and sentiment analysis. The output of this work prove that education-based social network is very effective and improvement for online communications. Experiments show the efficiency of the introduced method which can be helpful and profitable for education instructors.",2017,CEUR Workshop Proceedings,0,@ growth of social networking ha gained much interest @ @ research community in recent year @ social networking technology a @ e-learning tool seems promising @ education instructor to combine distance education @ several analysis research of social medium @ conducted @ detection opinion leader @ @ @ of @ existing algorithm proposed @ community determination @ destined to commercial use in @ work @ @ a @ approach @ detecting opinion leader based on analyzing online learning community interaction @ in fact @ aim to identify learner behavior and attitude in social network site a productive online tool @ learning @ to achieve @ purpose @ describe a method of performing detecting opinion leader by @ machine learning technique @ @ focus on @ application of text mining and sentiment analysis @ @ output of @ work prove @ education-based social network is @ effective and improvement @ online communication @ experiment @ @ efficiency of @ introduced method @ @ @ helpful and profitable @ education instructor @ 
2014,A regression-based SVD parallelization using overlapping folds for textual data,"One of the most difficult issues in text mining is high dimensionality caused by a large number of features (keywords). While various multivariate analyses, such as PCA and SVD (in information retrieval, called LSI), are developed to solve this curse of high dimensionality, they are computationally costly. This paper investigates a regression-based reconstruction method that enables parallelization of PCA/SVD by decomposing a document-term matrix into a set of sub-matrices with consideration of overlapped terms, and then to re-assemble using regression technique. To evaluate our method, we utilize two text datasets in the UCI Machine Learning Repository, called “Bag of Words” and “Reuter 50 50”. To measure the closeness between two documents, cosine similarity is applied while the accuracy is measured in the form of rank order mismatch. Finally, the result shows that, the matrices decomposition and re-assembly can preserve the quality of relation/representation. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ of @ @ difficult issue in text mining is high dimensionality caused by a @ number of feature @ keywords @ @ @ various multivariate analysis @ a pca and svd @ in information retrieval called lsi @ @ developed to solve @ curse of high dimensionality @ @ computationally costly @ @ @ investigates a regression-based reconstruction method @ enables parallelization of pca svd by decomposing a document-term matrix @ a set of sub-matrices @ consideration of overlapped term and @ to re-assemble @ regression technique @ to evaluate @ method @ utilize @ text datasets in @ uci machine learning repository called bag of word and reuter @ to measure @ closeness @ @ document cosine similarity is applied @ @ accuracy is measured in @ form of rank order mismatch @ finally @ @ @ @ @ matrix decomposition and re-assembly @ preserve @ quality of relation representation @ @ international publishing ag @ 
2015,"2nd International Conference on Advanced Intelligent Systems and Informatics, AISI 2016","The proceedings contain 86 papers. The special focus in this conference is on Intelligent Language Processing Track, Intelligent Systems Track and Informatics Track. The topics include: Further investigations for documents information retrieval based on DWT; knowledge representation in intelligent tutoring system; multimodal graph-based dependency parsing of natural language; an enhanced distance based similarity measure for user based recommendations; semantic-based feature reduction approach for E-mail classification; statistical machine translation context modelling with recurrent neural network and LDA; optimizing fuzzy inference systems for improving speech emotion recognition; towards improving sentiment analysis in Arabic; an Arabic keyphrase extraction algorithm; lexicon free Arabic speech recognition recipe; natural language processing for Arabic metaphors; a conceptual approach; an automatic diacritization system for Arabic; using text mining to analyze real estate classifieds; security as a service model for cloud storage; content based image retrieval with hadoop; an enhanced distributed database design over the cloud environment; abrupt cut detection in news videos using dominant colors representation; new quantum image steganography scheme with hadamard transformation; optimize BpNN using new breeder genetic algorithm; a neural network approach for binary hashing in image retrieval; particle swarm optimization with random forests for handwritten Arabic recognition system; local influence maximization in social networks; controlling Rumor cascade over social networks; a behavioral action sequences process design; WEMA to Speed up NIDS packet header detection engine and CNN for handwritten Arabic digits recognition based on LeNet-5.",2017,Advances in Intelligent Systems and Computing,0,@ proceeding contain @ @ @ special focus in @ conference is on intelligent language processing track intelligent system track and informatics track @ @ topic include @ @ investigation @ document information retrieval based on dwt @ knowledge representation in intelligent tutoring system @ multimodal graph-based dependency parsing of natural language @ @ enhanced distance based similarity measure @ user based recommendation @ semantic-based feature reduction approach @ e-mail classification @ statistical machine translation context modelling @ recurrent neural network and lda @ optimizing fuzzy inference system @ improving speech emotion recognition @ towards improving sentiment analysis in arabic @ @ arabic keyphrase extraction algorithm @ lexicon free arabic speech recognition recipe @ natural language processing @ arabic metaphor @ a conceptual approach @ @ automatic diacritization system @ arabic @ @ text mining to analyze real estate classified @ security a a service model @ cloud storage @ content based image retrieval @ hadoop @ @ enhanced distributed database design @ @ cloud environment @ abrupt cut detection in news video @ dominant color representation @ @ quantum image steganography scheme @ hadamard transformation @ optimize bpnn @ @ breeder genetic algorithm @ a neural network approach @ binary hashing in image retrieval @ particle swarm optimization @ random forest @ handwritten arabic recognition system @ local influence maximization in social network @ controlling rumor cascade @ social network @ a behavioral action sequence process design @ wema to speed up nids packet header detection engine and cnn @ handwritten arabic digit recognition based on lenet @ 
2016,Social media sentiment polarity analysis: A novel approach to promote business performance and consumer decision-making,"In order to have a clear understanding of the market structure as well as the customer trends toward various products, there is a need for every company to collect, monitor, and analyze the user data generated online. In this paper, the online reviews of products from two leading camera manufacturers have been utilized to analyze the user trends. After preprocessing the data, sentiment analysis techniques have been employed to mine the textual content of customers’ opinion and classify them into different polarities according to the theoretical conceptualization of service and performance. The sentiment analysis results using Support Vector Machine provide a high level of accuracy in encapsulating and measuring the sentiments of customers toward the products and services as compared to the other text mining strategies. Further, a competitive analysis technique based on K-means clustering has been implemented to examine the most frequent word which is discussed by the customer. The combination of these two methods provides benefit not only to obtain the best classification but also to help the user focus on the most relevant categories that meet his/her interest. © Springer Nature Singapore Pte Ltd. 2017.",2017,Advances in Intelligent Systems and Computing,0,in order to @ a clear understanding of @ market structure a well a @ customer trend toward various product @ is a need @ every company to collect monitor and analyze @ user data generated online @ in @ @ @ online review of product @ @ leading camera manufacturer @ @ utilized to analyze @ user trend @ @ preprocessing @ data sentiment analysis technique @ @ employed to mine @ textual content of customer opinion and classify @ @ different polarity according to @ theoretical conceptualization of service and performance @ @ sentiment analysis @ @ support vector machine provide a high level of accuracy in encapsulating and measuring @ sentiment of customer toward @ product and service a compared to @ @ text mining strategy @ @ a competitive analysis technique based on k-means clustering ha @ implemented to examine @ @ frequent word @ is discussed by @ customer @ @ combination of @ @ method provides benefit not only to obtain @ best classification @ @ to help @ user focus on @ @ relevant category @ meet @ @ interest @ @ nature singapore pte ltd @ @ 
2017,Extracting domain-specific stopwords for text classifiers,"In this paper, an automatic generation of domain-specific stopwords from a large labeled corpus is proposed. In the majority of text mining tasks, stopwords are removed according to a standard stopword list and/or using high and low document frequencies. In this paper, a new approach for stopword extraction, based on the notion of backward filter-level performance and data sparsity index, is proposed. First, based on the proposed model to evaluate the extracted stopwords, we examine high document frequency filtering for stopword reduction. Secondly, a new algorithm for building general and domain-specific stopword lists is proposed. For the method, it is assumed that a set of candidate stopwords must have a minimum information content and prediction capacity that is measured by the performance of a classifier. We show that to avoid obtaining the classifier performance, it can be estimated by the sparsity of the training dataset. Moreover, it is confirmed that even if a given term ranking measure can perform well for the feature selection, the measure is not necessarily efficient for selecting poor features (stopwords). According to the comparative study, the newly devised approach offers more promising results that guarantee a minimum information loss by filtering out most stopwords. © 2017 - IOS Press and the authors. All rights reserved.",2017,Intelligent Data Analysis,8,in @ @ @ automatic generation of domain-specific stopwords @ a @ labeled corpus is proposed @ in @ majority of text mining task stopwords @ removed according to a standard stopword list and @ @ high and low document frequency @ in @ @ a @ approach @ stopword extraction based on @ notion of backward filter-level performance and data sparsity index is proposed @ first based on @ proposed model to evaluate @ extracted stopwords @ examine high document frequency filtering @ stopword reduction @ secondly a @ algorithm @ building general and domain-specific stopword list is proposed @ @ @ method @ is assumed @ a set of candidate stopwords must @ a minimum information content and prediction capacity @ is measured by @ performance of a classifier @ @ @ @ to avoid obtaining @ classifier performance @ @ @ estimated by @ sparsity of @ training dataset @ moreover @ is confirmed @ even if a given term ranking measure @ perform well @ @ feature selection @ measure is not necessarily efficient @ selecting poor feature @ stopwords @ @ according to @ comparative study @ newly devised approach offer more promising @ @ guarantee a minimum information loss by filtering @ @ stopwords @ io @ and @ author @ @ right reserved @ 
2018,Predicting the evolution of service value features from user reviews for continuous service improvement,"Facing with a highly competitive service market where customers have more choices on services to fulfill their demands, service providers have to improve their services continuously to make them adapt to constantly-changing value expectations of customers. An enormous quantity of reviews published by customers who have experienced services is an essential basis for service providers to understand which fine-grained features are cared more by customers and what others are less. In this paper, we present a method (VFAMine) for extracting Service Value Features (VF) from review texts by text mining and measuring customers’ attention degrees on VFs by sentiment analysis. As a result, a Time-series Service Value Feature Distribution model (TSVFD) is constructed to delineate the evolution history of attention degrees on various VFs. To help providers identify VFs which are to be extensively concerned by customers and improve them in advance, we give a convolutional sliding window and random forest based algorithm (CSRF) for predicting the future trend of the attention degree on one VF, either for a single service or for services belonging to the same region/domain. In terms of Maximum Information Coefficient (MIC) based correlation analysis, we find that there are latent correlations between the evolution history of different VFs, and such correlation would help service providers improve multiple correlated VFs together. Experiments are conducted on a Yelp dataset and the results demonstrate the effectiveness of our approach. © Springer International Publishing AG 2017.",2017,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,facing @ a highly competitive service market @ customer @ more choice on service to fulfill @ demand service provider @ to improve @ service continuously to make @ adapt to constantly-changing value expectation of customer @ @ enormous quantity of review published by customer @ @ experienced service is @ essential basis @ service provider to understand @ fine-grained feature @ cared more by customer and @ others @ le @ in @ @ @ @ a method @ vfamine @ @ extracting service value feature @ vf @ @ review text by text mining and measuring customer attention degree on vfs by sentiment analysis @ a a @ a time-series service value feature distribution model @ tsvfd @ is constructed to delineate @ evolution history of attention degree on various vfs @ to help provider identify vfs @ @ to @ extensively concerned by customer and improve @ in advance @ give a convolutional sliding window and random forest based algorithm @ csrf @ @ predicting @ future trend of @ attention degree on @ vf either @ a single service @ @ service belonging to @ @ region domain @ in term of maximum information coefficient @ mic @ based correlation analysis @ find @ @ @ latent correlation @ @ evolution history of different vfs and @ correlation would help service provider improve multiple correlated vfs together @ experiment @ conducted on a yelp dataset and @ @ demonstrate @ effectiveness of @ approach @ @ international publishing ag @ 
2019,Computer assisted assignment of ICD codes for primary admission diagnostic in ICUs,"The intensive care units (ICUs) provide a constant monitoring and specialized support to patients with acute critical conditions, assuring timely interventions to rapid changes. A major determinant of the patient care in ICUs is the primary admission diagnosis. A typical diagnosis includes a nosological entity or syndrome name, with the possibility to describe the clinical condition and the patient health state. This diagnosis is the starting point to establish intervention plans and to devise epidemiological studies. In the ICU physicians are in charge to define this diagnosis. Diagnoses in ICUs are commonly described in natural language. However, a common practice is to assign a normalized code from the international classification of diseases and related health problems (ICD). Unfortunately, this codification task is time expensive and requires highly specialized medical knowledge. In this work, we introduce a text mining system to automatically recover ICD codes for diagnosis of admission in ICUs. The system is based on a novel hierarchical recovery approach which is well suited to representation used in the ICD code. The proposed approach was evaluated by using a set of 1206 codified descriptions written in Spanish language corresponding to diagnoses in an real ICU. The results suggest that this approach may account for a considerable percentage of the diagnoses codified by the expert in the ICU. In particular, the F1 measure was 0.21 ± 0.06 with a mean precision average of 0.3. © Springer International Publishing AG 2017.",2017,Communications in Computer and Information Science,1,@ intensive care unit @ icu @ provide a constant monitoring and specialized support to patient @ acute critical condition assuring timely intervention to rapid change @ a major determinant of @ patient care in icu is @ primary admission diagnosis @ a typical diagnosis includes a nosological entity @ syndrome name @ @ possibility to describe @ clinical condition and @ patient health state @ @ diagnosis is @ starting point to establish intervention plan and to devise epidemiological study @ in @ icu physician @ in charge to define @ diagnosis @ diagnosis in icu @ commonly described in natural language @ however a common practice is to assign a normalized code @ @ international classification of disease and related health problem @ icd @ @ unfortunately @ codification task is time expensive and requires highly specialized medical knowledge @ in @ work @ introduce a text mining system to automatically recover icd code @ diagnosis of admission in icu @ @ system is based on a novel hierarchical recovery approach @ is well suited to representation used in @ icd code @ @ proposed approach wa evaluated by @ a set of codified description written in spanish language corresponding to diagnosis in @ real icu @ @ @ suggest @ @ approach may account @ a considerable percentage of @ diagnosis codified by @ expert in @ icu @ in particular @ f measure wa @ @ @ a mean precision average of @ @ @ international publishing ag @ 
2020,BELMiner: Adapting a rule-based relation extraction system to extract biological expression language statements from bio-medical literature evidence sentences,"Extracting meaningful relationships with semantic significance from biomedical literature is often a challenging task. BioCreative V track4 challenge for the first time has organized a comprehensive shared task to test the robustness of the text-mining algorithms in extracting semantically meaningful assertions from the evidence statement in biomedical text. In this work, we tested the ability of a rule-based semantic parser to extract Biological Expression Language (BEL) statements from evidence sentences culled out of biomedical literature as part of BioCreative V Track4 challenge. The system achieved an overall best Fmeasure of 21.29% in extracting the complete BEL statement. For relation extraction, the system achieved an F-measure of 65.13% on test data set. Our system achieved the best performance in five of the six criteria that was adopted for evaluation by the task organizers. Lack of ability to derive semantic inferences, limitation in the rule sets to map the textual extractions to BEL function were some of the reasons for low performance in extracting the complete BEL statement. Post shared task we also evaluated the impact of differential NER components on the ability to extract BEL statements on the test data sets besides making a single change in the rule sets that translate relation extractions into a BEL statement. There is a marked improvement by over 20% in the overall performance of the BELMiner's capability to extract BEL statement on the test set. The system is available as a REST-API at http://54.146.11.205:8484/BELXtractor/finder/. © The Author(s) 2017. Published by Oxford University Press.",2017,Database,10,extracting meaningful relationship @ semantic significance @ biomedical literature is often a challenging task @ biocreative v track challenge @ @ first time ha organized a comprehensive shared task to test @ robustness of @ text-mining algorithm in extracting semantically meaningful assertion @ @ evidence statement in biomedical text @ in @ work @ tested @ ability of a rule-based semantic parser to extract biological expression language @ bel @ statement @ evidence sentence culled @ of biomedical literature a part of biocreative v track challenge @ @ system achieved @ overall best fmeasure of @ in extracting @ complete bel statement @ @ relation extraction @ system achieved @ f-measure of @ on test data set @ @ system achieved @ best performance in five of @ six criterion @ wa adopted @ evaluation by @ task organizer @ lack of ability to derive semantic inference limitation in @ rule set to map @ textual extraction to bel function @ some of @ reason @ low performance in extracting @ complete bel statement @ post shared task @ @ evaluated @ impact of differential ner component on @ ability to extract bel statement on @ test data set besides making a single change in @ rule set @ translate relation extraction @ a bel statement @ @ is a marked improvement by @ in @ overall performance of @ belminer @ s capability to extract bel statement on @ test set @ @ system is available a a rest-api at http @ @ @ @ @ belxtractor finder @ @ author @ s @ @ published by oxford university @ @ 
2021,A sentiment analysis tool for determining the promotional success of fashion images on instagram,"Sentiment Analysis (SA) or Opinion Mining is the process of analysing natural language texts to detect an emotion or a pattern of emotions towards a certain product to make a decision about that product. SA is a topic of text mining, Natural Language Processing (NLP) and web mining disciplines. Research in SA is currently at its peak given the amount of data generated from social media networks. The concept is that consumers are expressing exactly what they need, want and expect from a product but on the other hand the companies don't have the tools to analyse and understand these feelings to satisfy these consumers accordingly. One of the applications that generate a high rate of reactions and sentiments in social networks is Instagram. This study focuses on analysing the reactions generated by the top 50 fashion houses on Instagram given their top 20 images with the highest number of likes. The approach taken in this study is to qualify the visual aesthetics of fashion images and to establish why some succeed on social media more than others. The basic question asked in this paper is whether there are certain visual aesthetics that appeal more to the user and are therefore more successful on social media than others as determined by a measure we introduce, 'Social Value'. To do so, a sentiment analysis tool is developed to measure the proposed social value of each image. An input of comments from each image will be processed. Each comment will go through a pre-processing phase; each word will be placed through a lexicon to identify if it is positive or negative. The output of the lexicon is a score value assigned to each comment to identify its degree of positivity, negativity, or it has no effect on the social value. Adding to these results, the number of likes and shares would also be taken into consideration quantifying the image's value. A cumulative result is then produced to determine the social value of an image.",2017,International Journal of Interactive Mobile Technologies,8,sentiment analysis @ sa @ @ opinion mining is @ process of analysing natural language text to detect @ emotion @ a pattern of emotion towards a certain product to make a decision @ @ product @ sa is a topic of text mining natural language processing @ nlp @ and web mining discipline @ research in sa is currently at @ peak given @ amount of data generated @ social medium network @ @ concept is @ consumer @ expressing exactly @ @ need want and expect @ a product @ on @ @ hand @ company @ @ t @ @ tool to analyse and understand @ feeling to satisfy @ consumer accordingly @ @ of @ application @ generate a high rate of reaction and sentiment in social network is instagram @ @ study focus on analysing @ reaction generated by @ top fashion house on instagram given @ top image @ @ highest number of like @ @ approach taken in @ study is to qualify @ visual aesthetic of fashion image and to establish @ some succeed on social medium more @ others @ @ basic question asked in @ @ is whether @ @ certain visual aesthetic @ appeal more to @ user and @ therefore more successful on social medium @ others a determined by a measure @ introduce @ social value @ @ to @ @ a sentiment analysis tool is developed to measure @ proposed social value of @ image @ @ input of comment @ @ image @ @ processed @ @ comment @ go @ a pre-processing phase @ @ word @ @ placed @ a lexicon to identify if @ is positive @ negative @ @ output of @ lexicon is a score value assigned to @ comment to identify @ degree of positivity negativity @ @ ha no effect on @ social value @ adding to @ @ @ number of like and share would @ @ taken @ consideration quantifying @ image @ s value @ a cumulative @ is @ produced to determine @ social value of @ image @ 
2022,Modern statistical and linguistic approaches to processing texts in natural languages,"Natural language processing (NLP) is a research area that focuses on studying the methods of computer analysis and synthesis of natural languages. The sources of information can include not only texts, but also audio and video data. In this article, we will focus on text mining. The analysis is divided into the following subtasks: information extraction, tonality analysis, question-answer systems, etc. In turn, information extraction also includes subtasks: named entity recognition (NER), relation extraction, extraction of keywords and word combinations (collocations). The methods of NLP are divided into linguistic (based on rules and grammars) and probabilistic; there are also hybrid methods that combine both approaches. The aim of this paper is to provide an overview of modern approaches to text processing using the example of the tasks of named entities recognition and identifying the relationships between them. © 2005 - 2016 JATIT & LLS. All rights reserved.",2016,Journal of Theoretical and Applied Information Technology,0,natural language processing @ nlp @ is a research area @ focus on studying @ method of computer analysis and synthesis of natural language @ @ source of information @ include not only text @ @ audio and video data @ in @ article @ @ focus on text mining @ @ analysis is divided @ @ following subtasks @ information extraction tonality analysis question-answer system etc @ in turn information extraction @ includes subtasks @ named entity recognition @ ner @ relation extraction extraction of keywords and word combination @ collocation @ @ @ method of nlp @ divided @ linguistic @ based on rule and grammar @ and probabilistic @ @ @ @ hybrid method @ combine @ approach @ @ aim of @ @ is to provide @ overview of modern approach to text processing @ @ example of @ task of named entity recognition and identifying @ relationship @ @ @ jatit lls @ @ right reserved @ 
2025,Text mining: Current trends and applications,"This chapter reveals the overview of text mining; text mining, patent analysis, and keyword selection;text mining and sentiment analysis in modern marketing; text mining applications in the biomedical sciences;and the multifaceted applications of text mining. Text mining is an advanced technology utilizedin business, marketing, biomedical sciences, education, and operations. Text mining offers a solution tomany problems, drawing on techniques concerning information retrieval, natural language processing,information extraction, and knowledge management. Through text mining, information can be extractedto derive summaries for the words contained in the documents. Text mining has the potential to increasethe research base available to business and society and to enable business to utilize the research basemore effectively. Economic and societal benefits of text mining include cost savings, productivity gains,innovative new service development, new business models, and new medical treatments. © 2017 by IGI Global. All rights reserved.",2016,Web Data Mining and the Development of Knowledge-Based Decision Support Systems,6,@ chapter reveals @ overview of text mining @ text mining patent analysis and keyword selection @ text mining and sentiment analysis in modern marketing @ text mining application in @ biomedical science @ and @ multifaceted application of text mining @ text mining is @ advanced technology utilizedin @ marketing biomedical science education and operation @ text mining offer a solution tomany problem drawing on technique concerning information retrieval natural language processing information extraction and knowledge management @ @ text mining information @ @ extractedto derive summary @ @ word contained in @ document @ text mining ha @ potential to increasethe research base available to @ and society and to enable @ to utilize @ research basemore effectively @ economic and societal benefit of text mining include cost saving productivity gain innovative @ service development @ @ model and @ medical treatment @ by igi global @ @ right reserved @ 
2031,Natural language processing as feature extraction method for building better predictive models,This chapter covers natural language processing techniques and their application in predicitve models development. Two case studies are presented. First case describes a project where textual descriptions of various situations in call center of one telecommunication company were processed in order to predict churn. Second case describes sentiment analysis of business news and describes practical and testing issues in text mining projects. Both case studies depict different approaches and are implemented in different tools. Language of the texts processed in these projects is Croatian which belongs to the Slavic group of languages with more complex morphologies and grammar rules than English. Chapter concludes with several points on the future research possible in this domain. © 2017 by IGI Global. All rights reserved.,2016,"Artificial Intelligence: Concepts, Methodologies, Tools, and Applications",0,@ chapter cover natural language processing technique and @ application in predicitve model development @ @ case study @ presented @ first case describes a project @ textual description of various situation in call center of @ telecommunication company @ processed in order to predict churn @ second case describes sentiment analysis of @ news and describes practical and testing issue in text mining project @ @ case study depict different approach and @ implemented in different tool @ language of @ text processed in @ project is croatian @ belongs to @ slavic group of language @ more complex morphology and grammar rule @ english @ chapter concludes @ several point on @ future research possible in @ domain @ by igi global @ @ right reserved @ 
2032,"Sentiment classification: Facebook' statuses mining in the ""Arabic spring"" era","In this work, we focus on the application of text mining and sentiment analysis techniques for analyzing Tunisian users' statuses updates on Facebook. We aim to extract useful information, about their sentiment and behavior, especially during the ""Arabic spring"" era. To achieve this task, we describe a method for sentiment analysis using Support Vector Machine and Naïve Bayes algorithms, and applying a combination of more than two features. The output of this work consists, on one hand, on the construction of a sentiment lexicon based on the Emoticons and Acronyms' lexicons that we developed based on the extracted statuses updates; and on the other hand, it consists on the realization of detailed comparative experiments between the above algorithms by creating a training model for sentiment classification. © 2017 by IGI Global. All rights reserved.",2016,"Artificial Intelligence: Concepts, Methodologies, Tools, and Applications",0,in @ work @ focus on @ application of text mining and sentiment analysis technique @ analyzing tunisian user @ status update on facebook @ @ aim to extract useful information @ @ sentiment and behavior especially @ @ @ arabic spring @ era @ to achieve @ task @ describe a method @ sentiment analysis @ support vector machine and naïve bayes algorithm and applying a combination of more @ @ feature @ @ output of @ work consists on @ hand on @ construction of a sentiment lexicon based on @ emoticon and acronym @ lexicon @ @ developed based on @ extracted status update @ and on @ @ hand @ consists on @ realization of detailed comparative experiment @ @ @ algorithm by creating a training model @ sentiment classification @ by igi global @ @ right reserved @ 
2040,DVM-based topic detection for microblog,"With the rise of microblog, topic detection in microblog posts has been a hotspot in natural language processing and text mining. Different from regular text, microblog post is a kind of short and idiomatic text. Microblog post contains little information, which brings great challenge for its topic detection. To address the issue of topic detection in microblog, a new single pass algorithm based on a double-vector model (DVM; Single Pass-DM) is proposed. First, a support vector machine (SVM) based algorithm is employed to filter irrelevant posts, thereby improving the accuracy of the algorithm. As for the representation model, on the basis of the traditional vector space model, a DVM that includes event and keyword vector is put forward. Subsequently, a combination of Jacoby,cosine and semantic similarity is used for similarity computation. Finally, some structural characteristics of microblog posts are used to support the topic detection problem. To validate the performance of the proposed algorithm, experiments are conducted on a real-world dataset. Experimental results show that, comparing with three benchmark algorithms SinglePass, Agglomerative Hierarchical Clustering (AHC) and Densitybased Spatial Clustering (DBSCAN), the performance of SinglePass-DM has been improved greatly.",2016,Journal of Digital Information Management,1,@ @ rise of microblog topic detection in microblog post ha @ a hotspot in natural language processing and text mining @ different @ regular text microblog post is a kind of short and idiomatic text @ microblog post contains little information @ brings great challenge @ @ topic detection @ to address @ issue of topic detection in microblog a @ single pas algorithm based on a double-vector model @ dvm @ single pass-dm @ is proposed @ first a support vector machine @ svm @ based algorithm is employed to filter irrelevant post thereby improving @ accuracy of @ algorithm @ a @ @ representation model on @ basis of @ traditional vector space model a dvm @ includes event and keyword vector is put forward @ subsequently a combination of jacoby cosine and semantic similarity is used @ similarity computation @ finally some structural characteristic of microblog post @ used to support @ topic detection problem @ to validate @ performance of @ proposed algorithm experiment @ conducted on a real-world dataset @ experimental @ @ @ comparing @ three benchmark algorithm singlepass agglomerative hierarchical clustering @ ahc @ and densitybased spatial clustering @ dbscan @ @ performance of singlepass-dm ha @ improved greatly @ 
2045,Discovering concept-level event associations from a text stream,We study an open text mining problem – discovering concept-level event associations from a text stream. We investigate the importance and challenge of this task and propose a novel solution by using event sequential patterns. The proposed approach can discover important event associations implicitly expressed. The discovered event associations are general and useful as knowledge for applications such as event prediction. © Springer International Publishing AG 2016.,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ study @ open text mining problem discovering concept-level event association @ a text stream @ @ investigate @ importance and challenge of @ task and propose a novel solution by @ event sequential pattern @ @ proposed approach @ discover important event association implicitly expressed @ @ discovered event association @ general and useful a knowledge @ application @ a event prediction @ @ international publishing ag @ 
2046,Conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability,"We propose a general framework for topic-specific summarization of large text corpora, and illustrate how it can be used for analysis in two quite different contexts: an Occupational Safety and Health Administration (OSHA) database of fatality and catastrophe reports (to facilitate surveillance for patterns in circumstances leading to injury or death), and legal decisions on workers' compensation claims (to explore relevant case law). Our summarization framework, built on sparse classification methods, is a compromise between simple word frequency-based methods currently in wide use, and more heavyweight, model-intensive methods such as latent Dirichlet allocation (LDA). For a particular topic of interest (e.g., mental health disability, or carbon monoxide exposure), we regress a labeling of documents onto the high-dimensional counts of all the other words and phrases in the documents. The resulting small set of phrases found as predictive are then harvested as the summary. Using a branch-and-bound approach, this method can incorporate phrases of arbitrary length, which allows for potentially rich summarization. We discuss how focus on the purpose of the summaries can inform choices of tuning parameters and model constraints. We evaluate this tool by comparing the computational time and summary statistics of the resulting word lists to three other methods in the literature. We also present a new R package, textreg. Overall, we argue that sparse methods have much to offer in text analysis and is a branch of research that should be considered further in this context. © 2016 Wiley Periodicals, Inc. Statistical Analysis and Data Mining: The ASA Data Science Journal, 2016. © 2016 Wiley Periodicals, Inc.",2016,Statistical Analysis and Data Mining,0,@ propose a general framework @ topic-specific summarization of @ text corpus and illustrate @ @ @ @ used @ analysis in @ quite different context @ @ occupational safety and health administration @ osha @ database of fatality and catastrophe report @ to facilitate surveillance @ pattern in circumstance leading to injury @ death @ and legal decision on worker @ compensation claim @ to explore relevant case law @ @ @ summarization framework built on sparse classification method is a compromise @ simple word frequency-based method currently in wide use and more heavyweight model-intensive method @ a latent dirichlet allocation @ lda @ @ @ a particular topic of interest @ e @ g @ mental health disability @ carbon monoxide exposure @ @ regress a labeling of document onto @ high-dimensional count of @ @ @ word and phrase in @ document @ @ resulting small set of phrase found a predictive @ @ harvested a @ summary @ @ a branch-and-bound approach @ method @ incorporate phrase of arbitrary length @ allows @ potentially rich summarization @ @ discus @ focus on @ purpose of @ summary @ inform choice of tuning parameter and model constraint @ @ evaluate @ tool by comparing @ computational time and summary statistic of @ resulting word list to three @ method in @ literature @ @ @ @ a @ r package textreg @ overall @ argue @ sparse method @ much to offer in text analysis and is a branch of research @ @ @ considered @ in @ context @ wiley periodical inc @ statistical analysis and data mining @ @ asa data science journal @ wiley periodical inc @ 
2047,CoTO: A novel approach for fuzzy aggregation of semantic similarity measures,"Semantic similarity measurement aims to determine the likeness between two text expressions that use different lexicographies for representing the same real object or idea. There are a lot of semantic similarity measures for addressing this problem. However, the best results have been achieved when aggregating a number of simple similarity measures. This means that after the various similarity values have been calculated, the overall similarity for a pair of text expressions is computed using an aggregation function of these individual semantic similarity values. This aggregation is often computed by means of statistical functions. In this work, we present CoTO (Consensus or Trade-Off) a solution based on fuzzy logic that is able to outperform these traditional approaches. © 2016 Elsevier B.V.",2016,Cognitive Systems Research,17,semantic similarity measurement aim to determine @ likeness @ @ text expression @ use different lexicography @ representing @ @ real object @ idea @ @ @ a lot of semantic similarity measure @ addressing @ problem @ however @ best @ @ @ achieved @ aggregating a number of simple similarity measure @ @ mean @ @ @ various similarity value @ @ calculated @ overall similarity @ a pair of text expression is computed @ @ aggregation function of @ individual semantic similarity value @ @ aggregation is often computed by mean of statistical function @ in @ work @ @ coto @ consensus @ trade-off @ a solution based on fuzzy logic @ is able to outperform @ traditional approach @ @ b @ v @ 
2069,eSAP: A decision support framework for enhanced sentiment analysis and polarity classification,"Sentiment analysis or opinion mining is an imperative research area of natural language processing. It is used to determine the writer's attitude or speaker's opinion towards a particular person, product or topic. Polarity or subjectivity classification is the process of categorizing a piece of text into positive or negative classes. In recent years, various supervised and unsupervised methods have been presented to accomplish sentiment polarity detection. SentiWordNet (SWN) has been extensively used as a lexical resource for opinion mining. This research incorporates SWN as the labeled training corpus where the sentiment scores are extracted based on the part of speech information. A vocabulary SWN-V with revised sentiment scores, generated from SWN, is then used for Support Vector Machines model learning and classification process. Based on this vocabulary, a framework named “Enhanced Sentiment Analysis and Polarity Classification (eSAP)” is proposed. Training, testing and evaluation of the proposed eSAP are conducted on seven benchmark datasets from various domains. 10-fold cross validated accuracy, precision, recall, and f-measure results averaged over seven datasets for the proposed framework are 80.82%, 80.83%, 80.94% and 80.81% respectively. A notable performance improvement of 13.4% in accuracy, 14.2% in precision, 6.9% in recall and 11.1% in f-measure is observed on average by evaluating the proposed eSAP against the baseline SWN classifier. State of the art performance comparison is conducted which also verifies the superiority of the proposed eSAP framework. © 2016 Elsevier Inc.",2016,Information Sciences,37,sentiment analysis @ opinion mining is @ imperative research area of natural language processing @ @ is used to determine @ writer @ s attitude @ speaker @ s opinion towards a particular person product @ topic @ polarity @ subjectivity classification is @ process of categorizing a piece of text @ positive @ negative class @ in recent year various supervised and unsupervised method @ @ presented to accomplish sentiment polarity detection @ sentiwordnet @ swn @ ha @ extensively used a a lexical resource @ opinion mining @ @ research incorporates swn a @ labeled training corpus @ @ sentiment score @ extracted based on @ part of speech information @ a vocabulary swn-v @ revised sentiment score generated @ swn is @ used @ support vector machine model learning and classification process @ based on @ vocabulary a framework named enhanced sentiment analysis and polarity classification @ esap @ is proposed @ training testing and evaluation of @ proposed esap @ conducted on seven benchmark datasets @ various domain @ fold cross validated accuracy precision recall and f-measure @ averaged @ seven datasets @ @ proposed framework @ @ @ @ and @ respectively @ a notable performance improvement of @ in accuracy @ in precision @ in recall and @ in f-measure is observed on average by evaluating @ proposed esap @ @ baseline swn classifier @ state of @ art performance comparison is conducted @ @ verifies @ superiority of @ proposed esap framework @ @ inc @ 
2070,Supervised classification of spam emails with natural language stylometry,"Email spam is one of the biggest threats to today’s Internet. To deal with this threat, there are long-established measures like supervised anti-spam filters. In this paper, we report the development and evaluation of sentinel—an anti-spam filter based on natural language and stylometry attributes. The performance of the filter is evaluated not only on non-personalized emails (i.e., emails collected randomly) but also on personalized emails (i.e., emails collected from particular individuals). Among the non-personalized datasets are CSDMC2010, SpamAssassin, and LingSpam, while the Enron-Spam collection comprises personalized emails. The proposed filter extracts natural language attributes from email text that are closely related to writer stylometry and generate classifiers using multiple learning algorithms. Experimental outcomes show that classifiers generated by meta-learning algorithms such as adaboostm1 and bagging are the best, performing equally well and surpassing the performance of a number of filters proposed in previous studies, while a random forest generated classifier is a close second. On the other hand, the performance of classifiers using support vector machine and Naïve Bayes is not satisfactory. In addition, we find much improved results on personalized emails and mixed results on non-personalized emails. © 2015, The Natural Computing Applications Forum.",2016,Neural Computing and Applications,15,email spam is @ of @ biggest threat to today s internet @ to deal @ @ threat @ @ long-established measure like supervised anti-spam filter @ in @ @ @ report @ development and evaluation of sentinel @ anti-spam filter based on natural language and stylometry attribute @ @ performance of @ filter is evaluated not only on non-personalized email @ i @ e @ email collected randomly @ @ @ on personalized email @ i @ e @ email collected @ particular individual @ @ among @ non-personalized datasets @ csdmc spamassassin and lingspam @ @ enron-spam collection comprises personalized email @ @ proposed filter extract natural language attribute @ email text @ @ closely related to writer stylometry and generate classifier @ multiple learning algorithm @ experimental outcome @ @ classifier generated by meta-learning algorithm @ a adaboostm and bagging @ @ best performing equally well and surpassing @ performance of a number of filter proposed in previous study @ a random forest generated classifier is a close second @ on @ @ hand @ performance of classifier @ support vector machine and naïve bayes is not satisfactory @ in addition @ find much improved @ on personalized email and mixed @ on non-personalized email @ @ natural computing application forum @ 
2071,A hybrid similarity measure method for patent portfolio analysis,"Similarity measures are fundamental tools for identifying relationships within or across patent portfolios. Many bibliometric indicators are used to determine similarity measures; for example, bibliographic coupling, citation and co-citation, and co-word distribution. This paper aims to construct a hybrid similarity measure method based on multiple indicators to analyze patent portfolios. Two models are proposed: categorical similarity and semantic similarity. The categorical similarity model emphasizes international patent classifications (IPCs), while the semantic similarity model emphasizes textual elements. We introduce fuzzy set routines to translate the rough technical (sub-) categories of IPCs into defined numeric values, and we calculate the categorical similarities between patent portfolios using membership grade vectors. In parallel, we identify and highlight core terms in a 3-level tree structure and compute the semantic similarities by comparing the tree-based structures. A weighting model is designed to consider: 1) the bias that exists between the categorical and semantic similarities, and 2) the weighting or integrating strategy for a hybrid method. A case study to measure the technological similarities between selected firms in China's medical device industry is used to demonstrate the reliability our method, and the results indicate the practical meaning of our method in a broad range of informetric applications. © 2016 Elsevier Ltd",2016,Journal of Informetrics,30,similarity measure @ fundamental tool @ identifying relationship within @ across patent portfolio @ many bibliometric indicator @ used to determine similarity measure @ @ example bibliographic coupling citation and co-citation and co-word distribution @ @ @ aim to construct a hybrid similarity measure method based on multiple indicator to analyze patent portfolio @ @ model @ proposed @ categorical similarity and semantic similarity @ @ categorical similarity model emphasizes international patent classification @ ipcs @ @ @ semantic similarity model emphasizes textual element @ @ introduce fuzzy set routine to translate @ rough technical @ sub @ category of ipcs @ defined numeric value and @ calculate @ categorical similarity @ patent portfolio @ membership grade vector @ in parallel @ identify and highlight core term in a level tree structure and compute @ semantic similarity by comparing @ tree-based structure @ a weighting model is designed to consider @ @ @ bias @ exists @ @ categorical and semantic similarity and @ @ weighting @ integrating strategy @ a hybrid method @ a case study to measure @ technological similarity @ selected firm in china @ s medical device industry is used to demonstrate @ reliability @ method and @ @ indicate @ practical meaning of @ method in a broad range of informetric application @ @ ltd
2086,Identifying and Profiling Key Sellers in Cyber Carding Community: AZSecure Text Mining System,"The past few years have witnessed millions of credit/debit cards flowing through the underground economy and ultimately causing significant financial loss. Examining key underground economy sellers has both practical and academic significance for cybercrime forensics and criminology research. Drawing on social media analytics, we have developed the AZSecure text mining system for identifying and profiling key sellers. The system identifies sellers using sentiment analysis of customer reviews and profiles sellers using topic modeling of advertisements. We evaluated the AZSecure system on eight international underground economy forums. The system significantly outperformed all benchmark machine-learning methods on identifying advertisement threads, classifying customer review sentiments, and profiling seller characteristics, with an average F-measure of about 80 percent to 90 percent. In our case study, we identified the famous carder, Rescator, who was affiliated with the Target breach, and captured important seller characteristics in terms of product type, payment options, and contact channels. Our research leverages social media analytics to probe into the underground economy in order to help law enforcement target key sellers and prevent future fraud. It also contributes to our understanding of the use of information technology in detecting deception in online systems. Copyright © Taylor & Francis Group, LLC.",2016,Journal of Management Information Systems,26,@ past @ year @ witnessed million of credit debit card flowing @ @ underground economy and ultimately causing significant financial loss @ examining key underground economy seller ha @ practical and @ significance @ cybercrime forensics and criminology research @ drawing on social medium analytics @ @ developed @ azsecure text mining system @ identifying and profiling key seller @ @ system identifies seller @ sentiment analysis of customer review and profile seller @ topic modeling of advertisement @ @ evaluated @ azsecure system on eight international underground economy forum @ @ system significantly outperformed @ benchmark machine-learning method on identifying advertisement thread classifying customer review sentiment and profiling seller characteristic @ @ average f-measure of @ percent to percent @ in @ case study @ identified @ famous carder rescator @ wa affiliated @ @ target breach and captured important seller characteristic in term of product type payment option and contact channel @ @ research leverage social medium analytics to probe @ @ underground economy in order to help law enforcement target key seller and prevent future fraud @ @ @ contributes to @ understanding of @ use of information technology in detecting deception in online system @ @ taylor francis group llc @ 
2087,Senti-CS: Building a lexical resource for sentiment analysis using subjective feature selection and normalized Chi-Square-based feature weight generation,"Sentiment analysis involves the detection of sentiment content of text using natural language processing. Natural language processing is a very challenging task due to syntactic ambiguities, named entity recognition, use of slangs, jargons, sarcasm, abbreviations and contextual sensitivity. Sentiment analysis can be performed using supervised as well as unsupervised approaches. As the amount of data grows, unsupervised approaches become vital as they cut down on the learning time and the requirements for availability of a labelled dataset. Sentiment lexicons provide an easy application of unsupervised algorithms for text classification. SentiWordNet is a lexical resource widely employed by many researchers for sentiment analysis and polarity classification. However, the reported performance levels need improvement. The proposed research is focused on raising the performance of SentiWordNet3.0 by using it as a labelled corpus to build another sentiment lexicon, named Senti-CS. The part of speech information, usage based ranks and sentiment scores are used to calculate Chi-Square-based feature weight for each unique subjective term/part-of-speech pair extracted from SentiWordNet3.0. This weight is then normalized in a range of −1 to +1 using min–max normalization. Senti-CS based sentiment analysis framework is presented and applied on a large dataset of 50000 movie reviews. These results are then compared with baseline SentiWordNet, Mutual Information and Information Gain techniques. State of the art comparison is performed for the Cornell movie review dataset. The analyses of results indicate that the proposed approach outperforms state-of-the-art classifiers. © 2016 Wiley Publishing Ltd",2016,Expert Systems,8,sentiment analysis involves @ detection of sentiment content of text @ natural language processing @ natural language processing is a @ challenging task due to syntactic ambiguity named entity recognition use of slang jargon sarcasm abbreviation and contextual sensitivity @ sentiment analysis @ @ performed @ supervised a well a unsupervised approach @ a @ amount of data grows unsupervised approach become vital a @ cut down on @ learning time and @ requirement @ availability of a labelled dataset @ sentiment lexicon provide @ easy application of unsupervised algorithm @ text classification @ sentiwordnet is a lexical resource widely employed by many researcher @ sentiment analysis and polarity classification @ however @ reported performance level need improvement @ @ proposed research is focused on raising @ performance of sentiwordnet @ by @ @ a a labelled corpus to build another sentiment lexicon named senti-cs @ @ part of speech information usage based rank and sentiment score @ used to calculate chi-square-based feature weight @ @ unique subjective term part-of-speech pair extracted @ sentiwordnet @ @ @ weight is @ normalized in a range of to @ min max normalization @ senti-cs based sentiment analysis framework is presented and applied on a @ dataset of movie review @ @ @ @ @ compared @ baseline sentiwordnet mutual information and information gain technique @ state of @ art comparison is performed @ @ cornell movie review dataset @ @ analysis of @ indicate @ @ proposed approach outperforms state-of-the-art classifier @ wiley publishing ltd
2088,Toy safety surveillance from online reviews,"Toy-related injuries account for a significant number of childhood injuries and the prevention of these injuries remains a goal for regulatory agencies and manufacturers. Text-mining is an increasingly prevalent method for uncovering the significance of words using big data. This research sets out to determine the effectiveness of text-mining in uncovering potentially dangerous children's toys. We develop a danger word list, also known as a “smoke word” list, from injury and recall text narratives. We then use the smoke word lists to score over one million Amazon reviews, with the top scores denoting potential safety concerns. We compare the smoke word list to conventional sentiment analysis techniques, in terms of both word overlap and effectiveness. We find that smoke word lists are highly distinct from conventional sentiment dictionaries and provide a statistically significant method for identifying safety concerns in children's toy reviews. Our findings indicate that text-mining is, in fact, an effective method for the surveillance of safety concerns in children's toys and could be a gateway to effective prevention of toy product-related injuries. © 2016 Elsevier B.V.",2016,Decision Support Systems,30,toy-related injury account @ a significant number of childhood injury and @ prevention of @ injury remains a goal @ regulatory agency and manufacturer @ text-mining is @ increasingly prevalent method @ uncovering @ significance of word @ big data @ @ research set @ to determine @ effectiveness of text-mining in uncovering potentially dangerous child @ s toy @ @ develop a danger word list @ known a a smoke word list @ injury and recall text narrative @ @ @ use @ smoke word list to score @ @ million amazon review @ @ top score denoting potential safety concern @ @ compare @ smoke word list to conventional sentiment analysis technique in term of @ word overlap and effectiveness @ @ find @ smoke word list @ highly distinct @ conventional sentiment dictionary and provide a statistically significant method @ identifying safety concern in child @ s toy review @ @ finding indicate @ text-mining is in fact @ effective method @ @ surveillance of safety concern in child @ s toy and could @ a gateway to effective prevention of toy product-related injury @ @ b @ v @ 
2089,Word-length algorithm for language identification of under-resourced languages,"Language identification is widely used in machine learning, text mining, information retrieval, and speech processing. Available techniques for solving the problem of language identification do require large amount of training text that are not available for under-resourced languages which form the bulk of the World's languages. The primary objective of this study is to propose a lexicon based algorithm which is able to perform language identification using minimal training data. Because language identification is often the first step in many natural language processing tasks, it is necessary to explore techniques that will perform language identification in the shortest possible time. Hence, the second objective of this research is to study the effect of the proposed algorithm on the run-time performance of language identification. Precision, recall, and F1 measures were used to determine the effectiveness of the proposed word length algorithm using datasets drawn from the Universal Declaration of Human Rights Act in 15 languages. The experimental results show good accuracy on language identification at the document level and at the sentence level based on the available dataset. The improved algorithm also showed significant improvement in run time performance compared with the spelling checker approach. © 2015 The Authors",2016,Journal of King Saud University - Computer and Information Sciences,9,language identification is widely used in machine learning text mining information retrieval and speech processing @ available technique @ solving @ problem of language identification @ require @ amount of training text @ @ not available @ under-resourced language @ form @ bulk of @ world @ s language @ @ primary objective of @ study is to propose a lexicon based algorithm @ is able to perform language identification @ minimal training data @ @ language identification is often @ first step in many natural language processing task @ is necessary to explore technique @ @ perform language identification in @ shortest possible time @ hence @ second objective of @ research is to study @ effect of @ proposed algorithm on @ run-time performance of language identification @ precision recall and f measure @ used to determine @ effectiveness of @ proposed word length algorithm @ datasets drawn @ @ universal declaration of human right act in language @ @ experimental @ @ good accuracy on language identification at @ document level and at @ sentence level based on @ available dataset @ @ improved algorithm @ showed significant improvement in run time performance compared @ @ spelling checker approach @ @ author
2090,News-based trading strategies,"The marvel of markets lies in the fact that dispersed information is instantaneously processed and used to adjust the price of goods, services and assets. Financial markets are particularly efficient when it comes to processing information; such information is typically embedded in textual news that is then interpreted by investors. Quite recently, researchers have started to automatically determine news sentiment in order to explain stock price movements. Interestingly, this so-called news sentiment works fairly well in explaining stock returns. In this paper, we design trading strategies that utilize textual news in order to obtain profits on the basis of novel information entering the market. We thus propose approaches for automated decision-making based on supervised and reinforcement learning. Altogether, we demonstrate how news-based data can be incorporated into an investment system. © 2016 Elsevier B.V.",2016,Decision Support Systems,30,@ marvel of market lie in @ fact @ dispersed information is instantaneously processed and used to adjust @ price of good service and asset @ financial market @ particularly efficient @ @ come to processing information @ @ information is typically embedded in textual news @ is @ interpreted by investor @ quite recently researcher @ started to automatically determine news sentiment in order to explain stock price movement @ interestingly @ so-called news sentiment work fairly well in explaining stock return @ in @ @ @ design trading strategy @ utilize textual news in order to obtain profit on @ basis of novel information entering @ market @ @ thus propose approach @ automated decision-making based on supervised and reinforcement learning @ altogether @ demonstrate @ news-based data @ @ incorporated @ @ investment system @ @ b @ v @ 
2091,Recognition of chemical entities using pattern matching and functional group classification,"The two main challenges in chemical entity recognition are: (i) New chemical compounds are constantly being synthesized infinitely. (ii) High ambiguity in chemical representation in which a chemical entity is being described by different nomenclatures. Therefore, the identification and maintenance of chemical terminologies is a tough task. Since most of the existing text mining methods followed the term-based approaches, the problems of polysemy and synonymy came into the picture. So, a Named Entity Recognition (NER) system based on pattern matching in chemical domain is developed to extract the chemical entities from chemical documents. The Tf-idf and PMI association measures are used to filter out the non-chemical terms. The F-score of 92.19% is achieved for chemical NER. This proposed method is compared with the baseline method and other existing approaches. As the final step, the filtered chemical entities are classified into sixteen functional groups. The classification is done using SVM One against All multiclass classification approach and achieved the accuracy of 87%. One-way ANOVA is used to test the quality of pattern matching method with the other existing chemical NER methods. Copyright © 2016, IGI Global.",2016,International Journal of Intelligent Information Technologies,1,@ @ main challenge in chemical entity recognition @ @ @ i @ @ chemical compound @ constantly @ synthesized infinitely @ @ ii @ high ambiguity in chemical representation in @ a chemical entity is @ described by different nomenclature @ therefore @ identification and maintenance of chemical terminology is a tough task @ since @ of @ existing text mining method followed @ term-based approach @ problem of polysemy and synonymy came @ @ picture @ @ a named entity recognition @ ner @ system based on pattern matching in chemical domain is developed to extract @ chemical entity @ chemical document @ @ tf-idf and pmi association measure @ used to filter @ @ non-chemical term @ @ f-score of @ is achieved @ chemical ner @ @ proposed method is compared @ @ baseline method and @ existing approach @ a @ final step @ filtered chemical entity @ classified @ sixteen functional group @ @ classification is done @ svm @ @ @ multiclass classification approach and achieved @ accuracy of @ one-way anova is used to test @ quality of pattern matching method @ @ @ existing chemical ner method @ @ igi global @ 
2092,Extracting drug-drug interactions from biomedical text using a feature-based kernel approach,"Discovering unknown drug interactions is of great importance for healthcare professionals since these interactions can become extremely dangerous and can affect patient’s safety. Since newly discovered drug interactions are reported in scientific papers, developing text mining techniques to automatically extract those interactions from unstructured texts is of great importance. All state-of-the-art systems evaluated on the standard DDIExtraction 2013 challenge corpus didn't exceed the threshold of 70%, which means that developing more powerful systems to manage this task still very important. In this paper we present a new feature-based kernel method to extract and classify drug interactions described in biomedical literature. Like many previous works, our method consists of two steps. First we detect interacting drug pairs, and then we classify each extracted pair into one of four interaction categories. To perform the first step, we have enhanced an existing feature-based system by adding new features, correction patterns, and trigger words. To perform the second step, we have built a new feature-based kernel classifier that exploit the lexical field particularity of each interaction type. This classifier is composed of 4 binary classifiers work sequentially. When evaluated on the DDIExtraction 2013 challenge corpus, our system achieved an F1-score of 71.14%, as compared to 69.75% and 68.4% reported by the top two state-of-the-art systems based respectively on Convolutional Neural Networks and graph kernel with context vectors methods. ï¿½ 2005 - 2016 JATIT & LLS. All rights reserved.",2016,Journal of Theoretical and Applied Information Technology,9,discovering unknown drug interaction is of great importance @ healthcare professional since @ interaction @ become extremely dangerous and @ affect patient s safety @ since newly discovered drug interaction @ reported in scientific @ developing text mining technique to automatically extract @ interaction @ unstructured text is of great importance @ @ state-of-the-art system evaluated on @ standard ddiextraction challenge corpus @ @ t exceed @ threshold of @ mean @ developing more powerful system to manage @ task still @ important @ in @ @ @ @ a @ feature-based kernel method to extract and classify drug interaction described in biomedical literature @ like many previous work @ method consists of @ step @ first @ detect @ drug pair and @ @ classify @ extracted pair @ @ of four interaction category @ to perform @ first step @ @ enhanced @ existing feature-based system by adding @ feature correction pattern and trigger word @ to perform @ second step @ @ built a @ feature-based kernel classifier @ exploit @ lexical field particularity of @ interaction type @ @ classifier is composed of binary classifier work sequentially @ @ evaluated on @ ddiextraction challenge corpus @ system achieved @ f score of @ a compared to @ and @ reported by @ top @ state-of-the-art system based respectively on convolutional neural network and graph kernel @ context vector method @ ï ½ jatit lls @ @ right reserved @ 
2096,A text feature-based approach for literature mining of lncRNA–protein interactions,"Long non-coding RNAs (lncRNAs) play important roles in regulating transcriptional and post-transcriptional levels. Currently, Knowledge of lncRNA and protein interactions (LPIs) is crucial for biomedical researches that are related to lncRNA. Many freshly discovered LPIs are stored in biomedical literature. With over one million new biomedical journal articles published every year, just keeping up with the novel finding requires automatically extracting information by text mining. To address this issue, we apply a text feature-based text mining approach to efficiently extract LPIs from biomedical literatures. Our approach consists of four steps. By employ natural language processing (NLP) technologies, this approach extracts text features from sentences that can precisely reflect the real LPIs. Our approach involves four steps including data collection, text pre-processing, structured representation, features extraction and training model and classification. The F-score performance of our approach achieves 79.5%, and the results indicate that the proposed approach can efficiently extract LPIs from biomedical literature. ï¿½ 2016 Elsevier B.V.",2016,Neurocomputing,10,long non-coding rna @ lncrnas @ play important role in regulating transcriptional and post-transcriptional level @ currently knowledge of lncrna and protein interaction @ lpis @ is crucial @ biomedical research @ @ related to lncrna @ many freshly discovered lpis @ stored in biomedical literature @ @ @ @ million @ biomedical journal article published every year @ keeping up @ @ novel finding requires automatically extracting information by text mining @ to address @ issue @ apply a text feature-based text mining approach to efficiently extract lpis @ biomedical literature @ @ approach consists of four step @ by employ natural language processing @ nlp @ technology @ approach extract text feature @ sentence @ @ precisely reflect @ real lpis @ @ approach involves four step including data collection text pre-processing structured representation feature extraction and training model and classification @ @ f-score performance of @ approach achieves @ and @ @ indicate @ @ proposed approach @ efficiently extract lpis @ biomedical literature @ ï ½ @ b @ v @ 
2099,Ensemble of keyword extraction methods and classifiers in text classification,"Automatic keyword extraction is an important research direction in text mining, natural language processing and information retrieval. Keyword extraction enables us to represent text documents in a condensed way. The compact representation of documents can be helpful in several applications, such as automatic indexing, automatic summarization, automatic classification, clustering and filtering. For instance, text classification is a domain with high dimensional feature space challenge. Hence, extracting the most important/relevant words about the content of the document and using these keywords as the features can be extremely useful. In this regard, this study examines the predictive performance of five statistical keyword extraction methods (most frequent measure based keyword extraction, term frequency-inverse sentence frequency based keyword extraction, co-occurrence statistical information based keyword extraction, eccentricity-based keyword extraction and TextRank algorithm) on classification algorithms and ensemble methods for scientific text document classification (categorization). In the study, a comprehensive study of comparing base learning algorithms (Naïve Bayes, support vector machines, logistic regression and Random Forest) with five widely utilized ensemble methods (AdaBoost, Bagging, Dagging, Random Subspace and Majority Voting) is conducted. To the best of our knowledge, this is the first empirical analysis, which evaluates the effectiveness of statistical keyword extraction methods in conjunction with ensemble learning algorithms. The classification schemes are compared in terms of classification accuracy, F-measure and area under curve values. To validate the empirical analysis, two-way ANOVA test is employed. The experimental analysis indicates that Bagging ensemble of Random Forest with the most-frequent based keyword extraction method yields promising results for text classification. For ACM document collection, the highest average predictive performance (93.80%) is obtained with the utilization of the most frequent based keyword extraction method with Bagging ensemble of Random Forest algorithm. In general, Bagging and Random Subspace ensembles of Random Forest yield promising results. The empirical analysis indicates that the utilization of keyword-based representation of text documents in conjunction with ensemble learning can enhance the predictive performance and scalability of text classification schemes, which is of practical importance in the application fields of text classification. © 2016 Elsevier Ltd. All rights reserved.",2016,Expert Systems with Applications,118,automatic keyword extraction is @ important research direction in text mining natural language processing and information retrieval @ keyword extraction enables u to represent text document in a condensed way @ @ compact representation of document @ @ helpful in several application @ a automatic indexing automatic summarization automatic classification clustering and filtering @ @ instance text classification is a domain @ high dimensional feature space challenge @ hence extracting @ @ important relevant word @ @ content of @ document and @ @ keywords a @ feature @ @ extremely useful @ in @ regard @ study examines @ predictive performance of five statistical keyword extraction method @ @ frequent measure based keyword extraction term frequency-inverse sentence frequency based keyword extraction co-occurrence statistical information based keyword extraction eccentricity-based keyword extraction and textrank algorithm @ on classification algorithm and ensemble method @ scientific text document classification @ categorization @ @ in @ study a comprehensive study of comparing base learning algorithm @ naïve bayes support vector machine logistic regression and random forest @ @ five widely utilized ensemble method @ adaboost bagging dagging random subspace and majority voting @ is conducted @ to @ best of @ knowledge @ is @ first empirical analysis @ evaluates @ effectiveness of statistical keyword extraction method in conjunction @ ensemble learning algorithm @ @ classification scheme @ compared in term of classification accuracy f-measure and area @ curve value @ to validate @ empirical analysis two-way anova test is employed @ @ experimental analysis indicates @ bagging ensemble of random forest @ @ most-frequent based keyword extraction method yield promising @ @ text classification @ @ acm document collection @ highest average predictive performance @ @ @ is obtained @ @ utilization of @ @ frequent based keyword extraction method @ bagging ensemble of random forest algorithm @ in general bagging and random subspace ensemble of random forest yield promising @ @ @ empirical analysis indicates @ @ utilization of keyword-based representation of text document in conjunction @ ensemble learning @ enhance @ predictive performance and scalability of text classification scheme @ is of practical importance in @ application field of text classification @ @ ltd @ @ right reserved @ 
2102,Question-driven topic-based extraction of Protein-Protein Interaction Methods from biomedical literature,"This paper proposes a novel topic-based model for identifying experimental mentions of Protein-Protein Interaction Method (PPIM) in the biomedical literature. The model combines topic-based classification models and some basic question-answering extraction techniques aiming at effectively detecting and identifying PPIM mentions on Protein-Protein Interactions. Unlike other state-of-the-art approaches, the approach captures underlying relationships within both input and output concept spaces by assuming the extraction task to be strongly driven by context provided by experts, usually in the form of a question to guide the search. Results indicate our topic-based question-driven approach obtained better results than other unsupervised learning probabilistic latent space models for detecting correct answers (PPIM mentions). © 2016 Elsevier Inc.",2016,Information Sciences,0,@ @ proposes a novel topic-based model @ identifying experimental mention of protein-protein interaction method @ ppim @ in @ biomedical literature @ @ model combine topic-based classification model and some basic question-answering extraction technique aiming at effectively detecting and identifying ppim mention on protein-protein interaction @ unlike @ state-of-the-art approach @ approach capture underlying relationship within @ input and output concept space by assuming @ extraction task to @ strongly driven by context provided by expert usually in @ form of a question to guide @ search @ @ indicate @ topic-based question-driven approach obtained better @ @ @ unsupervised learning probabilistic latent space model @ detecting correct answer @ ppim mention @ @ @ inc @ 
2104,Efficient Voting-Based Extractive Automatic Text Summarization Using Prominent Feature Set,"Automatic text summarization (ATS) is the process of generating a summary by condensing text document by a computer machine. In this paper, we explored voting-based extractive approaches for text summarization. The main issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize the quality of summary. Voting-based methods are sensitive to initial ranking process. We proposed reciprocal ranking-based sentence scoring approach that alleviates the feature weighting and initial ranking problem. The proposed approach uses a specific prominent set of features for initial ranking that further enhance the performance. Experimental results on Document Understating Conference 2002 data-set using ROUGE evaluation matrices shows that our proposed method performs better as compared to other voting-based methods. © 2016 IETE.",2016,IETE Journal of Research,2,automatic text summarization @ at @ is @ process of generating a summary by condensing text document by a computer machine @ in @ @ @ explored voting-based extractive approach @ text summarization @ @ main issue @ @ of @ feature-based at method is to find optimal feature weight @ sentence scoring to optimize @ quality of summary @ voting-based method @ sensitive to initial ranking process @ @ proposed reciprocal ranking-based sentence scoring approach @ alleviates @ feature weighting and initial ranking problem @ @ proposed approach us a specific prominent set of feature @ initial ranking @ @ enhance @ performance @ experimental @ on document understating conference data-set @ rouge evaluation matrix @ @ @ proposed method performs better a compared to @ voting-based method @ iete @ 
2106,Hierarchical classification in text mining for sentiment analysis of online news,"Sentiment analysis in text mining is a challenging task. Sentiment is subtly reflected by the tone and affective content of a writer’s words. Conventional text mining techniques, which are based on keyword frequencies, usually run short of accurately detecting such subjective information implied in the text. In this paper, we evaluate several popular classification algorithms, along with three filtering schemes. The filtering schemes progressively shrink the original dataset with respect to the contextual polarity and frequent terms of a document. We call this approach “hierarchical classification”. The effects of the approach in different combination of classification algorithms and filtering schemes are discussed over three sets of controversial online news articles where binary and multi-class classifications are applied. Meanwhile we use two methods to test this hierarchical classification model, and also have a comparison of the two methods. © 2015, Springer-Verlag Berlin Heidelberg.",2016,Soft Computing,25,sentiment analysis in text mining is a challenging task @ sentiment is subtly reflected by @ tone and affective content of a writer s word @ conventional text mining technique @ @ based on keyword frequency usually run short of accurately detecting @ subjective information implied in @ text @ in @ @ @ evaluate several popular classification algorithm along @ three filtering scheme @ @ filtering scheme progressively shrink @ original dataset @ respect to @ contextual polarity and frequent term of a document @ @ call @ approach hierarchical classification @ @ effect of @ approach in different combination of classification algorithm and filtering scheme @ discussed @ three set of controversial online news article @ binary and multi-class classification @ applied @ meanwhile @ use @ method to test @ hierarchical classification model and @ @ a comparison of @ @ method @ springer-verlag @ @ @ 
2107,Deteami research-transference project: Natural language processing technologies to the aid of pharmacy and pharmacosurveillance,"The goal of the Deteami project is to develop tools that make clinicians aware of adverse drug reactions stated in electronic health records of the clinical digital history.The records produced in hospitals are a valuable though nearly unexplored source of information among others due to the fact that are tough to get due to privacy and confidentiality restrictions. To leverage the clinicians work of reading and analyzing the health records looking for information about the health of the patients, in this project we explore the records automatically, identify among others disorder and drug entities, and infer medical information, in this case, adverse drug reactions. In this project a research-framework was settled with the Galdakao-Usansolo and Basurto Hospitals from Osakidetza (the Basque Health System). Osakidetza provided both the texts and the final user feedback, as well as, specialists that annotate the corpora, an in this way, we obtained a gold-standard. © 2016 Sociedad Española para el Procesamiento del Lenguaje Natural.",2016,Procesamiento de Lenguaje Natural,0,@ goal of @ deteami project is to develop tool @ make clinician aware of adverse drug reaction stated in electronic health record of @ clinical digital history @ @ record produced in hospital @ a valuable though nearly unexplored source of information among others due to @ fact @ @ tough to get due to privacy and confidentiality restriction @ to leverage @ clinician work of reading and analyzing @ health record looking @ information @ @ health of @ patient in @ project @ explore @ record automatically identify among others disorder and drug entity and infer medical information in @ case adverse drug reaction @ in @ project a research-framework wa settled @ @ galdakao-usansolo and basurto hospital @ osakidetza @ @ basque health system @ @ osakidetza provided @ @ text and @ final user feedback a well a specialist @ annotate @ corpus @ in @ way @ obtained a gold-standard @ sociedad española para el procesamiento del lenguaje natural @ 
2109,SPIRIT: A Tree Kernel-Based Method for Topic Person Interaction Detection,"The development of a topic in a set of topic documents is constituted by a series of person interactions at a specific time and place. Knowing the interactions of the persons mentioned in these documents is helpful for readers to better comprehend the documents. In this paper, we propose a topic person interaction detection method called SPIRIT, which classifies the text segments in a set of topic documents that convey person interactions. We design the rich interactive tree structure to represent syntactic, context, and semantic information of text, and this structure is incorporated into a tree-based convolution kernel to identify interactive segments. Experiment results based on real world topics demonstrate that the proposed rich interactive tree structure effectively detects the topic person interactions and that our method outperforms many well-known relation extraction and protein-protein interaction methods. © 1989-2012 IEEE.",2016,IEEE Transactions on Knowledge and Data Engineering,2,@ development of a topic in a set of topic document is constituted by a series of person interaction at a specific time and place @ knowing @ interaction of @ person mentioned in @ document is helpful @ reader to better comprehend @ document @ in @ @ @ propose a topic person interaction detection method called spirit @ classifies @ text segment in a set of topic document @ convey person interaction @ @ design @ rich interactive tree structure to represent syntactic context and semantic information of text and @ structure is incorporated @ a tree-based convolution kernel to identify interactive segment @ experiment @ based on real world topic demonstrate @ @ proposed rich interactive tree structure effectively detects @ topic person interaction and @ @ method outperforms many well-known relation extraction and protein-protein interaction method @ @ @ 
2110,String kernels for native language identification: Insights from behind the curtains,"The most common approach in text mining classification tasks is to rely on features like words, part-of-speech tags, stems, or some other high-level linguistic features. Recently, an approach that uses only character p-grams as features has been proposed for the task of native language identification (NLI). The approach obtained state-of-the-art results by combining several string kernels using multiple kernel learning. Despite the fact that the approach based on string kernels performs so well, several questions about this method remain unanswered. First, it is not clear why such a simple approach can compete with far more complex approaches that take words, lemmas, syntactic information, or even semantics into account. Second, although the approach is designed to be language independent, all experiments to date have been on English. This work is an extensive study that aims to systematically present the string kernel approach and to clarify the open questions mentioned above. A broad set of native language identification experiments were conducted to compare the string kernels approach with other state-of-the-art methods. The empirical results obtained in all of the experiments conducted in this work indicate that the proposed approach achieves state-of-the-art performance in NLI, reaching an accuracy that is 1:7% above the top scoring system of the 2013 NLI Shared Task. Furthermore, the results obtained on both the Arabic and the Norwegian corpora demonstrate that the proposed approach is language independent. In the Arabic native language identification task, string kernels show an increase of more than 17% over the best accuracy reported so far. The results of string kernels on Norwegian native language identification are also significantly better than the state-of-the-art approach. In addition, in a cross-corpus experiment, the proposed approach shows that it can also be topic independent, improving the state-of-the-art system by 32:3%. © 2016 Association for Computational Linguistics.",2016,Computational Linguistics,13,@ @ common approach in text mining classification task is to rely on feature like word part-of-speech tag stem @ some @ high-level linguistic feature @ recently @ approach @ us only character p-grams a feature ha @ proposed @ @ task of native language identification @ nli @ @ @ approach obtained state-of-the-art @ by combining several string kernel @ multiple kernel learning @ despite @ fact @ @ approach based on string kernel performs @ well several question @ @ method remain unanswered @ first @ is not clear @ @ a simple approach @ compete @ far more complex approach @ take word lemma syntactic information @ even semantics @ account @ second although @ approach is designed to @ language independent @ experiment to date @ @ on english @ @ work is @ extensive study @ aim to systematically @ @ string kernel approach and to clarify @ open question mentioned @ @ a broad set of native language identification experiment @ conducted to compare @ string kernel approach @ @ state-of-the-art method @ @ empirical @ obtained in @ of @ experiment conducted in @ work indicate @ @ proposed approach achieves state-of-the-art performance in nli reaching @ accuracy @ is @ @ @ top scoring system of @ nli shared task @ furthermore @ @ obtained on @ @ arabic and @ norwegian corpus demonstrate @ @ proposed approach is language independent @ in @ arabic native language identification task string kernel @ @ increase of more @ @ @ best accuracy reported @ far @ @ @ of string kernel on norwegian native language identification @ @ significantly better @ @ state-of-the-art approach @ in addition in a cross-corpus experiment @ proposed approach @ @ @ @ @ @ topic independent improving @ state-of-the-art system by @ @ association @ computational linguistics @ 
2111,C-BiLDA extracting cross-lingual topics from non-parallel texts by distinguishing shared from unshared content,"We study the problem of extracting cross-lingual topics from non-parallel multilingual text datasets with partially overlapping thematic content (e.g., aligned Wikipedia articles in two different languages). To this end, we develop a new bilingual probabilistic topic model called comparable bilingual latent Dirichlet allocation (C-BiLDA), which is able to deal with such comparable data, and, unlike the standard bilingual LDA model (BiLDA), does not assume the availability of document pairs with identical topic distributions. We present a full overview of C-BiLDA, and show its utility in the task of cross-lingual knowledge transfer for multi-class document classification on two benchmarking datasets for three language pairs. The proposed model outperforms the baseline LDA model, as well as the standard BiLDA model and two standard low-rank approximation methods (CL-LSI and CL-KCCA) used in previous work on this task. © 2015, The Author(s).",2016,Data Mining and Knowledge Discovery,8,@ study @ problem of extracting cross-lingual topic @ non-parallel multilingual text datasets @ partially overlapping thematic content @ e @ g @ aligned wikipedia article in @ different language @ @ to @ end @ develop a @ bilingual probabilistic topic model called comparable bilingual latent dirichlet allocation @ c-bilda @ @ is able to deal @ @ comparable data and unlike @ standard bilingual lda model @ bilda @ doe not assume @ availability of document pair @ identical topic distribution @ @ @ a full overview of c-bilda and @ @ utility in @ task of cross-lingual knowledge transfer @ multi-class document classification on @ benchmarking datasets @ three language pair @ @ proposed model outperforms @ baseline lda model a well a @ standard bilda model and @ standard low-rank approximation method @ cl-lsi and cl-kcca @ used in previous work on @ task @ @ author @ s @ @ 
2114,The added value of auxiliary data in sentiment analysis of Facebook posts,"The purpose of this study is to (1) assess the added value of information available before (i.e., leading) and after (i.e., lagging) the focal post's creation time in sentiment analysis of Facebook posts, (2) determine which predictors are most important, and (3) investigate the relationship between top predictors and sentiment. We build a sentiment prediction model, including leading information, lagging information, and traditional post variables. We benchmark Random Forest and Support Vector Machines using five times twofold cross-validation. The results indicate that both leading and lagging information increase the model's predictive performance. The most important predictors include the number of uppercase letters, the number of likes and the number of negative comments. A higher number of uppercase letters and likes increases the likelihood of a positive post, while a higher number of comments increases the likelihood of a negative post. The main contribution of this study is that it is the first to assess the added value of leading and lagging information in the context of sentiment analysis. © 2016 Elsevier B.V.",2016,Decision Support Systems,28,@ purpose of @ study is to @ @ ass @ added value of information available @ @ i @ e @ leading @ and @ @ i @ e @ lagging @ @ focal post @ s creation time in sentiment analysis of facebook post @ @ determine @ predictor @ @ important and @ @ investigate @ relationship @ top predictor and sentiment @ @ build a sentiment prediction model including leading information lagging information and traditional post variable @ @ benchmark random forest and support vector machine @ five time twofold cross-validation @ @ @ indicate @ @ leading and lagging information increase @ model @ s predictive performance @ @ @ important predictor include @ number of uppercase letter @ number of like and @ number of negative comment @ a higher number of uppercase letter and like increase @ likelihood of a positive post @ a higher number of comment increase @ likelihood of a negative post @ @ main contribution of @ study is @ @ is @ first to ass @ added value of leading and lagging information in @ context of sentiment analysis @ @ b @ v @ 
2115,Understanding the evolving academic landscape of library and information science through faculty hiring data,"Using a 40-year (from 1975 to 2015) hiring dataset of 642 library and Information science (LIS) faculty members from 44 US universities, this research reveals the disciplinary characteristics of LIS through several key aspects including gender, rank, country, university, major, and research area. Results show that genders and ranks among LIS faculty members are evenly distributed; geographically, more than 90 % of LIS faculty members received doctoral degrees in the US; meanwhile, 60 % of LIS faculty received Ph.D. in LIS, followed by Computer Science and Education; in regards to research interests, Human–Computer interaction, Digital Librarianship, Knowledge Organization and Management, and Information Behavior are the most popular research areas among LIS faculty members. Through a series of dynamic analyses, this study shows that the educational background of LIS faculty members is becoming increasingly diverse; in addition, research areas such as Human–Computer interaction, Social Network Analysis, Services for Children and Youth, Information Literacy, Information Ethics and Policy, and Data and Text Mining, Natural Language Processing, Machine Learning have received an increasing popularity. Predictive analyses are performed to discover trends on majors and research areas. Results show that the growth rate of LIS faculty members is linearly distributed. In addition, among faculty member’s Ph.D. majors, the share of LIS is decreasing while that the share of Computer Science is growing; among faculty members’ research areas, the share of Human–Computer interaction is on the rise. © 2016, Akadémiai Kiadó, Budapest, Hungary.",2016,Scientometrics,7,@ a year @ @ to @ hiring dataset of library and information science @ li @ faculty member @ u university @ research reveals @ disciplinary characteristic of li @ several key aspect including gender rank country university major and research area @ @ @ @ gender and rank among li faculty member @ evenly distributed @ geographically more @ of li faculty member received doctoral degree in @ u @ meanwhile of li faculty received ph @ @ @ in li followed by computer science and education @ in regard to research interest human computer interaction digital librarianship knowledge organization and management and information behavior @ @ @ popular research area among li faculty member @ @ a series of dynamic analysis @ study @ @ @ educational background of li faculty member is becoming increasingly diverse @ in addition research area @ a human computer interaction social network analysis service @ child and youth information literacy information ethic and policy and data and text mining natural language processing machine learning @ received @ increasing popularity @ predictive analysis @ performed to discover trend on major and research area @ @ @ @ @ growth rate of li faculty member is linearly distributed @ in addition among faculty member s ph @ @ @ major @ share of li is decreasing @ @ @ share of computer science is growing @ among faculty member research area @ share of human computer interaction is on @ rise @ akadémiai kiadó budapest hungary @ 
2121,Feature engineered relation extraction - Medical documents setting,"Purpose - Improving healthcare services by developing assistive technologies includes both the health aid devices and the analysis of the data collected by them. The acquired data modeled as a knowledge base give more insight into each patient's health status and needs. Therefore, the ultimate goal of a health-care system is obtaining recommendations provided by an assistive decision support system using such knowledge base, benefiting the patients, the physicians and the healthcare industry. This paper aims to define the knowledge flow for a medical assistive decision support system by structuring raw medical data and leveraging the knowledge contained in the data proposing solutions for efficient data search, medical investigation or diagnosis and medication prediction and relationship identification. Design/methodology/approach - The solution this paper proposes for implementing a medical assistive decision support system can analyze any type of unstructured medical documents which are processed by applying Natural Language Processing (NLP) tasks followed by semantic analysis, leading to the medical concept identification, thus imposing a structure on the input documents. The structured information is filtered and classified such that custom decisions regarding patients' health status can be made. The current research focuses on identifying the relationships between medical concepts as defined by the REMed (Relation Extraction from Medical documents) solution that aims at finding the patterns that lead to the classification of concept pairs into concept-to-concept relations. Findings - This paper proposed the REMed solution expressed as a multi-class classification problem tackled using the support vector machine classifier. Experimentally, this paper determined the most appropriate setup for the multi-class classification problem which is a combination of lexical, context, syntactic and grammatical features, as each feature category is good at representing particular relations, but not all. The best results we obtained are expressed as F1-measure of 74.9 per cent which is 1.4 per cent better than the results reported by similar systems. Research limitations/implications - The difficulty to discriminate between TrIP and TrAP relations revolves around the hierarchical relationship between the two classes as TrIP is a particular type (an instance) of TrAP. The intuition behind this behavior was that the classifier cannot discern the correct relations because of the bias toward the majority classes. The analysis was conducted by using only sentences from electronic health record that contain at least two medical concepts. This limitation was introduced by the availability of the annotated data with reported results, as relations were defined at sentence level. Originality/value - The originality of the proposed solution lies in the methodology to extract valuable information from the medical records via semantic searches; concept-to-concept relation identification; and recommendations for diagnosis, treatment and further investigations. The REMed solution introduces a learning-based approach for the automatic discovery of relations between medical concepts. We propose an original list of features: lexical - 3, context - 6, grammatical - 4 and syntactic - 4. The similarity feature introduced in this paper has a significant influence on the classification, and, to the best of the authors' knowledge, it has not been used as feature in similar solutions. © 2016 Emerald Group Publishing Limited.",2016,International Journal of Web Information Systems,7,purpose improving healthcare service by developing assistive technology includes @ @ health aid device and @ analysis of @ data collected by @ @ @ acquired data modeled a a knowledge base give more insight @ @ patient @ s health status and need @ therefore @ ultimate goal of a health-care system is obtaining recommendation provided by @ assistive decision support system @ @ knowledge base benefiting @ patient @ physician and @ healthcare industry @ @ @ aim to define @ knowledge flow @ a medical assistive decision support system by structuring raw medical data and leveraging @ knowledge contained in @ data proposing solution @ efficient data search medical investigation @ diagnosis and medication prediction and relationship identification @ design methodology approach @ solution @ @ proposes @ implementing a medical assistive decision support system @ analyze @ type of unstructured medical document @ @ processed by applying natural language processing @ nlp @ task followed by semantic analysis leading to @ medical concept identification thus imposing a structure on @ input document @ @ structured information is filtered and classified @ @ custom decision regarding patient @ health status @ @ made @ @ current research focus on identifying @ relationship @ medical concept a defined by @ remed @ relation extraction @ medical document @ solution @ aim at finding @ pattern @ lead to @ classification of concept pair @ concept-to-concept relation @ finding @ @ proposed @ remed solution expressed a a multi-class classification problem tackled @ @ support vector machine classifier @ experimentally @ @ determined @ @ appropriate setup @ @ multi-class classification problem @ is a combination of lexical context syntactic and grammatical feature a @ feature category is good at representing particular relation @ not @ @ @ best @ @ obtained @ expressed a f measure of @ per cent @ is @ per cent better @ @ @ reported by similar system @ research limitation implication @ difficulty to discriminate @ trip and trap relation revolves around @ hierarchical relationship @ @ @ class a trip is a particular type @ @ instance @ of trap @ @ intuition behind @ behavior wa @ @ classifier cannot discern @ correct relation @ of @ bias toward @ majority class @ @ analysis wa conducted by @ only sentence @ electronic health record @ contain at least @ medical concept @ @ limitation wa introduced by @ availability of @ annotated data @ reported @ a relation @ defined at sentence level @ originality value @ originality of @ proposed solution lie in @ methodology to extract valuable information @ @ medical record via semantic search @ concept-to-concept relation identification @ and recommendation @ diagnosis treatment and @ investigation @ @ remed solution introduces a learning-based approach @ @ automatic discovery of relation @ medical concept @ @ propose @ original list of feature @ lexical context grammatical and syntactic @ @ similarity feature introduced in @ @ ha a significant influence on @ classification and to @ best of @ author @ knowledge @ ha not @ used a feature in similar solution @ emerald group publishing limited @ 
2122,Sentiment analysis and classification using lexicon-based approach and addressing polarity shift problem,"As we know that in recent years e-commerce has been growing, so volume of online reviews on the web is also increasing for different sides due to which we can understand that the particular product or things are good for use or not and their current status in market. In Natural Language Processing (NLP) and text mining, different models and methods are useful for text representation and categorization purposes. Bag- Of-Word (BOW) model is one such model used to model the text. But polarity shift problem is a major factor in Bag-Of-Word model which can effect on classification performance of Sentiment Analysis. In our methodology, we address the polarity shift problem by detecting, removing and modifying negation from extracted review to identify where the sentiment orientation is actually changing in given review. Our main idea is Sentiment analysis and classification which is based on machine learning approach using Lexicon based antonym dictionary. We build system for Sentence-level sentiment classification. We first extract product reviews from one of the customized shopping portal. When extracted reviews are simple sentences then system is trained to directly find its opinion target, and classify it according to its sentiment polarities i.e. is positive, negative and neutral class labels. When extracted reviews are compound and complex sentences then we first split it into subsentences and build a model based on some rules to detect, remove and modify polarity shift in contrast of negation to identify where the sentiment orientation is changing in compound or complex sentences. After that, we classify review according to its polarity and determine the targets of opinion given in review. Furthermore, we extend our system for opinion summarization based on opinion features or aspects and graphically represent overall summary of Positive, Negative and Neutral sentiments of customer for each product. © 2005 - 2016 JATIT & LLS. All rights reserved.",2016,Journal of Theoretical and Applied Information Technology,2,a @ know @ in recent year e-commerce ha @ growing @ volume of online review on @ web is @ increasing @ different side due to @ @ @ understand @ @ particular product @ thing @ good @ use @ not and @ current status in market @ in natural language processing @ nlp @ and text mining different model and method @ useful @ text representation and categorization purpose @ bag of-word @ bow @ model is @ @ model used to model @ text @ @ polarity shift problem is a major factor in bag-of-word model @ @ effect on classification performance of sentiment analysis @ in @ methodology @ address @ polarity shift problem by detecting removing and modifying negation @ extracted review to identify @ @ sentiment orientation is actually changing in given review @ @ main idea is sentiment analysis and classification @ is based on machine learning approach @ lexicon based antonym dictionary @ @ build system @ sentence-level sentiment classification @ @ first extract product review @ @ of @ customized shopping portal @ @ extracted review @ simple sentence @ system is trained to directly find @ opinion target and classify @ according to @ sentiment polarity i @ e @ is positive negative and neutral class label @ @ extracted review @ compound and complex sentence @ @ first split @ @ subsentences and build a model based on some rule to detect remove and modify polarity shift in contrast of negation to identify @ @ sentiment orientation is changing in compound @ complex sentence @ @ @ @ classify review according to @ polarity and determine @ target of opinion given in review @ furthermore @ extend @ system @ opinion summarization based on opinion feature @ aspect and graphically represent overall summary of positive negative and neutral sentiment of customer @ @ product @ jatit lls @ @ right reserved @ 
2130,Multi-Objective Model Selection (MOMS)-based Semi-Supervised Framework for Sentiment Analysis,"Sentiment analysis has emerged as an active research field due to the rapid growth of user-generated content on the Internet. This research area analyzes the opinions and attitudes of masses toward products, movies, topics, individuals, and services. Various machine learning and text mining algorithms have been used for sentiment analysis and classification. The recent research concludes that domain-specific lexicons perform significantly better as compared to domain-independent lexicons. The proposed research aims at improving the performance of general-purpose lexicons utilizing machine learning algorithms. A semi-supervised framework based on “MOMS” is introduced in order to determine the feature weight by incorporating SentiWordNet, a well-known general-purpose sentiment lexicon. The feature weights are learned by support vector machine, and the classification performance is enhanced by using Multi-Objective Model Selection procedure. Subjectivity criterion is used to select the desired features, and the effects of feature selection with respect to their part-of-speech information are studied comprehensively. Experimental evaluation is performed on seven different benchmark datasets which includes Large movie review dataset, Multi-domain sentiment dataset, and Cornell movie review dataset. The comparison of the proposed approach is performed with state-of-the-art techniques, lexicon-based approaches, and other methods for sentiment analysis. The proposed framework results in high performance when compared to other research in this field. © 2016, Springer Science+Business Media New York.",2016,Cognitive Computation,18,sentiment analysis ha emerged a @ active research field due to @ rapid growth of user-generated content on @ internet @ @ research area analyzes @ opinion and attitude of mass toward product movie topic individual and service @ various machine learning and text mining algorithm @ @ used @ sentiment analysis and classification @ @ recent research concludes @ domain-specific lexicon perform significantly better a compared to domain-independent lexicon @ @ proposed research aim at improving @ performance of general-purpose lexicon utilizing machine learning algorithm @ a semi-supervised framework based on mom is introduced in order to determine @ feature weight by incorporating sentiwordnet a well-known general-purpose sentiment lexicon @ @ feature weight @ learned by support vector machine and @ classification performance is enhanced by @ multi-objective model selection procedure @ subjectivity criterion is used to select @ desired feature and @ effect of feature selection @ respect to @ part-of-speech information @ studied comprehensively @ experimental evaluation is performed on seven different benchmark datasets @ includes @ movie review dataset multi-domain sentiment dataset and cornell movie review dataset @ @ comparison of @ proposed approach is performed @ state-of-the-art technique lexicon-based approach and @ method @ sentiment analysis @ @ proposed framework @ in high performance @ compared to @ research in @ field @ @ science @ medium @ york @ 
2131,Dystemo: Distant supervision method for multi-category emotion recognition in tweets,"Emotion recognition in text has become an important research objective. It involves building classifiers capable of detecting human emotions for a specific application, for example, analyzing reactions to product launches, monitoring emotions at sports events, or discerning opinions in political debates. Most successful approaches rely heavily on costly manual annotation. To alleviate this burden, we propose a distant supervision method - Dystemo - for automatically producing emotion classifiers from tweets labeled using existing or easy-to-produce emotion lexicons. The goal is to obtain emotion classifiers that work more accurately for specific applications than available emotion lexicons. The success of this method depends mainly on a novel classifier - Balanced Weighted Voting (BWV) - designed to overcome the imbalance in emotion distribution in the initial dataset, and on novel heuristics for detecting neutral tweets. We demonstrate how Dystemo works using Twitter data about sports events, a fine-grained 20-category emotion model, and three different initial emotion lexicons. Through a series of carefully designed experiments, we confirm that Dystemo is effective both in extending initial emotion lexicons of small coverage to find correctly more emotional tweets and in correcting emotion lexicons of low accuracy to perform more accurately.",2016,ACM Transactions on Intelligent Systems and Technology,9,emotion recognition in text ha become @ important research objective @ @ involves building classifier capable of detecting human emotion @ a specific application @ example analyzing reaction to product launch monitoring emotion at sport event @ discerning opinion in political debate @ @ successful approach rely heavily on costly manual annotation @ to alleviate @ burden @ propose a distant supervision method dystemo @ automatically producing emotion classifier @ tweet labeled @ existing @ easy-to-produce emotion lexicon @ @ goal is to obtain emotion classifier @ work more accurately @ specific application @ available emotion lexicon @ @ success of @ method depends mainly on a novel classifier balanced weighted voting @ bwv @ designed to overcome @ imbalance in emotion distribution in @ initial dataset and on novel heuristic @ detecting neutral tweet @ @ demonstrate @ dystemo work @ twitter data @ sport event a fine-grained category emotion model and three different initial emotion lexicon @ @ a series of carefully designed experiment @ confirm @ dystemo is effective @ in extending initial emotion lexicon of small coverage to find correctly more emotional tweet and in correcting emotion lexicon of low accuracy to perform more accurately @ 
2133,TopPRF: A probabilistic framework for integrating topic space into pseudo relevance feedback,"Traditional pseudo relevance feedback (PRF) models choose top k feedback documents for query expansion and treat those documents equally. When k is determined, feedback terms are selected without considering the reliability of these documents for relevance. Because the performance of PRF is sensitive to the selection of feedback terms, noisy terms imported from these irrelevant documents or partially relevant documents will harm the final results extensively. Intuitively, terms in these documents should be considered less important for feedback term selection. Nonetheless, how to measure the reliability of feedback documents is a difficult problem. Recently, topic modeling has become more and more popular in the information retrieval (IR) area. In order to identify how reliable a feedback document is to be relevant, we attempt to adapt the topical information into PRF. However, topics are hard to be quantified and therefore the identification of topic is usually fuzzy. It is very challenging for integrating the obtained topical information effectively into IR and other text-processing-related areas. Current research work mainly focuses on mining relevant information from particular topics. This is extremely difficult when the boundaries of different topics are hard to define. In this article, we investigate a key factor of this problem, the topic number for topic modeling and how it makes topics ""fuzzy."" To effectively and efficiently apply topical information, we propose a new probabilistic framework, ""TopPRF,"" and threemodels, TS-COS, TS-EU, and TS-Entropy, via integrating ""Topic Space"" (TS) information into pseudo relevance feedback. Thesemethods discover how reliable a document is to be relevant through both term and topical information.When selecting feedback terms, candidate terms in more reliable feedback documents should obtain extra weights. Experimental results on various public collections justify that our proposed methods can significantly reduce the influence of ""fuzzy topics"" and obtain stable, good results over the strong baseline models. Our proposed probabilistic framework, TopPRF, and three topicspace- based models are capable of searching documents beyond traditional term matching only and provide a promising avenue for constructing better topic-space-based IR systems. Moreover, in-depth discussions and conclusions are made to help other researchers apply topical information effectively. © 2016 ACM.",2016,ACM Transactions on Information Systems,14,traditional pseudo relevance feedback @ prf @ model choose top k feedback document @ query expansion and treat @ document equally @ @ k is determined feedback term @ selected without considering @ reliability of @ document @ relevance @ @ @ performance of prf is sensitive to @ selection of feedback term noisy term imported @ @ irrelevant document @ partially relevant document @ harm @ final @ extensively @ intuitively term in @ document @ @ considered le important @ feedback term selection @ nonetheless @ to measure @ reliability of feedback document is a difficult problem @ recently topic modeling ha become more and more popular in @ information retrieval @ ir @ area @ in order to identify @ reliable a feedback document is to @ relevant @ attempt to adapt @ topical information @ prf @ however topic @ hard to @ quantified and therefore @ identification of topic is usually fuzzy @ @ is @ challenging @ integrating @ obtained topical information effectively @ ir and @ text-processing-related area @ current research work mainly focus on mining relevant information @ particular topic @ @ is extremely difficult @ @ boundary of different topic @ hard to define @ in @ article @ investigate a key factor of @ problem @ topic number @ topic modeling and @ @ make topic @ fuzzy @ @ to effectively and efficiently apply topical information @ propose a @ probabilistic framework @ topprf @ and threemodels ts-cos ts-eu and ts-entropy via integrating @ topic space @ @ t @ information @ pseudo relevance feedback @ thesemethods discover @ reliable a document is to @ relevant @ @ term and topical information @ @ selecting feedback term candidate term in more reliable feedback document @ obtain extra weight @ experimental @ on various public collection justify @ @ proposed method @ significantly reduce @ influence of @ fuzzy topic @ and obtain stable good @ @ @ strong baseline model @ @ proposed probabilistic framework topprf and three topicspace based model @ capable of searching document beyond traditional term matching only and provide a promising avenue @ constructing better topic-space-based ir system @ moreover in-depth discussion and conclusion @ made to help @ researcher apply topical information effectively @ acm @ 
2146,Gauzy knowledge sharing in conspiring environment using text mining,"Objective: The people all over the world are in need of gaining knowledge which is achieved by means of searching information through web services. Even though search results obtained from web services are relevant, the user is trying to obtain the best information providers. Methods: The existing approaches which is non-parametric generative model and infinite Hidden Markov Model (iHMM) of clustering technique does not yield better results for very large collections of web services. The iHMM model does not provide any security for datasets in a hierarchal structure. Thus to overcome such downsides of existing approach to a new technique can be used to help of Natural Language Processing (NLP). Findings: The new approach provides both tag recommendation and parental control. The tag recommendation is used to ping the best tags based on expert's bookmarks, and parental control provided by group owner is used to resolve the privacy issues. The web filter denies the unauthorized access. Thus, using NLP technique in the conspiring environment enables the web user to achieve fine grained knowledge. Applications: The paper defines the efficient way to provide tag recommendation on the web for large dataset users.",2016,Indian Journal of Science and Technology,2,objective @ @ people @ @ @ world @ in need of gaining knowledge @ is achieved by mean of searching information @ web service @ even though search @ obtained @ web service @ relevant @ user is trying to obtain @ best information provider @ method @ @ existing approach @ is non-parametric generative model and infinite hidden markov model @ ihmm @ of clustering technique doe not yield better @ @ @ @ collection of web service @ @ ihmm model doe not provide @ security @ datasets in a hierarchal structure @ thus to overcome @ downside of existing approach to a @ technique @ @ used to help of natural language processing @ nlp @ @ finding @ @ @ approach provides @ tag recommendation and parental control @ @ tag recommendation is used to ping @ best tag based on expert @ s bookmark and parental control provided by group owner is used to resolve @ privacy issue @ @ web filter denies @ unauthorized access @ thus @ nlp technique in @ conspiring environment enables @ web user to achieve fine grained knowledge @ application @ @ @ defines @ efficient way to provide tag recommendation on @ web @ @ dataset user @ 
2147,The linguistic construal of disciplinarity: A data-mining approach using register features,"We analyze the linguistic evolution of selected scientific disciplines over a 30-year time span (1970s to 2000s). Our focus is on four highly specialized disciplines at the boundaries of computer science that emerged during that time: computational linguistics, bioinformatics, digital construction, and microelectronics. Our analysis is driven by the question whether these disciplines develop a distinctive language use—both individually and collectively—over the given time period. The data set is the English Scientific Text Corpus (scitex), which includes texts from the 1970s/1980s and early 2000s. Our theoretical basis is register theory. In terms of methods, we combine corpus-based methods of feature extraction (various aggregated features [part-of-speech based], n-grams, lexico-grammatical patterns) and automatic text classification. The results of our research are directly relevant to the study of linguistic variation and languages for specific purposes (LSP) and have implications for various natural language processing (NLP) tasks, for example, authorship attribution, text mining, or training NLP tools. © 2015 ASIS&T",2016,Journal of the Association for Information Science and Technology,6,@ analyze @ linguistic evolution of selected scientific discipline @ a year time span @ s to s @ @ @ focus is on four highly specialized discipline at @ boundary of computer science @ emerged @ @ time @ computational linguistics bioinformatics digital construction and microelectronics @ @ analysis is driven by @ question whether @ discipline develop a distinctive language use @ individually and collectively @ @ given time period @ @ data set is @ english scientific text corpus @ scitex @ @ includes text @ @ s s and early s @ @ theoretical basis is register theory @ in term of method @ combine corpus-based method of feature extraction @ various aggregated feature part-of-speech based n-grams lexico-grammatical pattern @ and automatic text classification @ @ @ of @ research @ directly relevant to @ study of linguistic variation and language @ specific purpose @ lsp @ and @ implication @ various natural language processing @ nlp @ task @ example authorship attribution text mining @ training nlp tool @ asis t
2150,Learning the Multilingual Translation Representations for Question Retrieval in Community Question Answering via Non-Negative Matrix Factorization,"Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issues. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via non-negative matrix factorization. Experiments conducted on real CQA data sets show that our proposed approach is promising. © 2014 IEEE.",2016,IEEE/ACM Transactions on Audio Speech and Language Processing,17,community question answering @ cqa @ ha become @ increasingly popular research topic @ in @ @ @ focus on @ problem of question retrieval @ question retrieval in cqa @ automatically find @ @ relevant and recent question @ @ @ solved by @ user @ however @ word ambiguity and word mismatch problem bring @ @ challenge @ question retrieval in cqa @ state-of-the-art approach address @ issue by implicitly expanding @ queried question @ additional word @ phrase @ monolingual translation model @ @ useful @ effectiveness of @ model is highly dependent on @ availability of quality parallel monolingual corpus @ e @ g @ question-answer pair @ in @ absence of @ @ @ troubled by noise issue @ in @ work @ propose @ alternative way to address @ word ambiguity and word mismatch problem by taking advantage of potentially rich semantic information drawn @ @ language @ @ proposed method employ statistical machine translation to improve question retrieval and enriches @ question representation @ @ translated word @ @ language via non-negative matrix factorization @ experiment conducted on real cqa data set @ @ @ proposed approach is promising @ @ @ 
2151,Extracting PICO sentences from clinical trial reports using supervised distant supervision,"Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations. To this end, we propose a novel method - supervised distant supervision (SDS) - that uses a small amount of direct supervision to better exploit a large corpus of distantly labeled instances by learning to pseudo-annotate articles using the available DS. We show that this approach tends to outperform existing methods with respect to automated PICO extraction. ©2016 Byron C. Wallace, Joel Kuiper, Aakash Sharma, Mingxi Zhu and Iain J. Marshall.",2016,Journal of Machine Learning Research,51,systematic review underpin evidence based medicine @ ebm @ by addressing precise clinical question via comprehensive synthesis of @ relevant published evidence @ author of systematic review typically define a population problem intervention comparator and outcome @ a pico criterion @ of interest and @ retrieve appraise and synthesize @ @ @ report of clinical trial @ meet @ criterion @ identifying pico element in @ full-texts of trial report is thus a critical yet time-consuming step in @ systematic review process @ @ seek to expedite evidence synthesis by developing machine learning model to automatically extract sentence @ article relevant to pico element @ collecting a @ corpus of training data @ @ task would @ prohibitively expensive @ therefore @ derive distant supervision @ @ @ @ @ to train model @ @ conducted review @ @ entail heuristically deriving @ soft @ label @ @ available structured resource @ however @ @ access only to unstructured free-text summary of pico element @ corresponding article @ @ must derive @ @ @ desired sentence-level annotation @ to @ end @ propose a novel method supervised distant supervision @ sd @ @ us a small amount of direct supervision to better exploit a @ corpus of distantly labeled instance by learning to pseudo-annotate article @ @ available @ @ @ @ @ @ approach tends to outperform existing method @ respect to automated pico extraction @ byron c @ wallace joel kuiper aakash sharma mingxi zhu and iain j @ marshall @ 
2152,CMiner: Opinion Extraction and Summarization for Chinese Microblogs,"Sentiment analysis of microblog texts has drawn lots of attention in both the academic and industrial fields. However, most of the current work only focuses on polarity classification. In this paper, we present an opinion mining system for Chinese microblogs called CMiner. Instead of polarity classification, CMiner focuses on more complicated opinion mining tasks-opinion target extraction and opinion summarization. Novel algorithms are developed for the two tasks and integrated into the end-To-end system. CMiner can help to effectively understand the users' opinion towards different opinion targets in a microblog topic. Specially, we develop an unsupervised label propagation algorithm for opinion target extraction. The opinion targets of all messages in a topic are collectively extracted based on the assumption that similar messages may focus on similar opinion targets. In addition, we build an aspect-based opinion summarization framework for microblog topics. After getting the opinion targets of all the microblog messages in a topic, we cluster the opinion targets into several groups and extract representative targets and summaries for each group. A co-ranking algorithm is proposed to rank both the opinion targets and microblog sentences simultaneously. Experimental results on a benchmark dataset show the effectiveness of our system and the algorithms. © 1989-2012 IEEE.",2016,IEEE Transactions on Knowledge and Data Engineering,35,sentiment analysis of microblog text ha drawn lot of attention in @ @ @ and industrial field @ however @ of @ current work only focus on polarity classification @ in @ @ @ @ @ opinion mining system @ chinese microblogs called cminer @ instead of polarity classification cminer focus on more complicated opinion mining tasks-opinion target extraction and opinion summarization @ novel algorithm @ developed @ @ @ task and integrated @ @ end-to-end system @ cminer @ help to effectively understand @ user @ opinion towards different opinion target in a microblog topic @ specially @ develop @ unsupervised label propagation algorithm @ opinion target extraction @ @ opinion target of @ message in a topic @ collectively extracted based on @ assumption @ similar message may focus on similar opinion target @ in addition @ build @ aspect-based opinion summarization framework @ microblog topic @ @ getting @ opinion target of @ @ microblog message in a topic @ cluster @ opinion target @ several group and extract representative target and summary @ @ group @ a co-ranking algorithm is proposed to rank @ @ opinion target and microblog sentence simultaneously @ experimental @ on a benchmark dataset @ @ effectiveness of @ system and @ algorithm @ @ @ 
2153,Microblog Dimensionality Reduction-A Deep Learning Approach,"Exploring potentially useful information from huge amount of textual data produced by microblogging services has attracted much attention in recent years. An important preprocessing step of microblog text mining is to convert natural language texts into proper numerical representations. Due to the short-length characteristics of microblog texts, using term frequency vectors to represent microblog texts will cause 'sparse data' problem. Finding proper representations of microblog texts is a challenging issue. In this paper, we apply deep networks to map the high-dimensional representations of microblog texts to low-dimensional representations. To improve the result of dimensionality reduction, we take advantage of the semantic similarity derived from two types of microblog-specific information, namely the retweet relationship and hashtags. Two types of approaches, including modifying training data and modifying the training objective of deep networks, are proposed to make use of microblog-specific information. Experiment results show that the deep models perform better than traditional dimensionality reduction methods such as latent semantic analysis and latent Dirichlet allocation topic model, and the use of microblog-specific information can help to learn better representations. © 1989-2012 IEEE.",2016,IEEE Transactions on Knowledge and Data Engineering,16,exploring potentially useful information @ huge amount of textual data produced by microblogging service ha attracted much attention in recent year @ @ important preprocessing step of microblog text mining is to convert natural language text @ proper numerical representation @ due to @ short-length characteristic of microblog text @ term frequency vector to represent microblog text @ cause @ sparse data @ problem @ finding proper representation of microblog text is a challenging issue @ in @ @ @ apply deep network to map @ high-dimensional representation of microblog text to low-dimensional representation @ to improve @ @ of dimensionality reduction @ take advantage of @ semantic similarity derived @ @ type of microblog-specific information namely @ retweet relationship and hashtags @ @ type of approach including modifying training data and modifying @ training objective of deep network @ proposed to make use of microblog-specific information @ experiment @ @ @ @ deep model perform better @ traditional dimensionality reduction method @ a latent semantic analysis and latent dirichlet allocation topic model and @ use of microblog-specific information @ help to learn better representation @ @ @ 
2159,Big Data Analytics for Social Media,"A large volume of text data is being generated at a high velocity on a routine basis. Natural language processing (NLP) methods are used to parse text data and extract the most impactful entities. Understanding text is governed by grammatical rules of the language. Traditional methods of text analysis have been centered on syntactic methods, but there has been a systematic shift towards the use of statistical methods for text and language processing in recent years. These NLP techniques are central to building everyday applications such as search, recommendation systems, spell checkers, machine translation, question answering machines, etc. Several applications have been developed on top of text data streams. For example, detecting trending terms, potentially across languages, has proven useful in determining early signs of a flu outbreak. In a similar vein, detecting patterns/anomalies in chat messages in multiplayer games can potentially guide development of new features and can potentially help players to develop better strategies. Detecting change/anomalies in sentiment (derived via mining of text data streams) has direct applications in, for example, financial markets. Detection of anomalies in time series of terms, obtained from mining of text data streams, is nontrivial owing to, for example, but not limited to, presence of an underlying trend, seasonality and other data characteristics (which are mostly not accounted for by the existing techniques). Further, there is a tradeoff between accuracy and time to detect. © 2016 Elsevier Inc. All rights reserved.",2016,Big Data: Principles and Paradigms,0,a @ volume of text data is @ generated at a high velocity on a routine basis @ natural language processing @ nlp @ method @ used to parse text data and extract @ @ impactful entity @ understanding text is governed by grammatical rule of @ language @ traditional method of text analysis @ @ centered on syntactic method @ @ ha @ a systematic shift towards @ use of statistical method @ text and language processing in recent year @ @ nlp technique @ central to building everyday application @ a search recommendation system spell checker machine translation question answering machine etc @ several application @ @ developed on top of text data stream @ @ example detecting trending term potentially across language ha proven useful in determining early sign of a flu outbreak @ in a similar vein detecting pattern anomaly in chat message in multiplayer game @ potentially guide development of @ feature and @ potentially help player to develop better strategy @ detecting change anomaly in sentiment @ derived via mining of text data stream @ ha direct application in @ example financial market @ detection of anomaly in time series of term obtained @ mining of text data stream is nontrivial owing to @ example @ not limited to presence of @ underlying trend seasonality and @ data characteristic @ @ @ mostly not accounted @ by @ existing technique @ @ @ @ is a tradeoff @ accuracy and time to detect @ @ inc @ @ right reserved @ 
2160,TextFlows: A visual programming platform for text mining and natural language processing,"Text mining and natural language processing are fast growing areas of research, with numerous applications in business, science and creative industries. This paper presents TextFlows, a web-based text mining and natural language processing platform supporting workflow construction, sharing and execution. The platform enables visual construction of text mining workflows through a web browser, and the execution of the constructed workflows on a processing cloud. This makes TextFlows an adaptable infrastructure for the construction and sharing of text processing workflows, which can be reused in various applications. The paper presents the implemented text mining and language processing modules, and describes some precomposed workflows. Their features are demonstrated on three use cases: comparison of document classifiers and of different part-of-speech taggers on a text categorization problem, and outlier detection in document corpora. © 2016 Elsevier B.V.",2016,Science of Computer Programming,18,text mining and natural language processing @ fast growing area of research @ numerous application in @ science and creative industry @ @ @ @ textflows a web-based text mining and natural language processing platform supporting workflow construction sharing and execution @ @ platform enables visual construction of text mining workflow @ a web browser and @ execution of @ constructed workflow on a processing cloud @ @ make textflows @ adaptable infrastructure @ @ construction and sharing of text processing workflow @ @ @ reused in various application @ @ @ @ @ implemented text mining and language processing module and describes some precomposed workflow @ @ feature @ demonstrated on three use case @ comparison of document classifier and of different part-of-speech tagger on a text categorization problem and outlier detection in document corpus @ @ b @ v @ 
2162,"""Climate Change"" Frames Detection and Categorization Based on Generalized Concepts","The subliminal impact of framing of social, political and environmental issues such as climate change has been studied for a long time in political science and communications research. Media framing offers ""interpretative package"" for average citizens on how to make sense of climate change and its consequences to their livelihoods, how to deal with its negative impacts, and which mitigation or adaptation policies to support. A line of related work has used bag of words and word-level features to detect frames automatically in text. Such works face limitations since standard keyword based features may not generalize well to accommodate surface variations in text when different keywords are used for similar concepts. In this paper, we develop a new type of textual features that generalize (subject,verb,object) triplets extracted from text, by clustering them into high-level concepts. We utilize these concepts as features to detect frames in text. Our corpus comprises more than 45,000 climate change related sentences. Expert coders annotated those sentences as Frame/Non-Frame and framed sentences were mapped into one of four general frame categories: Solution, problem threat, cause, and motivation. Compared to uni-gram and bi-gram based models, classification and clustering using our generalized concepts yielded better discriminating features and a higher accuracy classifier with a 12% boost (i.e. from 74% to 83% in f-measure) and 0.91 clustering purity for Frame/Non-Frame detection. © 2016 World Scientific Publishing Company.",2016,International Journal of Semantic Computing,3,@ subliminal impact of framing of social political and environmental issue @ a climate change ha @ studied @ a long time in political science and communication research @ medium framing offer @ interpretative package @ @ average citizen on @ to make sense of climate change and @ consequence to @ livelihood @ to deal @ @ negative impact and @ mitigation @ adaptation policy to support @ a line of related work ha used bag of word and word-level feature to detect frame automatically in text @ @ work face limitation since standard keyword based feature may not generalize well to accommodate surface variation in text @ different keywords @ used @ similar concept @ in @ @ @ develop a @ type of textual feature @ generalize @ subject verb object @ triplet extracted @ text by clustering @ @ high-level concept @ @ utilize @ concept a feature to detect frame in text @ @ corpus comprises more @ climate change related sentence @ expert coder annotated @ sentence a frame non-frame and framed sentence @ mapped @ @ of four general frame category @ solution problem threat cause and motivation @ compared to uni-gram and bi-gram based model classification and clustering @ @ generalized concept yielded better discriminating feature and a higher accuracy classifier @ a boost @ i @ e @ @ to in f-measure @ and @ clustering purity @ frame non-frame detection @ world scientific publishing company @ 
2163,Measuring the semantic uncertainty of news events for evolution potential estimation,"The evolution potential estimation of news events can support the decision making of both corporations and governments. For example, a corporation could manage its public relations crisis in a timely manner if a negative news event about this corporation is known with large evolution potential in advance. However, existing state-of-the-art methods are mainly based on time series historical data, which are not suitable for the news events with limited historical data and bursty properties. In this article, we propose a purely content-based method to estimate the evolution potential of the news events. The proposed method considers a news event at a given time point as a system composed of different keywords, and the uncertainty of this system is defined and measured as the Semantic Uncertainty of this news event. At the same time, an uncertainty space is constructed with two extreme states: the most uncertain state and the most certain state. We believe that the Semantic Uncertainty has correlation with the content evolution of the news events, so it can be used to estimate the evolution potential of the news events. In order to verify the proposed method, we present detailed experimental setups and results measuring the correlation of the Semantic Uncertainty with the Content Change of news events using collected news events data. The results show that the correlation does exist and is stronger than the correlation of value from the time-series-based method with the Content Change. Therefore, we can use the Semantic Uncertainty to estimate the evolution potential of news events. © 2016 ACM.",2016,ACM Transactions on Information Systems,7,@ evolution potential estimation of news event @ support @ decision making of @ corporation and government @ @ example a corporation could manage @ public relation crisis in a timely manner if a negative news event @ @ corporation is known @ @ evolution potential in advance @ however existing state-of-the-art method @ mainly based on time series historical data @ @ not suitable @ @ news event @ limited historical data and bursty property @ in @ article @ propose a purely content-based method to estimate @ evolution potential of @ news event @ @ proposed method considers a news event at a given time point a a system composed of different keywords and @ uncertainty of @ system is defined and measured a @ semantic uncertainty of @ news event @ at @ @ time @ uncertainty space is constructed @ @ extreme state @ @ @ uncertain state and @ @ certain state @ @ believe @ @ semantic uncertainty ha correlation @ @ content evolution of @ news event @ @ @ @ used to estimate @ evolution potential of @ news event @ in order to verify @ proposed method @ @ detailed experimental setup and @ measuring @ correlation of @ semantic uncertainty @ @ content change of news event @ collected news event data @ @ @ @ @ @ correlation doe exist and is stronger @ @ correlation of value @ @ time-series-based method @ @ content change @ therefore @ @ use @ semantic uncertainty to estimate @ evolution potential of news event @ acm @ 
2166,Keyword extraction from Chinese text based on multidimensional weighted features,"This paper proposed to solve the problems of incomplete coverage and low accuracy in keyword extraction of Chinese text based on intrinsic feature of the Chinese language and an extraction method of multidimensional information weighted eigenvalues. This method combined theoretical analysis and experimental calculation to study the parts of speech, word position, word length, semantic similarity and word co-occurrence frequency in Chinese texts. By combining multidimensional data related to word frequency, word feature values, word similarity and word co-occurrence probability, we calculated that the weighted eigenvalues obtained by comparing precision rate, recall rate and F measure and concluded that the proposed method can give a better measure of the word accuracy than using word frequency or the basic eigenvalue methods alone. The conclusions obtained in this study provide reference values for keyword extraction and text mining.",2016,Journal of Digital Information Management,3,@ @ proposed to solve @ problem of incomplete coverage and low accuracy in keyword extraction of chinese text based on intrinsic feature of @ chinese language and @ extraction method of multidimensional information weighted eigenvalue @ @ method combined theoretical analysis and experimental calculation to study @ part of speech word position word length semantic similarity and word co-occurrence frequency in chinese text @ by combining multidimensional data related to word frequency word feature value word similarity and word co-occurrence probability @ calculated @ @ weighted eigenvalue obtained by comparing precision rate recall rate and f measure and concluded @ @ proposed method @ give a better measure of @ word accuracy @ @ word frequency @ @ basic eigenvalue method alone @ @ conclusion obtained in @ study provide reference value @ keyword extraction and text mining @ 
2169,Feature engineering for recognizing adverse drug reactions from twitter posts,"Social media platforms are emerging digital communication channels that provide aneasy way for common people to share their health and medication experiences online. With morepeople discussing their health information online publicly, social media platforms present a richsource of information for exploring adverse drug reactions (ADRs). ADRs are major public healthproblems that result in deaths and hospitalizations of millions of people. Unfortunately, not allADRs are identified before a drug is made available in the market. In this study, an ADR eventmonitoring system is developed which can recognize ADR mentions from a tweet and classify itsassertion. We explored several entity recognition features, feature conjunctions, and feature selectionand analyzed their characteristics and impacts on the recognition of ADRs, which have never beenstudied previously. The results demonstrate that the entity recognition performance for ADR canachieve an F-score of 0.562 on the PSB Social Media Mining shared task dataset, which outperformsthe partial-matching-based method by 0.122. After feature selection, the F-score can be furtherimproved by 0.026. This novel technique of text mining utilizing shared online social media data willopen an array of opportunities for researchers to explore various health related issues. © 2016 by the authors.",2016,Information (Switzerland),14,social medium platform @ emerging digital communication channel @ provide aneasy way @ common people to share @ health and medication experience online @ @ morepeople discussing @ health information online publicly social medium platform @ a richsource of information @ exploring adverse drug reaction @ adrs @ @ adrs @ major public healthproblems @ @ in death and hospitalization of million of people @ unfortunately not alladrs @ identified @ a drug is made available in @ market @ in @ study @ adr eventmonitoring system is developed @ @ recognize adr mention @ a tweet and classify itsassertion @ @ explored several entity recognition feature feature conjunction and feature selectionand analyzed @ characteristic and impact on @ recognition of adrs @ @ never beenstudied @ @ @ @ demonstrate @ @ entity recognition performance @ adr canachieve @ f-score of @ on @ psb social medium mining shared task dataset @ outperformsthe partial-matching-based method by @ @ @ feature selection @ f-score @ @ furtherimproved by @ @ @ novel technique of text mining utilizing shared online social medium data willopen @ array of opportunity @ researcher to explore various health related issue @ by @ author @ 
2174,Understanding big consumer opinion data for market-driven product design,"Big consumer data provide new opportunities for business administrators to explore the value to fulfil customer requirements (CRs). Generally, they are presented as purchase records, online behaviour, etc. However, distinctive characteristics of big data, Volume, Variety, Velocity and Value or 4Vs, lead to many conventional methods for customer understanding potentially fail to handle such data. A visible research gap with practical significance is to develop a framework to deal with big consumer data for CRs understanding. Accordingly, a research study is conducted to exploit the value of these data in the perspective of product designers. It starts with the identification of product features and sentiment polarities from big consumer opinion data. A Kalman filter method is then employed to forecast the trends of CRs and a Bayesian method is proposed to compare products. The objective is to help designers to understand the changes of CRs and their competitive advantages. Finally, using opinion data in Amazon.com, a case study is presented to illustrate how the proposed techniques are applied. This research is argued to incorporate an interdisciplinary collaboration between computer science and engineering design. It aims to facilitate designers by exploiting valuable information from big consumer data for market-driven product design. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",2016,International Journal of Production Research,61,big consumer data provide @ opportunity @ @ administrator to explore @ value to fulfil customer requirement @ cr @ @ generally @ @ presented a purchase record online behaviour etc @ however distinctive characteristic of big data volume variety velocity and value @ v lead to many conventional method @ customer understanding potentially fail to handle @ data @ a visible research gap @ practical significance is to develop a framework to deal @ big consumer data @ cr understanding @ accordingly a research study is conducted to exploit @ value of @ data in @ perspective of product designer @ @ start @ @ identification of product feature and sentiment polarity @ big consumer opinion data @ a kalman filter method is @ employed to forecast @ trend of cr and a bayesian method is proposed to compare product @ @ objective is to help designer to understand @ change of cr and @ competitive advantage @ finally @ opinion data in amazon @ com a case study is presented to illustrate @ @ proposed technique @ applied @ @ research is argued to incorporate @ interdisciplinary collaboration @ computer science and engineering design @ @ aim to facilitate designer by exploiting valuable information @ big consumer data @ market-driven product design @ informa uk limited trading a taylor francis group @ 
2177,A Survey of event extraction methods from text for decision support systems,"Event extraction, a specialized stream of information extraction rooted back into the 1980s, has greatly gained in popularity due to the advent of big data and the developments in the related fields of text mining and natural language processing. However, up to this date, an overview of this particular field remains elusive. Therefore, we give a summarization of event extraction techniques for textual data, distinguishing between data-driven, knowledge-driven, and hybrid methods, and present a qualitative evaluation of these. Moreover, we discuss common decision support applications of event extraction from text corpora. Last, we elaborate on the evaluation of event extraction systems and identify current research issues. © 2016 Elsevier B.V. All rights reserved.",2016,Decision Support Systems,55,event extraction a specialized stream of information extraction rooted back @ @ s ha greatly gained in popularity due to @ advent of big data and @ development in @ related field of text mining and natural language processing @ however up to @ date @ overview of @ particular field remains elusive @ therefore @ give a summarization of event extraction technique @ textual data distinguishing @ data-driven knowledge-driven and hybrid method and @ a qualitative evaluation of @ @ moreover @ discus common decision support application of event extraction @ text corpus @ last @ elaborate on @ evaluation of event extraction system and identify current research issue @ @ b @ v @ @ right reserved @ 
2179,A comparison between semi-supervised and supervised text mining techniques on detecting irony in Greek political tweets,"The present work describes a classification schema for irony detection in Greek political tweets. Our hypothesis states that humorous political tweets could predict actual election results. The irony detection concept is based on subjective perceptions, so only relying on human-annotator driven labor might not be the best route. The proposed approach relies on limited labeled training data, thus a semi-supervised approach is followed, where collective-learning algorithms take both labeled and unlabeled data into consideration. We compare the semi-supervised results with the supervised ones from a previous research of ours. The hypothesis is evaluated via a correlation study between the irony that a party receives on Twitter, its respective actual election results during the Greek parliamentary elections of May 2012, and the difference between these results and the ones of the preceding elections of 2009. © 2016 Elsevier Ltd. All rights reserved.",2016,Engineering Applications of Artificial Intelligence,34,@ @ work describes a classification schema @ irony detection in greek political tweet @ @ hypothesis state @ humorous political tweet could predict actual election @ @ @ irony detection concept is based on subjective perception @ only relying on human-annotator driven labor might not @ @ best route @ @ proposed approach relies on limited labeled training data thus a semi-supervised approach is followed @ collective-learning algorithm take @ labeled and unlabeled data @ consideration @ @ compare @ semi-supervised @ @ @ supervised @ @ a previous research of @ @ @ hypothesis is evaluated via a correlation study @ @ irony @ a party receives on twitter @ respective actual election @ @ @ greek parliamentary election of may and @ difference @ @ @ and @ @ of @ preceding election of @ @ ltd @ @ right reserved @ 
2180,A knowledge-based approach to Information Extraction for semantic interoperability in the archaeology domain,"The article presents a method for automatic semantic indexing of archaeological grey-literature reports using empirical (rule-based) Information Extraction techniques in combination with domain-specific knowledge organization systems. The semantic annotation system (OPTIMA) performs the tasks of Named Entity Recognition, Relation Extraction, Negation Detection, and Word-Sense Disambiguation using hand-crafted rules and terminological resources for associating contextual abstractions with classes of the standard ontology CIDOC Conceptual Reference Model (CRM) for cultural heritage and its archaeological extension, CRM-EH. Relation Extraction (RE) performance benefits from a syntactic-based definition of RE patterns derived from domain oriented corpus analysis. The evaluation also shows clear benefit in the use of assistive natural language processing (NLP) modules relating to Word-Sense Disambiguation, Negation Detection, and Noun Phrase Validation, together with controlled thesaurus expansion. The semantic indexing results demonstrate the capacity of rule-based Information Extraction techniques to deliver interoperable semantic abstractions (semantic annotations) with respect to the CIDOC CRM and archaeological thesauri. Major contributions include recognition of relevant entities using shallow parsing NLP techniques driven by a complimentary use of ontological and terminological domain resources and empirical derivation of context-driven RE rules for the recognition of semantic relationships from phrases of unstructured text. © 2015 ASIS&T.",2016,Journal of the Association for Information Science and Technology,12,@ article @ a method @ automatic semantic indexing of archaeological grey-literature report @ empirical @ rule-based @ information extraction technique in combination @ domain-specific knowledge organization system @ @ semantic annotation system @ optimum @ performs @ task of named entity recognition relation extraction negation detection and word-sense disambiguation @ hand-crafted rule and terminological resource @ associating contextual abstraction @ class of @ standard ontology cidoc conceptual reference model @ crm @ @ cultural heritage and @ archaeological extension crm-eh @ relation extraction @ @ @ performance benefit @ a syntactic-based definition of @ pattern derived @ domain oriented corpus analysis @ @ evaluation @ @ clear benefit in @ use of assistive natural language processing @ nlp @ module relating to word-sense disambiguation negation detection and noun phrase validation together @ controlled thesaurus expansion @ @ semantic indexing @ demonstrate @ capacity of rule-based information extraction technique to deliver interoperable semantic abstraction @ semantic annotation @ @ respect to @ cidoc crm and archaeological thesaurus @ major contribution include recognition of relevant entity @ shallow parsing nlp technique driven by a complimentary use of ontological and terminological domain resource and empirical derivation of context-driven @ rule @ @ recognition of semantic relationship @ phrase of unstructured text @ asis t @ 
2181,Management discussion and analysis in the US financial companies: A data mining analysis,"This research aims to analyse how managers react to firm’s financial conditions, in issuing the Management Discussion and Analysis (MD&A) and if MD&A content could be used to forecast firms’ future financial performance. To do so, we appeal to text mining techniques such as natural language processing and sentiment analysis. The main assumption is that the MD&A content varies depending on financial and economic conditions companies are experiencing. The study is conducted on a sample of US listed financial companies which experienced, between 1995 and 2011, different financial conditions, namely: (1) companies which filed for Chap. 11, thus having a high risk of bankruptcy; (2) companies not filing for Chap. 11, but with a medium risk of bankruptcy according to their economic and financial performance ratios; (3) companies not filing for Chap. 11, with a healthy financial situation. Empirical results reveal some interesting findings regarding the association between the bankruptcy risk levels and the content of the MD&A. This research also provides useful statistical instruments in supporting the stakeholders to investigate the reliability of the MD&As, examining the language used by the companies (effect), as response to financial conditions (cause). Text mining analysis allows to reveal some information that would otherwise remain implicit or even hidden behind complex periods and sentences. Contrary to our expectations, results suggest that companies experiencing high risk of bankruptcy use more positive words than those with medium and low bankruptcy risk. Also, findings show that companies with medium and low bankruptcy risk make a more appropriate use of positive and negative words. Moreover, we found that negative words contained in MD&A are an useful indicator to forecast a worsening of the main financial ratios. Regarding to the future researches, this study provides the starting point for analysing the role of MD&A in supporting the independent auditors’ reports. Recent studies show that audit firms often fail in predicting the bankruptcy risk of distressed companies and such an error could be due to the role of MD&A, as auditors may take it as a base for releasing their independent report. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Information Systems and Organisation,2,@ research aim to analyse @ manager react to firm s financial condition in issuing @ management discussion and analysis @ md a @ and if md a content could @ used to forecast firm future financial performance @ to @ @ @ appeal to text mining technique @ a natural language processing and sentiment analysis @ @ main assumption is @ @ md a content varies depending on financial and economic condition company @ experiencing @ @ study is conducted on a sample of u listed financial company @ experienced @ and different financial condition namely @ @ @ company @ filed @ chap @ thus @ a high risk of bankruptcy @ @ @ company not filing @ chap @ @ @ a medium risk of bankruptcy according to @ economic and financial performance ratio @ @ @ company not filing @ chap @ @ a healthy financial situation @ empirical @ reveal some interesting finding regarding @ association @ @ bankruptcy risk level and @ content of @ md a @ @ research @ provides useful statistical instrument in supporting @ stakeholder to investigate @ reliability of @ md a examining @ language used by @ company @ effect @ a response to financial condition @ cause @ @ text mining analysis allows to reveal some information @ would otherwise remain implicit @ even hidden behind complex period and sentence @ contrary to @ expectation @ suggest @ company experiencing high risk of bankruptcy use more positive word @ @ @ medium and low bankruptcy risk @ @ finding @ @ company @ medium and low bankruptcy risk make a more appropriate use of positive and negative word @ moreover @ found @ negative word contained in md a @ @ useful indicator to forecast a worsening of @ main financial ratio @ regarding to @ future research @ study provides @ starting point @ analysing @ role of md a in supporting @ independent auditor report @ recent study @ @ audit firm often fail in predicting @ bankruptcy risk of distressed company and @ @ error could @ due to @ role of md a a auditor may take @ a a base @ releasing @ independent report @ @ international publishing switzerland @ 
2182,On the naturalness of software,"Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, questionanswering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations-and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether (a) code can be usefully modeled by statistical language models and (b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very regular, and, in fact, even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area. © 2016 ACM.",2016,Communications of the ACM,27,natural language like english @ rich complex and powerful @ @ highly creative and graceful use of language like english and tamil by master like shakespeare and avvaiyar @ certainly delight and inspire @ @ in practice given cognitive constraint and @ exigency of daily life @ human utterance @ far simpler and much more repetitive and predictable @ in fact @ utterance @ @ @ usefully modeled @ modern statistical method @ @ fact ha led to @ phenomenal success of statistical approach to speech recognition natural language translation questionanswering and text mining and comprehension @ @ begin @ @ conjecture @ @ software is @ natural in @ sense @ @ is created by human at work @ @ @ attendant constraint and limitations-and thus like natural language @ is @ likely to @ repetitive and predictable @ @ @ proceed to ask whether @ a @ code @ @ usefully modeled by statistical language model and @ b @ @ model @ @ leveraged to support software engineer @ @ @ widely adopted n-gram model @ provide empirical evidence supportive of a positive answer to @ @ question @ @ @ @ code is @ @ regular and in fact even more @ @ natural language @ a @ example use of @ model @ @ developed a simple code completion engine @ java @ despite @ simplicity already improves eclipse @ s completion capability @ @ conclude @ @ by laying @ a vision @ future research in @ area @ acm @ 
2184,Multi-document summarization using closed patterns,"There are two main categories of multi-document summarization: term-based and ontology-based methods. A term-based method cannot deal with the problems of polysemy and synonymy. An ontology-based approach addresses such problems by taking into account of the semantic information of document content, but the construction of ontology requires lots of manpower. To overcome these open problems, this paper presents a pattern-based model for generic multi-document summarization, which exploits closed patterns to extract the most salient sentences from a document collection and reduce redundancy in the summary. Our method calculates the weight of each sentence of a document collection by accumulating the weights of its covering closed patterns with respect to this sentence, and iteratively selects one sentence that owns the highest weight and less similarity to the previously selected sentences, until reaching the length limitation. The sentence weight calculation by patterns reduces the dimension and captures more relevant information. Our method combines the advantages of the term-based and ontology-based models while avoiding their weaknesses. Empirical studies on the benchmark DUC2004 datasets demonstrate that our pattern-based method significantly outperforms the state-of-the-art methods. Multi-document summarization can be used to extract a particular individual's opinions in the form of closed patterns, from this individual's documents shared in social networks, hence provides a useful tool for further analyzing the individual's behavior and influence in group activities. © 2016 Elsevier Ltd. All rights reserved.",2016,Knowledge-Based Systems,38,@ @ @ main category of multi-document summarization @ term-based and ontology-based method @ a term-based method cannot deal @ @ problem of polysemy and synonymy @ @ ontology-based approach address @ problem by taking @ account of @ semantic information of document content @ @ construction of ontology requires lot of manpower @ to overcome @ open problem @ @ @ a pattern-based model @ generic multi-document summarization @ exploit closed pattern to extract @ @ salient sentence @ a document collection and reduce redundancy in @ summary @ @ method calculates @ weight of @ sentence of a document collection by accumulating @ weight of @ covering closed pattern @ respect to @ sentence and iteratively selects @ sentence @ owns @ highest weight and le similarity to @ @ selected sentence @ reaching @ length limitation @ @ sentence weight calculation by pattern reduces @ dimension and capture more relevant information @ @ method combine @ advantage of @ term-based and ontology-based model @ avoiding @ weakness @ empirical study on @ benchmark duc datasets demonstrate @ @ pattern-based method significantly outperforms @ state-of-the-art method @ multi-document summarization @ @ used to extract a particular individual @ s opinion in @ form of closed pattern @ @ individual @ s document shared in social network hence provides a useful tool @ @ analyzing @ individual @ s behavior and influence in group activity @ @ ltd @ @ right reserved @ 
2185,Recognizing emotions in text using ensemble of classifiers,"Emotions constitute a key factor in human nature and behavior. The most common way for people to express their opinions, thoughts and communicate with each other is via written text. In this paper, we present a sentiment analysis system for automatic recognition of emotions in text, using an ensemble of classifiers. The designed ensemble classifier schema is based on the notion of combining knowledge-based and statistical machine learning classification methods aiming to benefit from their merits and minimize their drawbacks. The ensemble schema is based on three classifiers; two are statistical (a Naïve Bayes and a Maximum Entropy learner) and the third one is a knowledge-based tool performing deep analysis of the natural language sentences. The knowledge-based tool analyzes the sentence's text structure and dependencies and implements a keyword-based approach, where the emotional state of a sentence is derived from the emotional affinity of the sentence's emotional parts. The ensemble classifier schema has been extensively evaluated on various forms of text such as, news headlines, articles and social media posts. The experimental results indicate quite satisfactory performance regarding the ability to recognize emotion presence in text and also to identify the polarity of the emotions. © 2016 Elsevier Ltd. All rights reserved.",2016,Engineering Applications of Artificial Intelligence,80,emotion constitute a key factor in human nature and behavior @ @ @ common way @ people to express @ opinion thought and communicate @ @ @ is via written text @ in @ @ @ @ a sentiment analysis system @ automatic recognition of emotion in text @ @ ensemble of classifier @ @ designed ensemble classifier schema is based on @ notion of combining knowledge-based and statistical machine learning classification method aiming to benefit @ @ merit and minimize @ drawback @ @ ensemble schema is based on three classifier @ @ @ statistical @ a naïve bayes and a maximum entropy learner @ and @ third @ is a knowledge-based tool performing deep analysis of @ natural language sentence @ @ knowledge-based tool analyzes @ sentence @ s text structure and dependency and implement a keyword-based approach @ @ emotional state of a sentence is derived @ @ emotional affinity of @ sentence @ s emotional part @ @ ensemble classifier schema ha @ extensively evaluated on various form of text @ a news headline article and social medium post @ @ experimental @ indicate quite satisfactory performance regarding @ ability to recognize emotion presence in text and @ to identify @ polarity of @ emotion @ @ ltd @ @ right reserved @ 
2189,Predicting political conflicts from polarized social media,"In a polarized society, rhetorical arguments are usually expressed by strong, extreme terms which by themselves carry a positive or negative sentiment in favor or against one side of a social debate or conflict. In a divided society, by detecting extreme terms in a document such as a blog post which is reflecting some political opinion, we are potentially able to automatically detect the sentiment of the text about the polarizing issue. On the other hand, during any social and political conflict in a polarized society, we can observe a shift from mainstream to extreme language or rhetoric. In this paper, a new metric called ""language gap"" is introduced to estimate the distance between mainstream and rhetoric in a social-political debate. Then we illustrate that there is a correlation between the language shift and social conflicts. In other words, the language shift can be used as a signal for predicting social conflicts in a divided society. © 2016-IOS Press and the authors. All rights reserved.",2016,Web Intelligence,0,in a polarized society rhetorical argument @ usually expressed by strong extreme term @ by @ carry a positive @ negative sentiment in favor @ @ @ side of a social debate @ conflict @ in a divided society by detecting extreme term in a document @ a a blog post @ is reflecting some political opinion @ @ potentially able to automatically detect @ sentiment of @ text @ @ polarizing issue @ on @ @ hand @ @ social and political conflict in a polarized society @ @ observe a shift @ mainstream to extreme language @ rhetoric @ in @ @ a @ metric called @ language gap @ is introduced to estimate @ distance @ mainstream and rhetoric in a social-political debate @ @ @ illustrate @ @ is a correlation @ @ language shift and social conflict @ in @ word @ language shift @ @ used a a signal @ predicting social conflict in a divided society @ io @ and @ author @ @ right reserved @ 
2199,Experimental evaluations of MapReduce in biomedical text mining,"In this paper, we demonstrate our development of two biomedical text mining applications: biomedical literature search (BLS) and biomedical association mining (BAM). While the former requires less computations, the latter is more computationally intensive. Experimental studies were conducted using Amazon Elastic MapReduce (EMR) with an input of 33,960 biomedical articles from TREC (Text REtrieval Conference) 2006 Genomics Track. Our experiment results indicated that both applications’ scalabilities were not linear in term of the number of computing nodes. Meanwhile, BAM achieved better scalability than BLS since BLS performed less computations and were primarily dominated by overheads such as JVM startup, scheduling, disk I/O, etc. These observations imply that existing MapReduce framework may not be suitable for on-line systems such as literature search that needs quick response. © Springer International Publishing Switzerland 2016.",2016,Advances in Intelligent Systems and Computing,0,in @ @ @ demonstrate @ development of @ biomedical text mining application @ biomedical literature search @ bls @ and biomedical association mining @ bam @ @ @ @ former requires le computation @ latter is more computationally intensive @ experimental study @ conducted @ amazon elastic mapreduce @ emr @ @ @ input of biomedical article @ trec @ text retrieval conference @ genomics track @ @ experiment @ indicated @ @ application scalability @ not linear in term of @ number of computing node @ meanwhile bam achieved better scalability @ bls since bls performed le computation and @ primarily dominated by overhead @ a jvm startup scheduling disk i @ etc @ @ observation imply @ existing mapreduce framework may not @ suitable @ on-line system @ a literature search @ need quick response @ @ international publishing switzerland @ 
2200,Towards High Performance Text Mining: A TextRank-based Method for Automatic Text Summarization,"As a typical unsupervised learning method, the TextRank algorithm performs well for arge-scale text mining, especially for automatic summarization or keyword extraction. However, TextRank only considers the similarities between sentences in the processes of automatic summarization and neglects information about text structure and context. To overcome these shortcomings, the authors propose an improved highly-scalable method, called iTextRank. When building a TextRank graph in their new method, the authors compute sentence similarities and adjust the weights of nodes by considering statistical and linguistic features, such as similarities in titles, paragraph structures, special sentences, sentence positions and lengths. Their analysis shows that the time complexity of iTextRank is comparable with TextRank. More importantly, two experiments show that iTextRank has a higher accuracy and lower recall rate than TextRank, and it is as effective as several popular online automatic summarization systems. © Copyright 2016, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",2016,International Journal of Grid and High Performance Computing,6,a a typical unsupervised learning method @ textrank algorithm performs well @ arge-scale text mining especially @ automatic summarization @ keyword extraction @ however textrank only considers @ similarity @ sentence in @ process of automatic summarization and neglect information @ text structure and context @ to overcome @ shortcoming @ author propose @ improved highly-scalable method called itextrank @ @ building a textrank graph in @ @ method @ author compute sentence similarity and adjust @ weight of node by considering statistical and linguistic feature @ a similarity in title paragraph structure special sentence sentence position and length @ @ analysis @ @ @ time complexity of itextrank is comparable @ textrank @ more importantly @ experiment @ @ itextrank ha a higher accuracy and lower recall rate @ textrank and @ is a effective a several popular online automatic summarization system @ @ igi global @ copying @ distributing in print @ electronic form without written permission of igi global is prohibited @ 
2201,A naïve Bayes baseline for early gesture recognition,"Early gesture/action recognition is the task of determining the identity of a gesture/action with as few information as possible. Although the topic is relatively new, there are some methods that address this problem. However, existing methods rely on complex modeling procedures, that do not necessarily paid off the computational effort. Thus, simple yet effective and efficient techniques are required for this task. This paper describes a new methodology for early gesture recognition based on the well known naïve Bayes classifier. The method is extremely simple and very fast, yet it compares favorably with more elaborated state of the art methodologies. The naïve baseline is based on three main observations: (1) the effectiveness of the naïve Bayes classifier in text mining problems; (2) the link between natural language processing and computer vision via the bag-of-words representation; and (3) the cumulative-evidence nature of the inference process of naïve Bayes. We evaluated the proposed method in several collections that included segmented and continuous video. Experimental results show that the proposed methodology compares favorably with state of the art methodologies that are more elaborated or were specifically designed for this purpose. © 2016 Elsevier B.V. All rights reserved.",2016,Pattern Recognition Letters,21,early gesture action recognition is @ task of determining @ identity of a gesture action @ a @ information a possible @ although @ topic is relatively @ @ @ some method @ address @ problem @ however existing method rely on complex modeling procedure @ @ not necessarily paid off @ computational effort @ thus simple yet effective and efficient technique @ required @ @ task @ @ @ describes a @ methodology @ early gesture recognition based on @ well known naïve bayes classifier @ @ method is extremely simple and @ fast yet @ compare favorably @ more elaborated state of @ art methodology @ @ naïve baseline is based on three main observation @ @ @ @ effectiveness of @ naïve bayes classifier in text mining problem @ @ @ @ link @ natural language processing and computer vision via @ bag-of-words representation @ and @ @ @ cumulative-evidence nature of @ inference process of naïve bayes @ @ evaluated @ proposed method in several collection @ included segmented and continuous video @ experimental @ @ @ @ proposed methodology compare favorably @ state of @ art methodology @ @ more elaborated @ @ specifically designed @ @ purpose @ @ b @ v @ @ right reserved @ 
2202,GeoSRS: A hybrid social recommender system for geolocated data,"We present GeoSRS, a hybrid recommender system for a popular location-based social network (LBSN), in which users are able to write short reviews on the places of interest they visit. Using state-of-the-art text mining techniques, our system recommends locations to users using as source the whole set of text reviews in addition to their geographical location. To evaluate our system, we have collected our own data sets by crawling the social network Foursquare. To do this efficiently, we propose the use of a parallel version of the Quadtree technique, which may be applicable to crawling/exploring other spatially distributed sources. Finally, we study the performance of GeoSRS on our collected data set and conclude that by combining sentiment analysis and text modeling, GeoSRS generates more accurate recommendations. The performance of the system improves as more reviews are available, which further motivates the use of large-scale crawling techniques such as the Quadtree. © 2015 Elsevier Ltd. All right sreserved.",2016,Information Systems,29,@ @ geosrs a hybrid recommender system @ a popular location-based social network @ lbsn @ in @ user @ able to write short review on @ place of interest @ visit @ @ state-of-the-art text mining technique @ system recommends location to user @ a source @ whole set of text review in addition to @ geographical location @ to evaluate @ system @ @ collected @ @ data set by crawling @ social network foursquare @ to @ @ efficiently @ propose @ use of a parallel version of @ quadtree technique @ may @ applicable to crawling exploring @ spatially distributed source @ finally @ study @ performance of geosrs on @ collected data set and conclude @ by combining sentiment analysis and text modeling geosrs generates more accurate recommendation @ @ performance of @ system improves a more review @ available @ @ motivates @ use of large-scale crawling technique @ a @ quadtree @ @ ltd @ @ right sreserved @ 
2203,Discovering hierarchical topic evolution in time-stamped documents,"The objective of this paper is to propose a hierarchical topic evolution model (HTEM) that can organize time-varying topics in a hierarchy and discover their evolutions with multiple timescales. In the proposed HTEM, topics near the root of the hierarchy are more abstract and also evolve in the longer timescales than those near the leaves. To achieve this goal, the distance-dependent Chinese restaurant process (ddCRP) is extended to a new nested process that is able to simultaneously model the dependencies among data and the relationship between clusters. The HTEM is proposed based on the new process for time-stamped documents, in which the timestamp is utilized to measure the dependencies among documents. Moreover, an efficient Gibbs sampler is developed for the proposed HTEM. Our experimental results on two popular real-world data sets verify that the proposed HTEM can capture coherent topics and discover their hierarchical evolutions. It also outperforms the baseline model in terms of likelihood on held-out data. © 2015 ASIS&T.",2016,Journal of the Association for Information Science and Technology,8,@ objective of @ @ is to propose a hierarchical topic evolution model @ htem @ @ @ organize time-varying topic in a hierarchy and discover @ evolution @ multiple timescales @ in @ proposed htem topic near @ root of @ hierarchy @ more abstract and @ evolve in @ longer timescales @ @ near @ leaf @ to achieve @ goal @ distance-dependent chinese restaurant process @ ddcrp @ is extended to a @ nested process @ is able to simultaneously model @ dependency among data and @ relationship @ cluster @ @ htem is proposed based on @ @ process @ time-stamped document in @ @ timestamp is utilized to measure @ dependency among document @ moreover @ efficient gibbs sampler is developed @ @ proposed htem @ @ experimental @ on @ popular real-world data set verify @ @ proposed htem @ capture coherent topic and discover @ hierarchical evolution @ @ @ outperforms @ baseline model in term of likelihood on held-out data @ asis t @ 
2205,A Novel Hybrid Text Summarization System for Punjabi Text,"Text summarization is the task of shortening text documents but retaining their overall meaning and information. A good summary should highlight the main concepts of any text document. Many statistical-based, location-based and linguistic-based techniques are available for text summarization. This paper has described a novel hybrid technique for automatic summarization of Punjabi text. Punjabi is an official language of Punjab State in India. There are very few linguistic resources available for Punjabi. The proposed summarization system is hybrid of conceptual-, statistical-, location- and linguistic-based features for Punjabi text. In this system, four new location-based features and two new statistical features (entropy measure and Z score) are used and results are very much encouraging. Support vector machine-based classifier is also used to classify Punjabi sentences into summary and non-summary sentences and to handle imbalanced data. Synthetic minority over-sampling technique is applied for over-sampling minority class data. Results of proposed system are compared with different baseline systems, and it is found that F score, Precision, Recall and ROUGE-2 score of our system are reasonably well as compared to other baseline systems. Moreover, summary quality of proposed system is comparable to the gold summary. © 2015, Springer Science+Business Media New York.",2016,Cognitive Computation,12,text summarization is @ task of shortening text document @ retaining @ overall meaning and information @ a good summary @ highlight @ main concept of @ text document @ many statistical-based location-based and linguistic-based technique @ available @ text summarization @ @ @ ha described a novel hybrid technique @ automatic summarization of punjabi text @ punjabi is @ official language of punjab state in india @ @ @ @ @ linguistic resource available @ punjabi @ @ proposed summarization system is hybrid of conceptual statistical location and linguistic-based feature @ punjabi text @ in @ system four @ location-based feature and @ @ statistical feature @ entropy measure and z score @ @ used and @ @ @ much encouraging @ support vector machine-based classifier is @ used to classify punjabi sentence @ summary and non-summary sentence and to handle imbalanced data @ synthetic minority over-sampling technique is applied @ over-sampling minority class data @ @ of proposed system @ compared @ different baseline system and @ is found @ f score precision recall and rouge score of @ system @ reasonably well a compared to @ baseline system @ moreover summary quality of proposed system is comparable to @ gold summary @ @ science @ medium @ york @ 
2206,Biomedical term extraction: overview and a new methodology,"Terminology extraction is an essential task in domain knowledge acquisition, as well as for information retrieval. It is also a mandatory first step aimed at building/enriching terminologies and ontologies. As often proposed in the literature, existing terminology extraction methods feature linguistic and statistical aspects and solve some problems related (but not completely) to term extraction, e.g. noise, silence, low frequency, large-corpora, complexity of the multi-word term extraction process. In contrast, we propose a cutting edge methodology to extract and to rank biomedical terms, covering all the mentioned problems. This methodology offers several measures based on linguistic, statistical, graphic and web aspects. These measures extract and rank candidate terms with excellent precision: we demonstrate that they outperform previously reported precision results for automatic term extraction, and work with different languages (English, French, and Spanish). We also demonstrate how the use of graphs and the web to assess the significance of a term candidate, enables us to outperform precision results. We evaluated our methodology on the biomedical GENIA and LabTestsOnline corpora and compared it with previously reported measures. © 2015, Springer Science+Business Media New York.",2016,Information Retrieval,38,terminology extraction is @ essential task in domain knowledge acquisition a well a @ information retrieval @ @ is @ a mandatory first step aimed at building enriching terminology and ontology @ a often proposed in @ literature existing terminology extraction method feature linguistic and statistical aspect and solve some problem related @ @ not completely @ to term extraction e @ g @ noise silence low frequency large-corpora complexity of @ multi-word term extraction process @ in contrast @ propose a cutting edge methodology to extract and to rank biomedical term covering @ @ mentioned problem @ @ methodology offer several measure based on linguistic statistical graphic and web aspect @ @ measure extract and rank candidate term @ excellent precision @ @ demonstrate @ @ outperform @ reported precision @ @ automatic term extraction and work @ different language @ english french and spanish @ @ @ @ demonstrate @ @ use of graph and @ web to ass @ significance of a term candidate enables u to outperform precision @ @ @ evaluated @ methodology on @ biomedical genia and labtestsonline corpus and compared @ @ @ reported measure @ @ science @ medium @ york @ 
2208,A process-centered knowledge model for analysis of technology innovation procedures,"Now, there are prodigiously expanding worldwide economic networks in the information society, which require their social structural changes through technology innovations. This paper so tries to formally define a process-centered knowledge model to be used to analyze policy-making procedures on technology innovations. The eventual goal of the proposed knowledge model is to apply itself to analyze a topic network based upon composite keywords from a document written in a natural language format during the technology innovation procedures. Knowledge model is created to topic network that compositing driven keyword through text mining from natural language in document. And we show that the way of analyzing knowledge model and automatically generating feature keyword and relation properties into topic networks. © 2016 KSII.",2016,KSII Transactions on Internet and Information Systems,2,now @ @ prodigiously expanding worldwide economic network in @ information society @ require @ social structural change @ technology innovation @ @ @ @ try to formally define a process-centered knowledge model to @ used to analyze policy-making procedure on technology innovation @ @ eventual goal of @ proposed knowledge model is to apply @ to analyze a topic network based upon composite keywords @ a document written in a natural language format @ @ technology innovation procedure @ knowledge model is created to topic network @ compositing driven keyword @ text mining @ natural language in document @ and @ @ @ @ way of analyzing knowledge model and automatically generating feature keyword and relation property @ topic network @ ksii @ 
2217,WSGM-SD: An approach to RESTful service discovery based on weighted service goal model,"Faced with the rapidly increasing Web services, it becomes a challenging issue for users to effectively and accurately discover and reuse services. Existing service discovery approaches are mainly developed for services with WSDL documents, while only a few attention is being paid to services described in natural language, especially to the RESTful services. Towards this issue, we introduce a Weighted service goal model (WSGM) by measuring the weights of service goals extracted from the textual descriptions of services. Based on the WSGM, a novel service discovery approach called Service discovery based on WSGM (WSGM-SD) is proposed. Experiments on Programmable Web, a public Web service repository, demonstrate the effectiveness of the proposed approach. © 2016 Chinese Institute of Electronics.",2016,Chinese Journal of Electronics,4,faced @ @ rapidly increasing web service @ becomes a challenging issue @ user to effectively and accurately discover and reuse service @ existing service discovery approach @ mainly developed @ service @ wsdl document @ only a @ attention is @ paid to service described in natural language especially to @ restful service @ towards @ issue @ introduce a weighted service goal model @ wsgm @ by measuring @ weight of service goal extracted @ @ textual description of service @ based on @ wsgm a novel service discovery approach called service discovery based on wsgm @ wsgm-sd @ is proposed @ experiment on programmable web a public web service repository demonstrate @ effectiveness of @ proposed approach @ chinese institute of electronics @ 
2226,Survey on Aspect-Level Sentiment Analysis,"The field of sentiment analysis, in which sentiment is gathered, analyzed, and aggregated from text, has seen a lot of attention in the last few years. The corresponding growth of the field has resulted in the emergence of various subareas, each addressing a different level of analysis or research question. This survey focuses on aspect-level sentiment analysis, where the goal is to find and aggregate sentiment on entities mentioned within documents or aspects of them. An in-depth overview of the current state-of-the-art is given, showing the tremendous progress that has already been made in finding both the target, which can be an entity as such, or some aspect of it, and the corresponding sentiment. Aspect-level sentiment analysis yields very fine-grained sentiment information which can be useful for applications in various domains. Current solutions are categorized based on whether they provide a method for aspect detection, sentiment analysis, or both. Furthermore, a breakdown based on the type of algorithm used is provided. For each discussed study, the reported performance is included. To facilitate the quantitative evaluation of the various proposed methods, a call is made for the standardization of the evaluation methodology that includes the use of shared data sets. Semantically-rich concept-centric aspect-level sentiment analysis is discussed and identified as one of the most promising future research direction. © 1989-2012 IEEE.",2016,IEEE Transactions on Knowledge and Data Engineering,266,@ field of sentiment analysis in @ sentiment is gathered analyzed and aggregated @ text ha seen a lot of attention in @ last @ year @ @ corresponding growth of @ field ha resulted in @ emergence of various subareas @ addressing a different level of analysis @ research question @ @ survey focus on aspect-level sentiment analysis @ @ goal is to find and aggregate sentiment on entity mentioned within document @ aspect of @ @ @ in-depth overview of @ current state-of-the-art is given showing @ tremendous progress @ ha already @ made in finding @ @ target @ @ @ @ entity a @ @ some aspect of @ and @ corresponding sentiment @ aspect-level sentiment analysis yield @ fine-grained sentiment information @ @ @ useful @ application in various domain @ current solution @ categorized based on whether @ provide a method @ aspect detection sentiment analysis @ @ @ furthermore a breakdown based on @ type of algorithm used is provided @ @ @ discussed study @ reported performance is included @ to facilitate @ quantitative evaluation of @ various proposed method a call is made @ @ standardization of @ evaluation methodology @ includes @ use of shared data set @ semantically-rich concept-centric aspect-level sentiment analysis is discussed and identified a @ of @ @ promising future research direction @ @ @ 
2227,Combining Sentiment Analysis with Socialization Bias in Social Networks for Stock Market Trend Prediction,"According to the indirect relationship between information and stock trend, information such as comments and tweets can be used for stock trend prediction. When conducting classification on text data, feature sparse issues occur during conversion between tweets and word vectors. Another problem is that the unreliability of average sentiment scores to indicate one day's sentiment. This is especially caused by the unbalanced number between positive and negative within one day, thus a large bias between sentiment and stock trend arises. In addion, information has social attributes when created and diffused in social networks, bias containing people's belief in social networks also have become socialization bias. In order to solve those problems, this work proposes a sentiment analysis based prediction model and an inverse bias algorithm. Instead of applying sentiment analysis to add sentiment related features, this work uses SentiWordNet to give an additional weight to the selected features, and applies two kinds of sentiment analysis to inverse the socialization bias. Aiming at labeling the tweets to sentiment related groups to help find socialization bias, this work also proposes an extended wordlist based on a semi-supervised Naïve Bayes classification algorithm. After finishing the inverse socialization bias, stock trends were used to label example sets. Different classification algorithms were compared in this work. The proposed model with SVM linear algorithm proves to yield accuracy of 90.33% at its best performance. © 2016 World Scientific Publishing Company.",2016,International Journal of Computational Intelligence and Applications,7,according to @ indirect relationship @ information and stock trend information @ a comment and tweet @ @ used @ stock trend prediction @ @ conducting classification on text data feature sparse issue occur @ conversion @ tweet and word vector @ another problem is @ @ unreliability of average sentiment score to indicate @ day @ s sentiment @ @ is especially caused by @ unbalanced number @ positive and negative within @ day thus a @ bias @ sentiment and stock trend arises @ in addion information ha social attribute @ created and diffused in social network bias containing people @ s belief in social network @ @ become socialization bias @ in order to solve @ problem @ work proposes a sentiment analysis based prediction model and @ inverse bias algorithm @ instead of applying sentiment analysis to add sentiment related feature @ work us sentiwordnet to give @ additional weight to @ selected feature and applies @ kind of sentiment analysis to inverse @ socialization bias @ aiming at labeling @ tweet to sentiment related group to help find socialization bias @ work @ proposes @ extended wordlist based on a semi-supervised naïve bayes classification algorithm @ @ finishing @ inverse socialization bias stock trend @ used to label example set @ different classification algorithm @ compared in @ work @ @ proposed model @ svm linear algorithm prof to yield accuracy of @ at @ best performance @ world scientific publishing company @ 
2228,"Data mining for building knowledge bases: Techniques, architectures and applications","Data mining techniques for extracting knowledge from text have been applied extensively to applications including question answering, document summarisation, event extraction and trend monitoring. However, current methods have mainly been tested on small-scale customised data sets for specific purposes. The availability of large volumes of data and high-velocity data streams (such as social media feeds) motivates the need to automatically extract knowledge from such data sources and to generalise existing approaches to more practical applications. Recently, several architectures have been proposed for what we call knowledge mining: integrating data mining for knowledge extraction from unstructured text (possibly making use of a knowledge base), and at the same time, consistently incorporating this new information into the knowledge base. After describing a number of existing knowledge mining systems, we review the state-of-the-art literature on both current text mining methods (emphasising stream mining) and techniques for the construction and maintenance of knowledge bases. In particular, we focus on mining entities and relations from unstructured text data sources, entity disambiguation, entity linking and question answering. We conclude by highlighting general trends in knowledge mining research and identifying problems that require further research to enable more extensive use of knowledge bases. © 2016 Cambridge University Press.",2016,Knowledge Engineering Review,5,data mining technique @ extracting knowledge @ text @ @ applied extensively to application including question answering document summarisation event extraction and trend monitoring @ however current method @ mainly @ tested on small-scale customised data set @ specific purpose @ @ availability of @ volume of data and high-velocity data stream @ @ a social medium feed @ motivates @ need to automatically extract knowledge @ @ data source and to generalise existing approach to more practical application @ recently several architecture @ @ proposed @ @ @ call knowledge mining @ integrating data mining @ knowledge extraction @ unstructured text @ possibly making use of a knowledge base @ and at @ @ time consistently incorporating @ @ information @ @ knowledge base @ @ describing a number of existing knowledge mining system @ review @ state-of-the-art literature on @ current text mining method @ emphasising stream mining @ and technique @ @ construction and maintenance of knowledge base @ in particular @ focus on mining entity and relation @ unstructured text data source entity disambiguation entity linking and question answering @ @ conclude by highlighting general trend in knowledge mining research and identifying problem @ require @ research to enable more extensive use of knowledge base @ cambridge university @ @ 
2240,Data and text mining techniques for classifying Arabic tweet polarity,"Sentiment analysis is a new task related to text mining that extracts opinions from textual data and classifies them into positive, negative or neutral. The goal of this paper is to determine the effect of applying stemming and n-gram techniques for Arabic texts (tweets) on sentiment classification. This study also aims at investigating the impact of feature selection on the performance of the classifier. For this reason, three classifiers Support Vector Machines (SVM), Naïve Bayes, (NB), and K-nearest neighbor (KNN) are used. The obtained results showed that the best results of performance are obtained when applying a hybrid representation which includes tokens with character 3-grams. The experiment results also revealed that the use of feature selection technique improves significantly the accuracy of the three classifiers for the task of opinion classification. Regarding The classifiers, SVM outperforms the other classifiers when using all the features, while when selecting the most relevant features by the SVM feature selection technique, SVM and NB provided the best results. © 2016, Digital Information Research Foundation. All rights reserved.",2016,Journal of Digital Information Management,18,sentiment analysis is a @ task related to text mining @ extract opinion @ textual data and classifies @ @ positive negative @ neutral @ @ goal of @ @ is to determine @ effect of applying stemming and n-gram technique @ arabic text @ tweet @ on sentiment classification @ @ study @ aim at investigating @ impact of feature selection on @ performance of @ classifier @ @ @ reason three classifier support vector machine @ svm @ naïve bayes @ nb @ and k-nearest neighbor @ knn @ @ used @ @ obtained @ showed @ @ best @ of performance @ obtained @ applying a hybrid representation @ includes token @ character gram @ @ experiment @ @ revealed @ @ use of feature selection technique improves significantly @ accuracy of @ three classifier @ @ task of opinion classification @ regarding @ classifier svm outperforms @ @ classifier @ @ @ @ feature @ @ selecting @ @ relevant feature by @ svm feature selection technique svm and nb provided @ best @ @ digital information research foundation @ @ right reserved @ 
2243,Learning semantic representation with neural networks for community question answering retrieval,"In community question answering (cQA), users pose queries (or questions) on portals like Yahoo! Answers which can then be answered by other users who are often knowledgeable on the subject. cQA is increasingly popular on the Web, due to its convenience and effectiveness in connecting users with queries and those with answers. In this article, we study the problem of finding previous queries (e.g., posed by other users) which may be similar to new queries, and adapting their answers as the answers to the new queries. A key challenge here is to the bridge the lexical gap between new queries and old answers. For example, ""company"" in the queries may correspond to ""firm"" in the answers. To address this challenge, past research has proposed techniques similar to machine translation that ""translate"" old answers to ones using the words in the new queries. However, a key limitation of these works is that they assume queries and answers are parallel texts, which is hardly true in reality. As a result, the translated or rephrased answers may not look intuitive. In this article, we propose a novel approach to learn the semantic representation of queries and answers by using a neural network architecture. The learned semantic level features are finally incorporated into a learning to rank framework. We have evaluated our approach using a large-scale data set. Results show that the approach can significantly outperform existing approaches. © 2015 Elsevier B.V. All rights reserved.",2016,Knowledge-Based Systems,50,in community question answering @ cqa @ user pose query @ @ question @ on portal like yahoo @ answer @ @ @ @ answered by @ user @ @ often knowledgeable on @ subject @ cqa is increasingly popular on @ web due to @ convenience and effectiveness in connecting user @ query and @ @ answer @ in @ article @ study @ problem of finding previous query @ e @ g @ posed by @ user @ @ may @ similar to @ query and adapting @ answer a @ answer to @ @ query @ a key challenge @ is to @ bridge @ lexical gap @ @ query and old answer @ @ example @ company @ in @ query may correspond to @ firm @ in @ answer @ to address @ challenge past research ha proposed technique similar to machine translation @ @ translate @ old answer to @ @ @ word in @ @ query @ however a key limitation of @ work is @ @ assume query and answer @ parallel text @ is hardly true in reality @ a a @ @ translated @ rephrased answer may not look intuitive @ in @ article @ propose a novel approach to learn @ semantic representation of query and answer by @ a neural network architecture @ @ learned semantic level feature @ finally incorporated @ a learning to rank framework @ @ @ evaluated @ approach @ a large-scale data set @ @ @ @ @ approach @ significantly outperform existing approach @ @ b @ v @ @ right reserved @ 
2244,SentiMI: Introducing point-wise mutual information with SentiWordNet to improve sentiment polarity detection,"Supervised learning has attracted much attention in recent years. As a consequence, many of the state-of-the-art algorithms are domain dependent as they require a labeled training corpus to learn the domain features. This requires the availability of labeled corpora which is a cumbersome task in itself. However, for text sentiment detection SentiWordNet (SWN) may be used. It is a vocabulary where terms are arranged in synonym groups called synsets. This research makes use of SentiWordNet and treats it as the labeled corpus for training. A sentiment dictionary, SentiMI, builds upon the mutual information calculated from these terms. A complete framework is developed by using feature selection and extracting mutual information, from SentiMI, for the selected features. Training, testing and evaluation of the proposed framework are conducted on a large dataset of 50,000 movie reviews. A notable performance improvement of 7% in accuracy, 14% in specificity, and 8% in F-measure is achieved by the proposed framework as compared to the baseline SentiWordNet classifier. Comparison with the state-of-the-art classifiers is also performed on widely used Cornell Movie Review dataset which also proves the effectiveness of the proposed approach. © 2015 Elsevier B.V.",2016,Applied Soft Computing Journal,58,supervised learning ha attracted much attention in recent year @ a a consequence many of @ state-of-the-art algorithm @ domain dependent a @ require a labeled training corpus to learn @ domain feature @ @ requires @ availability of labeled corpus @ is a cumbersome task in @ @ however @ text sentiment detection sentiwordnet @ swn @ may @ used @ @ is a vocabulary @ term @ arranged in synonym group called synset @ @ research make use of sentiwordnet and treat @ a @ labeled corpus @ training @ a sentiment dictionary sentimi build upon @ mutual information calculated @ @ term @ a complete framework is developed by @ feature selection and extracting mutual information @ sentimi @ @ selected feature @ training testing and evaluation of @ proposed framework @ conducted on a @ dataset of movie review @ a notable performance improvement of in accuracy in specificity and in f-measure is achieved by @ proposed framework a compared to @ baseline sentiwordnet classifier @ comparison @ @ state-of-the-art classifier is @ performed on widely used cornell movie review dataset @ @ prof @ effectiveness of @ proposed approach @ @ b @ v @ 
2252,Event causality extraction based on connectives analysis,"Causality is an important type of relation which is crucial in numerous tasks, such as predicting future events, generating scenario, question answering, textual entailment and discourse comprehension. Therefore, causality extraction is a fundamental task in text mining. Many efforts have been dedicated to extracting causality from texts utilizing patterns, constraints and machine learning techniques. This paper presents a new Restricted Hidden Naive Bayes model to extract causality from texts. Besides some commonly used features, such as contextual features, syntactic features, position features, we also utilize a new category feature of causal connectives. This new feature is obtained from the tree kernel similarity of sentences containing connectives. In previous studies, the features have been usually assumed to be independent, which is not the case in reality. The advantage of our model lies in its ability to cope with partial interactions among features so as to avoid over-fitting problem on Hidden Naive Bayes model, especially the interaction between the connective category and the syntactic structure of sentences. Evaluation on a public dataset shows that our method goes beyond all the baselines. © 2015.",2016,Neurocomputing,33,causality is @ important type of relation @ is crucial in numerous task @ a predicting future event generating scenario question answering textual entailment and discourse comprehension @ therefore causality extraction is a fundamental task in text mining @ many effort @ @ dedicated to extracting causality @ text utilizing pattern constraint and machine learning technique @ @ @ @ a @ restricted hidden naive bayes model to extract causality @ text @ besides some commonly used feature @ a contextual feature syntactic feature position feature @ @ utilize a @ category feature of causal connective @ @ @ feature is obtained @ @ tree kernel similarity of sentence containing connective @ in previous study @ feature @ @ usually assumed to @ independent @ is not @ case in reality @ @ advantage of @ model lie in @ ability to cope @ partial interaction among feature @ a to avoid over-fitting problem on hidden naive bayes model especially @ interaction @ @ connective category and @ syntactic structure of sentence @ evaluation on a public dataset @ @ @ method go beyond @ @ baseline @ @ 
2268,Infinite author topic model based on mixed gamma-negative binomial process,"Incorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditionaltext mining models has gained significant interests in the area of information retrieval, statistical natural language processing, andmachine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors'sinterests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) modelto resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determinethe number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in orderto capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma processthat accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm witheach conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets showthe capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topicssimultaneously. © 2015 IEEE.",2016,"Proceedings - IEEE International Conference on Data Mining, ICDM",9,incorporating @ side information of text corpus i @ e @ author time stamp and emotional tag @ @ traditionaltext mining model ha gained significant interest in @ area of information retrieval statistical natural language processing andmachine learning @ @ branch of @ work is @ so-called author topic model @ atm @ @ incorporates @ author @ sinterests a side information @ @ classical topic model @ however @ existing atm need to predefine @ number of topic @ is difficult and inappropriate in many real-world setting @ in @ @ @ propose @ infinite author topic @ iat @ modelto resolve @ issue @ instead of assigning a discrete probability on fixed number of topic @ use a stochastic process to determinethe number of topic @ @ data @ @ to @ specific @ extend a gamma-negative binomial process to three level in orderto capture @ author-document-keyword hierarchical structure @ furthermore @ document is assigned a mixed gamma processthat account @ @ multi-author @ s contribution towards @ document @ @ efficient gibbs sampling inference algorithm witheach conditional distribution @ closed-form is developed @ @ iat model @ experiment on several real-world datasets showthe capability of @ iat model to learn @ hidden topic author @ interest on @ topic and @ number of topicssimultaneously @ @ @ 
2271,Using text mining and natural language processing to support business decision: Towards a NooJ application,"Decision-making process has become extremely difficult especially for the large amount of textual data that companies must analyse to be competitive. The use of Natural Language Processing and Text mining in data discovery allows extracting knowledge from business texts that in the majority occur in unstructured form. The Decision Support System and the Information Technology departments face the new challenges that change poses, relying on linguistic analysis capabilities, no longer based on keyword research but on the syntactic properties, lexical and semantic word. In this paper, we focused on document-driven decision support, describing ways in which business communication performance can be improved by using a natural language interface as NooJ. In order to achieve our goals, we developed Linguistic Resources typically used in Economy knowledge domain, with regard to compound words and multiword atomic linguistic units (MWALUs). © Springer International Publishing AG 2016.",2016,Communications in Computer and Information Science,1,decision-making process ha become extremely difficult especially @ @ @ amount of textual data @ company must analyse to @ competitive @ @ use of natural language processing and text mining in data discovery allows extracting knowledge @ @ text @ in @ majority occur in unstructured form @ @ decision support system and @ information technology department face @ @ challenge @ change pose relying on linguistic analysis capability no longer based on keyword research @ on @ syntactic property lexical and semantic word @ in @ @ @ focused on document-driven decision support describing way in @ @ communication performance @ @ improved by @ a natural language interface a nooj @ in order to achieve @ goal @ developed linguistic resource typically used in economy knowledge domain @ regard to compound word and multiword atomic linguistic unit @ mwalus @ @ @ international publishing ag @ 
2274,Enhancing information accessibility of scientific publications with text mining and ontology,"We present an ongoing effort on utilizing text mining methods and existing biological ontologies to help readers to access the information contained in the scientific articles. Our approach includes using multiple strategies for biological entity detection and using association analysis on extracted analysis. The entity extraction processes utilizes regular expression rules, ontologies, and keyword dictionary to get a comprehensive list of biological entities. In addition to extract list of entities, we also apply natural language processing and association analysis techniques to generate inferences among entities and comparing to known relations documented in the existing ontologies. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,1,@ @ @ ongoing effort on utilizing text mining method and existing biological ontology to help reader to access @ information contained in @ scientific article @ @ approach includes @ multiple strategy @ biological entity detection and @ association analysis on extracted analysis @ @ entity extraction process utilizes regular expression rule ontology and keyword dictionary to get a comprehensive list of biological entity @ in addition to extract list of entity @ @ apply natural language processing and association analysis technique to generate inference among entity and comparing to known relation documented in @ existing ontology @ ceur-ws @ @ right reserved @ 
2276,Development of a machine learning framework for biomedical text mining,"Biomedical text mining (BTM) aims to create methods for searching and structuring knowledge extracted from biomedical literature. Named entity recognition (NER), a BTM task, seeks to identify mentions to biological entities in texts. Dictionaries, regular expressions, natural language processing and machine learning (ML) algorithms are used in this task. Over the last years, @Note2, an open-source software framework, which includes user-friendly interfaces for important tasks in BTM, has been developed, but it did not include ML-based methods. In this work, the development of a framework, BioTML, including a number of ML-based approaches for NER is proposed, to fill the gap between @Note2 and state-of-the-art ML approaches. BioTML was integrated in @Note2 as a novel plug-in, where Hidden Markov Models, Conditional Random Fields and Support Vector Machines were implemented to address NER tasks, working with a set of over 60 feature types used to train ML models. The implementation was supported in open-source software, such as MALLET, LibSVM, ClearNLP or OpenNLP. Several manually annotated corpora were used in the validation of BioTML. The results are promising, while there is room for improvement. © Springer International Publishing Switzerland 2016.",2016,Advances in Intelligent Systems and Computing,1,biomedical text mining @ btm @ aim to create method @ searching and structuring knowledge extracted @ biomedical literature @ named entity recognition @ ner @ a btm task seek to identify mention to biological entity in text @ dictionary regular expression natural language processing and machine learning @ ml @ algorithm @ used in @ task @ @ @ last year note @ open-source software framework @ includes user-friendly interface @ important task in btm ha @ developed @ @ @ not include ml-based method @ in @ work @ development of a framework biotml including a number of ml-based approach @ ner is proposed to fill @ gap @ note and state-of-the-art ml approach @ biotml wa integrated in note a a novel plug-in @ hidden markov model conditional random field and support vector machine @ implemented to address ner task working @ a set of @ feature type used to train ml model @ @ implementation wa supported in open-source software @ a mallet libsvm clearnlp @ opennlp @ several manually annotated corpus @ used in @ validation of biotml @ @ @ @ promising @ @ is room @ improvement @ @ international publishing switzerland @ 
2279,Analyzing unstructured Facebook social network data through web text mining: A study of online shopping firms in Turkey,"The large amounts of Facebook social network data which are generated and collected need to be analyzed for valuable decision making information about shopping firms in Turkey. In addition, analyzing social network data from outside the firms becomes a critical business need for the firms which actively use Facebook. To have a competitive advantage, firms must translate social media texts into something more quantitative to extract information. In this study, web text mining techniques are used to determine popular online shopping firms’ Facebook patterns. For this purpose, 200 popular Turkish companies’ web URLs are used. Web text mining through natural language processing techniques is examined. Similarity analysis and clustering are done. Consequently, the clusters of the Facebook websites and their relationships and similarities of the firms are obtained. © 2014, The Author(s) 2014.",2016,Information Development,6,@ @ amount of facebook social network data @ @ generated and collected need to @ analyzed @ valuable decision making information @ shopping firm in turkey @ in addition analyzing social network data @ outside @ firm becomes a critical @ need @ @ firm @ actively use facebook @ to @ a competitive advantage firm must translate social medium text @ something more quantitative to extract information @ in @ study web text mining technique @ used to determine popular online shopping firm facebook pattern @ @ @ purpose popular turkish company web url @ used @ web text mining @ natural language processing technique is examined @ similarity analysis and clustering @ done @ consequently @ cluster of @ facebook website and @ relationship and similarity of @ firm @ obtained @ @ author @ s @ @ 
2280,Text mining of related events from natural science literature,"We present an approach to text mining in areas where the entities of interest can not be defined in advance. Our system is aimed at finding related events in natural science literature, in particular, changing/increasing/decreasing variables in Marine science publications. It enables semantic search for events by abstracting from morphological, lexical-semantic and syntactic variations. In addition, generalisation of variables through syntactic pruning helps finding similar variables. Relations between events are induced from co-occurrence frequencies. Extracted information is stored in a property graph database and accessed using the Cypher query language. A user interface presents events as a graph to visualise their type, frequency and relation strength, in combination with their textual sources. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ approach to text mining in area @ @ entity of interest @ not @ defined in advance @ @ system is aimed at finding related event in natural science literature in particular changing increasing decreasing variable in marine science publication @ @ enables semantic search @ event by abstracting @ morphological lexical-semantic and syntactic variation @ in addition generalisation of variable @ syntactic pruning help finding similar variable @ relation @ event @ induced @ co-occurrence frequency @ extracted information is stored in a property graph database and accessed @ @ cypher query language @ a user interface @ event a a graph to visualise @ type frequency and relation strength in combination @ @ textual source @ @ international publishing ag @ 
2284,Text mining for information systems researchers: An annotated topic modeling tutorial,"Analysts have estimated that more than 80 percent of today’s data is stored in unstructured form (e.g., text, audio, image, video)—much of it expressed in rich and ambiguous natural language. Traditionally, to analyze natural language, one has used qualitative data-analysis approaches, such as manual coding. Yet, the size of text data sets obtained from the Internet makes manual analysis virtually impossible. In this tutorial, we discuss the challenges encountered when applying automated text-mining techniques in information systems research. In particular, we showcase how to use probabilistic topic modeling via Latent Dirichlet allocation, an unsupervised text-mining technique, with a LASSO multinomial logistic regression to explain user satisfaction with an IT artifact by automatically analyzing more than 12,000 online customer reviews. For fellow information systems researchers, this tutorial provides guidance for conducting text-mining studies on their own and for evaluating the quality of others. © 2016 by the Association for Information Systems.",2016,Communications of the Association for Information Systems,78,analyst @ estimated @ more @ percent of today s data is stored in unstructured form @ e @ g @ text audio image video @ much of @ expressed in rich and ambiguous natural language @ traditionally to analyze natural language @ ha used qualitative data-analysis approach @ a manual coding @ yet @ size of text data set obtained @ @ internet make manual analysis virtually impossible @ in @ tutorial @ discus @ challenge encountered @ applying automated text-mining technique in information system research @ in particular @ showcase @ to use probabilistic topic modeling via latent dirichlet allocation @ unsupervised text-mining technique @ a lasso multinomial logistic regression to explain user satisfaction @ @ @ artifact by automatically analyzing more @ online customer review @ @ fellow information system researcher @ tutorial provides guidance @ conducting text-mining study on @ @ and @ evaluating @ quality of others @ by @ association @ information system @ 
2286,Construction of a biodiversity knowledge repository using a text mining-based framework,"In our aim to make the information encapsulated by biodiversity literature more accessible and searchable, we have developed a text mining-based framework for automatically transforming text into a structured knowledge repository. A text mining workflow employing information extraction techniques, i.e., named entity recognition and relation extraction, was implemented in the Argo platform and was subsequently applied on biodiversity literature to extract structured information. The resulting annotations were stored in a repository following the emerging Open Annotation standard, thus promoting interoperability with external applications. Accessible as a SPARQL endpoint, the repository supports knowledge discovery over a huge amount of biodiversity literature by retrieving annotations matching user-specified queries.",2016,CEUR Workshop Proceedings,1,in @ aim to make @ information encapsulated by biodiversity literature more accessible and searchable @ @ developed a text mining-based framework @ automatically transforming text @ a structured knowledge repository @ a text mining workflow employing information extraction technique i @ e @ named entity recognition and relation extraction wa implemented in @ argo platform and wa subsequently applied on biodiversity literature to extract structured information @ @ resulting annotation @ stored in a repository following @ emerging open annotation standard thus promoting interoperability @ external application @ accessible a a sparql endpoint @ repository support knowledge discovery @ a huge amount of biodiversity literature by retrieving annotation matching user-specified query @ 
2287,A new big data framework for customer opinions polarity extraction,"Recently, we are talking about opinion mining: It refers to extract subjective information from text data using the natural language processing, text analysis and computational linguistics. Micro-blogging is one of the most popular Web 2.0 applications, such as Twitter which is evolved into a practical means for sharing opinions around different topics. It becomes a rich data sources for opinion mining and sentiment analysis. In this work, we interest by to study users opinions about an object in social networks, for example studying the opinion of users about “the Samsung brand” or “the nokia brand”, using text mining and NLP (Natural language processing) technologies. We propose a new ontological approach able to determinate the polarity of user post. This approach classify the users posts to negative, positive or neutral opinions. To validate the effectiveness of our approach, we used a dataset published by Bing Liu’s group in our approach experimentation. © Springer International Publishing Switzerland 2016.",2016,Communications in Computer and Information Science,4,recently @ @ talking @ opinion mining @ @ refers to extract subjective information @ text data @ @ natural language processing text analysis and computational linguistics @ micro-blogging is @ of @ @ popular web @ application @ a twitter @ is evolved @ a practical mean @ sharing opinion around different topic @ @ becomes a rich data source @ opinion mining and sentiment analysis @ in @ work @ interest by to study user opinion @ @ object in social network @ example studying @ opinion of user @ @ samsung brand @ @ nokia brand @ text mining and nlp @ natural language processing @ technology @ @ propose a @ ontological approach able to determinate @ polarity of user post @ @ approach classify @ user post to negative positive @ neutral opinion @ to validate @ effectiveness of @ approach @ used a dataset published by bing liu s group in @ approach experimentation @ @ international publishing switzerland @ 
2289,Information extraction for personalised services based on conference alerts,"Text mining is moderately new research area at the interaction of data mining, natural language processing (NLP), machine learning and information retrieval. The interconnected task, information extraction is a text transforming that places a specified set of significant items in a natural-language document. It distils organised data or knowledge from unstructured text by recognising references to named entities and additionally expressed relationships between such entities. We present a new schema for text mining as information extraction for prediction, which uses a learn information extraction system to transform text into more structures data which is then be further analysed or mine for discovering more general patterns and interesting relationships. This paper presents the work obtained by applying information extraction (IE) technique to a corpus of conference announcement posted on conference web newsgroups. The work is analysis of extracted essential name entities that were used to find the patterns of recent trends in research area and it also provide a platform to explore more on NLP aspects. © 2016 Inderscience Enterprises Ltd.",2016,"International Journal of Data Mining, Modelling and Management",1,text mining is moderately @ research area at @ interaction of data mining natural language processing @ nlp @ machine learning and information retrieval @ @ interconnected task information extraction is a text transforming @ place a specified set of significant item in a natural-language document @ @ distils organised data @ knowledge @ unstructured text by recognising reference to named entity and additionally expressed relationship @ @ entity @ @ @ a @ schema @ text mining a information extraction @ prediction @ us a learn information extraction system to transform text @ more structure data @ is @ @ @ analysed @ mine @ discovering more general pattern and interesting relationship @ @ @ @ @ work obtained by applying information extraction @ ie @ technique to a corpus of conference announcement posted on conference web newsgroups @ @ work is analysis of extracted essential name entity @ @ used to find @ pattern of recent trend in research area and @ @ provide a platform to explore more on nlp aspect @ inderscience enterprise ltd @ 
2294,Weibo mood towards stock market,"Behavioral economics and behavioral finance believe that public mood is correlated with economic indicators and financial decisions are significantly driven by emotions. A growing body of research has examined the correlation between stock market and social media public mood state. However most research is conducted on English social media websites, the number of research on how public mood states in Chinese social media websites affect the stock market in China is limited. This paper first summarizes the previous research on text mining and social media sentiment analysis. After that, we investigate whether measurements of collective public mood states derived from Weibo which is a social media website similar as Twitter but most posts are written in Chinese are correlated to the stock market price in China. We use a novel Chinese mood extracting method using two NLP (Natural Language Processing) tools: Jieba and Chinese Emotion Words Ontology to analyze the text content of daily Weibo posts. A Granger Causality analysis is then used to investigate the hypothesis that the extracted public mood or emotion states are predictive of the stock price movement in China. Our experimental results indicate that some public mood dimensions such as “Happiness” and “Disgust” are highly correlated with the change of stock price and we can use them to forecast the price movement. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,behavioral economics and behavioral finance believe @ public mood is correlated @ economic indicator and financial decision @ significantly driven by emotion @ a growing body of research ha examined @ correlation @ stock market and social medium public mood state @ however @ research is conducted on english social medium website @ number of research on @ public mood state in chinese social medium website affect @ stock market in china is limited @ @ @ first summarizes @ previous research on text mining and social medium sentiment analysis @ @ @ @ investigate whether measurement of collective public mood state derived @ weibo @ is a social medium website similar a twitter @ @ post @ written in chinese @ correlated to @ stock market price in china @ @ use a novel chinese mood extracting method @ @ nlp @ natural language processing @ tool @ jieba and chinese emotion word ontology to analyze @ text content of daily weibo post @ a granger causality analysis is @ used to investigate @ hypothesis @ @ extracted public mood @ emotion state @ predictive of @ stock price movement in china @ @ experimental @ indicate @ some public mood dimension @ a happiness and disgust @ highly correlated @ @ change of stock price and @ @ use @ to forecast @ price movement @ @ international publishing switzerland @ 
2295,A data- and ontology-driven text mining-based construction of reliability model to analyze and predict component failures,"A real-life reliability system is proposed by fusing the field warranty failure data with the failure modes extracted from unstructured repair verbatim data by using the ontology-based natural language processing technique to facilitate accurate estimation of component reliability. Traditionally, the reliability estimation process uses the warranty data, but it provides limited support to handle the “failure confounding” problem, whereby different failure modes associated with a component failure are confounded into a single failure mode. The resulting reliability estimation lacks the required level of precision. Because our model takes into account textual failure modes associated with component failures, it enhances the overall reliability estimation. The performance of our system is evaluated with the baseline system for predicting absolute errors by using the real-life data from the automotive domain, e.g., headlamp failure, collected at different miles exposures. In the best case, the absolute errors predicted by our model showed an improvement of 97 % with respect to the baseline model (without considering the failure modes), while in worst case, it was 71 %. © 2015, Springer-Verlag London.",2016,Knowledge and Information Systems,11,a real-life reliability system is proposed by fusing @ field warranty failure data @ @ failure mode extracted @ unstructured repair verbatim data by @ @ ontology-based natural language processing technique to facilitate accurate estimation of component reliability @ traditionally @ reliability estimation process us @ warranty data @ @ provides limited support to handle @ failure confounding problem whereby different failure mode associated @ a component failure @ confounded @ a single failure mode @ @ resulting reliability estimation lack @ required level of precision @ @ @ model take @ account textual failure mode associated @ component failure @ enhances @ overall reliability estimation @ @ performance of @ system is evaluated @ @ baseline system @ predicting absolute error by @ @ real-life data @ @ automotive domain e @ g @ headlamp failure collected at different mile exposure @ in @ best case @ absolute error predicted by @ model showed @ improvement of @ respect to @ baseline model @ without considering @ failure mode @ @ in worst case @ wa @ springer-verlag london @ 
2299,Differentiation and empirical analysis of reference types in legal documents,"This paper proposes an extensible model distinguishing between reference types within legal documents. It differentiates between four types of references, namely fully-explicit, semi-explicit, implicit, and tacit references. We conducted a case study on German laws to evaluate both: the model and the proposed differentiation of reference types. We adapted text mining algorithms to determine and classify the different references according to their type. The evaluation shows that the consideration of additional reference types heavily impacts the resulting network structure by inducing a plethora of new edges and relationships. This work extends the approaches made in network analysis and argues for the necessity of detailed differentiation between references throughout legal documents. © 2016 The authors and IOS Press. All rights reserved.",2016,Frontiers in Artificial Intelligence and Applications,5,@ @ proposes @ extensible model distinguishing @ reference type within legal document @ @ differentiates @ four type of reference namely fully-explicit semi-explicit implicit and tacit reference @ @ conducted a case study on german law to evaluate @ @ @ model and @ proposed differentiation of reference type @ @ adapted text mining algorithm to determine and classify @ different reference according to @ type @ @ evaluation @ @ @ consideration of additional reference type heavily impact @ resulting network structure by inducing a plethora of @ edge and relationship @ @ work extends @ approach made in network analysis and argues @ @ necessity of detailed differentiation @ reference throughout legal document @ @ author and io @ @ @ right reserved @ 
2300,Empowering bridging term discovery for cross-domain literature mining in the textflows platform,"Given its immense growth, scientific literature can be explored to reveal new discoveries, based on yet uncovered relations between knowledge from different, relatively isolated fields of research specialization. This chapter proposes a bisociation-based text mining approach, which shows to be effective for cross-domain knowledge discovery. The proposed cross-domain literature mining functionality, including text acquisition, text preprocessing, and bisociative cross-domain literature mining facilities, is made publicly available within a new browserbased workflow execution engine TextFlows, which supports visual construction and execution of text mining and natural language processing (NLP) workflows. To support bisociative cross-domain literature mining, the TextFlows platform includes implementations of several elementary and ensemble heuristics that guide the expert in the process of exploring new cross-context bridging terms. We have extended the TextFlows platform with several components, which—together with document exploration and visualization features of the CrossBee humancomputer interface—make it a powerful, user-friendly text analysis tool for exploratory cross-domain knowledge discovery. Another novelty of the developed technology is the enabled use of controlled vocabularies to improve bridging term extraction. The potential of the developed functionality was showcased in two medical benchmark domains. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,given @ immense growth scientific literature @ @ explored to reveal @ discovery based on yet uncovered relation @ knowledge @ different relatively isolated field of research specialization @ @ chapter proposes a bisociation-based text mining approach @ @ to @ effective @ cross-domain knowledge discovery @ @ proposed cross-domain literature mining functionality including text acquisition text preprocessing and bisociative cross-domain literature mining facility is made publicly available within a @ browserbased workflow execution engine textflows @ support visual construction and execution of text mining and natural language processing @ nlp @ workflow @ to support bisociative cross-domain literature mining @ textflows platform includes implementation of several elementary and ensemble heuristic @ guide @ expert in @ process of exploring @ cross-context bridging term @ @ @ extended @ textflows platform @ several component @ together @ document exploration and visualization feature of @ crossbee humancomputer interface make @ a powerful user-friendly text analysis tool @ exploratory cross-domain knowledge discovery @ another novelty of @ developed technology is @ enabled use of controlled vocabulary to improve bridging term extraction @ @ potential of @ developed functionality wa showcased in @ medical benchmark domain @ @ international publishing ag @ 
2302,Extending full text search for legal document collections using word embeddings,"Traditional full text search allows fast search for exact matches. However, full text search is not optimal to deal with synonyms or semantically related terms and phrases. In this paper we explore a novel method that provides the ability to find not only exact matches, but also semantically similar parts for arbitrary length search queries. We achieve this without the application of ontologies, but base our approach on Word Embeddings. Recently, Word Embeddings have been applied successfully for many natural language processing tasks. We argue that our method is well suited for legal document collections and examine its applicability for two different use cases: We conduct a case study on a stand-alone law, in particular the EU Data Protection Directive 94/46/EC (EU-DPD) in order to extract obligations. Secondly, from a collection of publicly available templates for German rental contracts we retrieve similar provisions. © 2016 The authors and IOS Press. All rights reserved.",2016,Frontiers in Artificial Intelligence and Applications,12,traditional full text search allows fast search @ exact match @ however full text search is not optimal to deal @ synonym @ semantically related term and phrase @ in @ @ @ explore a novel method @ provides @ ability to find not only exact match @ @ semantically similar part @ arbitrary length search query @ @ achieve @ without @ application of ontology @ base @ approach on word embeddings @ recently word embeddings @ @ applied successfully @ many natural language processing task @ @ argue @ @ method is well suited @ legal document collection and examine @ applicability @ @ different use case @ @ conduct a case study on a stand-alone law in particular @ eu data protection directive ec @ eu-dpd @ in order to extract obligation @ secondly @ a collection of publicly available template @ german rental contract @ retrieve similar provision @ @ author and io @ @ @ right reserved @ 
2305,Information extraction using distant supervision and semantic similarities,"Information extraction is one of the main research tasks in natural language processing and text mining that extracts useful information from unstructured sentences. Information extraction techniques include named entity recognition, relation extraction, and co-reference resolution. Among them, relation extraction refers to a task that extracts semantic relations between entities such as personal and geographic names in documents. This is an important research area, which is used in knowledge base construction and question and answering systems. This study presents relation extraction using a distant supervision learning technique among semi-supervised learning methods, which have been spotlighted in recent years to reduce human manual work and costs required for supervised learning. That is, this study proposes a method that can improve relation extraction by improving a distant supervision learning technique by applying a clustering method to create a learning corpus and semantic analysis for relation extraction that is difficult to identify using existing distant supervision. Through comparison experiments of various semantic similarity comparison methods, similarity calculation methods that are useful to relation extraction using distant supervision are searched, and a large number of accurate relation triples can be extracted using the proposed structural advantages and semantic similarity comparison.",2016,Advances in Electrical and Computer Engineering,3,information extraction is @ of @ main research task in natural language processing and text mining @ extract useful information @ unstructured sentence @ information extraction technique include named entity recognition relation extraction and co-reference resolution @ among @ relation extraction refers to a task @ extract semantic relation @ entity @ a personal and geographic name in document @ @ is @ important research area @ is used in knowledge base construction and question and answering system @ @ study @ relation extraction @ a distant supervision learning technique among semi-supervised learning method @ @ @ spotlighted in recent year to reduce human manual work and cost required @ supervised learning @ @ is @ study proposes a method @ @ improve relation extraction by improving a distant supervision learning technique by applying a clustering method to create a learning corpus and semantic analysis @ relation extraction @ is difficult to identify @ existing distant supervision @ @ comparison experiment of various semantic similarity comparison method similarity calculation method @ @ useful to relation extraction @ distant supervision @ searched and a @ number of accurate relation triple @ @ extracted @ @ proposed structural advantage and semantic similarity comparison @ 
2311,Deep bi-directional long short-term memory neural networks for sentiment analysis of social data,"Sentiment analysis (SA) has been attracting a lot of studies in the field of natural language processing and text mining. Recently, there are many algorithm’s enhancements in various SA applications are investigated and introduced. Deep Convolutional Neural Networks (DCNNs) have recently been shown to give the state-of-the-art performance on sentiment classification of social data. Although, these solutions effectively address issues of multi-levels features presentation but having some limitations of temporal modeling. In addition, the Bidirectional Long Short-Term Memory (BLTSM) conventional models have encountered some limitations in presentation with multi-level features but can keep track of the temporal information while enabling deep representations in the data. In this paper, we propose to use Deep Bi-directional Long Short-Term Memory (DBLSTM) architecture with multi-levels feature presentation for sentiment polarity classification (SPC) on social data. By using DBLSTM, we can exploit more level features than BLTSM and inherit temporal modeling in BLTSM. Moreover, the language of social data is very informal with misspellings and abbreviations. One word can be appeared in multiple formalities, which is a challenge in word-level models. We use character-level as input of DBLSTM neural network (called Character DBLSTM - CDBLSTM) for learning sentence level presentation. The experimental results show that the performance of our model is competitive with state-of-the-art of SPC on Twitter’s data. Our model achieves 85.86% accuracy on Stanford Twitter Sentiment corpus (STS) and 84.82% accuracy on the subtasks B of SemEval-2016 Task 4 corpus. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,sentiment analysis @ sa @ ha @ attracting a lot of study in @ field of natural language processing and text mining @ recently @ @ many algorithm s enhancement in various sa application @ investigated and introduced @ deep convolutional neural network @ dcnns @ @ recently @ @ to give @ state-of-the-art performance on sentiment classification of social data @ although @ solution effectively address issue of multi-levels feature presentation @ @ some limitation of temporal modeling @ in addition @ bidirectional long short-term memory @ bltsm @ conventional model @ encountered some limitation in presentation @ multi-level feature @ @ keep track of @ temporal information @ enabling deep representation in @ data @ in @ @ @ propose to use deep bi-directional long short-term memory @ dblstm @ architecture @ multi-levels feature presentation @ sentiment polarity classification @ spc @ on social data @ by @ dblstm @ @ exploit more level feature @ bltsm and inherit temporal modeling in bltsm @ moreover @ language of social data is @ informal @ misspelling and abbreviation @ @ word @ @ appeared in multiple formality @ is a challenge in word-level model @ @ use character-level a input of dblstm neural network @ called character dblstm cdblstm @ @ learning sentence level presentation @ @ experimental @ @ @ @ performance of @ model is competitive @ state-of-the-art of spc on twitter s data @ @ model achieves @ accuracy on stanford twitter sentiment corpus @ sts @ and @ accuracy on @ subtasks b of semeval task corpus @ @ international publishing ag @ 
2313,Making sense of massive amounts of scientific publications: The scientific knowledge miner project,"The World Wide Web has become the hugest repository ever for scientific publications and it continues to increase at an unprecedented rate. Nevertheless, this information overload makes the exploration of this content a very time-consuming task. In this landscape, the availability of text mining tools to characterize and explore distinctive features of the scientific literature is mandatory. We present the Scientific Knowledge Miner (SKM) Project, that aims to investigate new approaches and frameworks to facilitate the extraction of knowledge from scientific publications across different disciplines. More specifically, we will focus on citation characterization, recommendation and scientific document summarization.",2016,CEUR Workshop Proceedings,2,@ world wide web ha become @ hugest repository ever @ scientific publication and @ continues to increase at @ unprecedented rate @ nevertheless @ information overload make @ exploration of @ content a @ time-consuming task @ in @ landscape @ availability of text mining tool to characterize and explore distinctive feature of @ scientific literature is mandatory @ @ @ @ scientific knowledge miner @ skm @ project @ aim to investigate @ approach and framework to facilitate @ extraction of knowledge @ scientific publication across different discipline @ more specifically @ @ focus on citation characterization recommendation and scientific document summarization @ 
2318,"Effects of negation, shifters, jargon, abbreviations and emoticons in sentiment analysis","Some problems present in the treatment of opinions are: the use of informal, ironic and sarcastic language, abbreviations, orthographic and typographic mistakes, semantic compositionality, the cultural level and knowledge of language. These problems impose greater difficulty on opinion mining than on text mining in general. Therefore, the aim of our research is to develop computational solutions to solve some of these problems, contributing to improve the processing of opinions and consequently to obtain more effective polarity detection. As a result of this research some resources were developed to manage jargons, emoticons, valence shifters and negations. These resources are applicable in any opinion mining system that requires them for mining opinions in Spanish or English. The experimental study from the application of the proposed resources showed values of accuracy and F1 higher than those obtained by calculating the polarity without incorporating those resources. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,2,some problem @ in @ treatment of opinion @ @ @ use of informal ironic and sarcastic language abbreviation orthographic and typographic mistake semantic compositionality @ cultural level and knowledge of language @ @ problem impose greater difficulty on opinion mining @ on text mining in general @ therefore @ aim of @ research is to develop computational solution to solve some of @ problem contributing to improve @ processing of opinion and consequently to obtain more effective polarity detection @ a a @ of @ research some resource @ developed to manage jargon emoticon valence shifter and negation @ @ resource @ applicable in @ opinion mining system @ requires @ @ mining opinion in spanish @ english @ @ experimental study @ @ application of @ proposed resource showed value of accuracy and f higher @ @ obtained by calculating @ polarity without incorporating @ resource @ ceur-ws @ @ right reserved @ 
2319,Metabolite named entity recognition: A hybrid approach,"Since labor intensive and time consuming issue, manual curation in metabolic information extraction currently was replaced by text mining (TM). While TM in metabolic domain has been attempted previously, it is still challenging due to variety of specific terms and their meanings in different contexts. Named Entity Recognition (NER) generally used to identify interested keyword (protein and metabolite terms) in sentence, this preliminary task therefore highly influences the performance of metabolic TM framework. Conditional Random Fields (CRFs) NER has been actively used during a last decade, because it explicitly outperforms other approaches. However, an efficient CRFs-based NER depends purely on a quality of corpus which is a nontrivial task to produce. This paper introduced a hybrid solution which combines CRFsbased NER, dictionary usage, and complementary modules (constructed from existing corpus) in order to improve the performance of metabolic NER and another similar domain. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,since labor intensive and time consuming issue manual curation in metabolic information extraction currently wa replaced by text mining @ tm @ @ @ tm in metabolic domain ha @ attempted @ @ is still challenging due to variety of specific term and @ meaning in different context @ named entity recognition @ ner @ generally used to identify interested keyword @ protein and metabolite term @ in sentence @ preliminary task therefore highly influence @ performance of metabolic tm framework @ conditional random field @ crfs @ ner ha @ actively used @ a last decade @ @ explicitly outperforms @ approach @ however @ efficient crfs-based ner depends purely on a quality of corpus @ is a nontrivial task to produce @ @ @ introduced a hybrid solution @ combine crfsbased ner dictionary usage and complementary module @ constructed @ existing corpus @ in order to improve @ performance of metabolic ner and another similar domain @ @ international publishing ag @ 
2321,KNOW at the social book search lab 2016 mining track,"This paper describes our system for the mining task of the Social Book Search Lab in 2016. The track consisted of two task, the classification of book request postings and the task of linking book iden-tiffers with references mentioned within the text. For the classification task we used text mining features like n-grams and vocabulary size, but also included advanced features like average spelling errors found within the text. Here two datasets were provided by the organizers for this task which were evaluated separately. The second task, the linking of book titles to a work identiffer, was addressed by an approach based on lookup tables. For the dataset of the first task our approach was ranked third, following two baseline approaches of the organizers with an accuracy of 91 percent. For the second dataset we achieved second place with an accuracy of 82 percent. Our approach secured the first place with an F-score of 33.50 for the second task.",2016,CEUR Workshop Proceedings,2,@ @ describes @ system @ @ mining task of @ social book search lab in @ @ track consisted of @ task @ classification of book request posting and @ task of linking book iden-tiffers @ reference mentioned within @ text @ @ @ classification task @ used text mining feature like n-grams and vocabulary size @ @ included advanced feature like average spelling error found within @ text @ @ @ datasets @ provided by @ organizer @ @ task @ @ evaluated separately @ @ second task @ linking of book title to a work identiffer wa addressed by @ approach based on lookup table @ @ @ dataset of @ first task @ approach wa ranked third following @ baseline approach of @ organizer @ @ accuracy of percent @ @ @ second dataset @ achieved second place @ @ accuracy of percent @ @ approach secured @ first place @ @ f-score of @ @ @ second task @ 
2322,Studying the cohesion evolution of genes related to chronic lymphocytic leukemia using semantic similarity in gene ontology and self-organizing maps,"A significant body of work on biomedical text mining is aimed at uncovering meaningful associations between biological entities, including genes. This has the potential to offer new insights for research, uncovering hidden links between genes involved in critical pathways and processes. Recently, high-throughput studies have started to unravel the genetic landscape of chronic lymphocytic leukemia (CLL), the most common adult leukemia. CLL displays remarkable clinical heterogeneity, likely reflecting its underlying biological heterogeneity which, despite all progress, still remains insufficiently characterized and understood. This paper deploys an ontology-based semantic similarity combined with self-organizing maps for studying the temporal evolution of cohesion among CLL-related genes and the extracted information. Three consecutive time periods are considered and groups of genes are derived therein. Our preliminary results indicated that our proposed gene groupings are meaningful and that the temporal dimension indeed impacted the gene cohesion, leaving a lot of room for further promising investigations.",2016,CEUR Workshop Proceedings,0,a significant body of work on biomedical text mining is aimed at uncovering meaningful association @ biological entity including gene @ @ ha @ potential to offer @ insight @ research uncovering hidden link @ gene involved in critical pathway and process @ recently high-throughput study @ started to unravel @ genetic landscape of chronic lymphocytic leukemia @ cll @ @ @ common adult leukemia @ cll display remarkable clinical heterogeneity likely reflecting @ underlying biological heterogeneity @ despite @ progress still remains insufficiently characterized and understood @ @ @ deploys @ ontology-based semantic similarity combined @ self-organizing map @ studying @ temporal evolution of cohesion among cll-related gene and @ extracted information @ three consecutive time period @ considered and group of gene @ derived therein @ @ preliminary @ indicated @ @ proposed gene grouping @ meaningful and @ @ temporal dimension indeed impacted @ gene cohesion leaving a lot of room @ @ promising investigation @ 
2323,Malay word stemmer to stem standard and slang word patterns on social media,"Word stemmer is a text preprocessing tool used in many artificial intelligence applications such as text mining, text categorization and information retrieval. It is used to stem derived words into their respective root words. Many researchers have proposed word stemmers for Malay language using various stemming approaches. Since the proliferation of social media, there are various word patterns have been used by social media users in which the existing word stemmers do not support in their stemming rules. These word patterns are slang words or informal conversation words which are used in daily conversation. Therefore, this paper proposes the new word stemmer for Malay language that able to stem standard and slang words. This paper also examines the differences between standard words and slang words. The experimental results show that the proposed word stemmer able to stem standard affixation and reduplication words and also stem slang affixation and reduplication words with better stemming accuracy. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,word stemmer is a text preprocessing tool used in many artificial intelligence application @ a text mining text categorization and information retrieval @ @ is used to stem derived word @ @ respective root word @ many researcher @ proposed word stemmer @ malay language @ various stemming approach @ since @ proliferation of social medium @ @ various word pattern @ @ used by social medium user in @ @ existing word stemmer @ not support in @ stemming rule @ @ word pattern @ slang word @ informal conversation word @ @ used in daily conversation @ therefore @ @ proposes @ @ word stemmer @ malay language @ able to stem standard and slang word @ @ @ @ examines @ difference @ standard word and slang word @ @ experimental @ @ @ @ proposed word stemmer able to stem standard affixation and reduplication word and @ stem slang affixation and reduplication word @ better stemming accuracy @ @ international publishing switzerland @ 
2325,TOPIE: An open-source opinion mining pipeline to analyze consumers’ sentiment in Brazilian Portuguese,"The growth of social media and user-generated content (UGC) on the Internet provides a huge quantity of information that allows discovering the experiences, opinions, and feelings of users or customers. These electronic Word of Mouth statements expressed on the web are prevalent in business and service industry to enable a customer to share his/her point of view. However, it is impossible for humans to fully understand it in a reasonable amount of time. Opinion mining (also known as Sentiment Analysis) is a sub-field of text mining in which the main task is to extract opinions from UGC. Thus, this work presents an open source pipeline to analyze the costumer’s opinion or sentiment in Twitter about products and services offered by Brazilian companies. The pipeline is based on General Architecture for Text Engineering (GATE) framework and the proposed hybrid method combines lexicon-based, supervised learning, and rulebased approaches. Case studies performed on Twitter real data achieved precision of almost 70%. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ growth of social medium and user-generated content @ ugc @ on @ internet provides a huge quantity of information @ allows discovering @ experience opinion and feeling of user @ customer @ @ electronic word of mouth statement expressed on @ web @ prevalent in @ and service industry to enable a customer to share @ @ point of view @ however @ is impossible @ human to fully understand @ in a reasonable amount of time @ opinion mining @ @ known a sentiment analysis @ is a sub-field of text mining in @ @ main task is to extract opinion @ ugc @ thus @ work @ @ open source pipeline to analyze @ costumer s opinion @ sentiment in twitter @ product and service offered by brazilian company @ @ pipeline is based on general architecture @ text engineering @ gate @ framework and @ proposed hybrid method combine lexicon-based supervised learning and rulebased approach @ case study performed on twitter real data achieved precision of almost @ @ international publishing switzerland @ 
2326,Actionable social media competitive analytics for understanding customer experiences,"A large amount of user-generated content is now freely available on social media sites. To increase their competitive advantage, companies need to monitor and analyze not only the customer-generated content on their own social media sites, but also the content on their competitors' social media sites. In this article, we describe a framework to integrate several techniques including quantitative analysis, text mining, and sentiment analysis for analyzing and comparing social media content from business competitors. Specifically, we conducted an in-depth case study which applies our developed framework to the analysis and comparison of social media content on the Facebook sites of the three largest drugstore chains in the United States: Walgreens, CVS, and Rite Aid. We found similarities and differences in the social media use among the three drugstore chains. We discuss the implications of our findings and provide recommendations to help companies develop their social media competitive analysis strategies.",2016,Journal of Computer Information Systems,47,a @ amount of user-generated content is now freely available on social medium site @ to increase @ competitive advantage company need to monitor and analyze not only @ customer-generated content on @ @ social medium site @ @ @ content on @ competitor @ social medium site @ in @ article @ describe a framework to integrate several technique including quantitative analysis text mining and sentiment analysis @ analyzing and comparing social medium content @ @ competitor @ specifically @ conducted @ in-depth case study @ applies @ developed framework to @ analysis and comparison of social medium content on @ facebook site of @ three largest drugstore chain in @ united state @ walgreens cv and rite aid @ @ found similarity and difference in @ social medium use among @ three drugstore chain @ @ discus @ implication of @ finding and provide recommendation to help company develop @ social medium competitive analysis strategy @ 
2328,Characterizing opinion mining: A systematic mapping study of the Portuguese language,"The growth of social media and user-generated content (UGC) on the Internet provides a huge quantity of information that allows discovering the experiences, opinions, and feelings of users or customers. Opinion Mining (OM) is a sub-field of text mining in which the main task is to extract opinions from UGC. Given that Portuguese is one of the most common spoken languages in the world, and it is also the second most frequent on Twitter, the goal of this work is to plot the landscape of current studies that relates the application of OM for Portuguese. A systematic mapping review (SMR) method was applied to search, select and to extract data from the included studies. Manual and automated searches retrieved 6075 studies up to year 2014, from which 25 articles were included. Almost 70 % of all approaches focus on the Brazilian Portuguese variant. Naïve Bayes and Support Vector Machine were the main classifiers and SentiLex-PT was the most used lexical resource. Portugal and Brazil are the main contributors in processing the Portuguese language. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,@ growth of social medium and user-generated content @ ugc @ on @ internet provides a huge quantity of information @ allows discovering @ experience opinion and feeling of user @ customer @ opinion mining @ om @ is a sub-field of text mining in @ @ main task is to extract opinion @ ugc @ given @ portuguese is @ of @ @ common spoken language in @ world and @ is @ @ second @ frequent on twitter @ goal of @ work is to plot @ landscape of current study @ relates @ application of om @ portuguese @ a systematic mapping review @ smr @ method wa applied to search select and to extract data @ @ included study @ manual and automated search retrieved study up to year @ @ article @ included @ almost of @ approach focus on @ brazilian portuguese variant @ naïve bayes and support vector machine @ @ main classifier and sentilex-pt wa @ @ used lexical resource @ portugal and brazil @ @ main contributor in processing @ portuguese language @ @ international publishing switzerland @ 
2329,Exploring performance of instance selection methods in text sentiment classification,"Sentiment analysis is the process of extracting subjective information in source materials. Sentiment analysis is a subfield of web and text mining. One major problem encountered in these areas is overwhelming amount of data available. Hence, instance selection and feature selection become two essential tasks for achieving scalability in machine learning based sentiment classification. Instance selection is a data reduction technique which aims to eliminate redundant, noisy data from the training dataset so that training time can be reduced, scalability and generalization ability can be enhanced. This paper examines the predictive performance of fifteen benchmark instance selection methods for text classification domain. The instance selection methods are evaluated by decision tree classifier (C4.5 algorithm) and radial basis function networks in terms of classification accuracy and data reduction rates. The experimental results indicate that the highest classification accuracies on C4.5 algorithm are generally obtained by model class selection method, while the highest classification accuracies on radial basis function networks are obtained by nearest centroid neighbor edition. © Springer International Publishing Switzerland 2016.",2016,Advances in Intelligent Systems and Computing,3,sentiment analysis is @ process of extracting subjective information in source material @ sentiment analysis is a subfield of web and text mining @ @ major problem encountered in @ area is overwhelming amount of data available @ hence instance selection and feature selection become @ essential task @ achieving scalability in machine learning based sentiment classification @ instance selection is a data reduction technique @ aim to eliminate redundant noisy data @ @ training dataset @ @ training time @ @ reduced scalability and generalization ability @ @ enhanced @ @ @ examines @ predictive performance of fifteen benchmark instance selection method @ text classification domain @ @ instance selection method @ evaluated by decision tree classifier @ c @ algorithm @ and radial basis function network in term of classification accuracy and data reduction rate @ @ experimental @ indicate @ @ highest classification accuracy on c @ algorithm @ generally obtained by model class selection method @ @ highest classification accuracy on radial basis function network @ obtained by nearest centroid neighbor edition @ @ international publishing switzerland @ 
2330,"Annotation process, guidelines and text corpus of small non-coding RNA molecules: The MiNCor for microRNA annotations","MicroRNA are small non-coding molecules that act as post-transcriptional regulators of gene expression in a wide spectrum of biological states. Mostly, the information about microRNA is embedded in unstructured data (text files) which needs specific text mining techniques for its retrieval and analysis. These are generally based on supervised (or semi-supervised) learning methods, which require collections of neatly annotated and categorised training data. In this study we propose a comprehensive granular annotation protocol for the annotation of non-coding RNA molecules, focusing primarily on microRNA mentions. This annotation protocol was used to construct a manually annotated corpus (MiNCor Gold) for microRNA mentions as well as a large semi-automatically generated microRNA mentions silver standard corpus (MiNCor Silver) and a large microRNA name dictionary. Therefore, the efficiency of these standards was evaluated using a named entity recognition (NER) system in comparison with another microRNA mentions standard freely available online. The NER system trained with our silver corpus showed a better performance, with higher precision (96,67% vs. 94,00%) and recall (97,57% vs. 95,00%) on their test data and on our (precision 89,26% vs. 88,97% and recall 90,03% vs. 86,74%). The corpora and guidelines are freely downloadable at http://zope.bioinfo.cnio.es/ mincor/minacor.tar.gz.",2016,CEUR Workshop Proceedings,0,microrna @ small non-coding molecule @ act a post-transcriptional regulator of gene expression in a wide spectrum of biological state @ mostly @ information @ microrna is embedded in unstructured data @ text file @ @ need specific text mining technique @ @ retrieval and analysis @ @ @ generally based on supervised @ @ semi-supervised @ learning method @ require collection of neatly annotated and categorised training data @ in @ study @ propose a comprehensive granular annotation protocol @ @ annotation of non-coding rna molecule focusing primarily on microrna mention @ @ annotation protocol wa used to construct a manually annotated corpus @ mincor gold @ @ microrna mention a well a a @ semi-automatically generated microrna mention silver standard corpus @ mincor silver @ and a @ microrna name dictionary @ therefore @ efficiency of @ standard wa evaluated @ a named entity recognition @ ner @ system in comparison @ another microrna mention standard freely available online @ @ ner system trained @ @ silver corpus showed a better performance @ higher precision @ v @ @ and recall @ v @ @ on @ test data and on @ @ precision v @ and recall v @ @ @ @ corpus and guideline @ freely downloadable at http @ zope @ bioinfo @ cnio @ e mincor minacor @ tar @ gz @ 
2332,A novel codification technique for tacit knowledge in software industry using datamining techniques,"Tacit knowledge is an important resource which comes from experience and insight, and is not in any pre-recorded form. But, it has a strong contribution to the success of decision making procedure. This paper focuses on the summarization of various efforts of codification methodologies to convert tacit to explicit knowledge of an IT company, where the later plays a key role in decision making. This paper also tries to bring out the lacuna or technical gaps of various methodologies and propose a novel method to capture the tacit technical knowledge, so that the technical knowledge transfer in case of staff relocation does not affect the growth of the small scale IT industry. The challenge in software development life cycle is clearly captured by product management software tools like JIRA. We are using text mining techniques on these reports to gather all the tacit knowledge, of technical person assigned to the project. Added to that, from our scrum report, we will extract consolidate generic knowledge. These two, will ensure that we capture most of the needed tacit knowledge of the project coding or maintenance phase. The mining results from the data are promising and future work is to include sentimental analysis. © Springer India 2016.",2016,Advances in Intelligent Systems and Computing,1,tacit knowledge is @ important resource @ come @ experience and insight and is not in @ pre-recorded form @ @ @ ha a strong contribution to @ success of decision making procedure @ @ @ focus on @ summarization of various effort of codification methodology to convert tacit to explicit knowledge of @ @ company @ @ later play a key role in decision making @ @ @ @ try to bring @ @ lacuna @ technical gap of various methodology and propose a novel method to capture @ tacit technical knowledge @ @ @ technical knowledge transfer in case of staff relocation doe not affect @ growth of @ small scale @ industry @ @ challenge in software development life cycle is clearly captured by product management software tool like jira @ @ @ @ text mining technique on @ report to gather @ @ tacit knowledge of technical person assigned to @ project @ added to @ @ @ scrum report @ @ extract consolidate generic knowledge @ @ @ @ ensure @ @ capture @ of @ needed tacit knowledge of @ project coding @ maintenance phase @ @ mining @ @ @ data @ promising and future work is to include sentimental analysis @ @ india @ 
2333,ECSTRA-INSERM @ CLEF eHealth2016-Task 2: ICD10 Code Extraction from Death Certificates,"This paper describes the participation of ECSTRA-INSERM team at CLEF eHealth 2016, task 2.C. The task involves extracting ICD10 codes from death certificates, mainly described with short plain texts. We cast the task as a machine learning problem involving the prediction of the ICD10 codes (categorical variable) from the raw text transformed into a bag-of-words matrix. We rely on probabilistic topic models that we evaluate against classical classifiers such as SVM and Naive Bayes. We demonstrate the effectiveness of topic models for this task in terms of prediction accuracy and result interpretation.",2016,CEUR Workshop Proceedings,9,@ @ describes @ participation of ecstra-inserm team at clef ehealth task @ c @ @ task involves extracting icd code @ death certificate mainly described @ short plain text @ @ cast @ task a a machine learning problem involving @ prediction of @ icd code @ categorical variable @ @ @ raw text transformed @ a bag-of-words matrix @ @ rely on probabilistic topic model @ @ evaluate @ classical classifier @ a svm and naive bayes @ @ demonstrate @ effectiveness of topic model @ @ task in term of prediction accuracy and @ interpretation @ 
2337,Text analysis system for measuring the influence of news articles on intraday price changes in financial markets,"This study constructs a text analysis system for analyzing financial markets. This system enables us to investigate the influence of news article on intraday price changes. In this study, we examine the automobile companies in Japan to analyze the relationship between news articles and stock price reactions. As a result of empirical analyses, we confirmed that stock prices reflect news information in a timely manner. These results are suggestive from both academic and practical view points. More detailed analyses are planned for the future. © Springer International Publishing Switzerland 2016.",2016,"Smart Innovation, Systems and Technologies",1,@ study construct a text analysis system @ analyzing financial market @ @ system enables u to investigate @ influence of news article on intraday price change @ in @ study @ examine @ automobile company in japan to analyze @ relationship @ news article and stock price reaction @ a a @ of empirical analysis @ confirmed @ stock price reflect news information in a timely manner @ @ @ @ suggestive @ @ @ and practical view point @ more detailed analysis @ planned @ @ future @ @ international publishing switzerland @ 
2338,Actor Identification and relevance filtering in movie reviews,"With a large amount of data it is not always useful to run analyses on the entire corpus. Sometimes, it is helpful to previously preprocess data by filtering relevant information in order to form a fitting basis for the examination of particular aspects such as sentiment analysis. As a result, the amount of data that needs to be explored is reduced and concentrated, and thus the performance is enhanced. For example, a correct recognition of the rating of acting performances in movie reviews assumes that only judgements on the movie's actors are used as a basis. In this paper, we discuss different approaches for a rulebased selection of sentences from movie reviews. Our aim is the filtering of sentences in order to facilitate analyses about single actors. Thereby actor identification is used to preselect a set of sentences that mention a specific actor. This is done individually for every actor involved in the movie. Furthermore, filtering is used to identify sentences that not only mention an actor but also state facts about him. To evaluate the developed methods, a test corpus consisting of ten movies with 30 reviews each, taken from the online movie platform IMDb, was built. Based on this data and the presented feature selection rules, an average F1 score of 77.9% is achieved as best result.",2016,CEUR Workshop Proceedings,0,@ a @ amount of data @ is not always useful to run analysis on @ entire corpus @ sometimes @ is helpful to @ preprocess data by filtering relevant information in order to form a fitting basis @ @ examination of particular aspect @ a sentiment analysis @ a a @ @ amount of data @ need to @ explored is reduced and concentrated and thus @ performance is enhanced @ @ example a correct recognition of @ rating of acting performance in movie review assumes @ only judgement on @ movie @ s actor @ used a a basis @ in @ @ @ discus different approach @ a rulebased selection of sentence @ movie review @ @ aim is @ filtering of sentence in order to facilitate analysis @ single actor @ thereby actor identification is used to preselect a set of sentence @ mention a specific actor @ @ is done individually @ every actor involved in @ movie @ furthermore filtering is used to identify sentence @ not only mention @ actor @ @ state fact @ @ @ to evaluate @ developed method a test corpus consisting of ten movie @ review @ taken @ @ online movie platform imdb wa built @ based on @ data and @ presented feature selection rule @ average f score of @ is achieved a best @ @ 
2339,Disentangling the structure of tables in scientific literature,"Within the scientific literature, tables are commonly used to present factual and statistical information in a compact way, which is easy to digest by readers. The ability to ""understand"" the structure of tables is key for information extraction in many domains. However, the complexity and variety of presentation layouts and value formats makes it difficult to automatically extract roles and relationships of table cells. In this paper, we present a model that structures tables in a machine readable way and a methodology to automatically disentangle and transform tables into the modelled data structure. The method was tested in the domain of clinical trials: it achieved an F-score of 94.26% for cell function identification and 94.84% for identification of inter-cell relationships. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,within @ scientific literature table @ commonly used to @ factual and statistical information in a compact way @ is easy to digest by reader @ @ ability to @ understand @ @ structure of table is key @ information extraction in many domain @ however @ complexity and variety of presentation layout and value format make @ difficult to automatically extract role and relationship of table cell @ in @ @ @ @ a model @ structure table in a machine readable way and a methodology to automatically disentangle and transform table @ @ modelled data structure @ @ method wa tested in @ domain of clinical trial @ @ achieved @ f-score of @ @ cell function identification and @ @ identification of inter-cell relationship @ @ international publishing switzerland @ 
2340,Descriptive document clustering via discriminant learning in a co-embedded space of multilevel similarities,"Descriptive document clustering aims at discovering clusters of semantically interrelated documents together with meaningful labels to summarize the content of each document cluster. In this work, we propose a novel descriptive clustering framework, referred to as CEDL. It relies on the formulation and generation of 2 types of heterogeneous objects, which correspond to documents and candidate phrases, using multilevel similarity information. CEDL is composed of 5 main processing stages. First, it simultaneously maps the documents and candidate phrases into a common co-embedded space that preserves higher-order, neighbor-based proximities between the combined sets of documents and phrases. Then, it discovers an approximate cluster structure of documents in the common space. The third stage extracts promising topic phrases by constructing a discriminant model where documents along with their cluster memberships are used as training instances. Subsequently, the final cluster labels are selected from the topic phrases using a ranking scheme using multiple scores based on the extracted co-embedding information and the discriminant output. The final stage polishes the initial clusters to reduce noise and accommodate the multitopic nature of documents. The effectiveness and competitiveness of CEDL is demonstrated qualitatively and quantitatively with experiments using document databases from different application fields. © 2014 ASIS&T",2016,Journal of the Association for Information Science and Technology,10,descriptive document clustering aim at discovering cluster of semantically interrelated document together @ meaningful label to summarize @ content of @ document cluster @ in @ work @ propose a novel descriptive clustering framework referred to a cedl @ @ relies on @ formulation and generation of type of heterogeneous object @ correspond to document and candidate phrase @ multilevel similarity information @ cedl is composed of main processing stage @ first @ simultaneously map @ document and candidate phrase @ a common co-embedded space @ preserve higher-order neighbor-based proximity @ @ combined set of document and phrase @ @ @ discovers @ approximate cluster structure of document in @ common space @ @ third stage extract promising topic phrase by constructing a discriminant model @ document along @ @ cluster membership @ used a training instance @ subsequently @ final cluster label @ selected @ @ topic phrase @ a ranking scheme @ multiple score based on @ extracted co-embedding information and @ discriminant output @ @ final stage polish @ initial cluster to reduce noise and accommodate @ multitopic nature of document @ @ effectiveness and competitiveness of cedl is demonstrated qualitatively and quantitatively @ experiment @ document database @ different application field @ asis t
2342,“Part of speech tagging – A corpus based approach”,"POS tagging, an ideal way to augment a corpus is an imperative abstraction for text mining. However with an increase in the amount of linguistic errors and distinctive fashion of language ambiguities, the data filtered by POS tagging is noisier. In this paper, probabilistic tagging and tagging based on Markov models are combined to estimate the association probabilities. Based on this combined approach, error estimation model is defined. Comparison study is made on different corpus available in NLTK such as Crubadan, Brown and INSPEC. The results obtained by the proposed methodologies show a drastic increase in the accuracy rate of about 98% when compared to the existing algorithms which shows an average of 96% accurate. The performance measure is plotted to calculate the error ratio across the maximum-likelihood estimation. © Springer Nature Singapore Pte Ltd. 2016.",2016,Communications in Computer and Information Science,0,po tagging @ ideal way to augment a corpus is @ imperative abstraction @ text mining @ however @ @ increase in @ amount of linguistic error and distinctive fashion of language ambiguity @ data filtered by po tagging is noisier @ in @ @ probabilistic tagging and tagging based on markov model @ combined to estimate @ association probability @ based on @ combined approach error estimation model is defined @ comparison study is made on different corpus available in nltk @ a crubadan brown and inspec @ @ @ obtained by @ proposed methodology @ a drastic increase in @ accuracy rate of @ @ compared to @ existing algorithm @ @ @ average of accurate @ @ performance measure is plotted to calculate @ error ratio across @ maximum-likelihood estimation @ @ nature singapore pte ltd @ @ 
2348,Text sentiment computation for online forums hotspot detection,"The user generated content on the web grows rapidly in this emergent information age. The tremendous growth of content available in forums, blogs, news reports, etc., are having large volume of public opinion information, it is essential to analyse in time and understands the trends of their opinion correctly. The evolutionary changes in technology make use of such information to capture only the user's essence and finally the useful information are exposed to information seekers. Most of the existing research on text information processing, focus on the factual domain rather than the opinion domain. In this paper, we detect online hotspot forums by computing sentiment analysis for text data available in each forum. This approach analyses the forum text data and computes value for each word of text. The proposed approach combines K-means clustering and support vector machine with PSO (SVM-PSO) classification algorithm that can be used to group the forums into two clusters forming hotspot forums and non-hotspot forums within the current time span. The proposed system accuracy is compared with the other classification algorithms such as Naïve Bayes, decision tree and SVM. The experiment helps to identify that K-means and SVM-PSO together achieve highly consistent results. Copyright © 2016 Inderscience Enterprises Ltd.",2016,International Journal of Information and Communication Technology,0,@ user generated content on @ web grows rapidly in @ emergent information age @ @ tremendous growth of content available in forum blog news report etc @ @ @ @ volume of public opinion information @ is essential to analyse in time and understands @ trend of @ opinion correctly @ @ evolutionary change in technology make use of @ information to capture only @ user @ s essence and finally @ useful information @ exposed to information seeker @ @ of @ existing research on text information processing focus on @ factual domain rather @ @ opinion domain @ in @ @ @ detect online hotspot forum by computing sentiment analysis @ text data available in @ forum @ @ approach analysis @ forum text data and computes value @ @ word of text @ @ proposed approach combine k-means clustering and support vector machine @ pso @ svm-pso @ classification algorithm @ @ @ used to group @ forum @ @ cluster forming hotspot forum and non-hotspot forum within @ current time span @ @ proposed system accuracy is compared @ @ @ classification algorithm @ a naïve bayes decision tree and svm @ @ experiment help to identify @ k-means and svm-pso together achieve highly consistent @ @ @ inderscience enterprise ltd @ 
2353,LiCord: Language independent content word finder,"Content Words (CWs) are important segments of the text. In text mining, we utilize them for various purposes such as topic identification, document summarization, question answering etc. Usually, the identification of CWs requires various language dependent tools. However, such tools are not available for many languages and developing of them for all languages is costly. On the other hand, because of recent growth of text contents in various languages, language independent text mining carries great potentiality. To mine text automatically, the language tool independent CWs finding is a requirement. In this research, we devise a framework that identifies text segments into CWs in a language independent way. We identify some structural features that relate text segments into CWs. We devise the features over a large text corpus and apply machine learning-based classification that classifies the segments into CWs. The proposed framework only uses large text corpus and some training examples, apart from these, it does not require any language specific tool. We conduct experiments of our framework for three different languages: English, Vietnamese and Indonesian, and found that it works with more than 83 % accuracy. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,content word @ cws @ @ important segment of @ text @ in text mining @ utilize @ @ various purpose @ a topic identification document summarization question answering etc @ usually @ identification of cws requires various language dependent tool @ however @ tool @ not available @ many language and developing of @ @ @ language is costly @ on @ @ hand @ of recent growth of text content in various language language independent text mining carry great potentiality @ to mine text automatically @ language tool independent cws finding is a requirement @ in @ research @ devise a framework @ identifies text segment @ cws in a language independent way @ @ identify some structural feature @ relate text segment @ cws @ @ devise @ feature @ a @ text corpus and apply machine learning-based classification @ classifies @ segment @ cws @ @ proposed framework only us @ text corpus and some training example apart @ @ @ doe not require @ language specific tool @ @ conduct experiment of @ framework @ three different language @ english vietnamese and indonesian and found @ @ work @ more @ accuracy @ @ international publishing switzerland @ 
2354,A comprehensive comparison of two MEDLINE annotators for disease and gene linkage: Sometimes less is more,"Text mining is popular in biomedical applications because it allows retrieving highly relevant information. Particularly for us, it is quite practical in linking diseases to the genes involved in them. However text mining involves multiple challenges, such as (1) recognizing named entities (e.g., diseases and genes) inside the text, (2) constructing specific vocabularies that efficiently represent the available text, and (3) applying the correct statistical criteria to link biomedical entities with each other. We have previously developed Beegle, a tool that allows prioritizing genes for any search query of interest. The method starts with a search phase, where relevant genes are identified via the literature. Once known genes are identified, a second phase allows prioritizing novel candidate genes through a data fusion strategy. Many aspects of our method could be potentially improved. Here we evaluate two MEDLINE annotators that recognize biomedical entities inside a given abstract using different dictionaries and annotation strategies. We compare the contribution of each of the two annotators in associating genes with diseases under different vocabulary settings. Somewhat surprisingly, with fewer recognized entities and a more compact vocabulary, we obtain better associations between genes and diseases. We also propose a novel but simple association criterion to link genes with diseases, which relies on recognizing only gene entities inside the biomedical text. These refinements significantly improve the performance of our method. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,text mining is popular in biomedical application @ @ allows retrieving highly relevant information @ particularly @ u @ is quite practical in linking disease to @ gene involved in @ @ however text mining involves multiple challenge @ a @ @ recognizing named entity @ e @ g @ disease and gene @ inside @ text @ @ constructing specific vocabulary @ efficiently represent @ available text and @ @ applying @ correct statistical criterion to link biomedical entity @ @ @ @ @ @ @ developed beegle a tool @ allows prioritizing gene @ @ search query of interest @ @ method start @ a search phase @ relevant gene @ identified via @ literature @ @ known gene @ identified a second phase allows prioritizing novel candidate gene @ a data fusion strategy @ many aspect of @ method could @ potentially improved @ @ @ evaluate @ medline annotator @ recognize biomedical entity inside a given abstract @ different dictionary and annotation strategy @ @ compare @ contribution of @ of @ @ annotator in associating gene @ disease @ different vocabulary setting @ somewhat surprisingly @ fewer recognized entity and a more compact vocabulary @ obtain better association @ gene and disease @ @ @ propose a novel @ simple association criterion to link gene @ disease @ relies on recognizing only gene entity inside @ biomedical text @ @ refinement significantly improve @ performance of @ method @ @ international publishing switzerland @ 
2355,Sentiment analysis at document level,"Sentiment analysis becomes a very active research area in the text mining field. It aims to extract people’s opinions, sentiments, and subjectivity from the texts. Sentiment analysis can be performed at three levels: at document level, at sentence level and at aspect level. An important part of research effort focuses on document level sentiment classification, including works on opinion classification of reviews. This survey paper tackles a comprehensive overview of the last update of sentiment analysis at document level. The main target of this survey is to give nearly full image of sentiment analysis techniques at this level. In addition, some future research issues are also presented. © Springer Nature Singapore Pte Ltd. 2016.",2016,Communications in Computer and Information Science,8,sentiment analysis becomes a @ active research area in @ text mining field @ @ aim to extract people s opinion sentiment and subjectivity @ @ text @ sentiment analysis @ @ performed at three level @ at document level at sentence level and at aspect level @ @ important part of research effort focus on document level sentiment classification including work on opinion classification of review @ @ survey @ tackle a comprehensive overview of @ last update of sentiment analysis at document level @ @ main target of @ survey is to give nearly full image of sentiment analysis technique at @ level @ in addition some future research issue @ @ presented @ @ nature singapore pte ltd @ @ 
2358,Cross-genre age and gender identification in social media,"This paper1 gives a brief description on the methods adopted for the task of author-profiling as part of the competition PAN 2016 [1]. Author profiling is the task of predicting the author's age and gender from his/her writing. In this paper, we follow a two-level ensemble ap-proach to tackle the cross-genre author profiling task where training doc-uments and testing documents are from different genres. We use the soft-voting approach to build the classification ensemble. To include various feature sets, we first train logistic regression models using the extracted word n-gram, character n-gram, and part-of-speech n-gram features for each genre. We then ensemble single-genre predictive models trained on the blog, social media and Twitter data sources, to build our multi-genre ensemble approach. The experimental results indicate that our approach performs well in both single-genre and cross-genre author profiling tasks.",2016,CEUR Workshop Proceedings,1,@ @ give a brief description on @ method adopted @ @ task of author-profiling a part of @ competition pan @ author profiling is @ task of predicting @ author @ s age and gender @ @ @ writing @ in @ @ @ follow a two-level ensemble ap-proach to tackle @ cross-genre author profiling task @ training doc-uments and testing document @ @ different genre @ @ use @ soft-voting approach to build @ classification ensemble @ to include various feature set @ first train logistic regression model @ @ extracted word n-gram character n-gram and part-of-speech n-gram feature @ @ genre @ @ @ ensemble single-genre predictive model trained on @ blog social medium and twitter data source to build @ multi-genre ensemble approach @ @ experimental @ indicate @ @ approach performs well in @ single-genre and cross-genre author profiling task @ 
2362,Discovering alias for chemical material with NGD,"The complexity of chemical substance names makes it difficult to fully describe chemical substances using just several keywords. We usually find related information through search engines or look up an online chemical dic-tionary. However, the chemical material names used in academy usually translated from English, and the same chemicals often have many different aliases. This English Chinese translation creates many problems when querying information for chemicals. Recent studies have proposed to use Normalized Google Distance (NGD) to determine semantic relevance between two words. Therefore, this study proposes to find alias based on NGD with two methods, namely, novel and category affixed methods. The Experimental results show that the latter method can derive better result. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ complexity of chemical substance name make @ difficult to fully describe chemical substance @ @ several keywords @ @ usually find related information @ search engine @ look up @ online chemical dic-tionary @ however @ chemical material name used in academy usually translated @ english and @ @ chemical often @ many different alias @ @ english chinese translation creates many problem @ querying information @ chemical @ recent study @ proposed to use normalized google distance @ ngd @ to determine semantic relevance @ @ word @ therefore @ study proposes to find alias based on ngd @ @ method namely novel and category affixed method @ @ experimental @ @ @ @ latter method @ derive better @ @ @ international publishing switzerland @ 
2365,Semi-supervised knowledge extraction for detection of drugs and their effects,"New Psychoactive Substances (NPS) are drugs that lay in a grey area of legislation, since they are not internationally and officially banned, possibly leading to their not prosecutable trade. The exacerbation of the phenomenon is that NPS can be easily sold and bought online. Here, we consider large corpora of textual posts, published on online forums specialized on drug discussions, plus a small set of known substances and associated effects, which we call seeds.We propose a semisupervised approach to knowledge extraction, applied to the detection of drugs (comprising NPS) and effects from the corpora under investigation. Based on the very small set of initial seeds, the work highlights how a contrastive approach and context deduction are effective in detecting substances and effects from the corpora. Our promising results, which feature a F1 score close to 0.9, pave the way for shortening the detection time of new psychoactive substances, once these are discussed and advertised on the Internet. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ psychoactive substance @ np @ @ drug @ lay in a grey area of legislation since @ @ not internationally and officially banned possibly leading to @ not prosecutable trade @ @ exacerbation of @ phenomenon is @ np @ @ easily sold and bought online @ @ @ consider @ corpus of textual post published on online forum specialized on drug discussion plus a small set of known substance and associated effect @ @ call seed @ @ propose a semisupervised approach to knowledge extraction applied to @ detection of drug @ comprising np @ and effect @ @ corpus @ investigation @ based on @ @ small set of initial seed @ work highlight @ a contrastive approach and context deduction @ effective in detecting substance and effect @ @ corpus @ @ promising @ @ feature a f score close to @ pave @ way @ shortening @ detection time of @ psychoactive substance @ @ @ discussed and advertised on @ internet @ @ international publishing ag @ 
2366,Topic-sentiment mining from multiple text collections,Topic-sentiment mining is a challenging task for many applications. This paper presents a topic-sentiment joint model in order to mine topics and their sentimental polarities from multiple text collections. Text collections are represented with a mixture of components and modeled via the hierarchical Dirichlet process which can determine the number of components automatically. Each component consists of topic words and its sentiments. The model can mine topics with different proportions and different sentimental polarities as well as one positive and one negative topic for each collection. Experiments on two text collections from Chinese news media and microblog show that our model can find meaningful topics and their different sentimental polarities. Experiments on Multi-Domain Sentiment Dataset show that our model is better than the JST-alike models on parameter settings for topic-sentiment mining. © Springer International Publishing AG 2016.,2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,topic-sentiment mining is a challenging task @ many application @ @ @ @ a topic-sentiment joint model in order to mine topic and @ sentimental polarity @ multiple text collection @ text collection @ represented @ a mixture of component and modeled via @ hierarchical dirichlet process @ @ determine @ number of component automatically @ @ component consists of topic word and @ sentiment @ @ model @ mine topic @ different proportion and different sentimental polarity a well a @ positive and @ negative topic @ @ collection @ experiment on @ text collection @ chinese news medium and microblog @ @ @ model @ find meaningful topic and @ different sentimental polarity @ experiment on multi-domain sentiment dataset @ @ @ model is better @ @ jst-alike model on parameter setting @ topic-sentiment mining @ @ international publishing ag @ 
2367,Folksonomy Based Trend Analysis on Community Question Answering Sites: A Perspective on Software Technologies,"In the booming era of online social media environment, community question answering (CQA) sites have become one of the popular resources for software engineers and software industries. Software engineers are increasingly sharing their questions and answers on CQA sites. Aims of the CQA sites are to provide useful and relevant information to the users. Analysis information of major programming languages using trend analysis can be useful for software engineers to understand the technological evolutions and popularity. Since most of the CQA sites consist of user assigned tags by which folksonomy can be efficiently utilized for developing suitable algorithm to find the trend of key programming technologies. In this paper, two techniques of trend analysis, namely, ARIMA time series model and fuzzy time series model have been applied on the tagging data to identify the trend. Trend analysis is being carried on key programming languages, namely, c#, Java, PHP, and python. In this paper, quality of the trend is measured by entropy, ZTrend, and quality of forecast is measured by MMRE, burst trend for key programming languages. © 2016 IEEE.",2016,IEEE Access,4,in @ booming era of online social medium environment community question answering @ cqa @ site @ become @ of @ popular resource @ software engineer and software industry @ software engineer @ increasingly sharing @ question and answer on cqa site @ aim of @ cqa site @ to provide useful and relevant information to @ user @ analysis information of major programming language @ trend analysis @ @ useful @ software engineer to understand @ technological evolution and popularity @ since @ of @ cqa site consist of user assigned tag by @ folksonomy @ @ efficiently utilized @ developing suitable algorithm to find @ trend of key programming technology @ in @ @ @ technique of trend analysis namely arima time series model and fuzzy time series model @ @ applied on @ tagging data to identify @ trend @ trend analysis is @ carried on key programming language namely c java php and python @ in @ @ quality of @ trend is measured by entropy ztrend and quality of forecast is measured by mmre burst trend @ key programming language @ @ @ 
2370,Predicting abnormal bank stock returns using textual analysis of annual reports – A neural network approach,"This paper aims to extract both sentiment and bag-of-words information from the annual reports of U.S. banks. The sentiment analysis is based on two commonly used finance-specific dictionaries, while the bag-of-words are selected according to their tf-idf. We combine these features with financial indicators to predict abnormal bank stock returns using a neural network with dropout regularization and rectified linear units. We show that this method outperforms other machine learning algorithms (Naïve Bayes, Support Vector Machine, C4.5 decision tree, and k-nearest neighbour classifier) in predicting positive/negative abnormal stock returns. Thus, this neural network seems to be well suited for text classification tasks working with sparse high-dimensional data. We also show that the quality of the prediction significantly increased when using the combination of financial indicators and bigrams and trigrams, respectively. © Springer International Publishing Switzerland 2016.",2016,Communications in Computer and Information Science,7,@ @ aim to extract @ sentiment and bag-of-words information @ @ annual report of u @ s @ bank @ @ sentiment analysis is based on @ commonly used finance-specific dictionary @ @ bag-of-words @ selected according to @ tf-idf @ @ combine @ feature @ financial indicator to predict abnormal bank stock return @ a neural network @ dropout regularization and rectified linear unit @ @ @ @ @ method outperforms @ machine learning algorithm @ naïve bayes support vector machine c @ decision tree and k-nearest neighbour classifier @ in predicting positive negative abnormal stock return @ thus @ neural network seems to @ well suited @ text classification task working @ sparse high-dimensional data @ @ @ @ @ @ quality of @ prediction significantly increased @ @ @ combination of financial indicator and bigram and trigram respectively @ @ international publishing switzerland @ 
2371,Unsupervised trained functional discourse parser for e-learning materials scaffolding,"The article describes the way of automatic segmentation of natural language text into fragments with different functional semantics. The proposed solution is based on the analysis of how the various parts of speech are distributed through the text. The amount and variety of nouns, verbs and adjectives is calculated for a set of sliding windows with the same length. The text is divided into fragments using clustering of windows set. We considered two clustering methods: ISODATA and a method based on the minimum spanning tree. The results of comparison of the methods with each other and with the manually text markup are shown. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ article describes @ way of automatic segmentation of natural language text @ fragment @ different functional semantics @ @ proposed solution is based on @ analysis of @ @ various part of speech @ distributed @ @ text @ @ amount and variety of noun verb and adjective is calculated @ a set of sliding window @ @ @ length @ @ text is divided @ fragment @ clustering of window set @ @ considered @ clustering method @ isodata and a method based on @ minimum spanning tree @ @ @ of comparison of @ method @ @ @ and @ @ manually text markup @ @ @ @ international publishing switzerland @ 
2373,Learning to extract adverse drug reaction events from electronic health records in Spanish,"Objective: To tackle the extraction of adverse drug reaction events in electronic health records. The challenge stands in inferring a robust prediction model from highly unbalanced data. According to our manually annotated corpus, only 6% of the drug-disease entity pairs trigger a positive adverse drug reaction event and this low ratio makes machine learning tough. Method: We present a hybrid system utilising a self-developed morpho-syntactic and semantic analyser for medical texts in Spanish. It performs named entity recognition of drugs and diseases and adverse drug reaction event extraction. The event extraction stage operates using rule-based and machine learning techniques. Results: We assess both the base classifiers, namely a knowledge-based model and an inferred classifier, and also the resulting hybrid system. Moreover, for the machine learning approach, an analysis of each particular bio-cause triggering the adverse drug reaction is carried out. Conclusions: One of the contributions of the machine learning based system is its ability to deal with both intra-sentence and inter-sentence events in a highly skewed classification environment. Moreover, the knowledge-based and the inferred model are complementary in terms of precision and recall. While the former provides high precision and low recall, the latter is the other way around. As a result, an appropriate hybrid approach seems to be able to benefit from both approaches and also improve them. This is the underlying motivation for selecting the hybrid approach. In addition, this is the first system dealing with real electronic health records in Spanish. © 2016 Elsevier Ltd",2016,Expert Systems with Applications,21,objective @ to tackle @ extraction of adverse drug reaction event in electronic health record @ @ challenge stand in inferring a robust prediction model @ highly unbalanced data @ according to @ manually annotated corpus only of @ drug-disease entity pair trigger a positive adverse drug reaction event and @ low ratio make machine learning tough @ method @ @ @ a hybrid system utilising a self-developed morpho-syntactic and semantic analyser @ medical text in spanish @ @ performs named entity recognition of drug and disease and adverse drug reaction event extraction @ @ event extraction stage operates @ rule-based and machine learning technique @ @ @ @ ass @ @ base classifier namely a knowledge-based model and @ inferred classifier and @ @ resulting hybrid system @ moreover @ @ machine learning approach @ analysis of @ particular bio-cause triggering @ adverse drug reaction is carried @ @ conclusion @ @ of @ contribution of @ machine learning based system is @ ability to deal @ @ intra-sentence and inter-sentence event in a highly skewed classification environment @ moreover @ knowledge-based and @ inferred model @ complementary in term of precision and recall @ @ @ former provides high precision and low recall @ latter is @ @ way around @ a a @ @ appropriate hybrid approach seems to @ able to benefit @ @ approach and @ improve @ @ @ is @ underlying motivation @ selecting @ hybrid approach @ in addition @ is @ first system dealing @ real electronic health record in spanish @ @ ltd
2375,Exploring the leading authors and journals in major topics by citation sentences and topic modeling,"Citation plays an important role in understanding the knowledge sharing among scholars. Citation sentences embed useful contents that signify the influence of cited authors on shared ideas, and express own opinion of citing authors to others' articles. The purpose of the study is to provide a new lens to analyze the topical relationship embedded in the citation sentences in an integrated manner. To this end, we extract citation sentences from full-text articles in the field of Oncology. In addition, we adopt Author-Journal-Topic (AJT) model to take both authors and journals into consideration of topic analysis. For the study, we collect the 6,360 full-text articles from PubMed Central and select the top 15 journals on Oncology. By applying AJT model, we identify what the major topics are shared among researchers in Oncology and which authors and journal lead the idea exchange in sub-disciplines of Oncology.",2016,CEUR Workshop Proceedings,5,citation play @ important role in understanding @ knowledge sharing among scholar @ citation sentence embed useful content @ signify @ influence of cited author on shared idea and express @ opinion of citing author to others @ article @ @ purpose of @ study is to provide a @ lens to analyze @ topical relationship embedded in @ citation sentence in @ integrated manner @ to @ end @ extract citation sentence @ full-text article in @ field of oncology @ in addition @ adopt author-journal-topic @ ajt @ model to take @ author and journal @ consideration of topic analysis @ @ @ study @ collect @ full-text article @ pubmed central and select @ top journal on oncology @ by applying ajt model @ identify @ @ major topic @ shared among researcher in oncology and @ author and journal lead @ idea exchange in sub-disciplines of oncology @ 
2376,An empirical assessment of citation information in scientific summarization,"Considering the recent substantial growth of the publication rate of scientific results, nowadays the availability of effective and automated techniques to summarize scientific articles is of utmost importance. In this paper we investigate if and how we can exploit the citations of an article in order to better identify its relevant excerpts. By relying on the BioSumm2014 dataset, we evaluate the variation in performance of extractive summarization approaches when we consider the citations to extend or select the contents of an article to summarize. We compute the maximum ROUGE-2 scores that can be obtained when we summarize a paper by considering its contents together with its citations. We show that the inclusion of citation-related information brings to the generation of better summaries. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,considering @ recent substantial growth of @ publication rate of scientific @ nowadays @ availability of effective and automated technique to summarize scientific article is of utmost importance @ in @ @ @ investigate if and @ @ @ exploit @ citation of @ article in order to better identify @ relevant excerpt @ by relying on @ biosumm dataset @ evaluate @ variation in performance of extractive summarization approach @ @ consider @ citation to extend @ select @ content of @ article to summarize @ @ compute @ maximum rouge score @ @ @ obtained @ @ summarize a @ by considering @ content together @ @ citation @ @ @ @ @ inclusion of citation-related information brings to @ generation of better summary @ @ international publishing switzerland @ 
2377,Pattern and semantic analysis to improve unsupervised techniques for opinion target identification,"This research employs patterns and semantic analysis to improve the existing unsupervised opinion targets extraction technique. Two steps are employed to identify opinion targets: candidate selection and opinion targets selection. For candidate selection; a combined lexical based syntactic pattern is identified. For opinion targets selection, a hybrid approach that combines the existing likelihood ratio test technique with semantic based relatedness is proposed. The existing approach basically extracts frequently observed targets in text. However, analysis shows that not all target features occur frequently in the texts. Hence the hybrid technique is proposed to extract both frequent and infrequent targets. The proposed algorithm employs incremental approach to improve the performance of existing unsupervised mining of features by extracting infrequent features through semantic relatedness with frequent features based on lexical dictionary. Empirical results show that the hybrid technique with combined patterns outperforms the existing techniques.",2016,Kuwait Journal of Science,8,@ research employ pattern and semantic analysis to improve @ existing unsupervised opinion target extraction technique @ @ step @ employed to identify opinion target @ candidate selection and opinion target selection @ @ candidate selection @ a combined lexical based syntactic pattern is identified @ @ opinion target selection a hybrid approach @ combine @ existing likelihood ratio test technique @ semantic based relatedness is proposed @ @ existing approach basically extract frequently observed target in text @ however analysis @ @ not @ target feature occur frequently in @ text @ hence @ hybrid technique is proposed to extract @ frequent and infrequent target @ @ proposed algorithm employ incremental approach to improve @ performance of existing unsupervised mining of feature by extracting infrequent feature @ semantic relatedness @ frequent feature based on lexical dictionary @ empirical @ @ @ @ hybrid technique @ combined pattern outperforms @ existing technique @ 
2378,Large scale sentiment analysis on twitter with spark,"Sentiment analysis on Twitter data has attracted much attention recently. One of the system's key features, is the immediacy in communication with other users in an easy, user-friendly and fast way. Consequently, people tend to express their feelings freely, which makes Twitter an ideal source for accumulating a vast amount of opinions towards a wide diversity of topics. This amount of information offers huge potential and can be harnessed to receive the sentiment tendency towards these topics. However, since none can invest an infinite amount of time to read through these tweets, an automated decision making approach is necessary. Nevertheless, most existing solutions are limited in centralized environments only. Thus, they can only process at most a few thousand tweets. Such a sample, is not representative to define the sentiment polarity towards a topic due to the massive number of tweets published daily. In this paper, we go one step further and develop a novel method for sentiment learning in the Spark framework. Our algorithm exploits the hashtags and emoticons inside a tweet, as sentiment labels, and proceeds to a classification procedure of diverse sentiment types in a parallel and distributed manner. Moreover, we utilize Bloom filters to compact the storage size of intermediate data and boost the performance of our algorithm. Through an extensive experimental evaluation, we prove that our solution is efficient, robust and scalable and confirm the quality of our sentiment identification. © 2016, Copyright is with the authors.",2016,CEUR Workshop Proceedings,3,sentiment analysis on twitter data ha attracted much attention recently @ @ of @ system @ s key feature is @ immediacy in communication @ @ user in @ easy user-friendly and fast way @ consequently people tend to express @ feeling freely @ make twitter @ ideal source @ accumulating a vast amount of opinion towards a wide diversity of topic @ @ amount of information offer huge potential and @ @ harnessed to receive @ sentiment tendency towards @ topic @ however since none @ invest @ infinite amount of time to read @ @ tweet @ automated decision making approach is necessary @ nevertheless @ existing solution @ limited in centralized environment only @ thus @ @ only process at @ a @ thousand tweet @ @ a sample is not representative to define @ sentiment polarity towards a topic due to @ massive number of tweet published daily @ in @ @ @ go @ step @ and develop a novel method @ sentiment learning in @ spark framework @ @ algorithm exploit @ hashtags and emoticon inside a tweet a sentiment label and proceeds to a classification procedure of diverse sentiment type in a parallel and distributed manner @ moreover @ utilize bloom filter to compact @ storage size of intermediate data and boost @ performance of @ algorithm @ @ @ extensive experimental evaluation @ prove @ @ solution is efficient robust and scalable and confirm @ quality of @ sentiment identification @ @ is @ @ author @ 
2379,Text clustering and text summarization on the use of side information,"Clustering algorithm order information focuses on persuading social events concentrated around their similarity to abuse important data from data focuses. The end place of clustering these properties (text) has huge measure of information. It is difficult to measure relative data in light of the way in which the rate of the information is not clear. In such cases, it can be risky to partner side-data into the mining technique, since it can either build the nature of the representation for the mining system, then again add noise to the methodology. In various content mining applications, side-information is accessible nearby the content reports. Such text documents may be of a few sorts, for instance, record provenance information, the connections in the file, user access conduct from web logs, or other non-text based characteristics which are embedded into the content record. Such qualities may contain a massive measure of data for clustering purposes in the proposed system merge summarization methods. While executing the COATES estimation we used summarization system which is the union of duplicated clusters what’s more, give last summary. COATES cluster algorithms we get the clusters on the establishment of substance what’s more, auxiliary attributes. So in this project, an algorithm is designed, in order to give an effective clustering algorithm. Two algorithms are used in this project for clustering. In this paper COATES algorithm (this algorithm combines classical partitioning algorithms with probabilistic models) is used and the proposed system implements hierarchical algorithm which is compared with COATES algorithm and also implements the merging and summary generation algorithm which produces the summary or pure data for the user’s convenience. © Springer Science+Business Media Singapore 2016.",2016,Advances in Intelligent Systems and Computing,0,clustering algorithm order information focus on persuading social event concentrated around @ similarity to abuse important data @ data focus @ @ end place of clustering @ property @ text @ ha huge measure of information @ @ is difficult to measure relative data in light of @ way in @ @ rate of @ information is not clear @ in @ case @ @ @ risky to partner side-data @ @ mining technique since @ @ either build @ nature of @ representation @ @ mining system @ @ add noise to @ methodology @ in various content mining application side-information is accessible nearby @ content report @ @ text document may @ of a @ sort @ instance record provenance information @ connection in @ file user access conduct @ web log @ @ non-text based characteristic @ @ embedded @ @ content record @ @ quality may contain a massive measure of data @ clustering purpose in @ proposed system merge summarization method @ @ executing @ coates estimation @ used summarization system @ is @ union of duplicated cluster @ s more give last summary @ coates cluster algorithm @ get @ cluster on @ establishment of substance @ s more auxiliary attribute @ @ in @ project @ algorithm is designed in order to give @ effective clustering algorithm @ @ algorithm @ used in @ project @ clustering @ in @ @ coates algorithm @ @ algorithm combine classical partitioning algorithm @ probabilistic model @ is used and @ proposed system implement hierarchical algorithm @ is compared @ coates algorithm and @ implement @ merging and summary generation algorithm @ produce @ summary @ pure data @ @ user s convenience @ @ science @ medium singapore @ 
2380,Improvement in quality of extractive text summaries using modified reciprocal ranking,"Due to increasing amount of text data available in WWW, it becomes time consuming for information system users to explore every text source in detail. Automatic text summarization (ATS) is the process of generating summary by condensing text document automatically by a computer machine that can save users precious time. Major issue with most of the feature-based ATS methods is to find optimal feature weights for sentence scoring to optimize quality of text summary. This paper presents a novel voting-based approach that use modified reciprocal ranking approach which alleviates the issue of feature weighting and. Proposed approach use a specific prominent set of features for initial ranking that further boosts the performance. Experimental results on DUC 2002 dataset using ROUGE evaluation matrices show that our proposed voting approach performs better when compared to other statistical-and voting-based methods. © Springer Science+Business Media Singapore 2016.",2016,Advances in Intelligent Systems and Computing,0,due to increasing amount of text data available in www @ becomes time consuming @ information system user to explore every text source in detail @ automatic text summarization @ at @ is @ process of generating summary by condensing text document automatically by a computer machine @ @ save user precious time @ major issue @ @ of @ feature-based at method is to find optimal feature weight @ sentence scoring to optimize quality of text summary @ @ @ @ a novel voting-based approach @ use modified reciprocal ranking approach @ alleviates @ issue of feature weighting and @ proposed approach use a specific prominent set of feature @ initial ranking @ @ boost @ performance @ experimental @ on duc dataset @ rouge evaluation matrix @ @ @ proposed voting approach performs better @ compared to @ statistical-and voting-based method @ @ science @ medium singapore @ 
2381,A social stability analysis system based on web sensitive information mining,"Researches on domestic social stability analysis mainly focus on construction of social stability theory, architecture and index, while few pay attention on quantitative analysis. In this paper, a social stability supervising framework is proposed based on sensitive Web information mining, semantic pattern matching and quantitative calculating. A sensitive information knowledge base is constructed by analyzing sensitive information about social environment, national harmonious and happy index of people’s live in natural language online news texts from Internet, and recognizing hot keywords as well as the event trends led by the keywords. A social stability index theoretic model and a quantitative calculating model are proposed to evaluate social stability quantitatively. Parameters of the calculating model are determined by employing social investigations and an iterative feedback learning method. A prototype system is built on proposed framework and experiments are conducted on six frontier provinces, e.g., Xinjiang and Tibet. The result of an average accurate of 73.29 % shows the effectiveness of the proposed model. © Springer Science+Business Media Singapore 2016.",2016,Communications in Computer and Information Science,0,research on domestic social stability analysis mainly focus on construction of social stability theory architecture and index @ @ pay attention on quantitative analysis @ in @ @ a social stability supervising framework is proposed based on sensitive web information mining semantic pattern matching and quantitative calculating @ a sensitive information knowledge base is constructed by analyzing sensitive information @ social environment national harmonious and happy index of people s live in natural language online news text @ internet and recognizing hot keywords a well a @ event trend led by @ keywords @ a social stability index theoretic model and a quantitative calculating model @ proposed to evaluate social stability quantitatively @ parameter of @ calculating model @ determined by employing social investigation and @ iterative feedback learning method @ a prototype system is built on proposed framework and experiment @ conducted on six frontier province e @ g @ xinjiang and tibet @ @ @ of @ average accurate of @ @ @ effectiveness of @ proposed model @ @ science @ medium singapore @ 
2382,What makes consumers unsatisfied with your products: Review analysis at a fine-grained level,"Online product reviews contain valuable information regarding customer requirements (CRs). Intelligent analysis of a large volume of online CRs attracts interest from researchers in various fields. However, many research studies only concern sentiment polarity in the product feature level. With these results, designers still need to read a list of reviews to absorb comprehensive CRs. In this research, online reviews are analyzed at a fine-grained level. In particular, aspects of product features and detailed reasons of consumers are extracted from online reviews to inform designers regarding what leads to unsatisfied opinions. This research starts from the identification of product features and the sentiment analysis with the help of pros and cons reviews. Next, the approach of conditional random fields is employed to detect aspects of product features and detailed reasons from online reviews jointly. In addition, a co-clustering algorithm is devised to group similar aspects and reasons to provide a concise description about CRs. Finally, utilizing customer reviews of six mobiles in Amazon.com, a case study is presented to illustrate how the proposed approaches benefit product designers in the elicitation of CRs by the analysis of online opinion data. © 2015 Elsevier Ltd.",2016,Engineering Applications of Artificial Intelligence,24,online product review contain valuable information regarding customer requirement @ cr @ @ intelligent analysis of a @ volume of online cr attracts interest @ researcher in various field @ however many research study only concern sentiment polarity in @ product feature level @ @ @ @ designer still need to read a list of review to absorb comprehensive cr @ in @ research online review @ analyzed at a fine-grained level @ in particular aspect of product feature and detailed reason of consumer @ extracted @ online review to inform designer regarding @ lead to unsatisfied opinion @ @ research start @ @ identification of product feature and @ sentiment analysis @ @ help of pro and con review @ next @ approach of conditional random field is employed to detect aspect of product feature and detailed reason @ online review jointly @ in addition a co-clustering algorithm is devised to group similar aspect and reason to provide a concise description @ cr @ finally utilizing customer review of six mobile in amazon @ com a case study is presented to illustrate @ @ proposed approach benefit product designer in @ elicitation of cr by @ analysis of online opinion data @ @ ltd @ 
2383,A comparison between preprocessing techniques for sentiment analysis in Twitter,"In recent years, Sentiment Analysis has become one of the most interesting topics in AI research due to its promising commercial benefits. An important step in a Sentiment Analysis system for text mining is the preprocessing phase, but it is often underestimated and not extensively covered in literature. In this work, our aim is to highlight the importance of preprocessing techniques and show how they can improve system accuracy. In particular, some different preprocessing methods are presented and the accuracy of each of them is compared with the others. The purpose of this comparison is to evaluate which techniques are effective. In this paper, we also present the reasons why the accuracy improves, by means of a precise analysis of each method. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,34,in recent year sentiment analysis ha become @ of @ @ interesting topic in ai research due to @ promising commercial benefit @ @ important step in a sentiment analysis system @ text mining is @ preprocessing phase @ @ is often underestimated and not extensively covered in literature @ in @ work @ aim is to highlight @ importance of preprocessing technique and @ @ @ @ improve system accuracy @ in particular some different preprocessing method @ presented and @ accuracy of @ of @ is compared @ @ others @ @ purpose of @ comparison is to evaluate @ technique @ effective @ in @ @ @ @ @ @ reason @ @ accuracy improves by mean of a precise analysis of @ method @ ceur-ws @ @ right reserved @ 
2384,"One tagger, many uses: Illustrating the power of ontologies in dictionary-based named entity recognition","Automatic annotation of text is an important complement to manual annotation, because the latter is highly labour intensive. We have developed a fast dictionary-based named entity recognition (NER) system and addressed a wide variety of biomedical problems by applied it to text from many different sources. We have used this tagger both in real-time tools to support curation efforts and in pipelines for populating databases through bulk processing of entire Medline, the open-access subset of PubMed Central, NIH grant abstracts, FDA drug labels, electronic health records, and the Encyclopedia of Life. Despite the simplicity of the approach, it typically achieves 80-90% precision and 70-80% recall. Many of the underlying dictionaries were built from open biomedical ontologies, which further facilitate integration of the text-mining results with evidence from other sources. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,0,automatic annotation of text is @ important complement to manual annotation @ @ latter is highly labour intensive @ @ @ developed a fast dictionary-based named entity recognition @ ner @ system and addressed a wide variety of biomedical problem by applied @ to text @ many different source @ @ @ used @ tagger @ in real-time tool to support curation effort and in pipeline @ populating database @ bulk processing of entire medline @ open-access subset of pubmed central nih grant abstract fda drug label electronic health record and @ encyclopedia of life @ despite @ simplicity of @ approach @ typically achieves precision and recall @ many of @ underlying dictionary @ built @ open biomedical ontology @ @ facilitate integration of @ text-mining @ @ evidence @ @ source @ ceur-ws @ @ right reserved @ 
2385,BOWL: Bag of word clusters text representation using word embeddings,"The text representation is fundamental for text mining and information retrieval. The Bag Of Words (BOW) and its variants (e.g. TF-IDF) are very basic text representation methods. Although the BOW and TF-IDF are simple and perform well in tasks like classification and clustering, its representation efficiency is extremely low. Besides, word level semantic similarity is not captured which results failing to capture text level similarity in many situations. In this paper, we propose a straightforward Bag Of Word cLusters (BOWL) representation for texts in a higher level, much lower dimensional space. We exploit the word embeddings to group semantically close words and consider them as a whole. The word embeddings are trained on a large corpus and incorporate extensive knowledge. We demonstrate on three benchmark datasets and two tasks, that BOWL representation shows significant advantages in terms of representation accuracy and efficiency. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ text representation is fundamental @ text mining and information retrieval @ @ bag of word @ bow @ and @ variant @ e @ g @ tf-idf @ @ @ basic text representation method @ although @ bow and tf-idf @ simple and perform well in task like classification and clustering @ representation efficiency is extremely low @ besides word level semantic similarity is not captured @ @ failing to capture text level similarity in many situation @ in @ @ @ propose a straightforward bag of word cluster @ bowl @ representation @ text in a higher level much lower dimensional space @ @ exploit @ word embeddings to group semantically close word and consider @ a a whole @ @ word embeddings @ trained on a @ corpus and incorporate extensive knowledge @ @ demonstrate on three benchmark datasets and @ task @ bowl representation @ significant advantage in term of representation accuracy and efficiency @ @ international publishing ag @ 
2386,Extending framenet to machine learning domain,"In recent years, several ontological resources have been proposed to model machine learning domain. However, they do not provide a direct link to linguistic data. In this paper, we propose a linguistic resource, a set of several semantic frames with associated annotated initial corpus in machine learning domain, we coined MLFrameNet. We have bootstrapped the process of (manual) frame creation by text mining on the set of 1293 articles from the Machine Learning Journal from about 100 volumes of the journal. It allowed us to find frequent occurences of words and bigrams serving as candidates for lexical units and frame elements. We bridge the gap between linguistics analysis and formal ontologies by typing the frame elements with semantic types from the DMOP domain ontology. The resulting resource is aimed to facilitate tasks such as knowledge extraction, question answering, summarization etc. in machine learning domain.",2016,CEUR Workshop Proceedings,1,in recent year several ontological resource @ @ proposed to model machine learning domain @ however @ @ not provide a direct link to linguistic data @ in @ @ @ propose a linguistic resource a set of several semantic frame @ associated annotated initial corpus in machine learning domain @ coined mlframenet @ @ @ bootstrapped @ process of @ manual @ frame creation by text mining on @ set of article @ @ machine learning journal @ @ volume of @ journal @ @ allowed u to find frequent occurences of word and bigram serving a candidate @ lexical unit and frame element @ @ bridge @ gap @ linguistics analysis and formal ontology by typing @ frame element @ semantic type @ @ dmop domain ontology @ @ resulting resource is aimed to facilitate task @ a knowledge extraction question answering summarization etc @ in machine learning domain @ 
2387,Integrated feature selection methods using metaheuristic algorithms for sentiment analysis,"In text mining, the feature selection process can potentially improve classification accuracy by reducing the high-dimensional feature space to a low-dimensional feature space resulting in an optimal subset of available features. In this paper, a hybrid method and two meta-heuristic algorithms are employed to find an optimal feature subset. The feature selection task is performed in two steps: first, different feature subsets (called local-solutions) are obtained using a hybrid filter and wrapper approaches to reduce high-dimensional feature space; second, local-solutions are integrated using two meta-heuristic algorithms (namely, the harmony search algorithm and the genetic algorithm) in order to find an optimal feature subset. The results of a wide range of comparative experiments on three widely-used datasets in sentiment analysis show that the proposed method for feature selection outperforms other baseline methods in terms of accuracy. © Springer-Verlag Berlin Heidelberg 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,in text mining @ feature selection process @ potentially improve classification accuracy by reducing @ high-dimensional feature space to a low-dimensional feature space resulting in @ optimal subset of available feature @ in @ @ a hybrid method and @ meta-heuristic algorithm @ employed to find @ optimal feature subset @ @ feature selection task is performed in @ step @ first different feature subset @ called local-solutions @ @ obtained @ a hybrid filter and wrapper approach to reduce high-dimensional feature space @ second local-solutions @ integrated @ @ meta-heuristic algorithm @ namely @ harmony search algorithm and @ genetic algorithm @ in order to find @ optimal feature subset @ @ @ of a wide range of comparative experiment on three widely-used datasets in sentiment analysis @ @ @ proposed method @ feature selection outperforms @ baseline method in term of accuracy @ springer-verlag @ @ @ 
2391,Gold-standard ontology-based annotation of concepts in biomedical text in the CRAFT corpus: Updates and extensions,"Ontologies are increasingly used for semantic integration across disparate curated biomedical resources, while gold-standard annotated corpora are needed for accurate training and evaluation of text-mining tools. Bringing together the respective power of these, we created the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of full-length, open-access biomedical journal articles that have been manually annotated both syntactically and semantically with select Open Biomedical Ontologies (OBOs), the first release of which includes ∼100,000 annotations of concepts mentioned in the text of 67 articles and mapped to the classes of eight prominent OBOs. Here we present our continuing work on the corpus, including updated versions of these annotations with newer versions of the ontologies, new annotations made with two additional OBOs, annotations made with newly created extension classes defined in terms of existing classes of the ontologies, and new annotations of roots of prefixed and suffixed words. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,0,ontology @ increasingly used @ semantic integration across disparate curated biomedical resource @ gold-standard annotated corpus @ needed @ accurate training and evaluation of text-mining tool @ bringing together @ respective power of @ @ created @ colorado richly annotated full-text @ craft @ corpus a collection of full-length open-access biomedical journal article @ @ @ manually annotated @ syntactically and semantically @ select open biomedical ontology @ obos @ @ first release of @ includes annotation of concept mentioned in @ text of article and mapped to @ class of eight prominent obos @ @ @ @ @ continuing work on @ corpus including updated version of @ annotation @ newer version of @ ontology @ annotation made @ @ additional obos annotation made @ newly created extension class defined in term of existing class of @ ontology and @ annotation of root of prefixed and suffixed word @ ceur-ws @ @ right reserved @ 
2393,Stance analysis for debates on traditional chinese medicine at Tianya forum,"Internet and social media devices have created a new public space for debates on societal topics. This paper applies text mining methods to conduct stance analysis of on-line debates with the illustration of debates on traditional Chinese medicine (TCM) at one famous Chinese BBS Tianya Froum. After crawling and preprocessing data, logistic regression is adopted to get a domain lexicon. Words in the lexicon are taken as features to automatically distinguish stances. Furthermore a topic model latent Dirichlet allocation (LDA) is utilized to discover shared topics of different camps. Then further analysis is conducted to detect the focused technical terms of TCM and human names referred during the debates. The classification results reveal that using domain discriminating words as features of classifier outperforms taking nouns, verbs, adjectives and adverbs as features. The results of topic modeling and further analysis enable us to see how the different camps express their stances. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,internet and social medium device @ created a @ public space @ debate on societal topic @ @ @ applies text mining method to conduct stance analysis of on-line debate @ @ illustration of debate on traditional chinese medicine @ tcm @ at @ famous chinese bb tianya froum @ @ crawling and preprocessing data logistic regression is adopted to get a domain lexicon @ word in @ lexicon @ taken a feature to automatically distinguish stance @ furthermore a topic model latent dirichlet allocation @ lda @ is utilized to discover shared topic of different camp @ @ @ analysis is conducted to detect @ focused technical term of tcm and human name referred @ @ debate @ @ classification @ reveal @ @ domain discriminating word a feature of classifier outperforms taking noun verb adjective and adverb a feature @ @ @ of topic modeling and @ analysis enable u to see @ @ different camp express @ stance @ @ international publishing switzerland @ 
2394,Sentiment analysis of customer reviews using robust hierarchical bidirectional recurrent neural network,"With tremendous growth of online content, sentiment analysis of customer reviews has become an active research topic for machine learning community. However, due to variety of products being reviewed online traditional methods do not give desirable results. As number of reviews expand, it is essential to develop robust sentiment analysis model capable of extracting product aspects and determine sentiments adhering to various accuracy measures. Here, hierarchical bidirectional recurrent neural network (HBRNN) is developed in order to characterize sentiment specific aspects in review data available at DBS Text Mining Challenge. HBRNN predicts aspect sentiments vector at reviewlevel.HBRNNis optimized by fine tuning different network parameters and compared with methods like long short termmemory (LSTM) and bidirectional LSTM (BLSTM). The methods are evaluated with highly skewed data. All models are evaluated using precision, recall and F1 scores. The results on experimental dataset indicate superiority of HBRNN over other techniques. © Springer International Publishing Switzerland 2016.",2016,Advances in Intelligent Systems and Computing,10,@ tremendous growth of online content sentiment analysis of customer review ha become @ active research topic @ machine learning community @ however due to variety of product @ reviewed online traditional method @ not give desirable @ @ a number of review expand @ is essential to develop robust sentiment analysis model capable of extracting product aspect and determine sentiment adhering to various accuracy measure @ @ hierarchical bidirectional recurrent neural network @ hbrnn @ is developed in order to characterize sentiment specific aspect in review data available at db text mining challenge @ hbrnn predicts aspect sentiment vector at reviewlevel @ hbrnnis optimized by fine tuning different network parameter and compared @ method like long short termmemory @ lstm @ and bidirectional lstm @ blstm @ @ @ method @ evaluated @ highly skewed data @ @ model @ evaluated @ precision recall and f score @ @ @ on experimental dataset indicate superiority of hbrnn @ @ technique @ @ international publishing switzerland @ 
2395,Identifying crop specific named entities from agriculture domain using semantic vector,"Named entity extraction is the most primitive task in the field of text mining. This paper is a preliminary attempt to identify domain specific named entities, specifically crop names from text documents in Agriculture domain. The task is challenging as the names of these entities are very generic and hence word level features are not very helpful in differentiating them from routine words. Thus in this paper we have suggested a semantic vector based approach. Two different methods have been suggested, that are based on exploiting the context of the words in order to extract these entities. The methods accept few seed entities, identify their context and then find other words that are sharing the similar context. These words sharing the similar context are expected to be newly identified entities. Considering this as an initial attempt, the results are motivating and inspire us to move further in this direction. © Springer India 2016.",2016,Advances in Intelligent Systems and Computing,1,named entity extraction is @ @ primitive task in @ field of text mining @ @ @ is a preliminary attempt to identify domain specific named entity specifically crop name @ text document in agriculture domain @ @ task is challenging a @ name of @ entity @ @ generic and hence word level feature @ not @ helpful in differentiating @ @ routine word @ thus in @ @ @ @ suggested a semantic vector based approach @ @ different method @ @ suggested @ @ based on exploiting @ context of @ word in order to extract @ entity @ @ method accept @ seed entity identify @ context and @ find @ word @ @ sharing @ similar context @ @ word sharing @ similar context @ expected to @ newly identified entity @ considering @ a @ initial attempt @ @ @ motivating and inspire u to move @ in @ direction @ @ india @ 
2397,Conceptual modeling with Formal Concept Analysis on natural language texts,"The paper presents conceptual modelling technique on natural language texts. This technique combines the usage of two conceptual modeling paradigms: conceptual graphs and Formal Concept Analysis. Conceptual graphs serve as semantic models of text sentences and the data source for concept lattice - the basic conceptual model in Formal Concept Analysis. With the use of conceptual graphs the Text Mining problems of Named Entity Recognition and Relations Extraction are solved. Then these solutions are applied for creating concept lattice. The main problem investigated in the paper is the problem of creating formal contexts on a set of conceptual graphs. Its solution is based on the analysis of semantic roles and conceptual patterns in conceptual graphs. Concept lattice built on textual data is applied for knowledge extraction. Knowledge, sometimes interpreted as facts, can be extracted by using navigation in the lattice and interpretation its concepts and hierarchical links between them. Experimental investigation of the proposed technique is performed on the annotated textual corpus consisted of descriptions of biotopes of bacteria.",2016,CEUR Workshop Proceedings,3,@ @ @ conceptual modelling technique on natural language text @ @ technique combine @ usage of @ conceptual modeling paradigm @ conceptual graph and formal concept analysis @ conceptual graph serve a semantic model of text sentence and @ data source @ concept lattice @ basic conceptual model in formal concept analysis @ @ @ use of conceptual graph @ text mining problem of named entity recognition and relation extraction @ solved @ @ @ solution @ applied @ creating concept lattice @ @ main problem investigated in @ @ is @ problem of creating formal context on a set of conceptual graph @ @ solution is based on @ analysis of semantic role and conceptual pattern in conceptual graph @ concept lattice built on textual data is applied @ knowledge extraction @ knowledge sometimes interpreted a fact @ @ extracted by @ navigation in @ lattice and interpretation @ concept and hierarchical link @ @ @ experimental investigation of @ proposed technique is performed on @ annotated textual corpus consisted of description of biotopes of bacteria @ 
2399,SS4MCT: A statistical stemmer for morphologically complex texts,"There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complex texts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in different text mining tasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ multiple attempt to resolve various inflection matching problem in information retrieval @ stemming is a common approach to @ end @ among many technique @ stemming statistical stemming ha @ @ to @ effective in a number of language particularly highly inflected language @ in @ @ @ propose a method @ finding affix in different position of a word @ common statistical technique heavily rely on string similarity in term of prefix and suffix matching @ since infix @ common in irregular informal inflection in morphologically complex text @ is required to find infix @ stemming @ in @ @ @ propose a method whose aim is to find statistical inflectional rule based on minimum edit distance table of word pair and @ likelihood of @ rule in a language @ @ rule @ used to statistically stem word and @ @ used in different text mining task @ experimental @ on clef and clef english-persian clir task indicate @ @ proposed method significantly outperforms @ @ baseline in term of map @ @ international publishing switzerland @ 
2400,PHMM: Stemming on Persian texts using statistical stemmer based on hidden Markov model,"Stemming is the process of finding the main morpheme of a word and it is used in natural language processing, text mining and information retrieval systems. A stemmer extracts the stem of the words. Persian stemmers are classified into three main classes: structural stemmers, dictionary based stemmers, and statistical stemmers. The precision of structural stemmers is low and the expenses of dictionary based stemmers is high; therefore, the main goal of this research was to design and implement a statistical stemmer based on Hidden Markov Model with high precision in order to reduce the size of indexed file and increase the speed of information retrieval systems. In the present study, the proposed stemmer finds the prefixes and suffixes of a word and removes them, so that the rest of the word is considered to be the stem. But there are some exceptions in Persian words which would be considered as a stem mistakenly. So, at first a dictionary of Persian stemmers was collected and after that the proposed stemmer searched a word in the dictionary, if the word was not there, the stemmer found the stem of it by HMM based stemmer. This stemmer was tested in Bijankhan corpus and Hamshahri test collection. The results showed increment in mean average precision and recall. The speed of the Information retrieval system was increased and the size of indexed files were decreased by the algorithm.",2016,International Journal of Information Science and Management,3,stemming is @ process of finding @ main morpheme of a word and @ is used in natural language processing text mining and information retrieval system @ a stemmer extract @ stem of @ word @ persian stemmer @ classified @ three main class @ structural stemmer dictionary based stemmer and statistical stemmer @ @ precision of structural stemmer is low and @ expense of dictionary based stemmer is high @ therefore @ main goal of @ research wa to design and implement a statistical stemmer based on hidden markov model @ high precision in order to reduce @ size of indexed file and increase @ speed of information retrieval system @ in @ @ study @ proposed stemmer find @ prefix and suffix of a word and remove @ @ @ @ rest of @ word is considered to @ @ stem @ @ @ @ some exception in persian word @ would @ considered a a stem mistakenly @ @ at first a dictionary of persian stemmer wa collected and @ @ @ proposed stemmer searched a word in @ dictionary if @ word wa not @ @ stemmer found @ stem of @ by hmm based stemmer @ @ stemmer wa tested in bijankhan corpus and hamshahri test collection @ @ @ showed increment in mean average precision and recall @ @ speed of @ information retrieval system wa increased and @ size of indexed file @ decreased by @ algorithm @ 
2401,BRONCO: Biomedical entity Relation ONcology COrpus for extracting gene-variant-disease-drug relations,"Comprehensive knowledge of genomic variants in a biological context is key for precision medicine. As next-generation sequencing technologies improve, the amount of literature containing genomic variant data, such as new functions or related phenotypes, rapidly increases. Because numerous articles are published every day, it is almost impossible to manually curate all the variant information from the literature. Many researchers focus on creating an improved automated biomedical natural language processing (BioNLP) method that extracts useful variants and their functional information from the literature. However, there is no gold-standard data set that contains texts annotated with variants and their related functions. To overcome these limitations, we introduce a Biomedical entity Relation ONcology COrpus (BRONCO) that contains more than 400 variants and their relations with genes, diseases, drugs and cell lines in the context of cancer and anti-tumor drug screening research. The variants and their relations were manually extracted from 108 full-text articles. BRONCO can be utilized to evaluate and train new methods used for extracting biomedical entity relations from full-text publications, and thus be a valuable resource to the biomedical text mining research community. Using BRONCO, we quantitatively and qualitatively evaluated the performance of three state-of-the-art BioNLP methods. We also identified their shortcomings, and suggested remedies for each method. We implemented post-processing modules for the three BioNLP methods, which improved their performance. © The Author(s) 2016.",2016,Database,19,comprehensive knowledge of genomic variant in a biological context is key @ precision medicine @ a next-generation sequencing technology improve @ amount of literature containing genomic variant data @ a @ function @ related phenotype rapidly increase @ @ numerous article @ published every day @ is almost impossible to manually curate @ @ variant information @ @ literature @ many researcher focus on creating @ improved automated biomedical natural language processing @ bionlp @ method @ extract useful variant and @ functional information @ @ literature @ however @ is no gold-standard data set @ contains text annotated @ variant and @ related function @ to overcome @ limitation @ introduce a biomedical entity relation oncology corpus @ bronco @ @ contains more @ variant and @ relation @ gene disease drug and cell line in @ context of cancer and anti-tumor drug screening research @ @ variant and @ relation @ manually extracted @ full-text article @ bronco @ @ utilized to evaluate and train @ method used @ extracting biomedical entity relation @ full-text publication and thus @ a valuable resource to @ biomedical text mining research community @ @ bronco @ quantitatively and qualitatively evaluated @ performance of three state-of-the-art bionlp method @ @ @ identified @ shortcoming and suggested remedy @ @ method @ @ implemented post-processing module @ @ three bionlp method @ improved @ performance @ @ author @ s @ @ 
2402,Knowledge extraction and modeling from scientific publications,"During the last decade the amount of scientific articles available online has substantially grown in parallel with the adoption of the Open Access publishing model. Nowadays researchers, as well as any other interested actor, are often overwhelmed by the enormous and continuously growing amount of publications to consider in order to perform any complete and careful assessment of scientific literature. As a consequence, new methodologies and automated tools to ease the extraction, semantic representation and browsing of information from papers are necessary. We propose a platform to automatically extract, enrich and characterize several structural and semantic aspects of scientific publications, representing them as RDF datasets. We analyze papers by relying on the scientific Text Mining Framework developed in the context of the European Project Dr. Inventor. We evaluate how the Framework supports two core scientific text analysis tasks: rhetorical sentence classification and extractive text summarization. To ease the exploration of the distinct facets of scientific knowledge extracted by our platform, we present a set of tailored Web visualizations. We provide on-line access to both the RDF datasets and the Web visualizations generated by mining the papers of the 2015 ACL-IJCNLP Conference. © Springer International Publishing AG 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),16,@ @ last decade @ amount of scientific article available online ha substantially grown in parallel @ @ adoption of @ open access publishing model @ nowadays researcher a well a @ @ interested actor @ often overwhelmed by @ enormous and continuously growing amount of publication to consider in order to perform @ complete and careful assessment of scientific literature @ a a consequence @ methodology and automated tool to ease @ extraction semantic representation and browsing of information @ @ @ necessary @ @ propose a platform to automatically extract enrich and characterize several structural and semantic aspect of scientific publication representing @ a rdf datasets @ @ analyze @ by relying on @ scientific text mining framework developed in @ context of @ european project dr @ inventor @ @ evaluate @ @ framework support @ core scientific text analysis task @ rhetorical sentence classification and extractive text summarization @ to ease @ exploration of @ distinct facet of scientific knowledge extracted by @ platform @ @ a set of tailored web visualization @ @ provide on-line access to @ @ rdf datasets and @ web visualization generated by mining @ @ of @ acl-ijcnlp conference @ @ international publishing ag @ 
2403,Biomedical disease name entity recognition using NCBI corpus,"Named Entity Recognition (NER) in biomedical literature is a very active research area. NER is a crucial component of biomedical text mining because it allows for information retrieval, reasoning and knowledge discovery. Much research has been carried out in this area using semantic type categories, such as ""DNA"", ""RNA"", ""proteins"" and ""genes"". However, disease NER has not received its needed attention yet, specifically human disease NER. Traditional machine learning approaches lack the precision for disease NER, due to their dependence on token level features, sentence level features and the integration of features, such as orthographic, contextual and linguistic features. In this paper a method for disease NER is proposed which utilizes sentence and token level features based on Conditional Random Fields using the NCBI disease corpus. Our system utilizes rich features including orthographic, contextual, affixes, bigrams, part of speech and stem based features. Using these feature sets our approach has achieved a maximum F-score of 94% for the training set by applying 10 fold cross validation for semantic labeling of the NCBI disease corpus. For testing and development corpus the model has achieved an F-score of 88% and 85% respectively. © 2016, CEUR-WS. All rights reserved.",2016,CEUR Workshop Proceedings,0,named entity recognition @ ner @ in biomedical literature is a @ active research area @ ner is a crucial component of biomedical text mining @ @ allows @ information retrieval reasoning and knowledge discovery @ much research ha @ carried @ in @ area @ semantic type category @ a @ dna @ @ rna @ @ protein @ and @ gene @ @ however disease ner ha not received @ needed attention yet specifically human disease ner @ traditional machine learning approach lack @ precision @ disease ner due to @ dependence on token level feature sentence level feature and @ integration of feature @ a orthographic contextual and linguistic feature @ in @ @ a method @ disease ner is proposed @ utilizes sentence and token level feature based on conditional random field @ @ ncbi disease corpus @ @ system utilizes rich feature including orthographic contextual affix bigram part of speech and stem based feature @ @ @ feature set @ approach ha achieved a maximum f-score of @ @ training set by applying fold cross validation @ semantic labeling of @ ncbi disease corpus @ @ testing and development corpus @ model ha achieved @ f-score of and respectively @ ceur-ws @ @ right reserved @ 
2404,An automatic workflow for the formalization of scholarly articles’ structural and semantic elements,"We present a workflow for the automatic transformation of scholarly literature to a Linked Open Data (LOD) compliant knowledge base to address Task 2 of the Semantic Publishing Challenge 2016. In this year’s task, we aim to extract various contextual information from full-text papers using a text mining pipeline that integrates LOD-based Named Entity Recognition (NER) and triplification of the detected entities. In our proposed approach, we leverage an existing NER tool to ground named entities, such as geographical locations, to their LOD resources. Combined with a rule-based approach, we demonstrate how we can extract both the structural (e.g., floats and sections) and semantic elements (e.g., authors and their respective affiliations) of the provided dataset’s documents. Finally, we integrate the LODeXporter, our flexible exporting module to represent the results as semantic triples in RDF format. As the result, we generate a scalable, TDB-based knowledge base that is interlinked with the LOD cloud, and a public SPARQL endpoint for the task’s queries. Our submission won the second place at the Sem- Pub2016 challenge Task 2 with an average 0.63 F-score. © Springer International Publishing Switzerland 2016.",2016,Communications in Computer and Information Science,1,@ @ a workflow @ @ automatic transformation of scholarly literature to a linked open data @ lod @ compliant knowledge base to address task of @ semantic publishing challenge @ in @ year s task @ aim to extract various contextual information @ full-text @ @ a text mining pipeline @ integrates lod-based named entity recognition @ ner @ and triplification of @ detected entity @ in @ proposed approach @ leverage @ existing ner tool to ground named entity @ a geographical location to @ lod resource @ combined @ a rule-based approach @ demonstrate @ @ @ extract @ @ structural @ e @ g @ float and section @ and semantic element @ e @ g @ author and @ respective affiliation @ of @ provided dataset s document @ finally @ integrate @ lodexporter @ flexible exporting module to represent @ @ a semantic triple in rdf format @ a @ @ @ generate a scalable tdb-based knowledge base @ is interlinked @ @ lod cloud and a public sparql endpoint @ @ task s query @ @ submission @ @ second place at @ sem pub challenge task @ @ average @ f-score @ @ international publishing switzerland @ 
2405,BiTeM at CLEFeHealth Evaluation Lab 2016Task 2: Multilingual informationextraction,"BiTeM/SIB Text Mining (http://bitem.hesge.ch/) is a University re-search group carrying over activities in semantic and text analytics applied to health and life sciences. This paper reports on the participation of our team at the CLEF eHealth 2016 evaluation lab. The processing applied to each evaluation corpus (QUAREO and CépiDC) was originally very similar. Our method is based on an Au-Tomatic Text Categorization (ATC) system. First, the system is set with a specific input ontology (French UMLS), and ATC assigns a rank list of related concepts to each document received in input. Then, a second module relocates all of the positive matches in the text, and normalizes the extracted entities. For the CépiDC corpus, the system was loaded with the Swiss ICD-10 GM thesaurus. However a late minute data transformation issue forced us to implement an ad hoc solution based on simple pat-Tern matching to comply with the constraints of the CépiDC challenge. We obtained an average precision of 62% on the QUAREO entity extraction (over MEDLINE/EMEA texts, and exact/inexact), 48% on normalizing this entities, and 59% on the CépiDC subtask. Enhancing the recall by expanding the coverage of the terminologies could be an interesting approach to improve this system at moderate labour costs.",2016,CEUR Workshop Proceedings,5,bitem sib text mining @ http @ bitem @ hesge @ ch @ is a university re-search group carrying @ activity in semantic and text analytics applied to health and life science @ @ @ report on @ participation of @ team at @ clef ehealth evaluation lab @ @ processing applied to @ evaluation corpus @ quareo and cépidc @ wa originally @ similar @ @ method is based on @ au-tomatic text categorization @ atc @ system @ first @ system is set @ a specific input ontology @ french umls @ and atc assigns a rank list of related concept to @ document received in input @ @ a second module relocates @ of @ positive match in @ text and normalizes @ extracted entity @ @ @ cépidc corpus @ system wa loaded @ @ swiss icd gm thesaurus @ however a late minute data transformation issue forced u to implement @ ad hoc solution based on simple pat-tern matching to comply @ @ constraint of @ cépidc challenge @ @ obtained @ average precision of on @ quareo entity extraction @ @ medline emea text and exact inexact @ on normalizing @ entity and on @ cépidc subtask @ enhancing @ recall by expanding @ coverage of @ terminology could @ @ interesting approach to improve @ system at moderate labour cost @ 
2406,MHCQ: A tool for measuring health care quality-using Sentimental Analysis,"Sentimental Analysis (SA) is a text mining or Opinion Mining (OM) approach in which the opinion and an emotion of human beings related to an entity is analyzed. Sentic computing is a multi-disciplinary approach to sentiment analysis that exploits both computer and society to better recognize, interpret and process opinions and sentiments that are available online. Sentic computing is mainly used to detect the emotions of the users and analyze free text opinions of unstructured data. Measuring the quality of health care is helpful in knowing the performance of the health care system and provides the way for improvement. This study has proposed an easy-to-use tool to monitor and analyze patient opinion on a regular basis and calculates the quality of health care systems that are available in the hospitals. Usually the patients are willing to express their opinions and feelings in free text, rather than simply filling in a questionnaire, speaking out their satisfaction. This proposed tool, MHCQ allows the patients to enter their health status using an android app and evaluates their health status by means of sentic computing. The performance parameters such as precision, recall and F-measure are used to measure the mood and feeling of the patients regarding the health care system provided. © Medwell Journals, 2016.",2016,Asian Journal of Information Technology,0,sentimental analysis @ sa @ is a text mining @ opinion mining @ om @ approach in @ @ opinion and @ emotion of human @ related to @ entity is analyzed @ sentic computing is a multi-disciplinary approach to sentiment analysis @ exploit @ computer and society to better recognize interpret and process opinion and sentiment @ @ available online @ sentic computing is mainly used to detect @ emotion of @ user and analyze free text opinion of unstructured data @ measuring @ quality of health care is helpful in knowing @ performance of @ health care system and provides @ way @ improvement @ @ study ha proposed @ easy-to-use tool to monitor and analyze patient opinion on a regular basis and calculates @ quality of health care system @ @ available in @ hospital @ usually @ patient @ willing to express @ opinion and feeling in free text rather @ simply filling in a questionnaire speaking @ @ satisfaction @ @ proposed tool mhcq allows @ patient to enter @ health status @ @ android app and evaluates @ health status by mean of sentic computing @ @ performance parameter @ a precision recall and f-measure @ used to measure @ mood and feeling of @ patient regarding @ health care system provided @ medwell journal @ 
2408,"Enhanced rules application order to stem affixation, reduplication and compounding words in malay texts","Word stemmer is an automated program to remove affixes, clitics and particles from derived words based on morphological structures of specific natural languages. It has been widely used for text preprocessing in many artificial intelligence applications. Furthermore, the performance of word stemmer to correctly stem derived words has an influence to the performance of information retrieval, text mining and text categorization applications. Despite of various stemming approaches were proposed in the past research, the existing word stemmers for Malay language still suffer from stemming errors. Moreover, the existing word stemmers partially consider morphological structures of Malay language in which only focused on affixation words instead of affixation, reduplication and compounding words, simultaneously. Therefore, this paper proposes an enhanced word stemmer using rule-based affixes removal and dictionary lookup methods called enhanced rule application order that is able to stem affixation, reduplication and compounding words and at the same time, is able to address possible stemming errors. This paper also examines possible root causes of affixation, reduplication and compounding stemming errors that could happen during word stemming process. The experimental results indicate that the proposed word stemmer is able to stem affixation, reduplication and compounding words with better stemming accuracy by using enhanced rule application order. © Springer International Publishing Switzerland 2016.",2016,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,word stemmer is @ automated program to remove affix clitics and particle @ derived word based on morphological structure of specific natural language @ @ ha @ widely used @ text preprocessing in many artificial intelligence application @ furthermore @ performance of word stemmer to correctly stem derived word ha @ influence to @ performance of information retrieval text mining and text categorization application @ despite of various stemming approach @ proposed in @ past research @ existing word stemmer @ malay language still suffer @ stemming error @ moreover @ existing word stemmer partially consider morphological structure of malay language in @ only focused on affixation word instead of affixation reduplication and compounding word simultaneously @ therefore @ @ proposes @ enhanced word stemmer @ rule-based affix removal and dictionary lookup method called enhanced rule application order @ is able to stem affixation reduplication and compounding word and at @ @ time is able to address possible stemming error @ @ @ @ examines possible root cause of affixation reduplication and compounding stemming error @ could happen @ word stemming process @ @ experimental @ indicate @ @ proposed word stemmer is able to stem affixation reduplication and compounding word @ better stemming accuracy by @ enhanced rule application order @ @ international publishing switzerland @ 
2409,An automatic method for CVSS score prediction using vulnerabilities description,"In this paper we introduce an objective method for CVSS score calculation. CVSS is a well known and mostly used method for giving priority to software vulnerabilities. Currently it is being calculated by some slightly subjective methods which require enough skill and knowledge. This research shows how we can benefit from natural language description of vulnerabilities for CVSS calculation. The data that were used for implementation and evaluation of the proposed models consists of the available CVE vulnerability descriptions and their corresponding CVSS scores from the OSVDB database. First, feature vectors were extracted using text mining tools and techniques, and then the SVM and Random-Forest algorithms as well as fuzzy systems were examined to predict the concerned CVSS scores. In spite of the fact that SVM and Random-Forest are mostly used and trusted methods in prediction, results of this research bear a witness that using fuzzy systems can give comparable and even better results. In addition, implementation of the fuzzy based system is much easier and faster. Although so far, there have been so little efforts in using the information embedded in textual materials regarding vulnerabilities, this research shows that it will be valuable to utilize them in systems security establishment. © 2016 - IOS Press and the authors. All rights reserved.",2016,Journal of Intelligent and Fuzzy Systems,14,in @ @ @ introduce @ objective method @ cv score calculation @ cv is a well known and mostly used method @ giving priority to software vulnerability @ currently @ is @ calculated by some slightly subjective method @ require enough skill and knowledge @ @ research @ @ @ @ benefit @ natural language description of vulnerability @ cv calculation @ @ data @ @ used @ implementation and evaluation of @ proposed model consists of @ available cve vulnerability description and @ corresponding cv score @ @ osvdb database @ first feature vector @ extracted @ text mining tool and technique and @ @ svm and random-forest algorithm a well a fuzzy system @ examined to predict @ concerned cv score @ in spite of @ fact @ svm and random-forest @ mostly used and trusted method in prediction @ of @ research bear a witness @ @ fuzzy system @ give comparable and even better @ @ in addition implementation of @ fuzzy based system is much easier and faster @ although @ far @ @ @ @ little effort in @ @ information embedded in textual material regarding vulnerability @ research @ @ @ @ @ valuable to utilize @ in system security establishment @ io @ and @ author @ @ right reserved @ 
2410,Fundamentals of sentiment analysis and its applications,"The problem of identifying people’s opinions expressed in written language is a relatively new and very active field of research. Having access to huge amount of data due to the ubiquity of Internet, has enabled researchers in different fields—such as natural language processing, machine learning and data mining, text mining, management and marketing and even psychology—to conduct research in order to discover people’s opinions and sentiments from the publicly available data sources. Sentiment analysis and opinion mining are typically done at various level of abstraction: document, sentence and aspect. Recently researchers are also investigating concept-level sentiment analysis, which is a form of aspect-level sentiment analysis in which aspects can be multi terms. Also recently research has started addressing sentiment analysis and opinion mining by using, modifying and extending topic modeling techniques. Topic models are probabilistic techniques for discovering the main themes existing in a collection of unstructured documents. In this book chapter we aim at addressing recent approaches to sentiment analysis, and explain this in the context of wider use. We start the chapter with a brief contextual introduction to the problem of sentiment analysis and opinion mining and extend our introduction with some of its applications in different domains. The main challenges in sentiment analysis and opinion mining are discussed, and different existing approaches to address these challenges are explained. Recent directions with respect to applying sentiment analysis and opinion mining are discussed. We will review these studies towards the end of this chapter, and conclude the chapter with new opportunities for research. © Springer International Publishing Switzerland 2016.",2016,Studies in Computational Intelligence,10,@ problem of identifying people s opinion expressed in written language is a relatively @ and @ active field of research @ @ access to huge amount of data due to @ ubiquity of internet ha enabled researcher in different field @ a natural language processing machine learning and data mining text mining management and marketing and even psychology to conduct research in order to discover people s opinion and sentiment @ @ publicly available data source @ sentiment analysis and opinion mining @ typically done at various level of abstraction @ document sentence and aspect @ recently researcher @ @ investigating concept-level sentiment analysis @ is a form of aspect-level sentiment analysis in @ aspect @ @ multi term @ @ recently research ha started addressing sentiment analysis and opinion mining by @ modifying and extending topic modeling technique @ topic model @ probabilistic technique @ discovering @ main theme existing in a collection of unstructured document @ in @ book chapter @ aim at addressing recent approach to sentiment analysis and explain @ in @ context of wider use @ @ start @ chapter @ a brief contextual introduction to @ problem of sentiment analysis and opinion mining and extend @ introduction @ some of @ application in different domain @ @ main challenge in sentiment analysis and opinion mining @ discussed and different existing approach to address @ challenge @ explained @ recent direction @ respect to applying sentiment analysis and opinion mining @ discussed @ @ @ review @ study towards @ end of @ chapter and conclude @ chapter @ @ opportunity @ research @ @ international publishing switzerland @ 
2412,Towards interval version of fuzzy synsets,"WordNet-like Lexical Databases (WLDs) group English words into synsets, being utilized in several text mining applications. Synsets were also open to criticism, because while synset members (wordsenses) are, in practice, considered as compeers, yet in theory not all of them represent the synset meaning with a same degree. Considering this criticism, fuzzy synsets (considering synsets as fuzzy sets) have been proposed. In this study, we show why the standard fuzzy synsets do not properly-enough model the membership uncertainty, and propose an upgraded version of them in which membership degrees are represented by intervals (similar to what in Interval Type 2 Fuzzy Sets). We present an algorithm for constructing the interval fuzzy version of WLDs of a language, given a large enough multicontextual corpus of documents and a precise enough word-sense-disambiguation (WSD) system of that language. Utilizing the algorithm, we produced interval fuzzy synsets of English WordNet (for the frequent-enough synsets). For evaluation, we compared the results with crowdsourced data, asking people to rate the min/max compatibility degree of wordsenses of a synset with its definition. Comparisons, promisingly, showed the algorithm accuracy. The algorithm has also the drawback of being applicable only for synsets with wordsenses having enough frequency in all the corpus categories. This drawback is going to be covered in our future work. © 2016 The authors and IOS Press. All rights reserved.",2016,Frontiers in Artificial Intelligence and Applications,0,wordnet-like lexical database @ wlds @ group english word @ synset @ utilized in several text mining application @ synset @ @ open to criticism @ @ synset member @ wordsenses @ @ in practice considered a compeer yet in theory not @ of @ represent @ synset meaning @ a @ degree @ considering @ criticism fuzzy synset @ considering synset a fuzzy set @ @ @ proposed @ in @ study @ @ @ @ standard fuzzy synset @ not properly-enough model @ membership uncertainty and propose @ upgraded version of @ in @ membership degree @ represented by interval @ similar to @ in interval type fuzzy set @ @ @ @ @ algorithm @ constructing @ interval fuzzy version of wlds of a language given a @ enough multicontextual corpus of document and a precise enough word-sense-disambiguation @ wsd @ system of @ language @ utilizing @ algorithm @ produced interval fuzzy synset of english wordnet @ @ @ frequent-enough synset @ @ @ evaluation @ compared @ @ @ crowdsourced data asking people to rate @ min max compatibility degree of wordsenses of a synset @ @ definition @ comparison promisingly showed @ algorithm accuracy @ @ algorithm ha @ @ drawback of @ applicable only @ synset @ wordsenses @ enough frequency in @ @ corpus category @ @ drawback is going to @ covered in @ future work @ @ author and io @ @ @ right reserved @ 
2417,Functional technology foresight. A novel methodology to identify emerging technologies,"The speed and complexity of the technology evolution faced by modern societies need new approaches to the analysis and understanding of the world. Indeed, an exclusive focus on technological goals can miss to recognize all the stakeholders of a technology and address real user needs; moreover, on the one hand low signals are becoming more and more important in fast evolving markets, on the other hand the excess of hype, fashions, or vested interests sometimes deeply alter indicators. However, the so called Big Data promise to be a huge low cost set of valuable information, available and affordable to all (SMEs included). But, analyzing them is not trivial especially if we deal with academic papers and patents. To tackle these issues, the present paper proposes to apply a powerful methodological tool called Functional Analysis to the Technology Foresight process. Actually the rigorous study of the functions, that an artefact should perform to satisfy the user needs, provides a universal and thus unifying point of view, which is able to correlate the user perspective on the product with its technical features. Functional reasoning has been applied to (i) detect possible patterns of development, spotting missing elements and highlighting strengths as well as potential sources of failure; (ii) to enhance traditional bibliometric tools such as the analysis of S-curves and (iii), integrated with a natural language processing analysis toolchain, tailored for patent documents, to identify emerging technologies. The paper describes the functional approach to technology foresight activity, presents how to integrate it with text mining algorithms and experts’ domain knowledge, and finally discusses its benefits in the context of Technology Foresight also from an economic point of view, showing that oresight is affordable also for Small and Medium Enterprises. © The Author(s) 2016.",2016,European Journal of Futures Research,5,@ speed and complexity of @ technology evolution faced by modern society need @ approach to @ analysis and understanding of @ world @ indeed @ exclusive focus on technological goal @ miss to recognize @ @ stakeholder of a technology and address real user need @ moreover on @ @ hand low signal @ becoming more and more important in fast evolving market on @ @ hand @ excess of hype fashion @ vested interest sometimes deeply alter indicator @ however @ @ called big data promise to @ a huge low cost set of valuable information available and affordable to @ @ smes included @ @ @ analyzing @ is not trivial especially if @ deal @ @ @ and patent @ to tackle @ issue @ @ @ proposes to apply a powerful methodological tool called functional analysis to @ technology foresight process @ actually @ rigorous study of @ function @ @ artefact @ perform to satisfy @ user need provides a universal and thus unifying point of view @ is able to correlate @ user perspective on @ product @ @ technical feature @ functional reasoning ha @ applied to @ i @ detect possible pattern of development spotting missing element and highlighting strength a well a potential source of failure @ @ ii @ to enhance traditional bibliometric tool @ a @ analysis of s-curves and @ iii @ integrated @ a natural language processing analysis toolchain tailored @ patent document to identify emerging technology @ @ @ describes @ functional approach to technology foresight activity @ @ to integrate @ @ text mining algorithm and expert domain knowledge and finally discus @ benefit in @ context of technology foresight @ @ @ economic point of view showing @ oresight is affordable @ @ small and medium enterprise @ @ author @ s @ @ 
2446,Dimensionality reduction for text preprocessing in text mining using NLTK,"Today we are enjoying the fruits of digital era. The technology now blossoms with the speed of lightening causing turbulence for creating many smart devices. The impact of such a revolution has led to a huge source of information. Perceiving a meticulous piece of data by data querying imposes an enormous amount of time and therefore accuracy becomes a challenge. The result of an information retrieval system for the user query is comparative to the index of data storage. In this paper, the techniques of Natural Language Processing such as Tokenization, Stop Word Removal and Stemming is revisited by combining with various methodologies of Data Mining namely Clustering and Classification in order to achieve increased efficiency, accuracy and decreased time quotient. The result obtained with such an approach is analyzed through a comparative study based on the parameters namely domain of token, feature of tokens, number of tokens generated and time taken for this preprocessing. © Research India Publications.",2015,International Journal of Applied Engineering Research,0,today @ @ enjoying @ fruit of digital era @ @ technology now blossom @ @ speed of lightening causing turbulence @ creating many smart device @ @ impact of @ a revolution ha led to a huge source of information @ perceiving a meticulous piece of data by data querying imposes @ enormous amount of time and therefore accuracy becomes a challenge @ @ @ of @ information retrieval system @ @ user query is comparative to @ index of data storage @ in @ @ @ technique of natural language processing @ a tokenization stop word removal and stemming is revisited by combining @ various methodology of data mining namely clustering and classification in order to achieve increased efficiency accuracy and decreased time quotient @ @ @ obtained @ @ @ approach is analyzed @ a comparative study based on @ parameter namely domain of token feature of token number of token generated and time taken @ @ preprocessing @ research india publication @ 
2449,Structuring Tweets for improving Twitter search,"Spam and wildly varying documents make searching in Twitter challenging. Most Twitter search systems generally treat a Tweet as a plain text when modeling relevance. However, a series of conventions allows users to Tweet in structural ways using a combination of different blocks of texts. These blocks include plain texts, hashtags, links, mentions, etc. Each block encodes a variety of communicative intent and the sequence of these blocks captures changing discourse. Previous work shows that exploiting the structural information can improve the structured documents (e.g., web pages) retrieval. In this study we utilize the structure of Tweets, induced by these blocks, for Twitter retrieval and Twitter opinion retrieval. For Twitter retrieval, a set of features, derived from the blocks of text and their combinations, is used into a learning-to-rank scenario. We show that structuring Tweets can achieve state-of-the-art performance. Our approach does not rely on social media features, but when we do add this additional information, performance improves significantly. For Twitter opinion retrieval, we explore the question of whether structural information derived from the body of Tweets and opinionatedness ratings of Tweets can improve performance. Experimental results show that retrieval using a novel unsupervised opinionatedness feature based on structuring Tweets achieves comparable performance with a supervised method using manually tagged Tweets. Topic-related specific structured Tweet sets are shown to help with query-dependent opinion retrieval. © 2015 ASIS&T.",2015,Journal of the Association for Information Science and Technology,8,spam and wildly varying document make searching in twitter challenging @ @ twitter search system generally treat a tweet a a plain text @ modeling relevance @ however a series of convention allows user to tweet in structural way @ a combination of different block of text @ @ block include plain text hashtags link mention etc @ @ block encodes a variety of communicative intent and @ sequence of @ block capture changing discourse @ previous work @ @ exploiting @ structural information @ improve @ structured document @ e @ g @ web page @ retrieval @ in @ study @ utilize @ structure of tweet induced by @ block @ twitter retrieval and twitter opinion retrieval @ @ twitter retrieval a set of feature derived @ @ block of text and @ combination is used @ a learning-to-rank scenario @ @ @ @ structuring tweet @ achieve state-of-the-art performance @ @ approach doe not rely on social medium feature @ @ @ @ add @ additional information performance improves significantly @ @ twitter opinion retrieval @ explore @ question of whether structural information derived @ @ body of tweet and opinionatedness rating of tweet @ improve performance @ experimental @ @ @ retrieval @ a novel unsupervised opinionatedness feature based on structuring tweet achieves comparable performance @ a supervised method @ manually tagged tweet @ topic-related specific structured tweet set @ @ to help @ query-dependent opinion retrieval @ asis t @ 
2450,Computational Annotations: SCDL-NL as a Structured Annotation Language,"In this paper, we categorize ""semantics"" into ""taxonomical semantics"", ""syntactical semantics"" and ""formal semantics"". We propose a declarative meta-language SCDL-NL as the foundation of a general annotation language in which ""taxonomical and syntactical semantic"" information of a sentence can be clearly defined. Since pure natural language is too complicated to be used as a general annotation language, the annotation language imposes some restrictions on the English grammar so that it can be easily translated into SCDL-NL to facilitate information retrieval. © 2015 World Scientific Publishing Company.",2015,International Journal of Semantic Computing,0,in @ @ @ categorize @ semantics @ @ @ taxonomical semantics @ @ syntactical semantics @ and @ formal semantics @ @ @ propose a declarative meta-language scdl-nl a @ foundation of a general annotation language in @ @ taxonomical and syntactical semantic @ information of a sentence @ @ clearly defined @ since pure natural language is too complicated to @ used a a general annotation language @ annotation language imposes some restriction on @ english grammar @ @ @ @ @ easily translated @ scdl-nl to facilitate information retrieval @ world scientific publishing company @ 
2456,A novel social media competitive analytics framework with sentiment benchmarks,"In today's competitive business environment, there is a strong need for businesses to collect, monitor, and analyze user-generated data on their own and on their competitors' social media sites, such as Facebook, Twitter, and blogs. To achieve a competitive advantage, it is often necessary to listen to and understand what customers are saying about competitors' products and services. Current social media analytics frameworks do not provide benchmarks that allow businesses to compare customer sentiment on social media to easily understand where businesses are doing well and where they need to improve. In this paper, we present a social media competitive analytics framework with sentiment benchmarks that can be used to glean industry-specific marketing intelligence. Based on the idea of the proposed framework, new social media competitive analytics with sentiment benchmarks can be developed to enhance marketing intelligence and to identify specific actionable areas in which businesses are leading and lagging to further improve their customers' experience using customer opinions gleaned from social media. Guided by the proposed framework, an innovative business-driven social media competitive analytics tool named VOZIQ is developed. We use VOZIQ to analyze tweets associated with five large retail sector companies and to generate meaningful business insight reports. © 2015 Elsevier B.V. All rights reserved.",2015,Information and Management,118,in today @ s competitive @ environment @ is a strong need @ @ to collect monitor and analyze user-generated data on @ @ and on @ competitor @ social medium site @ a facebook twitter and blog @ to achieve a competitive advantage @ is often necessary to listen to and understand @ customer @ saying @ competitor @ product and service @ current social medium analytics framework @ not provide benchmark @ allow @ to compare customer sentiment on social medium to easily understand @ @ @ @ well and @ @ need to improve @ in @ @ @ @ a social medium competitive analytics framework @ sentiment benchmark @ @ @ used to glean industry-specific marketing intelligence @ based on @ idea of @ proposed framework @ social medium competitive analytics @ sentiment benchmark @ @ developed to enhance marketing intelligence and to identify specific actionable area in @ @ @ leading and lagging to @ improve @ customer @ experience @ customer opinion gleaned @ social medium @ guided by @ proposed framework @ innovative business-driven social medium competitive analytics tool named voziq is developed @ @ use voziq to analyze tweet associated @ five @ retail sector company and to generate meaningful @ insight report @ @ b @ v @ @ right reserved @ 
2457,Subjective well-being measurement based on Chinese grassroots blog text sentiment analysis,"In this study, we propose a new method to measure the subjective well-being (SWB) of Chinese people. Based upon the classic framework in psychology, our model constructs a system of multiple weighted emotions in positive and negative affect by applying a text-sentiment analysis. To study SWB in the Chinese context, we also establish and supplement our model with a new lexicon, Ren-CECps-SWB 2.0. Tests on the data of 7 years of grassroots blogs on Sina.com demonstrate the validity of our model. Employing the same data, we find interesting patterns of the SWB of Chinese people on weekly and monthly bases. © 2015 Published by Elsevier B.V.",2015,Information and Management,10,in @ study @ propose a @ method to measure @ subjective well-being @ swb @ of chinese people @ based upon @ classic framework in psychology @ model construct a system of multiple weighted emotion in positive and negative affect by applying a text-sentiment analysis @ to study swb in @ chinese context @ @ establish and supplement @ model @ a @ lexicon ren-cecps-swb @ @ test on @ data of year of grassroots blog on sina @ com demonstrate @ validity of @ model @ employing @ @ data @ find interesting pattern of @ swb of chinese people on weekly and monthly base @ published by @ b @ v @ 
2458,A polarity analysis framework for Twitter messages,"Social media, such as Twitter and Facebook, allow the creation, sharing and exchange of information among people, companies and brands. This information can be used for several purposes, such as to understand consumers and their preferences. In this direction, the sentiment analysis can be used as a feedback mechanism. This analysis corresponds to classifying a text according to the sentiment that the writer intended to transmit. A basic sentiment classifier determines the sentiment polarity (negative, neutral or positive) of a given text at the document, sentence, or feature/aspect level. Advanced types may consider other elements like the emotional state (e.g. angry, sad, happy), affective states (e.g. pleasure and pain), motivational states (e.g. hunger and curiosity), temperaments, among others. In general, there are two main approaches to attribute sentiment to tweets: based on knowledge; or based on machine learning algorithms. In the latter case, the learning algorithm requires a pre-classified data sample to determine the class of new data. Typically, the sample is pre-classified manually, making the process time consuming and reducing its real time applicability for big data. This paper proposes a polarity analysis framework for Twitter messages, which combines both approaches and an automatic contextual module. To assess the performance of the proposed framework, four text datasets from the literature are used. Five different types of classifiers were considered: Naïve Bayes (NB); Support Vector Machines (SVM); Decision Trees (J48); and Nearest Neighbors (KNN). The results show that the proposal is a suitable framework to automate the whole polarity analysis process, providing high accuracy levels and low false positive rates. © 2015 Elsevier Inc. All rights reserved.",2015,Applied Mathematics and Computation,86,social medium @ a twitter and facebook allow @ creation sharing and exchange of information among people company and brand @ @ information @ @ used @ several purpose @ a to understand consumer and @ preference @ in @ direction @ sentiment analysis @ @ used a a feedback mechanism @ @ analysis corresponds to classifying a text according to @ sentiment @ @ writer intended to transmit @ a basic sentiment classifier determines @ sentiment polarity @ negative neutral @ positive @ of a given text at @ document sentence @ feature aspect level @ advanced type may consider @ element like @ emotional state @ e @ g @ angry sad happy @ affective state @ e @ g @ pleasure and pain @ motivational state @ e @ g @ hunger and curiosity @ temperament among others @ in general @ @ @ main approach to attribute sentiment to tweet @ based on knowledge @ @ based on machine learning algorithm @ in @ latter case @ learning algorithm requires a pre-classified data sample to determine @ class of @ data @ typically @ sample is pre-classified manually making @ process time consuming and reducing @ real time applicability @ big data @ @ @ proposes a polarity analysis framework @ twitter message @ combine @ approach and @ automatic contextual module @ to ass @ performance of @ proposed framework four text datasets @ @ literature @ used @ five different type of classifier @ considered @ naïve bayes @ nb @ @ support vector machine @ svm @ @ decision tree @ j @ @ and nearest neighbor @ knn @ @ @ @ @ @ @ proposal is a suitable framework to automate @ whole polarity analysis process providing high accuracy level and low false positive rate @ @ inc @ @ right reserved @ 
2459,Discovering topic time from web news,"Topic time reflects the temporal feature of topics in Web news pages, which can be used to establish and analyze topic models for many time-sensitive text mining tasks. However, there are two critical challenges in discovering topic time from Web news pages. The first issue is how to normalize different kinds of temporal expressions within a Web news page, e.g., explicit and implicit temporal expressions, into a unified representation framework. The second issue is how to determine the right topic time for topics in Web news. Aiming at solving these two problems, we propose a systematic framework for discovering topic time from Web news. In particular, for the first issue, we propose a new approach that can effectively determine the appropriate referential time for implicit temporal expressions and further present an effective defuzzification algorithm to find the right explanation for a fuzzy temporal expression. For the second issue, we propose a relation model to describe the relationship between news topics and topic time. Based on this model, we design a new algorithm to extract topic time from Web news. We build a prototype system called Topic Time Parser (TTP) and conduct extensive experiments to measure the effectiveness of our proposal. The results suggest that our proposal is effective in both temporal expression normalization and topic time extraction. © 2015 Elsevier Ltd. All rights reserved.",2015,Information Processing and Management,9,topic time reflects @ temporal feature of topic in web news page @ @ @ used to establish and analyze topic model @ many time-sensitive text mining task @ however @ @ @ critical challenge in discovering topic time @ web news page @ @ first issue is @ to normalize different kind of temporal expression within a web news page e @ g @ explicit and implicit temporal expression @ a unified representation framework @ @ second issue is @ to determine @ right topic time @ topic in web news @ aiming at solving @ @ problem @ propose a systematic framework @ discovering topic time @ web news @ in particular @ @ first issue @ propose a @ approach @ @ effectively determine @ appropriate referential time @ implicit temporal expression and @ @ @ effective defuzzification algorithm to find @ right explanation @ a fuzzy temporal expression @ @ @ second issue @ propose a relation model to describe @ relationship @ news topic and topic time @ based on @ model @ design a @ algorithm to extract topic time @ web news @ @ build a prototype system called topic time parser @ ttp @ and conduct extensive experiment to measure @ effectiveness of @ proposal @ @ @ suggest @ @ proposal is effective in @ temporal expression normalization and topic time extraction @ @ ltd @ @ right reserved @ 
2471,Co-training for detecting hedges and their scope in biomedical texts,"To avoid extracting uncertain statements as factual information, the detection of hedges and their scope becomes an important step in biomedical text mining. The current approaches focus on learning the detection models only with the labeled data. However, such approaches cannot make further progress due to the limited amount of training data and the difference between the training and working data. We proposes a co-training approach to make use of the limited labeled data to leverage some amounts of unlabeled data for boosting the detection performances of hedge cues and their scope. Experiments are carried out on the biomedical corpus of the CoNLL 2010 Shared Task and on free data derived from biomedical literature. Both the test data of the corpus and the free data are used as the unlabeled data. Experiment results show that the test data helps more than the free data on both tasks. The best F-score achieved in hedge cue identification is 88.12% and for hedge scope detection it is 63.09%, which significantly outperform previous systems. Co-training system can transfer the distribution of the unlabeled data to the labeled training data to improve the performance on the unlabeled data effectively. Copyright © 2015 Binary Information Press.",2015,Journal of Computational Information Systems,0,to avoid extracting uncertain statement a factual information @ detection of hedge and @ scope becomes @ important step in biomedical text mining @ @ current approach focus on learning @ detection model only @ @ labeled data @ however @ approach cannot make @ progress due to @ limited amount of training data and @ difference @ @ training and working data @ @ proposes a co-training approach to make use of @ limited labeled data to leverage some amount of unlabeled data @ boosting @ detection performance of hedge cue and @ scope @ experiment @ carried @ on @ biomedical corpus of @ conll shared task and on free data derived @ biomedical literature @ @ @ test data of @ corpus and @ free data @ used a @ unlabeled data @ experiment @ @ @ @ test data help more @ @ free data on @ task @ @ best f-score achieved in hedge cue identification is @ and @ hedge scope detection @ is @ @ significantly outperform previous system @ co-training system @ transfer @ distribution of @ unlabeled data to @ labeled training data to improve @ performance on @ unlabeled data effectively @ @ binary information @ @ 
2474,Selection criteria for text mining approaches,"Text mining techniques include categorization of text, summarization, topic detection, concept extraction, search and retrieval, document clustering, etc. Each of these techniques can be used in finding some non-trivial information from a collection of documents. Text mining can also be employed to detect a document's main topic/theme which is useful in creating taxonomy from the document collection. Areas of applications for text mining include publishing, media, telecommunications, marketing, research, healthcare, medicine, etc. Text mining has also been applied on many applications on the World Wide Web for developing recommendation systems. We propose here a set of criteria to evaluate the effectiveness of text mining techniques in an attempt to facilitate the selection of appropriate technique. © 2014 Elsevier Ltd. All rights reserved.",2015,Computers in Human Behavior,49,text mining technique include categorization of text summarization topic detection concept extraction search and retrieval document clustering etc @ @ of @ technique @ @ used in finding some non-trivial information @ a collection of document @ text mining @ @ @ employed to detect a document @ s main topic theme @ is useful in creating taxonomy @ @ document collection @ area of application @ text mining include publishing medium telecommunication marketing research healthcare medicine etc @ text mining ha @ @ applied on many application on @ world wide web @ developing recommendation system @ @ propose @ a set of criterion to evaluate @ effectiveness of text mining technique in @ attempt to facilitate @ selection of appropriate technique @ @ ltd @ @ right reserved @ 
2478,Application of Word Embeddings in Biomedical Named Entity Recognition Tasks,"Biomedical named entity recognition (Bio- NER) is the fundamental task of biomedical text mining. Machine-learning-based approaches, such as conditional random fields (CRFs), have been widely applied in this area, but the accuracy of these systems is limited because of the finite annotated corpus. In this study, word embedding features are generated from an unlabeled corpus, which as extra word features are induced into the CRFs system for Bio-NER. To further improved performance, a post-processing algorithm is employed after the named entity recognition task. Experimental results show that the word embedding features generated from a larger unlabeled corpus achieves higher performance, and the use of word embedding features increases F-measure on INLPBA04 data from 71.50% to 71.77%. After applying the post-processing algorithm, the F-measure reaches 71.85%, which is superior to the results in most existing systems.",2015,Journal of Digital Information Management,14,biomedical named entity recognition @ bio ner @ is @ fundamental task of biomedical text mining @ machine-learning-based approach @ a conditional random field @ crfs @ @ @ widely applied in @ area @ @ accuracy of @ system is limited @ of @ finite annotated corpus @ in @ study word embedding feature @ generated @ @ unlabeled corpus @ a extra word feature @ induced @ @ crfs system @ bio-ner @ to @ improved performance a post-processing algorithm is employed @ @ named entity recognition task @ experimental @ @ @ @ word embedding feature generated @ a larger unlabeled corpus achieves higher performance and @ use of word embedding feature increase f-measure on inlpba data @ @ to @ @ @ applying @ post-processing algorithm @ f-measure reach @ @ is superior to @ @ in @ existing system @ 
2479,Exploring author name disambiguation on PubMed-scale,"Author name disambiguation (AND) creates a daunting challenge in that disambiguation techniques often draw false conclusions when applied to incomplete or incorrect publication data. It becomes a more critical issue in the biomedical domain where PubMed articles are written by a wide range of researchers internationally. To tackle this issue, we create a carefully hand-crafted training set drawn from the entire PubMed collection by going through multiple iterations. We assess the quality of our training set by comparing it with SCOPUS-based training set. In addition, for the performance enhancement of the AND techniques, we propose a new set of publication features extracted by text mining techniques. The results of the experiments show that all four supervised learning techniques (Random Forest, C4.5, KNN, and SVM) with the new publication features (called NER model) achieve improved performance over the baseline and hybrid edit distance model. © 2015 Elsevier Ltd.",2015,Journal of Informetrics,16,author name disambiguation @ and @ creates a daunting challenge in @ disambiguation technique often draw false conclusion @ applied to incomplete @ incorrect publication data @ @ becomes a more critical issue in @ biomedical domain @ pubmed article @ written by a wide range of researcher internationally @ to tackle @ issue @ create a carefully hand-crafted training set drawn @ @ entire pubmed collection by going @ multiple iteration @ @ ass @ quality of @ training set by comparing @ @ scopus-based training set @ in addition @ @ performance enhancement of @ and technique @ propose a @ set of publication feature extracted by text mining technique @ @ @ of @ experiment @ @ @ four supervised learning technique @ random forest c @ knn and svm @ @ @ @ publication feature @ called ner model @ achieve improved performance @ @ baseline and hybrid edit distance model @ @ ltd @ 
2492,A text mining system for deviation detection in financial documents,"Attempts to mine text documents to discover deviations or anomalies have increased in recent years due to the elevated amount of textual data in today's data repositories. Text mining assists in uncovering hidden information contents across multiple documents. Although various text mining tools are available, their focus is mainly to assist in data summarization or document classification. These tasks proved to be helpful, however; they do not provide semantic analysis and rigorous textual comparison to detect abnormal sentences that exist in the documents. In this paper, we describe a text mining system that is able to detect sentence deviations from a collection of financial documents. The system implements a dissimilarity function to compare sentences represented as graphs. Our evaluation on the proposed system revolves around experiments using financial statements of a bank. The findings provide valid evidence that the proposed system is able to identify deviating sentences occurring in the documents. The detected deviations can be beneficial for the authorities in order to improve their business decisions. © 2015 IOS Press and the authors. All rights reserved.",2015,Intelligent Data Analysis,3,attempt to mine text document to discover deviation @ anomaly @ increased in recent year due to @ elevated amount of textual data in today @ s data repository @ text mining assist in uncovering hidden information content across multiple document @ although various text mining tool @ available @ focus is mainly to assist in data summarization @ document classification @ @ task proved to @ helpful however @ @ @ not provide semantic analysis and rigorous textual comparison to detect abnormal sentence @ exist in @ document @ in @ @ @ describe a text mining system @ is able to detect sentence deviation @ a collection of financial document @ @ system implement a dissimilarity function to compare sentence represented a graph @ @ evaluation on @ proposed system revolves around experiment @ financial statement of a bank @ @ finding provide valid evidence @ @ proposed system is able to identify deviating sentence occurring in @ document @ @ detected deviation @ @ beneficial @ @ authority in order to improve @ @ decision @ io @ and @ author @ @ right reserved @ 
2494,Joint model for subsentence-level sentiment analysis with Markov logic,"Sentiment analysis mainly focuses on the study of one's opinions that express positive or negative sentiments. With the explosive growth of web documents, sentiment analysis is becoming a hot topic in both academic research and system design. Fine-grained sentiment analysis is traditionally solved as a 2-step strategy, which results in cascade errors. Although joint models, such as joint sentiment/topic and maximum entropy (MaxEnt)/latent Dirichlet allocation, are proposed to tackle this problem of sentiment analysis, they focus on the joint learning of both aspects and sentiments. Thus, they are not appropriate to solve the cascade errors for sentiment analysis at the sentence or subsentence level. In this article, we present a novel jointly fine-grained sentiment analysis framework at the subsentence level with Markov logic. First, we divide the task into 2 separate stages (subjectivity classification and polarity classification). Then, the 2 separate stages are processed, respectively, with different feature sets, which are implemented by local formulas in Markov logic. Finally, global formulas in Markov logic are adopted to realize the interactions of the 2 separate stages. The joint inference of subjectivity and polarity helps prevent cascade errors. Experiments on a Chinese sentiment data set manifest that our joint model brings significant improvements. © 2015 ASIS&T.",2015,Journal of the Association for Information Science and Technology,5,sentiment analysis mainly focus on @ study of @ @ s opinion @ express positive @ negative sentiment @ @ @ explosive growth of web document sentiment analysis is becoming a hot topic in @ @ research and system design @ fine-grained sentiment analysis is traditionally solved a a step strategy @ @ in cascade error @ although joint model @ a joint sentiment topic and maximum entropy @ maxent @ latent dirichlet allocation @ proposed to tackle @ problem of sentiment analysis @ focus on @ joint learning of @ aspect and sentiment @ thus @ @ not appropriate to solve @ cascade error @ sentiment analysis at @ sentence @ subsentence level @ in @ article @ @ a novel jointly fine-grained sentiment analysis framework at @ subsentence level @ markov logic @ first @ divide @ task @ separate stage @ subjectivity classification and polarity classification @ @ @ @ separate stage @ processed respectively @ different feature set @ @ implemented by local formula in markov logic @ finally global formula in markov logic @ adopted to realize @ interaction of @ separate stage @ @ joint inference of subjectivity and polarity help prevent cascade error @ experiment on a chinese sentiment data set manifest @ @ joint model brings significant improvement @ asis t @ 
2495,Intelligent inventory management system for public organization,"Intelligent inventory management system is an automated system that manages numerous public records produced by public domain using natural language processing and text mining. Organizing and preserving records' retention period is a critical process because it is directly coupled to the appraisal of records. Currently, only a small number of archivists is engaged in managing considerable amount of records and many public organizations are facing difficulties due to manpower shortage. This paper describes a management system that assists archivists by deciding retention period of holdings' inventory automatically. A series of processes are integrated within the system: beginning with modeling the step-by-step process of evaluating retention period by the archivist. This is followed by training the retention period of inventory that has already been confirmed by the archivist. Finally, recommending retention period of the record using TF-IDF (Term Frequency-Inverse Document Frequency) algorithm. This reduces the time and the cost required to appraise large volume of records and improve accuracy of retention period due to continuous learning of data. © 2015 International Information Institute.",2015,Information (Japan),0,intelligent inventory management system is @ automated system @ manages numerous public record produced by public domain @ natural language processing and text mining @ organizing and preserving record @ retention period is a critical process @ @ is directly coupled to @ appraisal of record @ currently only a small number of archivist is engaged in managing considerable amount of record and many public organization @ facing difficulty due to manpower shortage @ @ @ describes a management system @ assist archivist by deciding retention period of holding @ inventory automatically @ a series of process @ integrated within @ system @ beginning @ modeling @ step-by-step process of evaluating retention period by @ archivist @ @ is followed by training @ retention period of inventory @ ha already @ confirmed by @ archivist @ finally recommending retention period of @ record @ tf-idf @ term frequency-inverse document frequency @ algorithm @ @ reduces @ time and @ cost required to appraise @ volume of record and improve accuracy of retention period due to continuous learning of data @ international information institute @ 
2497,MWI-sum: A multilingual summarizer based on frequent weighted itemsets,"Multidocument summarization addresses the selection of a compact subset of highly informative sentences, i.e., the summary, from a collection of textual documents. To perform sentence selection, two parallel strategies have been proposed: (a) apply general-purpose techniques relying on datamining or information retrieval techniques, and/or (b) perform advanced linguistic analysis relying on semantics-based models (e.g., ontologies) to capture the actual sentence meaning. Since there is an increasing need for processing documents written in different languages, the attention of the research community has recently focused on summarizers based on strategy (a). This article presents a novelmultilingual summarizer, namely MWI-Sum (Multilingual Weighted Itemsetbased Summarizer), that exploits an itemset-based model to summarize collections of documents ranging over the same topic. Unlike previous approaches, it extracts frequent weighted itemsets tailored to the analyzed collection and uses them to drive the sentence selection process. Weighted itemsets represent correlations among multiple highly relevant terms that are neglected by previous approaches. The proposed approach makes minimal use of language-dependent analyses. Thus, it is easily applicable to document collections written in different languages. Experiments performed on benchmark and real-life collections, English-written and not, demonstrate that the proposed approach performs better than state-of-the-art multilingual document summarizers. © 2015 ACM.",2015,ACM Transactions on Information Systems,33,multidocument summarization address @ selection of a compact subset of highly informative sentence i @ e @ @ summary @ a collection of textual document @ to perform sentence selection @ parallel strategy @ @ proposed @ @ a @ apply general-purpose technique relying on datamining @ information retrieval technique and @ @ b @ perform advanced linguistic analysis relying on semantics-based model @ e @ g @ ontology @ to capture @ actual sentence meaning @ since @ is @ increasing need @ processing document written in different language @ attention of @ research community ha recently focused on summarizers based on strategy @ a @ @ @ article @ a novelmultilingual summarizer namely mwi-sum @ multilingual weighted itemsetbased summarizer @ @ exploit @ itemset-based model to summarize collection of document ranging @ @ @ topic @ unlike previous approach @ extract frequent weighted itemsets tailored to @ analyzed collection and us @ to drive @ sentence selection process @ weighted itemsets represent correlation among multiple highly relevant term @ @ neglected by previous approach @ @ proposed approach make minimal use of language-dependent analysis @ thus @ is easily applicable to document collection written in different language @ experiment performed on benchmark and real-life collection english-written and not demonstrate @ @ proposed approach performs better @ state-of-the-art multilingual document summarizers @ acm @ 
2498,Mining activation force defined dependency patterns for relation extraction,"Relation extraction is essential for most text mining tasks. Existing approaches on relation extraction are generally based on bootstrapping methodology which implies semantic drift problem. This paper presents a new approach to learn semantic dependency patterns, which can significantly alleviate this problem. To this end, a unique representation of activation force defined dependency pattern is presented. It is a trigger word mediated relation between an entity and its attribute value, and the trigger word is extracted by using the statistics of word activation forces between those words. The adaptability and the scalability of the framework are facilitated by the recursive and compositional bootstrap learning of patterns and seed pairs. To obtain insights of the reliability and applicability of the method, we applied it to the English Slot Filling task of Knowledge Base Population track at Text Analysis Conference 2013. Experimental results show that the proposed method has good performance in the implementation of English Slot Filling 2013 with the overall F1 value significantly higher than the best automatic result reported. The experimental results also demonstrate that the activation force based trigger word mining method plays an essential role in improving the performance. © 2015 Elsevier B.V. All rights reserved.",2015,Knowledge-Based Systems,9,relation extraction is essential @ @ text mining task @ existing approach on relation extraction @ generally based on bootstrapping methodology @ implies semantic drift problem @ @ @ @ a @ approach to learn semantic dependency pattern @ @ significantly alleviate @ problem @ to @ end a unique representation of activation force defined dependency pattern is presented @ @ is a trigger word mediated relation @ @ entity and @ attribute value and @ trigger word is extracted by @ @ statistic of word activation force @ @ word @ @ adaptability and @ scalability of @ framework @ facilitated by @ recursive and compositional bootstrap learning of pattern and seed pair @ to obtain insight of @ reliability and applicability of @ method @ applied @ to @ english slot filling task of knowledge base population track at text analysis conference @ experimental @ @ @ @ proposed method ha good performance in @ implementation of english slot filling @ @ overall f value significantly higher @ @ best automatic @ reported @ @ experimental @ @ demonstrate @ @ activation force based trigger word mining method play @ essential role in improving @ performance @ @ b @ v @ @ right reserved @ 
2499,Exploratory visual analysis and interactive pattern extraction from semi-structured data,"Semi-structured documents are a common type of data containing free text in natural language (unstructured data) as well as additional information about the document, or meta-data, typically following a schema or controlled vocabulary (structured data). Simultaneous analysis of unstructured and structured data enables the discovery of hidden relationships that cannot be identified from either of these sources when analyzed independently of each other. In this work, we present a visual text analytics tool for semi-structured documents (ViTA-SSD), that aims to support the user in the exploration and finding of insightful patterns in a visual and interactive manner in a semi-structured collection of documents. It achieves this goal by presenting to the user a set of coordinated visualizations that allows the linking of the metadata with interactively generated clusters of documents in such a way that relevant patterns can be easily spotted. The system contains two novel approaches in its back end: a feature-learning method to learn a compact representation of the corpus and a fast-clustering approach that has been redesigned to allow user supervision. These novel contributions make it possible for the user to interact with a large and dynamic document collection and to perform several text analytical tasks more efficiently. Finally, we present two use cases that illustrate the suitability of the system for in-depth interactive exploration of semi-structured document collections, two user studies, and results of several evaluations of our text-mining components. © 2015 ACM 2160-6455/2015/09-ART16 $15.00.",2015,ACM Transactions on Interactive Intelligent Systems,6,semi-structured document @ a common type of data containing free text in natural language @ unstructured data @ a well a additional information @ @ document @ meta-data typically following a schema @ controlled vocabulary @ structured data @ @ simultaneous analysis of unstructured and structured data enables @ discovery of hidden relationship @ cannot @ identified @ either of @ source @ analyzed independently of @ @ @ in @ work @ @ a visual text analytics tool @ semi-structured document @ vita-ssd @ @ aim to support @ user in @ exploration and finding of insightful pattern in a visual and interactive manner in a semi-structured collection of document @ @ achieves @ goal by presenting to @ user a set of coordinated visualization @ allows @ linking of @ metadata @ interactively generated cluster of document in @ a way @ relevant pattern @ @ easily spotted @ @ system contains @ novel approach in @ back end @ a feature-learning method to learn a compact representation of @ corpus and a fast-clustering approach @ ha @ redesigned to allow user supervision @ @ novel contribution make @ possible @ @ user to interact @ a @ and dynamic document collection and to perform several text analytical task more efficiently @ finally @ @ @ use case @ illustrate @ suitability of @ system @ in-depth interactive exploration of semi-structured document collection @ user study and @ of several evaluation of @ text-mining component @ acm -art @ @ 
2505,DIKEA: Exploiting Wikipedia for keyphrase extraction,"Automatic keyphrase extraction is the challenging task of assigning keyphrases to documents to capture the main topics. It assists many research areas in the field of text mining - indexing, clustering, and summarisation. A landmark research KEA (Keyphrase Extraction Algorithm) formulated the problem as a supervised machine learning problem and successfully applied a Naïve Bayes model to it. KEA showed great promise but its performance is not satisfactory. Its state-of-art extension KEA++ significantly improved its performance but relies on a domain specific vocabulary which is often not available or incomplete for other domains. We present a novel domain-independent system (DIKEA) which makes three main contributions to this field of research: utilising the largest online knowledge source available, Wikipedia, for keyphrase candidate selection; adding new features including a Wikipedia-based feature, link probability; and further boosting performance by using a multilayer perceptron network. Our experiments showed that DIKEA outperformed KEA++ while keeping the overall solution domain-independent. DIKEA was also tested on a benchmark dataset provided by a workshop on Semantic Evaluation (SemEval-2010), allowing comparisons with the 19 other related systems which participated. Our experiments show that DIKEA ranks first when considering only the top 5 keyphrases extracted from each document, and ranks second overall. © 2015 - IOS Press and the authors. All rights reserved.",2015,Web Intelligence,1,automatic keyphrase extraction is @ challenging task of assigning keyphrases to document to capture @ main topic @ @ assist many research area in @ field of text mining indexing clustering and summarisation @ a landmark research kea @ keyphrase extraction algorithm @ formulated @ problem a a supervised machine learning problem and successfully applied a naïve bayes model to @ @ kea showed great promise @ @ performance is not satisfactory @ @ state-of-art extension kea significantly improved @ performance @ relies on a domain specific vocabulary @ is often not available @ incomplete @ @ domain @ @ @ a novel domain-independent system @ dikea @ @ make three main contribution to @ field of research @ utilising @ largest online knowledge source available wikipedia @ keyphrase candidate selection @ adding @ feature including a wikipedia-based feature link probability @ and @ boosting performance by @ a multilayer perceptron network @ @ experiment showed @ dikea outperformed kea @ keeping @ overall solution domain-independent @ dikea wa @ tested on a benchmark dataset provided by a workshop on semantic evaluation @ semeval @ allowing comparison @ @ @ related system @ participated @ @ experiment @ @ dikea rank first @ considering only @ top keyphrases extracted @ @ document and rank second overall @ io @ and @ author @ @ right reserved @ 
2514,Influence diffusion detection using the influence style (INFUSE) model,"Blogs are readily available sources of opinions and sentiments that in turn could influence the opinions of the blog readers. Previous studies have attempted to infer influence from blog features, but they have ignored the possible influence styles that describe the different ways in which influence is exerted. We propose a novel approach to analyzing bloggers' influence styles and using the influence styles as features to improve the performance of influence diffusion detection among linked bloggers. The proposed influence style (INFUSE) model describes bloggers' influence through their engagement style, persuasion style, and persona. Methods used include similarity analysis to detect the creating-sharing aspect of engagement style, subjectivity analysis to measure persuasion style, and sentiment analysis to identify persona style. We further extend the INFUSE model to detect influence diffusion among linked bloggers based on the bloggers' influence styles. The INFUSE model performed well with an average F1 score of 76% compared with the in-degree and sentiment-value baseline approaches. Previous studies have focused on the existence of influence among linked bloggers in detecting influence diffusion, but our INFUSE model is shown to provide a fine-grained description of the manner in which influence is diffused based on the bloggers' influence styles. © 2015 ASIS&T.",2015,Journal of the Association for Information Science and Technology,2,blog @ readily available source of opinion and sentiment @ in turn could influence @ opinion of @ blog reader @ previous study @ attempted to infer influence @ blog feature @ @ @ ignored @ possible influence style @ describe @ different way in @ influence is exerted @ @ propose a novel approach to analyzing blogger @ influence style and @ @ influence style a feature to improve @ performance of influence diffusion detection among linked blogger @ @ proposed influence style @ infuse @ model describes blogger @ influence @ @ engagement style persuasion style and persona @ method used include similarity analysis to detect @ creating-sharing aspect of engagement style subjectivity analysis to measure persuasion style and sentiment analysis to identify persona style @ @ @ extend @ infuse model to detect influence diffusion among linked blogger based on @ blogger @ influence style @ @ infuse model performed well @ @ average f score of compared @ @ in-degree and sentiment-value baseline approach @ previous study @ focused on @ existence of influence among linked blogger in detecting influence diffusion @ @ infuse model is @ to provide a fine-grained description of @ manner in @ influence is diffused based on @ blogger @ influence style @ asis t @ 
2515,The influence of stemming on Indonesian tweet sentiment analysis,"Stemming has commonly used in some research about text mining, information retrieval, and natural language processing. However, there is an indication that stemming does not deliver significant influence toward accuracy in text classification. Hence, this research attempts to investigate the influence of the stemming process on Indonesian tweet sentiment analysis. Furthermore, this work examines about the difference effect between two conditions by involving stemming and without involving stemming on pre-preprocessing task. The experiments show that the accuracy difference for SVM using stemming in pre-processing acquired 0.67% and 1.34% higher than pre-processing without stemming, whereas, Naive Bayes obtained 0.23% and 1.12%. Finally, this research proves that stemming does not raise the accuracy either using SVM or Naive Bayes algorithm. © 2015, Institute of Advanced Engineering and Science. All rights reserved.",2015,"International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",2,stemming ha commonly used in some research @ text mining information retrieval and natural language processing @ however @ is @ indication @ stemming doe not deliver significant influence toward accuracy in text classification @ hence @ research attempt to investigate @ influence of @ stemming process on indonesian tweet sentiment analysis @ furthermore @ work examines @ @ difference effect @ @ condition by involving stemming and without involving stemming on pre-preprocessing task @ @ experiment @ @ @ accuracy difference @ svm @ stemming in pre-processing acquired @ and @ higher @ pre-processing without stemming whereas naive bayes obtained @ and @ @ finally @ research prof @ stemming doe not raise @ accuracy either @ svm @ naive bayes algorithm @ institute of advanced engineering and science @ @ right reserved @ 
2516,Natural language processing as feature extraction method for building better predictive models,"This chapter covers natural language processing techniques and their application in predicitve models development. Two case studies are presented. First case describes a project where textual descriptions of various situations in call center of one telecommunication company were processed in order to predict churn. Second case describes sentiment analysis of business news and describes practical and testing issues in text mining projects. Both case studies depict different approaches and are implemented in different tools. Language of the texts processed in these projects is Croatian which belongs to the Slavic group of languages with more complex morphologies and grammar rules than English. Chapter concludes with several points on the future research possible in this domain. © 2015, IGI Global. All rights reserved.",2015,Modern Computational Models of Semantic Discovery in Natural Language,3,@ chapter cover natural language processing technique and @ application in predicitve model development @ @ case study @ presented @ first case describes a project @ textual description of various situation in call center of @ telecommunication company @ processed in order to predict churn @ second case describes sentiment analysis of @ news and describes practical and testing issue in text mining project @ @ case study depict different approach and @ implemented in different tool @ language of @ text processed in @ project is croatian @ belongs to @ slavic group of language @ more complex morphology and grammar rule @ english @ chapter concludes @ several point on @ future research possible in @ domain @ igi global @ @ right reserved @ 
2517,"Sentiment classification: Facebook' statuses mining in the ""arabic spring"" era","In this work, we focus on the application of text mining and sentiment analysis techniques for analyzing Tunisian users' statuses updates on Facebook. We aim to extract useful information, about their sentiment and behavior, especially during the ""Arabic spring"" era. To achieve this task, we describe a method for sentiment analysis using Support Vector Machine and Naïve Bayes algorithms, and applying a combination of more than two features. The output of this work consists, on one hand, on the construction of a sentiment lexicon based on the Emoticons and Acronyms' lexicons that we developed based on the extracted statuses updates; and on the other hand, it consists on the realization of detailed comparative experiments between the above algorithms by creating a training model for sentiment classification. © 2015, IGI Global. All rights reserved.",2015,Modern Computational Models of Semantic Discovery in Natural Language,0,in @ work @ focus on @ application of text mining and sentiment analysis technique @ analyzing tunisian user @ status update on facebook @ @ aim to extract useful information @ @ sentiment and behavior especially @ @ @ arabic spring @ era @ to achieve @ task @ describe a method @ sentiment analysis @ support vector machine and naïve bayes algorithm and applying a combination of more @ @ feature @ @ output of @ work consists on @ hand on @ construction of a sentiment lexicon based on @ emoticon and acronym @ lexicon @ @ developed based on @ extracted status update @ and on @ @ hand @ consists on @ realization of detailed comparative experiment @ @ @ algorithm by creating a training model @ sentiment classification @ igi global @ @ right reserved @ 
2518,Incremental ontology population and enrichment through semantic-based text mining: An application for it audit domain,"Higher education and profebional trainings often apply innovative e-learning systems, where ontologies are used for structuring domain knowledge. To provide up-to-date knowledge for the students, ontology has to be maintained regularly. It is especially true for IT audit and security domain, because technology is changing fast. However manual ontology population and enrichment is a complex task that require profebional experience involving a lot of efforts. The authors' paper deals with the challenges and pobible solutions for semi-automatic ontology enrichment and population. ProMine has two main contributions; one is the semantic-based text mining approach for automatically identifying domain-specific knowledge elements; the other is the automatic categorization of these extracted knowledge elements by using Wiktionary. ProMine ontology enrichment solution was applied in IT audit domain of an e-learning system. After ten cycles of the application ProMine, the number of automatically identified new concepts are tripled and ProMine categorized new concepts with high precision and recall. © Copyright 2015, IGI Global.",2015,International Journal on Semantic Web and Information Systems,9,higher education and profebional training often apply innovative e-learning system @ ontology @ used @ structuring domain knowledge @ to provide up-to-date knowledge @ @ student ontology ha to @ maintained regularly @ @ is especially true @ @ audit and security domain @ technology is changing fast @ however manual ontology population and enrichment is a complex task @ require profebional experience involving a lot of effort @ @ author @ @ deal @ @ challenge and pobible solution @ semi-automatic ontology enrichment and population @ promine ha @ main contribution @ @ is @ semantic-based text mining approach @ automatically identifying domain-specific knowledge element @ @ @ is @ automatic categorization of @ extracted knowledge element by @ wiktionary @ promine ontology enrichment solution wa applied in @ audit domain of @ e-learning system @ @ ten cycle of @ application promine @ number of automatically identified @ concept @ tripled and promine categorized @ concept @ high precision and recall @ @ igi global @ 
2519,Using rhetorical structure in sentiment analysis,"Experts state that a detailed analysis of rhetorical structure highlights crucial sentiment-carrying text segments. A better understanding of a text's sentiment can be obtained by guiding the analysis by the text's rhetorical structure. They state that automated sentiment analysis is related to natural language processing, computational linguistics, and text mining. Deep linguistic analysis is a key success factor for sentiment-analysis systems, as it helps in dealing with compositionality or the way the semantic orientation of text is determined by the combined semantic orientations of its constituent phrases.",2015,Communications of the ACM,35,expert state @ a detailed analysis of rhetorical structure highlight crucial sentiment-carrying text segment @ a better understanding of a text @ s sentiment @ @ obtained by guiding @ analysis by @ text @ s rhetorical structure @ @ state @ automated sentiment analysis is related to natural language processing computational linguistics and text mining @ deep linguistic analysis is a key success factor @ sentiment-analysis system a @ help in dealing @ compositionality @ @ way @ semantic orientation of text is determined by @ combined semantic orientation of @ constituent phrase @ 
2522,At the interface of computational linguistics and statistics,"Computational linguistics encompasses a broad range of ideas and research areas, and only a brief introduction is possible here. We chose to include areas in computational linguistics where statisticians can contribute, hoping to provide inspiration to the reader. We describe three main aspects of this discipline-formal languages, information retrieval, and machine learning. These support the overarching goal, which is the representation and analysis of meaning from unstructured text. We then provide an example where text analysis has been applied to unstructured text fields in survey records and conclude with some applications and computational resources. © 2015 Wiley Periodicals, Inc.",2015,Wiley Interdisciplinary Reviews: Computational Statistics,3,computational linguistics encompasses a broad range of idea and research area and only a brief introduction is possible @ @ @ chose to include area in computational linguistics @ statistician @ contribute hoping to provide inspiration to @ reader @ @ describe three main aspect of @ discipline-formal language information retrieval and machine learning @ @ support @ overarching goal @ is @ representation and analysis of meaning @ unstructured text @ @ @ provide @ example @ text analysis ha @ applied to unstructured text field in survey record and conclude @ some application and computational resource @ wiley periodical inc @ 
2532,A rule-based approach to emotion cause detection for Chinese micro-blogs,"Emotion analysis and emotion cause extraction are key research tasks in natural language processing and public opinion mining. This paper presents a rule-based approach to emotion cause component detection for Chinese micro-blogs. Our research has important scientific values on social network knowledge discovery and data mining. It also has a great potential in analyzing the psychological processes of consumers. Firstly, this paper proposes a rule-based system underlying the conditions that trigger emotions based on an emotional model. Secondly, this paper extracts the corresponding cause events in fine-grained emotions from the results of events, actions of agents and aspects of objects. Meanwhile, it is reasonable to get the proportions of different cause components under different emotions by constructing the emotional lexicon and identifying different linguistic features, and the proposed approach is based on Bayesian probability. Finally, this paper presents the experiments on an emotion corpus of Chinese micro-blogs. The experimental results validate the feasibility of the approach. The existing problems and the further works are also present at the end. © 2015 Elsevier Ltd. All rights reserved.",2015,Expert Systems with Applications,45,emotion analysis and emotion cause extraction @ key research task in natural language processing and public opinion mining @ @ @ @ a rule-based approach to emotion cause component detection @ chinese micro-blogs @ @ research ha important scientific value on social network knowledge discovery and data mining @ @ @ ha a great potential in analyzing @ psychological process of consumer @ firstly @ @ proposes a rule-based system underlying @ condition @ trigger emotion based on @ emotional model @ secondly @ @ extract @ corresponding cause event in fine-grained emotion @ @ @ of event action of agent and aspect of object @ meanwhile @ is reasonable to get @ proportion of different cause component @ different emotion by constructing @ emotional lexicon and identifying different linguistic feature and @ proposed approach is based on bayesian probability @ finally @ @ @ @ experiment on @ emotion corpus of chinese micro-blogs @ @ experimental @ validate @ feasibility of @ approach @ @ existing problem and @ @ work @ @ @ at @ end @ @ ltd @ @ right reserved @ 
2534,CRFs based parallel biomedical named entity recognition algorithm employing MapReduce framework,"As the rapid growth of the biomedical literature, the model training time in biomedical named entity recognition increases sharply when dealing with large-scale training samples. How to increase the efficiency of named entity recognition in biomedical big data becomes one of the key problems in biomedical text mining. For the purposes of improving the recognition performance and reducing the training time, this paper proposes an optimization method for two-phase recognition using conditional random fields. In the first stage, each named entity boundary is detected to distinguish all real entities. In the second stage, we label the semantic class of the entity detected. To expedite the training speed, in these two phases, we implement the model training process on a parallel optimization program framework based on MapReduce. Through dividing the training set into several parts, the iterations in the training algorithm are designed as map tasks which can be executed simultaneously in a cluster, where each map function is designed to complete the calculation of a gradient vector component for each part in the training set. Our experiments show that the proposed method in this paper can achieve high performance with short training time, which has important implications for the current biological big data processing. © 2015, Springer Science+Business Media New York.",2015,Cluster Computing,24,a @ rapid growth of @ biomedical literature @ model training time in biomedical named entity recognition increase sharply @ dealing @ large-scale training sample @ @ to increase @ efficiency of named entity recognition in biomedical big data becomes @ of @ key problem in biomedical text mining @ @ @ purpose of improving @ recognition performance and reducing @ training time @ @ proposes @ optimization method @ two-phase recognition @ conditional random field @ in @ first stage @ named entity boundary is detected to distinguish @ real entity @ in @ second stage @ label @ semantic class of @ entity detected @ to expedite @ training speed in @ @ phase @ implement @ model training process on a parallel optimization program framework based on mapreduce @ @ dividing @ training set @ several part @ iteration in @ training algorithm @ designed a map task @ @ @ executed simultaneously in a cluster @ @ map function is designed to complete @ calculation of a gradient vector component @ @ part in @ training set @ @ experiment @ @ @ proposed method in @ @ @ achieve high performance @ short training time @ ha important implication @ @ current biological big data processing @ @ science @ medium @ york @ 
2542,Linguistic features for review helpfulness prediction,"Online reviews play a critical role in customer's purchase decision making process on the web. The reviews are often ranked based on user helpfulness votes to minimize the review information overload problem. This paper examines the factors that contribute towards helpfulness of online reviews and builds a predictive model. The proposed predictive model extracts novel linguistic category features by analysing the textual content of reviews. In addition, the model makes use of review metadata, subjectivity and readability related features for helpfulness prediction. Our experimental analysis on two real-life review datasets reveals that a hybrid set of features deliver the best predictive accuracy. We also show that the proposed linguistic category features are better predictors of review helpfulness for experience goods such as books, music, and video games. The findings of this study can provide new insights to e-commerce retailers for better organization and ranking of online reviews and help customers in making better product choices. © 2015 Elsevier Ltd. All rights reserved.",2015,Expert Systems with Applications,100,online review play a critical role in customer @ s purchase decision making process on @ web @ @ review @ often ranked based on user helpfulness vote to minimize @ review information overload problem @ @ @ examines @ factor @ contribute towards helpfulness of online review and build a predictive model @ @ proposed predictive model extract novel linguistic category feature by analysing @ textual content of review @ in addition @ model make use of review metadata subjectivity and readability related feature @ helpfulness prediction @ @ experimental analysis on @ real-life review datasets reveals @ a hybrid set of feature deliver @ best predictive accuracy @ @ @ @ @ @ proposed linguistic category feature @ better predictor of review helpfulness @ experience good @ a book music and video game @ @ finding of @ study @ provide @ insight to e-commerce retailer @ better organization and ranking of online review and help customer in making better product choice @ @ ltd @ @ right reserved @ 
2543,"Enhancing transport data collection through social media sources: Methods, challenges and opportunities for textual data","Social media data now enriches and supplements information flow in various sectors of society. The question addressed here is whether social media can act as a credible information source of sufficient quality to meet the needs of transport planners, operators, policy makers and the travelling public. A typology of primary transport data needs, current and new data sources is initially established, following which this study focuses on social media textual data in particular. Three sub-questions are investigated: the potential to use social media data alongside existing transport data, the technical challenges in extracting transport-relevant information from social media and the wider barriers to the uptake of this data. Following an overview of the text mining process to extract relevant information from the corpus, a review of the challenges this approach holds for the transport sector is given. These include ontologies, sentiment analysis, location names and measuring accuracy. Finally, institutional issues in the greater use of social media are highlighted, concluding that social media information has not yet been fully explored. The contribution of this study is in scoping the technical challenges in mining social media data within the transport context, laying the foundation for further research in this field. © The Institution of Engineering and Technology 2015.",2015,IET Intelligent Transport Systems,38,social medium data now enriches and supplement information flow in various sector of society @ @ question addressed @ is whether social medium @ act a a credible information source of sufficient quality to meet @ need of transport planner operator policy maker and @ travelling public @ a typology of primary transport data need current and @ data source is initially established following @ @ study focus on social medium textual data in particular @ three sub-questions @ investigated @ @ potential to use social medium data alongside existing transport data @ technical challenge in extracting transport-relevant information @ social medium and @ wider barrier to @ uptake of @ data @ following @ overview of @ text mining process to extract relevant information @ @ corpus a review of @ challenge @ approach hold @ @ transport sector is given @ @ include ontology sentiment analysis location name and measuring accuracy @ finally institutional issue in @ greater use of social medium @ highlighted concluding @ social medium information ha not yet @ fully explored @ @ contribution of @ study is in scoping @ technical challenge in mining social medium data within @ transport context laying @ foundation @ @ research in @ field @ @ institution of engineering and technology @ 
2547,Towards robust tags for scientific publications from natural language processing tools and Wikipedia,"In this work, two simple methods of tagging scientific publications with labels reflecting their content are presented and compared. As a first source of labels, Wikipedia is employed. A second label set is constructed from the noun phrases occurring in the analyzed corpus. The corpus itself consists of abstracts from 0.7 million scientific documents deposited in the ArXiv preprint collection. We present a comparison of both approaches, which shows that discussed methods are to a large extent complementary. Moreover, the results give interesting insights into the completeness of Wikipedia knowledge in various scientific domains. As a next step, we examine the statistical properties of the obtained tags. It turns out that both methods show qualitatively similar rank–frequency dependence, which is best approximated by the stretched exponential curve. The distribution of the number of distinct tags per document follows also the same distribution for both methods and is well described by the negative binomial distribution. The developed tags are meant for use as features in various text mining tasks. Therefore, as a final step we show the preliminary results on their application to topic modeling. © 2014, The Author(s).",2015,International Journal on Digital Libraries,1,in @ work @ simple method of tagging scientific publication @ label reflecting @ content @ presented and compared @ a a first source of label wikipedia is employed @ a second label set is constructed @ @ noun phrase occurring in @ analyzed corpus @ @ corpus @ consists of abstract @ @ million scientific document deposited in @ arxiv preprint collection @ @ @ a comparison of @ approach @ @ @ discussed method @ to a @ extent complementary @ moreover @ @ give interesting insight @ @ completeness of wikipedia knowledge in various scientific domain @ a a next step @ examine @ statistical property of @ obtained tag @ @ turn @ @ @ method @ qualitatively similar rank frequency dependence @ is best approximated by @ stretched exponential curve @ @ distribution of @ number of distinct tag per document follows @ @ @ distribution @ @ method and is well described by @ negative binomial distribution @ @ developed tag @ meant @ use a feature in various text mining task @ therefore a a final step @ @ @ preliminary @ on @ application to topic modeling @ @ author @ s @ @ 
2555,Automatic discovery of person-related named-entity in news articles based on verb analysis,"Verb is the most important word in a sentence as it asserts an action, events, feeling about the subject and object discussed in the sentence. For news articles, it is observable that there is always at least a verb attached to the person(s) mentioned in the news. As such, a hypothesis has been formed such that there must exist some verbs that specifically describe human being conducts within a news article. In this paper, we propose an approach which aims to identify named-entity (NE) that performs human activity automatically. More specifically, our approach attempts to identify person-related NE generally and “person name” predefined type specifically by studying the nature of verb that associated with human activity via TreeTagger, Stanford packages and WordNet. The experimental results show that it is viable to use verb in identifying “person name“entity type. In addition, our empirical study proves that the approach is applicable to small text size articles. Another significant contribution of our approach is that it does not require training data set and anaphora resolution. © 2013, Springer Science+Business Media New York.",2015,Multimedia Tools and Applications,1,verb is @ @ important word in a sentence a @ asserts @ action event feeling @ @ subject and object discussed in @ sentence @ @ news article @ is observable @ @ is always at least a verb attached to @ person @ s @ mentioned in @ news @ a @ a hypothesis ha @ formed @ @ @ must exist some verb @ specifically describe human @ conduct within a news article @ in @ @ @ propose @ approach @ aim to identify named-entity @ ne @ @ performs human activity automatically @ more specifically @ approach attempt to identify person-related ne generally and person name predefined type specifically by studying @ nature of verb @ associated @ human activity via treetagger stanford package and wordnet @ @ experimental @ @ @ @ is viable to use verb in identifying person name entity type @ in addition @ empirical study prof @ @ approach is applicable to small text size article @ another significant contribution of @ approach is @ @ doe not require training data set and anaphora resolution @ @ science @ medium @ york @ 
2570,Improving literature-based discovery with advanced text mining,"Automated Literature Based Discovery (LBD) generates new knowledge by combining what is already known in literature. Facilitating large-scale hypothesis testing and generation from huge collections of literature, LBD could significantly support research in biomedical sciences. However, the uptake of LBD by the scientific community has been limited. One of the key reasons for this is the limited nature of existing LBD methodology. Based on fairly shallow methods, current LBD captures only some of the information available in literature. We discuss how advanced Text Mining based on Information retrieval, Natural Language Processing and data mining could open the doors to much deeper, wider coverage and dynamic LBD better capable of evolving with science, in particular when combined with sophisticated, state-of-the-art knowledge discovery techniques. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,automated literature based discovery @ lbd @ generates @ knowledge by combining @ is already known in literature @ facilitating large-scale hypothesis testing and generation @ huge collection of literature lbd could significantly support research in biomedical science @ however @ uptake of lbd by @ scientific community ha @ limited @ @ of @ key reason @ @ is @ limited nature of existing lbd methodology @ based on fairly shallow method current lbd capture only some of @ information available in literature @ @ discus @ advanced text mining based on information retrieval natural language processing and data mining could open @ door to much deeper wider coverage and dynamic lbd better capable of evolving @ science in particular @ combined @ sophisticated state-of-the-art knowledge discovery technique @ @ international publishing switzerland @ 
2573,Emerging directions in predictive text mining,"In recent years, Text Mining has seen a tremendous spurt of growth as data scientists focus their attention on analyzing unstructured data. The main drivers for this growth have been big data as well as complex applications where the information in the text is often combined with other kinds of information in building predictive models. These applications require highly efficient and scalable algorithms to meet the overall performance demands. In this context, six main directions are identified where research in text mining is heading: Deep Learning, Topic Models, Graphical Modeling, Summarization, Sentiment Analysis, Learning from Unlabeled Text. Each direction has its own motivations and goals. There is some overlap of concepts because of the common themes of text and prediction. The predictive models involved are typically ones that involve meta-information or tags that could be added to the text. These tags can then be used in other text processing tasks such as information extraction. While the boundary between the fields of Text Mining and Natural Language Processing is becoming increasingly blurry, the importance of predictive models for various applications involving text means there is still substantial growth potential within the traditional sub-fields of text mining. These data-centric directions are also likely to influence future research in Natural Language Processing, especially in resource-poor languages and in multilingual texts. © 2015 John Wiley & Sons, Ltd.",2015,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,13,in recent year text mining ha seen a tremendous spurt of growth a data scientist focus @ attention on analyzing unstructured data @ @ main driver @ @ growth @ @ big data a well a complex application @ @ information in @ text is often combined @ @ kind of information in building predictive model @ @ application require highly efficient and scalable algorithm to meet @ overall performance demand @ in @ context six main direction @ identified @ research in text mining is heading @ deep learning topic model graphical modeling summarization sentiment analysis learning @ unlabeled text @ @ direction ha @ @ motivation and goal @ @ is some overlap of concept @ of @ common theme of text and prediction @ @ predictive model involved @ typically @ @ involve meta-information @ tag @ could @ added to @ text @ @ tag @ @ @ used in @ text processing task @ a information extraction @ @ @ boundary @ @ field of text mining and natural language processing is becoming increasingly blurry @ importance of predictive model @ various application involving text mean @ is still substantial growth potential within @ traditional sub-fields of text mining @ @ data-centric direction @ @ likely to influence future research in natural language processing especially in resource-poor language and in multilingual text @ john wiley son ltd @ 
2574,User-centered text mining (invited tutorial),"Historically, text mining methods for extracting “knowledge” from text have increased in sophistication by the incorporation of both statistical learning and symbolic natural language processing. However, in scenarios where a domain user aims to make sense of document collections and derive insights from them, domain knowledge is necessary to inform the analysis, and text mining needs to be complemented by text visualization and user-driven interaction with the analytic process. In this tutorial we introduce Visual Text Analytics as a multi-disciplinary field of research. We cover conceptual and practical methods and tools, review stateof-the-art systems that integrate text mining with visualization and user interaction, and identify promising research directions for making text mining more user-centered and accessible to users with an interest in domain-specific applications. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,historically text mining method @ extracting knowledge @ text @ increased in sophistication by @ incorporation of @ statistical learning and symbolic natural language processing @ however in scenario @ a domain user aim to make sense of document collection and derive insight @ @ domain knowledge is necessary to inform @ analysis and text mining need to @ complemented by text visualization and user-driven interaction @ @ analytic process @ in @ tutorial @ introduce visual text analytics a a multi-disciplinary field of research @ @ cover conceptual and practical method and tool review stateof-the-art system @ integrate text mining @ visualization and user interaction and identify promising research direction @ making text mining more user-centered and accessible to user @ @ interest in domain-specific application @ @ international publishing switzerland @ 
2579,An approach for text mining based on noun phrases,"The use of noun phrases as descriptors for text mining vectors has been proposed to overcome the poor semantic of the traditional bag-of-words (BOW). However, the solutions found in the literature are unsatisfactory, mainly due to the use of static definitions for noun phrases and the fact that noun phrases per se do not enable an adequate relevance representation since they are expressions that barely repeat. We present an approach to deal with these problems by (i) introducing a process that enables the definition of noun phrases interactively and (ii) considering similar noun phrases as a unique term. A case study compares both approaches, the one proposed in this paper and the other based on BOW. The main contribution of this paper is the improvement of the preprocessing phase of text mining, leading to better results in the overall process. © Springer International Publishing Switzerland 2015.",2015,"Smart Innovation, Systems and Technologies",0,@ use of noun phrase a descriptor @ text mining vector ha @ proposed to overcome @ poor semantic of @ traditional bag-of-words @ bow @ @ however @ solution found in @ literature @ unsatisfactory mainly due to @ use of static definition @ noun phrase and @ fact @ noun phrase per se @ not enable @ adequate relevance representation since @ @ expression @ barely repeat @ @ @ @ approach to deal @ @ problem by @ i @ introducing a process @ enables @ definition of noun phrase interactively and @ ii @ considering similar noun phrase a a unique term @ a case study compare @ approach @ @ proposed in @ @ and @ @ based on bow @ @ main contribution of @ @ is @ improvement of @ preprocessing phase of text mining leading to better @ in @ overall process @ @ international publishing switzerland @ 
2582,Evaluating classification power of linked admission data sources with text mining,"Lung cancer is a leading cause of death in developed countries. This paper presents a text mining system using Support Vector Machines for detecting lung cancer admissions. Performance of the system using different clinical data sources is evaluated. We use radiology reports as an initial data source and add other sources, such as pathology reports, patient demographic information and hospital admission information. Results show that mining over linked data sources significantly improves classification performance with a maximum F-Score improvement of 0.057.",2015,CEUR Workshop Proceedings,0,lung cancer is a leading cause of death in developed country @ @ @ @ a text mining system @ support vector machine @ detecting lung cancer admission @ performance of @ system @ different clinical data source is evaluated @ @ use radiology report a @ initial data source and add @ source @ a pathology report patient demographic information and hospital admission information @ @ @ @ mining @ linked data source significantly improves classification performance @ a maximum f-score improvement of @ @ 
2583,Supporting knowledge discovery for biodiversity,"A proposal for text mining as a support for knowledge discovery on biological descriptions is introduced. Our aim is both to sustain the curation of databases and to offer an alternative representation frame for accessing information in the biodiversity domain. We work on raw texts with minimum human intervention, applying natural language processing to integrate linguistic and domain knowledge in a mathematical model that makes it possible to capture concepts and relationships between them in a computable form, using conceptual graphs. This provides a reasoning basis for determining semantic disjointedness or subsumption, as well as sub and super-concept relationships. © 2015 Elsevier B.V. All rights reserved.",2015,Data and Knowledge Engineering,3,a proposal @ text mining a a support @ knowledge discovery on biological description is introduced @ @ aim is @ to sustain @ curation of database and to offer @ alternative representation frame @ accessing information in @ biodiversity domain @ @ work on raw text @ minimum human intervention applying natural language processing to integrate linguistic and domain knowledge in a mathematical model @ make @ possible to capture concept and relationship @ @ in a computable form @ conceptual graph @ @ provides a reasoning basis @ determining semantic disjointedness @ subsumption a well a sub and super-concept relationship @ @ b @ v @ @ right reserved @ 
2588,Impact of text mining application on financial footnotes analysis research in progress,"In recent decade and with the advent of the eXtensible Business Reporting Language (XBRL), financial reports have a great mutation in terms of a unified reporting process. Nevertheless, the unstructured part of financial reports, so called footnotes, remains as barrier facing an accurate automatic and real-time financial analysis. The purpose of this paper is to investigate whether the text mining approach is an appropriate solution to assist analyzing textual financial footnotes or not. The implemented text mining prototype is able to classify textual financial footnotes into related pre-defined categories automatically. This avoids manually reading of the entire text. Different text classification supervised algorithms have been compared, where the decision tree by 90.65% accuracy performs better rather than other deployed classifiers. This research provides preliminary insights about the impact of using a text mining approach on automatic financial footnote analysis in terms of saving time and increasing accuracy. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,in recent decade and @ @ advent of @ extensible @ reporting language @ xbrl @ financial report @ a great mutation in term of a unified reporting process @ nevertheless @ unstructured part of financial report @ called footnote remains a barrier facing @ accurate automatic and real-time financial analysis @ @ purpose of @ @ is to investigate whether @ text mining approach is @ appropriate solution to assist analyzing textual financial footnote @ not @ @ implemented text mining prototype is able to classify textual financial footnote @ related pre-defined category automatically @ @ avoids manually reading of @ entire text @ different text classification supervised algorithm @ @ compared @ @ decision tree by @ accuracy performs better rather @ @ deployed classifier @ @ research provides preliminary insight @ @ impact of @ a text mining approach on automatic financial footnote analysis in term of saving time and increasing accuracy @ @ international publishing switzerland @ 
2590,Extensive study on text representation models in text mining,"The ever-increasing volume of text data has captured the attention of researchers worldwide. This daily augmenting data is posing a serious challenge for current text representation models in text mining. There is an urgent need to improve the existing models to overcome this barrier. As a step to bridge this gap, this paper showcases a study and analysis of some promising established and upcoming models. Moreover, performance parameters like space and time requirements have been focused upon. Enhanced Vector Space Model (E-VSM) and Distance Graph Representation have been found to be most promising amongst the upcoming schemes. The relative strengths of these two models have been discussed vis-à-vis the Vector Space Model (VSM). The paper also presents a summary of Natural Language Processing(NLP) based models for a complete view of recent advancements in text representation. © Research India Publications.",2015,International Journal of Applied Engineering Research,0,@ ever-increasing volume of text data ha captured @ attention of researcher worldwide @ @ daily augmenting data is posing a serious challenge @ current text representation model in text mining @ @ is @ urgent need to improve @ existing model to overcome @ barrier @ a a step to bridge @ gap @ @ showcase a study and analysis of some promising established and upcoming model @ moreover performance parameter like space and time requirement @ @ focused upon @ enhanced vector space model @ e-vsm @ and distance graph representation @ @ found to @ @ promising amongst @ upcoming scheme @ @ relative strength of @ @ model @ @ discussed vis-à-vis @ vector space model @ vsm @ @ @ @ @ @ a summary of natural language processing @ nlp @ based model @ a complete view of recent advancement in text representation @ research india publication @ 
2591,Conceptual modeling for financial investment with text mining,"Although text-mining, sentiment analysis, and other forms of analysis have been carried out on financial investment applications, a significant amount of associated research is ad hoc searching for meaningful patterns. Other research in finance develops theory using limited data sets. These efforts are at two extremes. To bridge the gap between financial data analytics and finance domain theory, this research analyzes a specific conceptual model, the Business Intelligence Model (BIM), to identify constructs and concepts that could be beneficial for matching data analytics to domain theory. Doing so, provides a first step towards understanding how to effectively generate and validate domain theories that significantly benefit from data analytics. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,although text-mining sentiment analysis and @ form of analysis @ @ carried @ on financial investment application a significant amount of associated research is ad hoc searching @ meaningful pattern @ @ research in finance develops theory @ limited data set @ @ effort @ at @ extreme @ to bridge @ gap @ financial data analytics and finance domain theory @ research analyzes a specific conceptual model @ @ intelligence model @ bim @ to identify construct and concept @ could @ beneficial @ matching data analytics to domain theory @ @ @ provides a first step towards understanding @ to effectively generate and validate domain theory @ significantly benefit @ data analytics @ @ international publishing switzerland @ 
2593,Readerbench: An integrated cohesion-centered framework,"ReaderBench is an automated software framework designed to support both students and tutors by making use of text mining techniques, advanced natural language processing, and social network analysis tools. ReaderBench is centered on comprehension prediction and assessment based on a cohesion-based representation of the discourse applied on different sources (e.g., textual materials, behavior tracks, metacognitive explanations, Computer Supported Collaborative Learning – CSCL – conversations). Therefore, Reader‐ Bench can act as a Personal Learning Environment (PLE) which incorporates both individual and collaborative assessments. Besides the a priori evaluation of textual materials’ complexity presented to learners, our system supports the identification of reading strategies evident within the learners’ self-explanations or summaries. Moreover, ReaderBench integrates a dedicated cohesion-based module to assess participation and collaboration in CSCL conversations. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,readerbench is @ automated software framework designed to support @ student and tutor by making use of text mining technique advanced natural language processing and social network analysis tool @ readerbench is centered on comprehension prediction and assessment based on a cohesion-based representation of @ discourse applied on different source @ e @ g @ textual material behavior track metacognitive explanation computer supported collaborative learning cscl conversation @ @ therefore reader bench @ act a a personal learning environment @ ple @ @ incorporates @ individual and collaborative assessment @ besides @ a priori evaluation of textual material complexity presented to learner @ system support @ identification of reading strategy evident within @ learner self-explanations @ summary @ moreover readerbench integrates a dedicated cohesion-based module to ass participation and collaboration in cscl conversation @ @ international publishing switzerland @ 
2594,Interdependence of text mining quality and the input data preprocessing,"The paper focuses on preprocessing techniques application to short informal textual documents created in different natural languages. The goal is to evaluate the impact on the quality of the results and computational complexity of the text mining process designed to reveal knowledge hidden in the data. Extensive number of experiments were carried out with real world text data with correction of spelling errors, stemming, stop words removal, and their combinations applied. Support vector machine, decision trees, and k-means algorithms as the commonly used methods were considered to analyze the text data. The text mining quality was generally not influenced significantly, however, the positive impact represented by the decreased computational complexity was observed. © Springer International Publishing Switzerland 2015.",2015,Advances in Intelligent Systems and Computing,2,@ @ focus on preprocessing technique application to short informal textual document created in different natural language @ @ goal is to evaluate @ impact on @ quality of @ @ and computational complexity of @ text mining process designed to reveal knowledge hidden in @ data @ extensive number of experiment @ carried @ @ real world text data @ correction of spelling error stemming stop word removal and @ combination applied @ support vector machine decision tree and k-means algorithm a @ commonly used method @ considered to analyze @ text data @ @ text mining quality wa generally not influenced significantly however @ positive impact represented by @ decreased computational complexity wa observed @ @ international publishing switzerland @ 
2596,Text mining of news-headlines for FOREX market prediction: A Multi-layer Dimension Reduction Algorithm with semantics and sentiment,"In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones; or the use of general financial breaking news without any further filtration. In order to accomplish the above, this work produces a multi-layer algorithm that tackles each of the mentioned aspects of the text-mining problem at a designated layer. The first layer is termed the Semantic Abstraction Layer and addresses the problem of co-reference in text mining that is contributing to sparsity. Co-reference occurs when two or more words in a text corpus refer to the same concept. This work produces a custom approach by the name of Heuristic-Hypernyms Feature-Selection which creates a way to recognize words with the same parent-word to be regarded as one entity. As a result, prediction accuracy increases significantly at this layer which is attributed to appropriate noise-reduction from the feature-space. The second layer is termed Sentiment Integration Layer, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight by the name of SumScore that reflects investors' sentiment. Additionally, this layer reduces the dimensions by eliminating those that are of zero value in terms of sentiment and thereby improves prediction accuracy. The third layer encompasses a dynamic model creation algorithm, termed Synchronous Targeted Feature Reduction (STFR). It is suitable for the challenge at hand whereby the mining of a stream of text is concerned. It updates the models with the most recent information available and, more importantly, it ensures that the dimensions are reduced to the absolute minimum. The algorithm and each of its layers are extensively evaluated using real market data and news content across multiple years and have proven to be solid and superior to any other comparable solution. The proposed techniques implemented in the system, result in significantly high directional-accuracies of up to 83.33%. On top of a well-rounded multifaceted algorithm, this work contributes a much needed research framework for this context with a test-bed of data that must make future research endeavors more convenient. The produced algorithm is scalable and its modular design allows improvement in each of its layers in future research. This paper provides ample details to reproduce the entire system and the conducted experiments. © 2014 Elsevier Ltd. All rights reserved.",2015,Expert Systems with Applications,124,in @ @ a novel approach is proposed to predict intraday directional-movements of a currency-pair in @ foreign exchange market based on @ text of breaking financial news-headlines @ @ motivation behind @ work is twofold @ first although market-prediction @ text-mining is @ to @ a promising area of work in @ literature @ text-mining approach utilized in @ at @ stage @ not much beyond basic @ a @ is still @ emerging field @ @ work is @ effort to put more emphasis on @ text-mining method and tackle some specific aspect thereof @ @ weak in previous work namely @ @ problem of high dimensionality a well a @ problem of ignoring sentiment and semantics in dealing @ textual language @ @ research assumes @ addressing @ aspect of text-mining @ @ impact on @ quality of @ achieved @ @ @ proposed system prof @ assumption to @ right @ @ second part of @ motivation is to research a specific market namely @ foreign exchange market @ seems not to @ @ researched in @ previous work based on predictive text-mining @ therefore @ of @ work @ successfully demonstrate a predictive relationship @ @ specific market-type and @ textual data of news @ besides @ @ @ main component of @ motivation @ @ @ specific aspect @ make @ setup of @ proposed system and @ conducted experiment unique @ example @ use of news article-headlines only and not news article-bodies @ enables usage of short piece of text rather @ long @ @ @ @ use of general financial breaking news without @ @ filtration @ in order to accomplish @ @ @ work produce a multi-layer algorithm @ tackle @ of @ mentioned aspect of @ text-mining problem at a designated layer @ @ first layer is termed @ semantic abstraction layer and address @ problem of co-reference in text mining @ is contributing to sparsity @ co-reference occurs @ @ @ more word in a text corpus refer to @ @ concept @ @ work produce a custom approach by @ name of heuristic-hypernyms feature-selection @ creates a way to recognize word @ @ @ parent-word to @ regarded a @ entity @ a a @ prediction accuracy increase significantly at @ layer @ is attributed to appropriate noise-reduction @ @ feature-space @ @ second layer is termed sentiment integration layer @ integrates sentiment analysis capability @ @ algorithm by proposing a sentiment weight by @ name of sumscore @ reflects investor @ sentiment @ additionally @ layer reduces @ dimension by eliminating @ @ @ of zero value in term of sentiment and thereby improves prediction accuracy @ @ third layer encompasses a dynamic model creation algorithm termed synchronous targeted feature reduction @ stfr @ @ @ is suitable @ @ challenge at hand whereby @ mining of a stream of text is concerned @ @ update @ model @ @ @ recent information available and more importantly @ ensures @ @ dimension @ reduced to @ absolute minimum @ @ algorithm and @ of @ layer @ extensively evaluated @ real market data and news content across multiple year and @ proven to @ solid and superior to @ @ comparable solution @ @ proposed technique implemented in @ system @ in significantly high directional-accuracies of up to @ @ on top of a well-rounded multifaceted algorithm @ work contributes a much needed research framework @ @ context @ a test-bed of data @ must make future research endeavor more convenient @ @ produced algorithm is scalable and @ modular design allows improvement in @ of @ layer in future research @ @ @ provides ample detail to reproduce @ entire system and @ conducted experiment @ @ ltd @ @ right reserved @ 
2597,Sentiment-topic modeling in text mining,"In recent years, there has been a rapid growth of research interest in natural language processing that seeks to better understand sentiment or opinion expressed in text. There are several notable issues in most previous work in sentiment analysis, among them: the trained classifiers are domain-dependent; the labeled corpora required for training can be difficult to acquire from real-world text; and dependencies between sentiments and topics are not taken into consideration. In response to these limitations, a new family of probabilistic topic models, namely joint sentiment-topic models, have been developed, which are capable of detecting sentiment in connection with topic from text without using any labeled data for training. In addition, the sentiment-bearing topics extracted by the joint sentiment-topic models provide means for automatically discovering and summarizing opinions from a vast amount of user-generated data. © 2015 John Wiley & Sons, Ltd.",2015,Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,5,in recent year @ ha @ a rapid growth of research interest in natural language processing @ seek to better understand sentiment @ opinion expressed in text @ @ @ several notable issue in @ previous work in sentiment analysis among @ @ @ trained classifier @ domain-dependent @ @ labeled corpus required @ training @ @ difficult to acquire @ real-world text @ and dependency @ sentiment and topic @ not taken @ consideration @ in response to @ limitation a @ family of probabilistic topic model namely joint sentiment-topic model @ @ developed @ @ capable of detecting sentiment in connection @ topic @ text without @ @ labeled data @ training @ in addition @ sentiment-bearing topic extracted by @ joint sentiment-topic model provide mean @ automatically discovering and summarizing opinion @ a vast amount of user-generated data @ john wiley son ltd @ 
2598,Introduction,"Chapter 1 introduces the problem of extracting information from natural language unstructured documents, which is becoming more and more relevant in our “document society”. Despite the many useful applications that the information in these documents can potentiate, it is harder and harder to obtain the wanted information. Major problems result from the fact that much of the documents are in a format non usable by humans or machines. There is the need to create ways to extract relevant information from the vast amount of natural language sources. After this, the chapter presents, briefly, background information on Semantics, knowledge representation and Natural Language Processing, to support the presentation of the area of Information Extraction [IE, “the analysis of unstructured text in order to extract information about pre-specified types of events, entities or relationships, such as the relationship between disease and genes or disease and food items; in so doing value and insight are added to the data.” (Text mining of web-based medical content, Berlin, p 50)], its challenges, different approaches and general architecture, which is organized as a processing pipeline including domain independent components—tokenization, morphological analysis, part-of-speech tagging, syntactic parsing—and domain specific IE components—named entity recognition and co-reference resolution, relation identification, information fusion, among others. © 2015, The Authors.",2015,SpringerBriefs in Speech Technology,0,chapter introduces @ problem of extracting information @ natural language unstructured document @ is becoming more and more relevant in @ document society @ despite @ many useful application @ @ information in @ document @ potentiate @ is harder and harder to obtain @ wanted information @ major problem @ @ @ fact @ much of @ document @ in a format non usable by human @ machine @ @ is @ need to create way to extract relevant information @ @ vast amount of natural language source @ @ @ @ chapter @ briefly background information on semantics knowledge representation and natural language processing to support @ presentation of @ area of information extraction ie @ analysis of unstructured text in order to extract information @ pre-specified type of event entity @ relationship @ a @ relationship @ disease and gene @ disease and food item @ in @ @ value and insight @ added to @ data @ @ text mining of web-based medical content @ p @ @ challenge different approach and general architecture @ is organized a a processing pipeline including domain independent component tokenization morphological analysis part-of-speech tagging syntactic parsing and domain specific ie component named entity recognition and co-reference resolution relation identification information fusion among others @ @ author @ 
2599,Comparative study of various Persian stemmers in the field of information retrieval,"In linguistics, stemming is the operation of reducing words to their more general form, which is called the 'stem'. Stemming is an important step in information retrieval systems, natural language processing, and text mining. Information retrieval systems are evaluated by metrics like precision and recall and the fundamental superiority of an information retrieval system over another one is measured by them. Stemmers decrease the indexed file, increase the speed of information retrieval systems, and improve the performance of these systems by boosting precision and recall. There are few Persian stemmers and most of them work based on morphological rules. In this paper we carefully study Persian stemmers, which are classified into three main classes: structural stemmers, lookup table stemmers, and statistical stemmers. We describe the algorithms of each class carefully and present the weaknesses and strengths of each Persian stemmer. We also propose some metrics to compare and evaluate each stemmer by them. Copyright. © 2015 KIPS.",2015,Journal of Information Processing Systems,4,in linguistics stemming is @ operation of reducing word to @ more general form @ is called @ @ stem @ @ stemming is @ important step in information retrieval system natural language processing and text mining @ information retrieval system @ evaluated by metric like precision and recall and @ fundamental superiority of @ information retrieval system @ another @ is measured by @ @ stemmer decrease @ indexed file increase @ speed of information retrieval system and improve @ performance of @ system by boosting precision and recall @ @ @ @ persian stemmer and @ of @ work based on morphological rule @ in @ @ @ carefully study persian stemmer @ @ classified @ three main class @ structural stemmer lookup table stemmer and statistical stemmer @ @ describe @ algorithm of @ class carefully and @ @ weakness and strength of @ persian stemmer @ @ @ propose some metric to compare and evaluate @ stemmer by @ @ @ @ kip @ 
2600,Discovering types of spatial relations with a text mining approach,"Knowledge discovery from texts, particularly the identification of spatial information is a difficult task due to the complexity of texts written in natural language. Here we propose a method combining two statistical approaches (lexical and contextual analysis) and a text mining approach to automatically identify types of spatial relations. Experiments conducted on an English corpus are presented. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,knowledge discovery @ text particularly @ identification of spatial information is a difficult task due to @ complexity of text written in natural language @ @ @ propose a method combining @ statistical approach @ lexical and contextual analysis @ and a text mining approach to automatically identify type of spatial relation @ experiment conducted on @ english corpus @ presented @ @ international publishing switzerland @ 
2601,OntoMate: A text-mining tool aiding curation at the Rat Genome Database,"The Rat Genome Database (RGD) is the premier repository of rat genomic, genetic and physiologic data. Converting data from free text in the scientific literature to a structured format is one of the main tasks of all model organism databases. RGD spends considerable effort manually curating gene, Quantitative Trait Locus (QTL) and strain information. The rapidly growing volume of biomedical literature and the active research in the biological natural language processing (bioNLP) community have given RGD the impetus to adopt text-mining tools to improve curation efficiency. Recently, RGD has initiated a project to use OntoMate, an ontology-driven, concept-based literature search engine developed at RGD, as a replacement for the PubMed (http://www.ncbi.nlm.nih.gov/pubmed) search engine in the gene curation workflow. OntoMate tags abstracts with gene names, gene mutations, organism name and most of the 16 ontologies/vocabularies used at RGD. All terms/entities tagged to an abstract are listed with the abstract in the search results. All listed terms are linked both to data entry boxes and a term browser in the curation tool. OntoMate also provides user-activated filters for species, date and other parameters relevant to the literature search. Using the system for literature search and import has streamlined the process compared to using PubMed. The system was built with a scalable and open architecture, including features specifically designed to accelerate the RGD gene curation process. With the use of bioNLP tools, RGD has added more automation to its curation workflow. © The Author(s) 2015. Published by Oxford University Press.",2015,Database,9,@ rat genome database @ rgd @ is @ premier repository of rat genomic genetic and physiologic data @ converting data @ free text in @ scientific literature to a structured format is @ of @ main task of @ model organism database @ rgd spends considerable effort manually curating gene quantitative trait locus @ qtl @ and strain information @ @ rapidly growing volume of biomedical literature and @ active research in @ biological natural language processing @ bionlp @ community @ given rgd @ impetus to adopt text-mining tool to improve curation efficiency @ recently rgd ha initiated a project to use ontomate @ ontology-driven concept-based literature search engine developed at rgd a a replacement @ @ pubmed @ http @ www @ ncbi @ nlm @ nih @ gov pubmed @ search engine in @ gene curation workflow @ ontomate tag abstract @ gene name gene mutation organism name and @ of @ ontology vocabulary used at rgd @ @ term entity tagged to @ abstract @ listed @ @ abstract in @ search @ @ @ listed term @ linked @ to data entry box and a term browser in @ curation tool @ ontomate @ provides user-activated filter @ specie date and @ parameter relevant to @ literature search @ @ @ system @ literature search and import ha streamlined @ process compared to @ pubmed @ @ system wa built @ a scalable and open architecture including feature specifically designed to accelerate @ rgd gene curation process @ @ @ use of bionlp tool rgd ha added more automation to @ curation workflow @ @ author @ s @ @ published by oxford university @ @ 
2607,Automatic symptom extraction from texts to enhance knowledge discovery on rare diseases,"This paper reports ongoing researches on automatic symptom recognition towards diagnosis of rare diseases and knowledge acquisition on this subject. We describe a hybrid approach combining sequential pattern mining and natural language processing techniques in order to automate the discovery of symptoms from textual content. More precisely, ourweakly supervised approach uses linguistic knowledge to enhance an incremental pattern mining process, in order to filter and make a relevant use of the discovered patterns. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,@ @ report ongoing research on automatic symptom recognition towards diagnosis of rare disease and knowledge acquisition on @ subject @ @ describe a hybrid approach combining sequential pattern mining and natural language processing technique in order to automate @ discovery of symptom @ textual content @ more precisely ourweakly supervised approach us linguistic knowledge to enhance @ incremental pattern mining process in order to filter and make a relevant use of @ discovered pattern @ @ international publishing switzerland @ 
2608,Nlp methodology as guidance and verification of the data mining of survey ensanut 2012,"Data Mining represents the cutting edge when we think about extracting information; however it always implicates a considerable spent provided that it needs “structured data”. Following this idea, text mining appears in the horizon, as a little spent, reliable alternative. It is able to provide meaningful expert information without the availability of plenty of resources, all we need is a fair big (real big) corpus of text in order to conduct a research on almost every topic. By themselves, both approaches provide valuable information at the end, nevertheless what would happen if both processes were linked in a way that one approach’s results could be verify by the result of a second process? With this idea on mind we are relaying on one hypothesis this is possible to generate a bound between both mining process and using them back and forth to verify one another. Hence, we describe thoroughly both methodologies making a special emphasis on mentioning those phases which have a propensity to establish a strong bound between them. We found that bound in the fact that once a Natural Language Processing has been performed on the chosen corpora what we got as an output is a list of meaningful nouns which can be used as features that will guide in a reliable way a data mining process. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,data mining represents @ cutting edge @ @ think @ extracting information @ however @ always implicates a considerable spent provided @ @ need structured data @ following @ idea text mining appears in @ horizon a a little spent reliable alternative @ @ is able to provide meaningful expert information without @ availability of plenty of resource @ @ need is a fair big @ real big @ corpus of text in order to conduct a research on almost every topic @ by @ @ approach provide valuable information at @ end nevertheless @ would happen if @ process @ linked in a way @ @ approach s @ could @ verify by @ @ of a second process @ @ @ idea on mind @ @ relaying on @ hypothesis @ is possible to generate a bound @ @ mining process and @ @ back and forth to verify @ another @ hence @ describe thoroughly @ methodology making a special emphasis on mentioning @ phase @ @ a propensity to establish a strong bound @ @ @ @ found @ bound in @ fact @ @ a natural language processing ha @ performed on @ chosen corpus @ @ got a @ output is a list of meaningful noun @ @ @ used a feature @ @ guide in a reliable way a data mining process @ @ international publishing switzerland @ 
2611,Auto-FAQ-gen: Automatic frequently asked questions generation,"Using Frequently Asked Questions (FAQs) is a popular way of documenting the list of common questions on particular topics or specific contexts. Most FAQ pages on the Internet are static and can quickly become outdated. We propose to extend the existing work on question answering systems to generating FAQ lists. In conventional Question Answering (QA) systems, users are only allowed to express their queries in a natural language format. A new methodology is proposed to construct the questions by combining of question extraction and question generation methods. The proposed system accepts, extracts, or generates user questions in order to create, maintain, and improve the FAQs quality. In addition to present the basis of QA system, complimentary units will be added to conduct the FAQ list. The research proposed here will contribute to the field of Natural Language Processing, Text Mining, QA, particularly to provide high quality automatic FAQ generation and retrieval. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ frequently asked question @ faq @ is a popular way of documenting @ list of common question on particular topic @ specific context @ @ faq page on @ internet @ static and @ quickly become outdated @ @ propose to extend @ existing work on question answering system to generating faq list @ in conventional question answering @ qa @ system user @ only allowed to express @ query in a natural language format @ a @ methodology is proposed to construct @ question by combining of question extraction and question generation method @ @ proposed system accepts extract @ generates user question in order to create maintain and improve @ faq quality @ in addition to @ @ basis of qa system complimentary unit @ @ added to conduct @ faq list @ @ research proposed @ @ contribute to @ field of natural language processing text mining qa particularly to provide high quality automatic faq generation and retrieval @ @ international publishing switzerland @ 
2612,"MycoCLAP, the database for characterized lignocellulose-active proteins of fungal origin: Resource and text mining curation support","Enzymes active on components of lignocellulosic biomass are used for industrial applications ranging from food processing to biofuels production. These include a diverse array of glycoside hydrolases, carbohydrate esterases, polysaccharide lyases and oxidoreductases. Fungi are prolific producers of these enzymes, spurring fungal genome sequencing efforts to identify and catalogue the genes that encode them. To facilitate the functional annotation of these genes, biochemical data on over 800 fungal lignocellulose-degrading enzymes have been collected from the literature and organized into the searchable database, mycoCLAP (http://mycoclap.fungalgenomics.ca). First implemented in 2011, and updated as described here, mycoCLAP is capable of ranking search results according to closest biochemically characterized homologues: this improves the quality of the annotation, and significantly decreases the time required to annotate novel sequences. The database is freely available to the scientific community, as are the open source applications based on natural language processing developed to support the manual curation of mycoCLAP. © The Author(s) 2015. Published by Oxford University Press.",2015,Database,11,enzyme active on component of lignocellulosic biomass @ used @ industrial application ranging @ food processing to biofuels production @ @ include a diverse array of glycoside hydrolases carbohydrate esterases polysaccharide lyases and oxidoreductase @ fungi @ prolific producer of @ enzyme spurring fungal genome sequencing effort to identify and catalogue @ gene @ encode @ @ to facilitate @ functional annotation of @ gene biochemical data on @ fungal lignocellulose-degrading enzyme @ @ collected @ @ literature and organized @ @ searchable database mycoclap @ http @ mycoclap @ fungalgenomics @ ca @ @ first implemented in and updated a described @ mycoclap is capable of ranking search @ according to closest biochemically characterized homologues @ @ improves @ quality of @ annotation and significantly decrease @ time required to annotate novel sequence @ @ database is freely available to @ scientific community a @ @ open source application based on natural language processing developed to support @ manual curation of mycoclap @ @ author @ s @ @ published by oxford university @ @ 
2616,A domain-based approach to extract Arabic person names using N-grams and simple rules,"Named Entity Recognition (NER) is considered an important task in many human language technologies including information extraction, Natural Language Processing (NLP) and Machine Translation. This is believed to be a challenging task for Arabic language. Most of the existing research studies deal only with names that are found in Modern Standard Arabic (MSA) sources such as news. In this study, we aim at building Classical Arabic name list or Gazetteer which represents an important part of a lively Arabic literature and culture. To achieve this goal, we propose a new approach for extracting Arabic Person Names (APNs). This approach constitutes a new model for extracting named entities from unstructured Arabic text without the need for Part of Speech (POS) tagging and/or morphological analysis. The proposed approach is based on formulating a model that is established on a specific domain. For this study, we use an authentic text in the literature of Islamic-Arabic studies viz, the ""Hadith"". This domain is related to the Prophet Mohammad's Peace Be Upon Him (PBUH) sayings. To achieve aims of this study, we use NLP and text mining techniques to extract and build an accurate standard list of classical APNs. Also, We built a standard evaluation classical names list in order to evaluate our approach. Results show very good precision of around 84%. © Medwell Journals, 2015.",2015,Asian Journal of Information Technology,3,named entity recognition @ ner @ is considered @ important task in many human language technology including information extraction natural language processing @ nlp @ and machine translation @ @ is believed to @ a challenging task @ arabic language @ @ of @ existing research study deal only @ name @ @ found in modern standard arabic @ msa @ source @ a news @ in @ study @ aim at building classical arabic name list @ gazetteer @ represents @ important part of a lively arabic literature and culture @ to achieve @ goal @ propose a @ approach @ extracting arabic person name @ apns @ @ @ approach constitutes a @ model @ extracting named entity @ unstructured arabic text without @ need @ part of speech @ po @ tagging and @ morphological analysis @ @ proposed approach is based on formulating a model @ is established on a specific domain @ @ @ study @ use @ authentic text in @ literature of islamic-arabic study viz @ @ hadith @ @ @ domain is related to @ prophet mohammad @ s peace @ upon @ @ pbuh @ saying @ to achieve aim of @ study @ use nlp and text mining technique to extract and build @ accurate standard list of classical apns @ @ @ built a standard evaluation classical name list in order to evaluate @ approach @ @ @ @ good precision of around @ medwell journal @ 
2617,Text analysis pipelines,"The understanding of natural language is one of the primary abilities that provide the basis for human intelligence. Since the invention of computers, people have thought about how to operationalize this ability in software applications (Jurafsky and Martin 2009). The rise of the internet in the 1990s then made explicit the practical need for automatically processing natural language in order to access relevant information. Search engines, as a solution, have revolutionalized the way we can find such information ad-hoc in large amounts of text (Manning et al. 2008). Until today, however, search engines excel in finding relevant texts rather than in understanding what information is relevant in the texts. Chapter 1 has proposed text mining as a means to achieve progress towards the latter, thereby making information search more intelligent. At the heart of every text mining application lies the analysis of text, mostly realized in the form of text analysis pipelines. In this chapter, we present the basics required to follow the approaches of this book to improve such pipelines for enabling text mining ad-hoc on large amounts of text as well as the state of the art in this respect. Text mining combines techniques from information retrieval, natural language processing, and data mining. In Sect. 2.1, we first provide a focused overview of those techniques referred to in this book. Then, we define the text analysis processes and pipelines that we consider in our proposed approaches (Sect. 2.2). We evaluate the different approaches based on texts and pipelines from a number of case studies introduced in Sect. 2.3. Finally, Sect. 2.4 surveys and discusses related existing work in the broad context of ad-hoc large-scale text mining. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ understanding of natural language is @ of @ primary ability @ provide @ basis @ human intelligence @ since @ invention of computer people @ thought @ @ to operationalize @ ability in software application @ jurafsky and martin @ @ @ rise of @ internet in @ s @ made explicit @ practical need @ automatically processing natural language in order to access relevant information @ search engine a a solution @ revolutionalized @ way @ @ find @ information ad-hoc in @ amount of text @ manning et al @ @ @ @ today however search engine excel in finding relevant text rather @ in understanding @ information is relevant in @ text @ chapter ha proposed text mining a a mean to achieve progress towards @ latter thereby making information search more intelligent @ at @ heart of every text mining application lie @ analysis of text mostly realized in @ form of text analysis pipeline @ in @ chapter @ @ @ basic required to follow @ approach of @ book to improve @ pipeline @ enabling text mining ad-hoc on @ amount of text a well a @ state of @ art in @ respect @ text mining combine technique @ information retrieval natural language processing and data mining @ in sect @ @ @ first provide a focused overview of @ technique referred to in @ book @ @ @ define @ text analysis process and pipeline @ @ consider in @ proposed approach @ sect @ @ @ @ @ evaluate @ different approach based on text and pipeline @ a number of case study introduced in sect @ @ @ finally sect @ @ survey and discus related existing work in @ broad context of ad-hoc large-scale text mining @ @ international publishing switzerland @ 
2619,Reviewing classification approaches in sentiment analysis,"The advancement of web technologies has changed the way people share and express their opinions. People enthusiastically shared their thoughts and opinions via online media such as forums, blogs and social networks. The overwhelmed of online opinionated data have gained much attention by researchers especially in the field of text mining and natural language processing (NLP) to study in depth about sentiment analysis. There are several methods in classifying sentiment, including lexicon-based approach and machine learning approach. Each approach has its own advantages and disadvantages. However, there are not many literatures deliberate on the comparison of both approaches. This paper presents an overview of classification approaches in sentiment analysis. Various advantages and limitations of the sentiment classification approaches based on several criteria such as domain, classification type and accuracy are also discussed in this paper. © Springer Science+Business Media Singapore 2015.",2015,Communications in Computer and Information Science,17,@ advancement of web technology ha changed @ way people share and express @ opinion @ people enthusiastically shared @ thought and opinion via online medium @ a forum blog and social network @ @ overwhelmed of online opinionated data @ gained much attention by researcher especially in @ field of text mining and natural language processing @ nlp @ to study in depth @ sentiment analysis @ @ @ several method in classifying sentiment including lexicon-based approach and machine learning approach @ @ approach ha @ @ advantage and disadvantage @ however @ @ not many literature deliberate on @ comparison of @ approach @ @ @ @ @ overview of classification approach in sentiment analysis @ various advantage and limitation of @ sentiment classification approach based on several criterion @ a domain classification type and accuracy @ @ discussed in @ @ @ @ science @ medium singapore @ 
2620,"Internet Outages, the Eyewitness Accounts: Analysis of the Outages Mailing List","Understanding network reliability and outages is critical to the “health” of the Internet infrastructure. Unfortunately, our ability to analyze Internet outages has been hampered by the lack of access to public information from key players. In this paper, we leverage a somewhat unconventional dataset to analyze Internet reliability—the outages mailing list. The mailing list is an avenue for network operators to share information and insights about widespread outages. Using this unique dataset, we perform a first-of-its-kind longitudinal analysis of Internet outages from 2006 to 2013 using text mining and natural language processing techniques. We observe several interesting aspects of Internet outages: a large number of application and mobility issues that impact users, a rise in content, mobile issues, and discussion of large-scale DDoS attacks in recent years. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,understanding network reliability and outage is critical to @ health of @ internet infrastructure @ unfortunately @ ability to analyze internet outage ha @ hampered by @ lack of access to public information @ key player @ in @ @ @ leverage a somewhat unconventional dataset to analyze internet reliability @ outage mailing list @ @ mailing list is @ avenue @ network operator to share information and insight @ widespread outage @ @ @ unique dataset @ perform a first-of-its-kind longitudinal analysis of internet outage @ to @ text mining and natural language processing technique @ @ observe several interesting aspect of internet outage @ a @ number of application and mobility issue @ impact user a rise in content mobile issue and discussion of large-scale ddos attack in recent year @ @ international publishing switzerland @ 
2621,Recognition of patient-related named entities in Noisy Tele-Health Texts,"We explore methods for effectively extracting information from clinical narratives that are captured in a public health consulting phone service called HealthLink. Our research investigates the application of stateof- the-art natural language processing and machine learning to clinical narratives to extract information of interest. The currently available data consist of dialogues constructed by nurses while consulting patients by phone. Since the data are interviews transcribed by nurses during phone conversations, they include a significant volume and variety of noise. When we extract the patient-related information from the noisy data, we have to remove or correct at least two kinds of noise: explicit noise, which includes spelling errors, unfinished sentences, omission of sentence delimiters, and variants of terms, and implicit noise, which includes non-patient information and patient's untrustworthy information. To filter explicit noise, we propose our own biomedical term detection/normalization method: it resolves misspelling, term variations, and arbitrary abbreviation of terms by nurses. In detecting temporal terms, temperature, and other types of named entities (which show patients' personal information such as age and sex), we propose a bootstrapping-based pattern learning process to detect a variety of arbitrary variations of named entities. To address implicit noise, we propose a dependency path-based filtering method. The result of our denoising is the extraction of normalized patient information, and we visualize the named entities by constructing a graph that shows the relations between named entities. The objective of this knowledge discovery task is to identify associations between biomedical terms and to clearly expose the trends of patients' symptoms and concern; the experimental results show that we achieve reasonable performance with our noise reduction methods. © 2015 ACM.",2015,ACM Transactions on Intelligent Systems and Technology,1,@ explore method @ effectively extracting information @ clinical narrative @ @ captured in a public health consulting phone service called healthlink @ @ research investigates @ application of stateof the-art natural language processing and machine learning to clinical narrative to extract information of interest @ @ currently available data consist of dialogue constructed by nurse @ consulting patient by phone @ since @ data @ interview transcribed by nurse @ phone conversation @ include a significant volume and variety of noise @ @ @ extract @ patient-related information @ @ noisy data @ @ to remove @ correct at least @ kind of noise @ explicit noise @ includes spelling error unfinished sentence omission of sentence delimiters and variant of term and implicit noise @ includes non-patient information and patient @ s untrustworthy information @ to filter explicit noise @ propose @ @ biomedical term detection normalization method @ @ resolve misspelling term variation and arbitrary abbreviation of term by nurse @ in detecting temporal term temperature and @ type of named entity @ @ @ patient @ personal information @ a age and sex @ @ propose a bootstrapping-based pattern learning process to detect a variety of arbitrary variation of named entity @ to address implicit noise @ propose a dependency path-based filtering method @ @ @ of @ denoising is @ extraction of normalized patient information and @ visualize @ named entity by constructing a graph @ @ @ relation @ named entity @ @ objective of @ knowledge discovery task is to identify association @ biomedical term and to clearly expose @ trend of patient @ symptom and concern @ @ experimental @ @ @ @ achieve reasonable performance @ @ noise reduction method @ acm @ 
2622,Annotation event relation for chinese newswire text document,"Event is the coarse-grained form of knowledge representation compared with that of word or concept. Event has more semantic information than word or concept, which is the basic unit of knowing and understanding the real-world for human. There are inherent relations between different events in objective world. At present, there is no relatively full-fledged corpus about event relation in Natural Language Processing field. Therefore, in this paper, we regarded event as the unit of knowledge representation, then we semi-automatically annotated event relation in the aspects of semantic, timing, co-reference and co-participant relation for Chinese newswire news text documents. The purpose of this paper is to supply an evaluation benchmark for event relation detection and platform for text mining. The mean Kappa-score, which illustrated the consistency of annotation, is 93.5%. ©, 2015, Journal of Computational Information Systems. All right reserved.",2015,Journal of Computational Information Systems,0,event is @ coarse-grained form of knowledge representation compared @ @ of word @ concept @ event ha more semantic information @ word @ concept @ is @ basic unit of knowing and understanding @ real-world @ human @ @ @ inherent relation @ different event in objective world @ at @ @ is no relatively full-fledged corpus @ event relation in natural language processing field @ therefore in @ @ @ regarded event a @ unit of knowledge representation @ @ semi-automatically annotated event relation in @ aspect of semantic timing co-reference and co-participant relation @ chinese newswire news text document @ @ purpose of @ @ is to supply @ evaluation benchmark @ event relation detection and platform @ text mining @ @ mean kappa-score @ illustrated @ consistency of annotation is @ @ journal of computational information system @ @ right reserved @ 
2623,Improving cross-document knowledge discovery through content and link analysis of wikipedia knowledge,"The Vector Space Model (VSM) has been widely used in Natural Language Processing (NLP) for representing text documents as a Bag of Words (BOW). However, only document-level statistical information is recorded (e.g., document frequency, inverse document frequency) and word semantics cannot be captured. Improvement towards understanding the meaning of words in texts is a challenging task and sufficient background knowledge may need to be incorporated to provide a better semantic representation of texts. In this paper, we present a text mining model that can automatically discover semantic relationships between concepts across multiple documents (where the traditional search paradigm such as search engines cannot help much) and effectively integrate various evidences mined from Wikipedia knowledge. We propose this integration may effectively complement existing information contained in text corpus and facilitate the construction of a more comprehensive representation and retrieval framework. The experimental results demonstrate the search performance has been significantly enhanced against two competitive baselines. © Springer-Verlag Berlin Heidelberg 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ vector space model @ vsm @ ha @ widely used in natural language processing @ nlp @ @ representing text document a a bag of word @ bow @ @ however only document-level statistical information is recorded @ e @ g @ document frequency inverse document frequency @ and word semantics cannot @ captured @ improvement towards understanding @ meaning of word in text is a challenging task and sufficient background knowledge may need to @ incorporated to provide a better semantic representation of text @ in @ @ @ @ a text mining model @ @ automatically discover semantic relationship @ concept across multiple document @ @ @ traditional search paradigm @ a search engine cannot help much @ and effectively integrate various evidence mined @ wikipedia knowledge @ @ propose @ integration may effectively complement existing information contained in text corpus and facilitate @ construction of a more comprehensive representation and retrieval framework @ @ experimental @ demonstrate @ search performance ha @ significantly enhanced @ @ competitive baseline @ springer-verlag @ @ @ 
2625,On modeling learning communities,"The aim to support learning communities by Web 2.0 technology leads to their investigation through the prism of learning theories. However, self-observation and self-modeling requires appropriate tools and skills. In this paper we focus on users of a forum platform and propose several tools that perform community detection, social network analysis, text mining, natural language processing, and clustering. The outcomes serve as inputs for community models that are automatically established using the i* information modeling approach. Stakeholders can recognize issues in their communities by visual analytics of the models. Based on this, they can refine community learning processes and community environments by retrieving new community requirements from the models. The process of model establishment was evaluated by experts in i* information modeling and the results show their acceptance of the proposed techniques. This solution enables also support of various well-known modeling approaches (like IMS Learning Design). © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ aim to support learning community by web @ technology lead to @ investigation @ @ prism of learning theory @ however self-observation and self-modeling requires appropriate tool and skill @ in @ @ @ focus on user of a forum platform and propose several tool @ perform community detection social network analysis text mining natural language processing and clustering @ @ outcome serve a input @ community model @ @ automatically established @ @ i information modeling approach @ stakeholder @ recognize issue in @ community by visual analytics of @ model @ based on @ @ @ refine community learning process and community environment by retrieving @ community requirement @ @ model @ @ process of model establishment wa evaluated by expert in i information modeling and @ @ @ @ acceptance of @ proposed technique @ @ solution enables @ support of various well-known modeling approach @ like ims learning design @ @ @ international publishing switzerland @ 
2626,A novel framework for semantic discovery of web services using integrated semantic model,"Semantic web technology plays a very critical role in the automatic web service discovery by assigning formal semantics to the service descriptions. Practically, it is not feasible to explicitly annotate the formal semantics to millions of existing services. Further, in user context, the request formation for services in semantic web is a complex process as it requires the user to be technically aware of the underlying technologies of the web services, discovery frameworks, description languages and implementation details. In this paper, we propose a semantic framework that enables Web service discovery based on the combination of semantic and syntax information contained in the service profiles. This novel approach for automatic discovery of Web services employs measures of semantic relatedness, Natural Language Processing techniques and information retrieval based statistical models to match a user request. Additionally, we present an efficient semantic matching technique to compute the intra service semantic similarity scores which further facilitates semantic ranking of services. The efficiency of the proposed approach has been demonstrated through experimental evaluations which clearly show that high degree of automation can be achieved with high precision.The results have been further authenticated by providing comparisons with other Information Retrieval based methods.",2015,Infocommunications Journal,0,semantic web technology play a @ critical role in @ automatic web service discovery by assigning formal semantics to @ service description @ practically @ is not feasible to explicitly annotate @ formal semantics to million of existing service @ @ in user context @ request formation @ service in semantic web is a complex process a @ requires @ user to @ technically aware of @ underlying technology of @ web service discovery framework description language and implementation detail @ in @ @ @ propose a semantic framework @ enables web service discovery based on @ combination of semantic and syntax information contained in @ service profile @ @ novel approach @ automatic discovery of web service employ measure of semantic relatedness natural language processing technique and information retrieval based statistical model to match a user request @ additionally @ @ @ efficient semantic matching technique to compute @ intra service semantic similarity score @ @ facilitates semantic ranking of service @ @ efficiency of @ proposed approach ha @ demonstrated @ experimental evaluation @ clearly @ @ high degree of automation @ @ achieved @ high precision @ @ @ @ @ @ authenticated by providing comparison @ @ information retrieval based method @ 
2627,HITS@FIRE task 2015: Twitter based named entity recognizer for Indian languages,"Natural Language processing (NLP) in its pure sense, is a platform that provides the ability for transforming natural language text to useful information. Named Entity Recognition (NER) is a key task in NLP for classification of named entities in natural languages. Though, there are several algorithms for named entity classification, identifying named entities in twitter data is a demanding task. Loads of information are being shared by people in twitter on a daily basis. This information is unstructured and often contains important information about organizations, politics, disasters, promotional advertisements etc. In this paper, we provide a NER that can effectively classify named entities in twitter data for Indian Languages such as English, Hindi and Tamil. POS, Chunk, Suffix, Prefix information has been used for training in Conditional Random Fields (CRF) based NER Model. CRF is a popular model for labeling and classification in text mining. Performance analysis was done using n-fold validation and F-measure. A maximum precision of 93.82 for English, 92.28 for Hindi and 86.94 for Tamil twitter data was achieved through N fold validation. Results provided by ESM-IL share task in terms of precision for English is 50.48, for Hindi is 81.49 and for Tamil 70.42. The proposed algorithm has a higher classification accuracy and it is achieved through n-fold validation.",2015,CEUR Workshop Proceedings,4,natural language processing @ nlp @ in @ pure sense is a platform @ provides @ ability @ transforming natural language text to useful information @ named entity recognition @ ner @ is a key task in nlp @ classification of named entity in natural language @ though @ @ several algorithm @ named entity classification identifying named entity in twitter data is a demanding task @ load of information @ @ shared by people in twitter on a daily basis @ @ information is unstructured and often contains important information @ organization politics disaster promotional advertisement etc @ in @ @ @ provide a ner @ @ effectively classify named entity in twitter data @ indian language @ a english hindi and tamil @ po chunk suffix prefix information ha @ used @ training in conditional random field @ crf @ based ner model @ crf is a popular model @ labeling and classification in text mining @ performance analysis wa done @ n-fold validation and f-measure @ a maximum precision of @ @ english @ @ hindi and @ @ tamil twitter data wa achieved @ n fold validation @ @ provided by esm-il share task in term of precision @ english is @ @ hindi is @ and @ tamil @ @ @ proposed algorithm ha a higher classification accuracy and @ is achieved @ n-fold validation @ 
2631,Conclusion,"The ability of performing text mining ad-hoc in the large has the potential to essentially improve the way people find information today in terms of speed and quality, both in everyday web search and in big data analytics. More complex information needs can be fulfilled immediately, and previously hidden information can be accessed. At the heart of every text mining application, relevant information is inferred from natural language texts by a text analysis process. Mostly, such a process is realized in the form of a pipeline that sequentially executes a number of information extraction, text classification, and other natural language processing algorithms. As a matter of fact, text mining is studied in the field of computational linguistics, which we consider from a computer science perspective in this book. Besides the fundamental challenge of inferring relevant information effectively, we have revealed the automatic design of a text analysis pipeline and the optimization of a pipeline’s run-time efficiency and domain robustness as major requirements for the enablement of ad-hoc large-scale text mining. Then, we have investigated the research question of how to exploit knowledge about a text analysis process and information obtained within the process to approach these requirements. To this end, we have developed different models and algorithms that can be employed to address information needs ad-hoc on large numbers of texts. The algorithms rely on classical and statistical techniques from artificial intelligence, namely, planning, truth maintenance, and informed search as well as supervised and self-supervised learning. All algorithms have been analyzed formally, implemented as software, and evaluated experimentally. In Sect. 6.1, we summarize our main findings and their contributions to different areas of computational linguistics. We outline that they have both scientific and practical impact on the state of the art in text mining. However, far from every problem of ad-hoc large-scale text mining has been solved or even approached at all in this book. In the words of Alan Turing, we can therefore already see plenty there that needs to be done in the given and in new directions of future research (Sect. 6.2). Also, some of our main ideas may be beneficial for other problems from computer science or even from other fields of application, as we finally sketch at the end. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ ability of performing text mining ad-hoc in @ @ ha @ potential to essentially improve @ way people find information today in term of speed and quality @ in everyday web search and in big data analytics @ more complex information need @ @ fulfilled immediately and @ hidden information @ @ accessed @ at @ heart of every text mining application relevant information is inferred @ natural language text by a text analysis process @ mostly @ a process is realized in @ form of a pipeline @ sequentially executes a number of information extraction text classification and @ natural language processing algorithm @ a a matter of fact text mining is studied in @ field of computational linguistics @ @ consider @ a computer science perspective in @ book @ besides @ fundamental challenge of inferring relevant information effectively @ @ revealed @ automatic design of a text analysis pipeline and @ optimization of a pipeline s run-time efficiency and domain robustness a major requirement @ @ enablement of ad-hoc large-scale text mining @ @ @ @ investigated @ research question of @ to exploit knowledge @ a text analysis process and information obtained within @ process to approach @ requirement @ to @ end @ @ developed different model and algorithm @ @ @ employed to address information need ad-hoc on @ number of text @ @ algorithm rely on classical and statistical technique @ artificial intelligence namely planning truth maintenance and informed search a well a supervised and self-supervised learning @ @ algorithm @ @ analyzed formally implemented a software and evaluated experimentally @ in sect @ @ @ summarize @ main finding and @ contribution to different area of computational linguistics @ @ outline @ @ @ @ scientific and practical impact on @ state of @ art in text mining @ however far @ every problem of ad-hoc large-scale text mining ha @ solved @ even approached at @ in @ book @ in @ word of alan turing @ @ therefore already see plenty @ @ need to @ done in @ given and in @ direction of future research @ sect @ @ @ @ @ some of @ main idea may @ beneficial @ @ problem @ computer science @ even @ @ field of application a @ finally sketch at @ end @ @ international publishing switzerland @ 
2632,Emotion-based music information retrieval using lyrics,"In this paper, we present a study on emotion-based music information retrieval using lyrics information. Listeners want to search the lyrics of music suitable for his/her emotion (impression of music), by using an information system from music libraries. As a solution of listeners’ needs, we have designed a system that retrieve the lyrics of music based on the emotion (or the impression) suitable for a listener’s feelings that the listener has selected, from 9 emotions and 9 impressions. We select the words, i.e. verb and adjective, from the bridge part of the lyrics of music that express emotion in lyrics by using natural language processing. We summarize the words into the representative words by using a dictionary of synonyms. We make a model that estimates a listener’s 9 emotion/impression of the representative words by using a machine learning method. And listeners want to understand why the recommended music by a system is suitable for his/her emotion/impression. Therefore, we select the representative words most related to a listener’s emotion/impression and we use the selected words as the explanation of reason to a listener. We have made each model of emotion and impression for 9 subjects and have evaluated the accuracy of the model. We also have investigated the selected representative words related to emotion/impression. © IFIP International Federation for Information Processing 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,in @ @ @ @ a study on emotion-based music information retrieval @ lyric information @ listener want to search @ lyric of music suitable @ @ @ emotion @ impression of music @ by @ @ information system @ music library @ a a solution of listener need @ @ designed a system @ retrieve @ lyric of music based on @ emotion @ @ @ impression @ suitable @ a listener s feeling @ @ listener ha selected @ emotion and impression @ @ select @ word i @ e @ verb and adjective @ @ bridge part of @ lyric of music @ express emotion in lyric by @ natural language processing @ @ summarize @ word @ @ representative word by @ a dictionary of synonym @ @ make a model @ estimate a listener s emotion impression of @ representative word by @ a machine learning method @ and listener want to understand @ @ recommended music by a system is suitable @ @ @ emotion impression @ therefore @ select @ representative word @ related to a listener s emotion impression and @ use @ selected word a @ explanation of reason to a listener @ @ @ made @ model of emotion and impression @ subject and @ evaluated @ accuracy of @ model @ @ @ @ investigated @ selected representative word related to emotion impression @ ifip international federation @ information processing @ 
2635,Text mining in social media for security threats,"We discuss techniques for information extraction from texts, and present two applications that use these techniques. We focus in particular on social media texts (Twitter messages), which present challenges for the information extraction techniques because they are noisy and short. The first application is extracting the locations mentioned in Twittermessages, and the second one is detecting the location of the users based on all the tweets written by each user. The same techniques can be used for extracting other kinds of information from social media texts, with the purpose ofmonitoring the topics, events, emotions, or locations of interest to security and defence applications. © Springer International Publishing Switzerland 2016.",2015,Studies in Computational Intelligence,2,@ discus technique @ information extraction @ text and @ @ application @ use @ technique @ @ focus in particular on social medium text @ twitter message @ @ @ challenge @ @ information extraction technique @ @ @ noisy and short @ @ first application is extracting @ location mentioned in twittermessages and @ second @ is detecting @ location of @ user based on @ @ tweet written by @ user @ @ @ technique @ @ used @ extracting @ kind of information @ social medium text @ @ purpose ofmonitoring @ topic event emotion @ location of interest to security and defence application @ @ international publishing switzerland @ 
2638,Exploration of known and unknown early symptoms of cervical cancer and development of a symptom spectrum - Outline of a data and text mining based approach,"This position paper delineates the structure of some experiments to detect early symptoms of cervical cancer. We are using a large corpora of electronic patient records texts in Swedish from Karolinska University Hospital from the years 2009-2010, where we extracted in total 1,660 patient records with the ICD-10 diagnosis code C53 for cervical cancer. We used a Named Entity Recogniser called Clinical Entity Finder to detect the diagnosis and symptoms expressed in these clinical texts containing in total 2,988,118 words. We found 28,218 symptoms and diagnoses on these 1,660 patients. We present some initial findings, and discuss them and propose a set of experiments to find possible early symptoms and/or a spectrum of early symptoms of cervical cancer. Copyright © 2015 for the individual papers by the papers' authors.",2015,CEUR Workshop Proceedings,0,@ position @ delineates @ structure of some experiment to detect early symptom of cervical cancer @ @ @ @ a @ corpus of electronic patient record text in swedish @ karolinska university hospital @ @ year @ @ extracted in total patient record @ @ icd diagnosis code c @ cervical cancer @ @ used a named entity recogniser called clinical entity finder to detect @ diagnosis and symptom expressed in @ clinical text containing in total word @ @ found symptom and diagnosis on @ patient @ @ @ some initial finding and discus @ and propose a set of experiment to find possible early symptom and @ a spectrum of early symptom of cervical cancer @ @ @ @ individual @ by @ @ @ author @ 
2639,Temporal expression extraction with extensive feature type selection and a posteriori label adjustment,"The automatic extraction of temporal information from written texts is pivotal for many Natural Language Processing applications such as question answering, text summarisation and information retrieval. It allows to filter information and infer temporal flows of events. This paper presents ManTIME, a general domain temporal expression identification and normalisation system, and systematically explores the impact of different features and training corpora on the performance. The identification phase combines the use of conditional random fields along with a post-processing pipeline, whereas the normalisation phase is carried out using NorMA, an open-source rule-based temporal normaliser. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in the results based on gazetteers, shallow parsing and propositional noun phrases labels on top of the morpho-lexical features. We also show that the use of silver data (alone or in addition to the human-annotated ones) does not improve the performance. We evaluate six combinations of training data and post-processing pipeline with respect to the TempEval-3 benchmark test set. The best run achieved 0.95 (precision), 0.85 (recall) and 0.90 (Fβ=1) in the identification phase. Normalisation accuracies are 0.86 (for type attribute) and 0.77 (for value attribute). The proposed approach ranked 3rd in the TempEval-3 challenge (task A) as the best performing machine learning-based system among 21 participants. © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license.",2015,Data and Knowledge Engineering,5,@ automatic extraction of temporal information @ written text is pivotal @ many natural language processing application @ a question answering text summarisation and information retrieval @ @ allows to filter information and infer temporal flow of event @ @ @ @ mantime a general domain temporal expression identification and normalisation system and systematically explores @ impact of different feature and training corpus on @ performance @ @ identification phase combine @ use of conditional random field along @ a post-processing pipeline whereas @ normalisation phase is carried @ @ norma @ open-source rule-based temporal normaliser @ @ investigate @ performance variation @ respect to different feature type @ specifically @ @ @ @ use of wordnet-based feature in @ identification task negatively affect @ overall performance and @ @ is no statistically significant difference in @ @ based on gazetteer shallow parsing and propositional noun phrase label on top of @ morpho-lexical feature @ @ @ @ @ @ use of silver data @ alone @ in addition to @ human-annotated @ @ doe not improve @ performance @ @ evaluate six combination of training data and post-processing pipeline @ respect to @ tempeval benchmark test set @ @ best run achieved @ @ precision @ @ @ recall @ and @ @ fβ @ in @ identification phase @ normalisation accuracy @ @ @ @ type attribute @ and @ @ @ value attribute @ @ @ proposed approach ranked rd in @ tempeval challenge @ task a @ a @ best performing machine learning-based system among participant @ @ author @ published by @ b @ v @ @ is @ open access article @ @ cc by license @ 
2643,Extracting information from electronic medical records to identify obesity status of a patient based on comorbidities and bodyweight measures,"Obesity is a chronic disease with an increasing impact on the world’s population. In this work, we present a method to identify obesity using text mining techniques and information related to body weight measures and obesity comorbidities. We used a dataset of 2412 de-identified medical records that contains labels for two classification problems. The first classification problem recognizes between obesity, overweight, normal weight, and underweight. The second problem of classification corresponds to the obesity types under the obesity category to recognize between super obesity, morbid obesity, severe obesity and moderate obesity. We used a Bag of Words approach to represent the records together with unigram and bigram representation of the features. We used Support Vector Machine and Naïve Bayes together with ten-fold cross validation to evaluate and compare performances. In general, our results show that Support Vector Machine obtains better performances than Naïve Bayes for both classification problems. We also observed that bigram representation improves performance compared with unigram representation. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,obesity is a chronic disease @ @ increasing impact on @ world s population @ in @ work @ @ a method to identify obesity @ text mining technique and information related to body weight measure and obesity comorbidities @ @ used a dataset of de-identified medical record @ contains label @ @ classification problem @ @ first classification problem recognizes @ obesity overweight normal weight and underweight @ @ second problem of classification corresponds to @ obesity type @ @ obesity category to recognize @ super obesity morbid obesity severe obesity and moderate obesity @ @ used a bag of word approach to represent @ record together @ unigram and bigram representation of @ feature @ @ used support vector machine and naïve bayes together @ ten-fold cross validation to evaluate and compare performance @ in general @ @ @ @ support vector machine obtains better performance @ naïve bayes @ @ classification problem @ @ @ observed @ bigram representation improves performance compared @ unigram representation @ @ international publishing switzerland @ 
2646,Using semantics and NLP in experimental protocols,"In this paper we present SMART Protocols, a semantic and NLP-based infrastructure for processing and enacting experimental protocols. Our contribution is twofold; on the one hand, SMART Protocols delivers a semantic layer that represents the knowledge encoded in experimental protocols. On the other hand, it builds the groundwork for making use of such semantics within an NLP framework. We emphasize the semantic and NLP components, namely the SMART Protocols (SP) Ontology, the Sample Instrument Reagent Objective (SIRO) model and the text mining integrative architecture GATE. The SIRO model defines an extended layer of metadata for experimental protocols; SIRO is also a Minimal Information (MI) model conceived in the same realm as the Patient Intervention Comparison Outcome (PICO) model that supports search, retrieval and classification purposes. By combining comprehensive vocabularies with NLP rules and gazetteers, we identify meaningful parts of speech in experimental protocols. Moreover, in cases for which SIRO is not available, our NLP automatically extracts it; also, searching for queries such as: What bacteria have been used in protocols for persister cells isolation is possible. © Copyright 2015 for the individual papers by the papers' authors.",2015,CEUR Workshop Proceedings,0,in @ @ @ @ smart protocol a semantic and nlp-based infrastructure @ processing and enacting experimental protocol @ @ contribution is twofold @ on @ @ hand smart protocol delivers a semantic layer @ represents @ knowledge encoded in experimental protocol @ on @ @ hand @ build @ groundwork @ making use of @ semantics within @ nlp framework @ @ emphasize @ semantic and nlp component namely @ smart protocol @ sp @ ontology @ sample instrument reagent objective @ siro @ model and @ text mining integrative architecture gate @ @ siro model defines @ extended layer of metadata @ experimental protocol @ siro is @ a minimal information @ mi @ model conceived in @ @ realm a @ patient intervention comparison outcome @ pico @ model @ support search retrieval and classification purpose @ by combining comprehensive vocabulary @ nlp rule and gazetteer @ identify meaningful part of speech in experimental protocol @ moreover in case @ @ siro is not available @ nlp automatically extract @ @ @ searching @ query @ a @ @ bacteria @ @ used in protocol @ persister cell isolation is possible @ @ @ @ individual @ by @ @ @ author @ 
2649,On developing extraction rules for mining informal scientific references from altmetric data sources,"Altmetrics measure scientific impact outside of traditional scientific literature. We identify mentions of scientific research or entities like researchers, academic or research organizations in a corpus containing blogs, articles, news items etc. We first manually analyse the corpus for patterns of such informal mentions and then apply text mining techniques by developing extraction rules for mining informal mentions. We apply them to our development corpus and present our results. This work takes us closer to developing concrete altmetrics for determining research impact on news and public discourse ultimately leading to measuring impact of scientific research on government policies. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,altmetrics measure scientific impact outside of traditional scientific literature @ @ identify mention of scientific research @ entity like researcher @ @ research organization in a corpus containing blog article news item etc @ @ first manually analyse @ corpus @ pattern of @ informal mention and @ apply text mining technique by developing extraction rule @ mining informal mention @ @ apply @ to @ development corpus and @ @ @ @ @ work take u closer to developing concrete altmetrics @ determining research impact on news and public discourse ultimately leading to measuring impact of scientific research on government policy @ @ international publishing switzerland @ 
2650,New metrics and related statistical approaches for efficient mining in very large and highly multidimensional databases,"As regard to the evolution of the concept of text and to the continuous growth of textual information of multiple nature which is available online, one of the important issues for linguists and information analysts for building up assumptions and validating models is to exploit efficient tools for textual analysis, able to adapt to large volumes of heterogeneous data, often changing and of distributed nature. We propose in this communication to look at new statistical methods that fit into this framework but that can also extent their application range to the more general context of dynamic numerical data. For that purpose, we have recently proposed an alternative metric based on feature maximization. The principle of this metric is to define a measure of compromise between generality and discrimination based altogether on the properties of the data which are specific to each group of a partition and on those which are shared between groups. One of the key advantages of this method is that it is operational in an incremental mode both on clustering (i.e. unsupervised classification) and on traditional categorization. We have shown that it allowed to very efficiently solve complex multidimensional problems related to unsupervised analysis of textual or linguistic data, like topic tracking with data changing over time or automatic classification in natural language processing (NLP) context. It can also adapt to the traditional discriminant analysis, often exploited in text mining, or to automatic text indexing or summarization, with performance that are far superior to conventional methods. In a more general way, this approach that freed from the exploitation of parameters can be exploited as an accurate feature selection and data resampling method in any numerical or non numerical context. We will present the general principles of feature maximization and we will especially return to its successful applications in the supervised framework, comparing its performance with those of the state of the art methods on reference databases. © Springer International Publishing Switzerland 2015.",2015,Communications in Computer and Information Science,1,a regard to @ evolution of @ concept of text and to @ continuous growth of textual information of multiple nature @ is available online @ of @ important issue @ linguist and information analyst @ building up assumption and validating model is to exploit efficient tool @ textual analysis able to adapt to @ volume of heterogeneous data often changing and of distributed nature @ @ propose in @ communication to look at @ statistical method @ fit @ @ framework @ @ @ @ extent @ application range to @ more general context of dynamic numerical data @ @ @ purpose @ @ recently proposed @ alternative metric based on feature maximization @ @ principle of @ metric is to define a measure of compromise @ generality and discrimination based altogether on @ property of @ data @ @ specific to @ group of a partition and on @ @ @ shared @ group @ @ of @ key advantage of @ method is @ @ is operational in @ incremental mode @ on clustering @ i @ e @ unsupervised classification @ and on traditional categorization @ @ @ @ @ @ allowed to @ efficiently solve complex multidimensional problem related to unsupervised analysis of textual @ linguistic data like topic tracking @ data changing @ time @ automatic classification in natural language processing @ nlp @ context @ @ @ @ adapt to @ traditional discriminant analysis often exploited in text mining @ to automatic text indexing @ summarization @ performance @ @ far superior to conventional method @ in a more general way @ approach @ freed @ @ exploitation of parameter @ @ exploited a @ accurate feature selection and data resampling method in @ numerical @ non numerical context @ @ @ @ @ general principle of feature maximization and @ @ especially return to @ successful application in @ supervised framework comparing @ performance @ @ of @ state of @ art method on reference database @ @ international publishing switzerland @ 
2656,Extracting nested biomedical entity relations by tagging dependency chains,"Biomedical event extraction is an important research topic in the field of biomedical text mining. However, much research work is required before event extraction systems become applicable. Thus, we proposed a novel and efficient approach for extracting nested biomedical events. First, using dependency parsing, we extracted the target sequences that containedbiomedical entity (trigger/argument) chains. Second, the Condition Random Fields (CRFs) model was used to tag the entity chains which represented the nested argument-trigger edges. Thirdly, the post-processing step was used to output the event.This method is a new attempt to treat the biomedical event extraction as a sequence tagging problem.The experiment resultsshowed that we got the performance of 47.3 in F-score which is promising when compared with the joint ML-based system i BioNLP-ST2013. Furthermore, we estimated the results of the trigger detection, which outperformed the state-of-the-art systems on the same corpus. Therefore, our work is a positive contribution to the biomedical text mining community. © 2015 Kavala Institute of Technology.",2015,Journal of Engineering Science and Technology Review,1,biomedical event extraction is @ important research topic in @ field of biomedical text mining @ however much research work is required @ event extraction system become applicable @ thus @ proposed a novel and efficient approach @ extracting nested biomedical event @ first @ dependency parsing @ extracted @ target sequence @ containedbiomedical entity @ trigger argument @ chain @ second @ condition random field @ crfs @ model wa used to tag @ entity chain @ represented @ nested argument-trigger edge @ thirdly @ post-processing step wa used to output @ event @ @ method is a @ attempt to treat @ biomedical event extraction a a sequence tagging problem @ @ experiment resultsshowed @ @ got @ performance of @ in f-score @ is promising @ compared @ @ joint ml-based system i bionlp-st @ furthermore @ estimated @ @ of @ trigger detection @ outperformed @ state-of-the-art system on @ @ corpus @ therefore @ work is a positive contribution to @ biomedical text mining community @ kavala institute of technology @ 
2658,Summarization of documents by finding key sentences based on social network analysis,"Finding key sentences or paragraphs from a document is an important and challenging problem. In recent years, the amount of text data has grown astronomically and this growth has produced a great demand for text summarization. In the present study, we propose a new text summarization process by text mining and social network methods. To demonstrate the applicability of the proposed summarization procedure, we used Martin Luther King, Jr’s public speech © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,finding key sentence @ paragraph @ a document is @ important and challenging problem @ in recent year @ amount of text data ha grown astronomically and @ growth ha produced a great demand @ text summarization @ in @ @ study @ propose a @ text summarization process by text mining and social network method @ to demonstrate @ applicability of @ proposed summarization procedure @ used martin luther king jr s public speech @ international publishing switzerland @ 
2659,Dr. Inventor framework: Extracting structured information from scientific publications,"Even if research communities and publishing houses are putting increasing efforts in delivering scientific articles as structured texts, nowadays a considerable part of on-line scientific literature is still available in layout-oriented data formats, like PDF, lacking any explicit structural or semantic information. As a consequence the bootstrap of textual analysis of scientific papers is often a time-consuming activity. We present the first version of the Dr. Inventor Framework, a publicly available collection of scientific text mining components useful to prevent or at least mitigate this problem. Thanks to the integration and the customization of several text mining tools and on-line services, the Dr. Inventor Framework is able to analyze scientific publications both in plain text and PDF format, making explicit and easily accessible core aspects of their structure and semantics. The facilities implemented by the Framework include the extraction of structured textual contents, the discursive characterization of sentences, the identifications of the structural elements of both papers header and bibliographic entries and the generation of graph based representations of text excerpts. The Framework is distributed as a Java library. We describe in detail the scientific mining facilities included in the Framework and present two use cases where the Framework is respectively exploited to boost scientific creativity and to generate RDF graphs from scientific publications. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),28,even if research community and publishing house @ putting increasing effort in delivering scientific article a structured text nowadays a considerable part of on-line scientific literature is still available in layout-oriented data format like pdf lacking @ explicit structural @ semantic information @ a a consequence @ bootstrap of textual analysis of scientific @ is often a time-consuming activity @ @ @ @ first version of @ dr @ inventor framework a publicly available collection of scientific text mining component useful to prevent @ at least mitigate @ problem @ thanks to @ integration and @ customization of several text mining tool and on-line service @ dr @ inventor framework is able to analyze scientific publication @ in plain text and pdf format making explicit and easily accessible core aspect of @ structure and semantics @ @ facility implemented by @ framework include @ extraction of structured textual content @ discursive characterization of sentence @ identification of @ structural element of @ @ header and bibliographic entry and @ generation of graph based representation of text excerpt @ @ framework is distributed a a java library @ @ describe in detail @ scientific mining facility included in @ framework and @ @ use case @ @ framework is respectively exploited to boost scientific creativity and to generate rdf graph @ scientific publication @ @ international publishing switzerland @ 
2661,A combined resource of biomedical terminology and its statistics,"In this paper, we present a large biomedical term resource automatically compiled from the terminology of a selection of biomedical databases. The resource has a very simple and intuitive format and therefore can be easily embedded into a system for biomedical text mining and used as a linguistic resource. It is continuously updated and a user interface makes it possible to compile a new term resource according to individual requirements by selecting specific databases to be included. We present statistics for each included biomedical entity type separately as well as in the context of the combined terminology.",2015,CEUR Workshop Proceedings,4,in @ @ @ @ a @ biomedical term resource automatically compiled @ @ terminology of a selection of biomedical database @ @ resource ha a @ simple and intuitive format and therefore @ @ easily embedded @ a system @ biomedical text mining and used a a linguistic resource @ @ is continuously updated and a user interface make @ possible to compile a @ term resource according to individual requirement by selecting specific database to @ included @ @ @ statistic @ @ included biomedical entity type separately a well a in @ context of @ combined terminology @ 
2662,IICE: Web tool for automatic identification of chemical entities and interactions,"Automatic methods are being developed and applied to transform textual biomedical information into machine-readable formats. Machine learning techniques have been a prominent approach to this problem. However, there is still a lack of systems that are easily accessible to users. For this reason, we developed a web tool to facilitate the access to our text mining framework, IICE (Identifying Interactions between Chemical Entities). This tool annotates the input text with chemical entities and identifies the interactions described between these entities. Various options are available, which can be manipulated to control the algorithms employed by the framework and to the output formats. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,automatic method @ @ developed and applied to transform textual biomedical information @ machine-readable format @ machine learning technique @ @ a prominent approach to @ problem @ however @ is still a lack of system @ @ easily accessible to user @ @ @ reason @ developed a web tool to facilitate @ access to @ text mining framework iice @ identifying interaction @ chemical entity @ @ @ tool annotates @ input text @ chemical entity and identifies @ interaction described @ @ entity @ various option @ available @ @ @ manipulated to control @ algorithm employed by @ framework and to @ output format @ @ international publishing switzerland @ 
2664,Disambiguation of semantic types in complex noun phrases for extracting candidate terms,"Mapping concepts from medical resources to structured medical documents is a prerequisite for many automatic document processing tasks. These resources are characterised by an abundance of material to represent any given concept. Moreover, the resources may include ambiguous terms in unstructured form that lead to distorted results in automating biomedical text mining. This paper is an exploratory study on disambiguation of semantic types for extracting a structured taxonomy from unstructured reports. Specifically, the terms that will be disambiguated are terms that have more than one semantic type in the Unified Medical Language System (UMLS) Metathesaurus. We suggest a word sense disambiguation algorithm that utilises the UMLS is-a hierarchy, augmented with a higher level representing semantic groups, as a knowledge base. The purpose is to explore all possible commonalities to classify simple or composed candidate terms with the Nearest Common Kinship (NCK). Experiments with the training corpora provide encouraging results. © 2015 Inderscience Enterprises Ltd.",2015,"International Journal of Metadata, Semantics and Ontologies",0,mapping concept @ medical resource to structured medical document is a prerequisite @ many automatic document processing task @ @ resource @ characterised by @ abundance of material to represent @ given concept @ moreover @ resource may include ambiguous term in unstructured form @ lead to distorted @ in automating biomedical text mining @ @ @ is @ exploratory study on disambiguation of semantic type @ extracting a structured taxonomy @ unstructured report @ specifically @ term @ @ @ disambiguated @ term @ @ more @ @ semantic type in @ unified medical language system @ umls @ metathesaurus @ @ suggest a word sense disambiguation algorithm @ utilises @ umls is-a hierarchy augmented @ a higher level representing semantic group a a knowledge base @ @ purpose is to explore @ possible commonality to classify simple @ composed candidate term @ @ nearest common kinship @ nck @ @ experiment @ @ training corpus provide encouraging @ @ inderscience enterprise ltd @ 
2665,Novel search engine supporting specific drug queries and literature management,"The growing concern for acquired microbial resistance is promoting the publication of a large number of clinical and biological antimicrobial studies. Most of these publications can be obtained by searching the PubMed database, but the broad scope and huge size of this collection make the search challenging, and time consuming. This paper presents an advanced search engine for the screening of up-to-date information on drug-related experimental studies. The main contributions lay on the resource-oriented architecture and the semantic analysis of the documents. The RESTful API enables the use of the searchable collection by different user interfaces whereas text mining tools support domain-specific document labeling, scoring and indexing. A small search engine demo indexing articles on antimicrobial peptide research is available at http://sing.ei.uvigo.es/sds/. The source code is also accessible from the same homepage and freely available under MIT License. © Springer International Publishing Switzerland 2015.",2015,Advances in Intelligent Systems and Computing,0,@ growing concern @ acquired microbial resistance is promoting @ publication of a @ number of clinical and biological antimicrobial study @ @ of @ publication @ @ obtained by searching @ pubmed database @ @ broad scope and huge size of @ collection make @ search challenging and time consuming @ @ @ @ @ advanced search engine @ @ screening of up-to-date information on drug-related experimental study @ @ main contribution lay on @ resource-oriented architecture and @ semantic analysis of @ document @ @ restful api enables @ use of @ searchable collection by different user interface whereas text mining tool support domain-specific document labeling scoring and indexing @ a small search engine demo indexing article on antimicrobial peptide research is available at http @ sing @ ei @ uvigo @ e sd @ @ source code is @ accessible @ @ @ homepage and freely available @ mit license @ @ international publishing switzerland @ 
2668,Overview of the PAN/CLEF 2015 evaluation lab,"This paper presents an overview of the PAN/CLEF evaluation lab. During the last decade, PAN has been established as the main forum of text mining research focusing on the identification of personal traits of authors left behind in texts unintentionally. PAN 2015 comprises three tasks: plagiarism detection, author identification and author profiling studying important variations of these problems. In plagiarism detection, community-driven corpus construction is introduced as a new way of developing evaluation resources with diversity. In author identification, cross-topic and cross-genre author verification (where the texts of known and unknown authorship do not match in topic and/or genre) is introduced. A new corpus was built for this challenging, yet realistic, task covering four languages. In author profiling, in addition to usual author demographics, such as gender and age, five personality traits are introduced (openness, conscientiousness, extraversion, agreeableness, and neuroticism) and a new corpus of Twitter messages covering four languages was developed. In total, 53 teams participated in all three tasks of PAN 2015 and, following the practice of previous editions, software submissions were required and evaluated within the TIRA experimentation framework. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),28,@ @ @ @ overview of @ pan clef evaluation lab @ @ @ last decade pan ha @ established a @ main forum of text mining research focusing on @ identification of personal trait of author left behind in text unintentionally @ pan comprises three task @ plagiarism detection author identification and author profiling studying important variation of @ problem @ in plagiarism detection community-driven corpus construction is introduced a a @ way of developing evaluation resource @ diversity @ in author identification cross-topic and cross-genre author verification @ @ @ text of known and unknown authorship @ not match in topic and @ genre @ is introduced @ a @ corpus wa built @ @ challenging yet realistic task covering four language @ in author profiling in addition to usual author demographic @ a gender and age five personality trait @ introduced @ openness conscientiousness extraversion agreeableness and neuroticism @ and a @ corpus of twitter message covering four language wa developed @ in total team participated in @ three task of pan and following @ practice of previous edition software submission @ required and evaluated within @ tira experimentation framework @ @ international publishing switzerland @ 
2669,EXTraccion de RElaciones entre Conceptos Medicos,"This project addresses extraction of medical concepts relationship in scientific documents, medical records and general information on the Internet, in several languages by using advanced Natural Language Processing and Information Retrieval techniques and tools. The project aims to show, through two use cases, the benefits of the application of language technology in the health sector. © 2015 Sociedad Española para el Procesamiento del Lenguaje Natural.",2015,Procesamiento de Lenguaje Natural,1,@ project address extraction of medical concept relationship in scientific document medical record and general information on @ internet in several language by @ advanced natural language processing and information retrieval technique and tool @ @ project aim to @ @ @ use case @ benefit of @ application of language technology in @ health sector @ sociedad española para el procesamiento del lenguaje natural @ 
2673,Towards computation of novel ideas from corpora of scientific text,"In this work we present a method for the computation of novel ‘ideas’ from corpora of scientific text. The system functions by first detecting concept noun-phrases within the titles and abstracts of publications using Part-Of-Speech tagging, before classifying these into sets of problem and solution phrases via a target-word matching approach. By defining an idea as a co-occurring <problem,solution> pair, Known-idea triples can be constructed through the additional assignment of a relevance value (computed via either phrase co-occurrence or an ‘idea frequency-inverse document frequency’ score). The resulting triples are then fed into a collaborative filtering algorithm, where problem-phrases are considered as users and solution-phrases as the items to be recommended. The final output is a ranked list of novel idea candidates, which hold potential for researchers to integrate into their hypothesis generation processes. This approach is evaluated using a subset of publications from the journal Science, with precision, recall and F-Measure results for a variety of model parametrizations indicating that the system is capable of generating useful novel ideas in an automated fashion. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9,in @ work @ @ a method @ @ computation of novel idea @ corpus of scientific text @ @ system function by first detecting concept noun-phrases within @ title and abstract of publication @ part-of-speech tagging @ classifying @ @ set of problem and solution phrase via a target-word matching approach @ by defining @ idea a a co-occurring problem solution pair known-idea triple @ @ constructed @ @ additional assignment of a relevance value @ computed via either phrase co-occurrence @ @ idea frequency-inverse document frequency score @ @ @ resulting triple @ @ fed @ a collaborative filtering algorithm @ problem-phrases @ considered a user and solution-phrases a @ item to @ recommended @ @ final output is a ranked list of novel idea candidate @ hold potential @ researcher to integrate @ @ hypothesis generation process @ @ approach is evaluated @ a subset of publication @ @ journal science @ precision recall and f-measure @ @ a variety of model parametrizations indicating @ @ system is capable of generating useful novel idea in @ automated fashion @ @ international publishing switzerland @ 
2674,A tweet classification model based on dynamic and static component topic vectors,This paper presents an unsupervised architecture for retrieving and ranking conceptually related tweets which can be used in real time. We present a model for ranking tweets with respect to topic relevance in order to improve the accuracy of information extraction. The proposed architecture uses concept enrichment from a knowledge source in order to expand the concept beyond the search keywords. The enriched concept is used to determine similarity levels between tweets and the given concept followed by a ranking of those tweets based on different similarity values. Tweets above a certain similarity threshold are considered as useful for providing relevant information (this is not part of this paper). We obtained precision values up to 0.81 and F values up to 0.61 for a tweet corpus of 2400 Tweets on the topic related to 2014 NZ general elections. © Springer International Publishing Switzerland 2015.,2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ @ @ unsupervised architecture @ retrieving and ranking conceptually related tweet @ @ @ used in real time @ @ @ a model @ ranking tweet @ respect to topic relevance in order to improve @ accuracy of information extraction @ @ proposed architecture us concept enrichment @ a knowledge source in order to expand @ concept beyond @ search keywords @ @ enriched concept is used to determine similarity level @ tweet and @ given concept followed by a ranking of @ tweet based on different similarity value @ tweet @ a certain similarity threshold @ considered a useful @ providing relevant information @ @ is not part of @ @ @ @ @ obtained precision value up to @ and f value up to @ @ a tweet corpus of tweet on @ topic related to nz general election @ @ international publishing switzerland @ 
2678,Improving opinion retrieval in social media by combining features-based coreferencing and memory-based learning,"Social networks messaging typically contains a lot of implicit linguistic information partially due to restrictions on a message's length (i.e., few named entities, short sentences, no discourse structure, etc.). This may significantly impact several applications including opinion mining, sentiment analysis, etc., as data collection tasks such as opinion retrieval tasks will fail to obtain all the relevant messages whenever the target topic, objects, or features are not explicit within the texts. In order to address these issues, in this paper a novel adaptive approach for opinion retrieval is proposed. It combines natural-language co-referencing techniques, features-based linguistic preprocessing and memory-based learning to resolving implicit co-referencing within informal opinion texts by using underlying hierarchies of thread messages. Experiments were conducted to assess the ability of the model to improve opinion retrieval by resolving implicit entities and features, showing the promise of our opinion retrieval approach when compared to state-of-the-art methods using text data from social networks. © 2014 Elsevier Inc.",2015,Information Sciences,10,social network messaging typically contains a lot of implicit linguistic information partially due to restriction on a message @ s length @ i @ e @ @ named entity short sentence no discourse structure etc @ @ @ @ may significantly impact several application including opinion mining sentiment analysis etc @ a data collection task @ a opinion retrieval task @ fail to obtain @ @ relevant message whenever @ target topic object @ feature @ not explicit within @ text @ in order to address @ issue in @ @ a novel adaptive approach @ opinion retrieval is proposed @ @ combine natural-language co-referencing technique features-based linguistic preprocessing and memory-based learning to resolving implicit co-referencing within informal opinion text by @ underlying hierarchy of thread message @ experiment @ conducted to ass @ ability of @ model to improve opinion retrieval by resolving implicit entity and feature showing @ promise of @ opinion retrieval approach @ compared to state-of-the-art method @ text data @ social network @ @ inc @ 
2679,Global machine learning for spatial ontology population,"Understanding spatial language is important in many applications such as geographical information systems, human computer interaction or text-to-scene conversion. Due to the challenges of designing spatial ontologies, the extraction of spatial information from natural language still has to be placed in a well-defined framework. In this work, we propose an ontology which bridges between cognitive-linguistic spatial concepts in natural language and multiple qualitative spatial representation and reasoning models. To make a mapping between natural language and the spatial ontology, we propose a novel global machine learning framework for ontology population. In this framework we consider relational features and background knowledge which originate from both ontological relationships between the concepts and the structure of the spatial language. The advantage of the proposed global learning model is the scalability of the inference, and the flexibility for automatically describing text with arbitrary semantic labels that form a structured ontological representation of its content. The machine learning framework is evaluated with SemEval-2012 and SemEval-2013 data from the spatial role labeling task. © 2014 Elsevier B.V.",2015,Journal of Web Semantics,21,understanding spatial language is important in many application @ a geographical information system human computer interaction @ text-to-scene conversion @ due to @ challenge of designing spatial ontology @ extraction of spatial information @ natural language still ha to @ placed in a well-defined framework @ in @ work @ propose @ ontology @ bridge @ cognitive-linguistic spatial concept in natural language and multiple qualitative spatial representation and reasoning model @ to make a mapping @ natural language and @ spatial ontology @ propose a novel global machine learning framework @ ontology population @ in @ framework @ consider relational feature and background knowledge @ originate @ @ ontological relationship @ @ concept and @ structure of @ spatial language @ @ advantage of @ proposed global learning model is @ scalability of @ inference and @ flexibility @ automatically describing text @ arbitrary semantic label @ form a structured ontological representation of @ content @ @ machine learning framework is evaluated @ semeval and semeval data @ @ spatial role labeling task @ @ b @ v @ 
2680,Supporting system for quiz in large class - Automatic keyword extraction and browsing interface -,"We focus on developing an e-learning system that supports the grasping of misunderstanding from descriptive answers. We propose real-time keyword extraction and an interface for grasping misunderstanding based on extracted keywords. The system extracts keywords without extra information. Teachers find major misunderstandings by using the proposed interface, which consists of two views - keyword and description. Using these views, teachers browse answers in three steps - finding keywords, reading around keywords, and reading full answers. We use experiments to demonstrate the effectiveness of our system, this proposed keyword extraction extracts expected words. Subjects evaluate the proposed interface for its effectiveness in grasping misunderstandings. Using our proposed, teachers found major misunderstandings quickly and easily.",2015,Journal of Advanced Computational Intelligence and Intelligent Informatics,6,@ focus on developing @ e-learning system @ support @ grasping of misunderstanding @ descriptive answer @ @ propose real-time keyword extraction and @ interface @ grasping misunderstanding based on extracted keywords @ @ system extract keywords without extra information @ teacher find major misunderstanding by @ @ proposed interface @ consists of @ view keyword and description @ @ @ view teacher browse answer in three step finding keywords reading around keywords and reading full answer @ @ use experiment to demonstrate @ effectiveness of @ system @ proposed keyword extraction extract expected word @ subject evaluate @ proposed interface @ @ effectiveness in grasping misunderstanding @ @ @ proposed teacher found major misunderstanding quickly and easily @ 
2681,A plagiarism detection system for Arabic documents,"This paper proposes a new plagiarism detection system devoted to Arabic text documents. This system is based on modeling the relation between documents and their n-gram phrases. Part-of-Speech tagging is applied on the examined documents to support in resolving the morphological ambiguity during text normalization. Text indexing and stop-words removal are performed, employing a new morphological analysis based method. Heuristic pairwise phrase matching algorithm is used to build the documents TF-IDF model, considering substitution of words with their synonyms. The hidden associations of the unique n-gram phrases contained in the documents are investigated using the Latent Semantic Analysis. Then, the pairwise document similarity scores are derived from the Singular Value Decomposition computations. The performance of the proposed system was confirmed through experiments with various data sets, exhibiting promising capabilities in identifying literal and some types of intelligent plagiarism. Finally, the proposed system was compared to Plagiarism-Checker-X, and the proposed system outperformed Plagiarism-Checker-X, especially for intelligent plagiarism. © Springer International Publishing Switzerland 2015.",2015,Advances in Intelligent Systems and Computing,7,@ @ proposes a @ plagiarism detection system devoted to arabic text document @ @ system is based on modeling @ relation @ document and @ n-gram phrase @ part-of-speech tagging is applied on @ examined document to support in resolving @ morphological ambiguity @ text normalization @ text indexing and stop-words removal @ performed employing a @ morphological analysis based method @ heuristic pairwise phrase matching algorithm is used to build @ document tf-idf model considering substitution of word @ @ synonym @ @ hidden association of @ unique n-gram phrase contained in @ document @ investigated @ @ latent semantic analysis @ @ @ pairwise document similarity score @ derived @ @ singular value decomposition computation @ @ performance of @ proposed system wa confirmed @ experiment @ various data set exhibiting promising capability in identifying literal and some type of intelligent plagiarism @ finally @ proposed system wa compared to plagiarism-checker-x and @ proposed system outperformed plagiarism-checker-x especially @ intelligent plagiarism @ @ international publishing switzerland @ 
2683,Improving peer-to-peer communication in e-learning by development of an advanced messaging system,"This chapter presents an advanced messaging system, whose goal is to improve the peer-to-peer communication in e-Learning. The improvement is based on the ability of the developed system to produce information that is highly related to the informational needs of the person, who accesses it. The system is an intelligent one because it integrates a classifcation procedure for retrieval of the messages that have a high potential of being interesting to peers. It uses as input data activity logs obtained by monitoring the communication that takes place within the e-Learning platform. The main data analysis goal is to create a user’s model, for which derived classes are in close relation with specifc set of messages. The outcome is in the form of a tool that allows learners to receive a set of recommended messages that is highly to be interesting for them. The tool analyzes the user’s features, classifes them and according with the class label obtained set of messages. The tool also acts as a message indexing system by storing messages in correlation with labels assigned to learners. A classical classifcation procedure is used for obtaining a labeling. The data used to train the classifer is gathered from the on-line educational environment and contains all the necessary information (i.e., the features) regarding the activities performed by learners on the platform. The high quality of the system is based also on a text-mining module that uses stemming, annotation, and concept detection for a proper assignment of messages to learner’s labels. © Springer International Publishing Switzerland 2015",2015,"Smart Innovation, Systems and Technologies",0,@ chapter @ @ advanced messaging system whose goal is to improve @ peer-to-peer communication in e-learning @ @ improvement is based on @ ability of @ developed system to produce information @ is highly related to @ informational need of @ person @ access @ @ @ system is @ intelligent @ @ @ integrates a classifcation procedure @ retrieval of @ message @ @ a high potential of @ interesting to peer @ @ us a input data activity log obtained by monitoring @ communication @ take place within @ e-learning platform @ @ main data analysis goal is to create a user s model @ @ derived class @ in close relation @ specifc set of message @ @ outcome is in @ form of a tool @ allows learner to receive a set of recommended message @ is highly to @ interesting @ @ @ @ tool analyzes @ user s feature classifes @ and according @ @ class label obtained set of message @ @ tool @ act a a message indexing system by storing message in correlation @ label assigned to learner @ a classical classifcation procedure is used @ obtaining a labeling @ @ data used to train @ classifer is gathered @ @ on-line educational environment and contains @ @ necessary information @ i @ e @ @ feature @ regarding @ activity performed by learner on @ platform @ @ high quality of @ system is based @ on a text-mining module @ us stemming annotation and concept detection @ a proper assignment of message to learner s label @ @ international publishing switzerland 
2684,Evolving fuzzy grammar for crime texts categorization,"Text mining refers to the activity of identifying useful information from natural language text. This is one of the criteria practiced in automated text categorization. Machine learning (ML) based methods are the popular solution for this problem. However, the developed models typically provide low expressivity and lacking in human-understandable representation. In spite of being highly efficient, the ML based methods are established in train-test setting, and when the existing model is found insufficient, the whole processes need to be reinvented which implies train-test-retrain and is typically time consuming. Furthermore, retraining the model is not usually practical and feasible option whenever there is continuous change. This paper introduces the evolving fuzzy grammar (EFG) method for crime texts categorization. In this method, the learning model is built based on a set of selected text fragments which are then transformed into their underlying structure called fuzzy grammars. The fuzzy notion is used because the grammar matching, parsing and derivation involve uncertainty. Fuzzy union operator is also used to combine and transform individual text fragment grammars into more general representations of the learned text fragments. The set of learned fuzzy grammars is influenced by the evolution in the seen pattern; the learned model is slightly changed (incrementally) as adaptation, which does not require the conventional redevelopment. The performance of EFG in crime texts categorization is evaluated against expert-tagged real incidents summaries and compared against C4.5, support vector machines, naïve Bayes, boosting, and k-nearest neighbour methods. Results show that the EFG algorithm produces results that are close in performance with the other ML methods while being highly interpretable, easily integrated into a more comprehensive grammar system and with lower model retraining adaptability time. © 2014 Elsevier B.V. All rights reserved.",2015,Applied Soft Computing,7,text mining refers to @ activity of identifying useful information @ natural language text @ @ is @ of @ criterion practiced in automated text categorization @ machine learning @ ml @ based method @ @ popular solution @ @ problem @ however @ developed model typically provide low expressivity and lacking in human-understandable representation @ in spite of @ highly efficient @ ml based method @ established in train-test setting and @ @ existing model is found insufficient @ whole process need to @ reinvented @ implies train-test-retrain and is typically time consuming @ furthermore retraining @ model is not usually practical and feasible option whenever @ is continuous change @ @ @ introduces @ evolving fuzzy grammar @ efg @ method @ crime text categorization @ in @ method @ learning model is built based on a set of selected text fragment @ @ @ transformed @ @ underlying structure called fuzzy grammar @ @ fuzzy notion is used @ @ grammar matching parsing and derivation involve uncertainty @ fuzzy union operator is @ used to combine and transform individual text fragment grammar @ more general representation of @ learned text fragment @ @ set of learned fuzzy grammar is influenced by @ evolution in @ seen pattern @ @ learned model is slightly changed @ incrementally @ a adaptation @ doe not require @ conventional redevelopment @ @ performance of efg in crime text categorization is evaluated @ expert-tagged real incident summary and compared @ c @ support vector machine naïve bayes boosting and k-nearest neighbour method @ @ @ @ @ efg algorithm produce @ @ @ close in performance @ @ @ ml method @ @ highly interpretable easily integrated @ a more comprehensive grammar system and @ lower model retraining adaptability time @ @ b @ v @ @ right reserved @ 
2685,Sentiment classification based on LDA using SMO classifier,"Opinion Mining is also known as Sentiment Analysis which refers to the use of natural language processing, text analysis and computational linguistics to identify and extract the subjective information. A commonly adopted framework generates structured review summaries with aspects and opinions. Recently topic models have been used to identify the meaningful review aspects, Topic modeling is a form of text mining, a way of identifying patterns in a corpus. . In the proposed system, Latent Dirichlet Allocation (LDA) model is introduced to discover both the aspects and aspect specific opinion words. Support Vector Machine (SVM) and Sequential Minimal Optimization (SMO) classifier is used to classify the topics preferable to each and every document. © Research India Publications.",2015,International Journal of Applied Engineering Research,5,opinion mining is @ known a sentiment analysis @ refers to @ use of natural language processing text analysis and computational linguistics to identify and extract @ subjective information @ a commonly adopted framework generates structured review summary @ aspect and opinion @ recently topic model @ @ used to identify @ meaningful review aspect topic modeling is a form of text mining a way of identifying pattern in a corpus @ @ in @ proposed system latent dirichlet allocation @ lda @ model is introduced to discover @ @ aspect and aspect specific opinion word @ support vector machine @ svm @ and sequential minimal optimization @ smo @ classifier is used to classify @ topic preferable to @ and every document @ research india publication @ 
2688,"Protein entity name recognition using orthographic, morphological and proteinhood features","Protein name identification in text is an important and challenging fundamental precursor in biomedical information processing. For example, accurate identification of protein names affects the finding of proteinprotein interactions from biomedical literature. In this paper, we present an efficient protein name identification technique based on a rich set of features: orthographic, morphological as well as Proteinhood features which are introduced newly in this study. The method was evaluated on GENIA corpus with the use of different machine learning algorithms. The highest values for precision 92.1%, recall 86.5% and F-measure 89.2% were achieved on Random Forest, while reducing the training and testing time significantly. We studied and showed the impact of the Proteinhood feature in protein identification as well as the effect of tuning the parameters of the machine learning algorithm.",2015,Journal of Advanced Computational Intelligence and Intelligent Informatics,1,protein name identification in text is @ important and challenging fundamental precursor in biomedical information processing @ @ example accurate identification of protein name affect @ finding of proteinprotein interaction @ biomedical literature @ in @ @ @ @ @ efficient protein name identification technique based on a rich set of feature @ orthographic morphological a well a proteinhood feature @ @ introduced newly in @ study @ @ method wa evaluated on genia corpus @ @ use of different machine learning algorithm @ @ highest value @ precision @ recall @ and f-measure @ @ achieved on random forest @ reducing @ training and testing time significantly @ @ studied and showed @ impact of @ proteinhood feature in protein identification a well a @ effect of tuning @ parameter of @ machine learning algorithm @ 
2689,Sentiment analysis in transcribed utterances,"A single phone call can make or break a valuable customer-organization relationship. Maintaining good quality of service can lead to customer loyalty, which affects profitability. Traditionally, customer feedback is mainly collected by interviews, questionnaires, and surveys; the major drawback of these data collection methods is in their limited scale. The growing amount of research conducted in the field of sentiment analysis, combined with advances in text processing and Artificial Intelligence, has led us be the first to present an intelligent system for mining sentiment from transcribed utterances—wherein the noisiness property and short length poses extra challenges to sentiment analysis. Our aim is to detect and process affective factors from multiple layers of information, and study the effectiveness and robustness of each factor type independently, by proposing a tailored machine learning paradigm. Three types of factors are related to the textual content while two overlook it. Experiments are carried out on two datasets of transcribed phone conversations, obtained from real-world telecommunication companies. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,a single phone call @ make @ break a valuable customer-organization relationship @ maintaining good quality of service @ lead to customer loyalty @ affect profitability @ traditionally customer feedback is mainly collected by interview questionnaire and survey @ @ major drawback of @ data collection method is in @ limited scale @ @ growing amount of research conducted in @ field of sentiment analysis combined @ advance in text processing and artificial intelligence ha led u @ @ first to @ @ intelligent system @ mining sentiment @ transcribed utterance wherein @ noisiness property and short length pose extra challenge to sentiment analysis @ @ aim is to detect and process affective factor @ multiple layer of information and study @ effectiveness and robustness of @ factor type independently by proposing a tailored machine learning paradigm @ three type of factor @ related to @ textual content @ @ overlook @ @ experiment @ carried @ on @ datasets of transcribed phone conversation obtained @ real-world telecommunication company @ @ international publishing switzerland @ 
2690,Messaging activity reconstruction with sentiment polarity identification,"Sentiment Analysis aims to extract information related to the emotional state of the person that produced a text document and also describe the sentiment polarity of the short or long message. This kind of information might be useful to a forensic analyst because it provides indications about the psychological state of the person under investigation at a given time. In this paper we use machine-learning algorithms to classify short texts (SMS), which could be found in the internal memory of a smartphone and extract the mood of the person that sent them. The basic goal of our method is to achieve low False Positive Rates. Moreover, we present two visualization schemes with the intention to provide the ability to digital forensic analysts to see graphical representations of the messaging activity of their suspects and therefore focus on specific areas of interest reducing their workload. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,sentiment analysis aim to extract information related to @ emotional state of @ person @ produced a text document and @ describe @ sentiment polarity of @ short @ long message @ @ kind of information might @ useful to a forensic analyst @ @ provides indication @ @ psychological state of @ person @ investigation at a given time @ in @ @ @ use machine-learning algorithm to classify short text @ sm @ @ could @ found in @ internal memory of a smartphone and extract @ mood of @ person @ sent @ @ @ basic goal of @ method is to achieve low false positive rate @ moreover @ @ @ visualization scheme @ @ intention to provide @ ability to digital forensic analyst to see graphical representation of @ messaging activity of @ suspect and therefore focus on specific area of interest reducing @ workload @ @ international publishing switzerland @ 
2691,Probabilistic topic modeling in multilingual settings: An overview of its methodology and applications,"Probabilistic topic models are unsupervised generative models which model document content as a two-step generation process, that is, documents are observed as mixtures of latent concepts or topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable texts. We define multilingual probabilistic topic modeling (MuPTM) and present the first full overview of the current research, methodology, advantages and limitations in MuPTM. As a representative example, we choose a natural extension of the omnipresent LDA model to multilingual settings called bilingual LDA (BiLDA). We provide a thorough overview of this representative multilingual model from its high-level modeling assumptions down to its mathematical foundations. We demonstrate how to use the data representation by means of output sets of (i) per-topic word distributions and (ii) per-document topic distributions coming from a multilingual probabilistic topic model in various real-life cross-lingual tasks involving different languages, without any external language pair dependent translation resource: (1) cross-lingual event-centered news clustering, (2) cross-lingual document classification, (3) cross-lingual semantic similarity, and (4) cross-lingual information retrieval. We also briefly review several other applications present in the relevant literature, and introduce and illustrate two related modeling concepts: topic smoothing and topic pruning. In summary, this article encompasses the current research in multilingual probabilistic topic modeling. By presenting a series of potential applications, we reveal the importance of the language-independent and language pair independent data representations by means of MuPTM. We provide clear directions for future research in the field by providing a systematic overview of how to link and transfer aspect knowledge across corpora written in different languages via the shared space of latent cross-lingual topics, that is, how to effectively employ learned per-topic word distributions and per-document topic distributions of any multilingual probabilistic topic model in various cross-lingual applications. © 2014 Elsevier Ltd. All rights reserved.",2015,Information Processing and Management,54,probabilistic topic model @ unsupervised generative model @ model document content a a two-step generation process @ is document @ observed a mixture of latent concept @ topic @ topic @ probability distribution @ vocabulary word @ recently a significant research effort ha @ invested @ transferring @ probabilistic topic modeling concept @ monolingual to multilingual setting @ novel topic model @ @ designed to work @ parallel and comparable text @ @ define multilingual probabilistic topic modeling @ muptm @ and @ @ first full overview of @ current research methodology advantage and limitation in muptm @ a a representative example @ choose a natural extension of @ omnipresent lda model to multilingual setting called bilingual lda @ bilda @ @ @ provide a thorough overview of @ representative multilingual model @ @ high-level modeling assumption down to @ mathematical foundation @ @ demonstrate @ to use @ data representation by mean of output set of @ i @ per-topic word distribution and @ ii @ per-document topic distribution coming @ a multilingual probabilistic topic model in various real-life cross-lingual task involving different language without @ external language pair dependent translation resource @ @ @ cross-lingual event-centered news clustering @ @ cross-lingual document classification @ @ cross-lingual semantic similarity and @ @ cross-lingual information retrieval @ @ @ briefly review several @ application @ in @ relevant literature and introduce and illustrate @ related modeling concept @ topic smoothing and topic pruning @ in summary @ article encompasses @ current research in multilingual probabilistic topic modeling @ by presenting a series of potential application @ reveal @ importance of @ language-independent and language pair independent data representation by mean of muptm @ @ provide clear direction @ future research in @ field by providing a systematic overview of @ to link and transfer aspect knowledge across corpus written in different language via @ shared space of latent cross-lingual topic @ is @ to effectively employ learned per-topic word distribution and per-document topic distribution of @ multilingual probabilistic topic model in various cross-lingual application @ @ ltd @ @ right reserved @ 
2693,A hybrid method to extract triggers in biomedical events,"Biomedical event extraction from literature is a new research topic in the field of biomedical text mining. Trigger detection is a crucial and challenging subtask in this complex task. A method based on the conditional random field (CRF) model combined with support vector machine (SVM) is developed in this study to detect biomedical event triggers. Rich features, including lexical and structural, are used in model training. Experiment results show the promising perfor-mance of the method, which achieved an F-score of 72.07 on the BioNLP2013-ST corpus. Different from the other similar systems, the valid proteins that participate in the event are identified which is helpful to detect the triggers. We believe this work is a positive contribution to the biomedical text mining community by easing biomedical event extraction.",2015,Journal of Digital Information Management,5,biomedical event extraction @ literature is a @ research topic in @ field of biomedical text mining @ trigger detection is a crucial and challenging subtask in @ complex task @ a method based on @ conditional random field @ crf @ model combined @ support vector machine @ svm @ is developed in @ study to detect biomedical event trigger @ rich feature including lexical and structural @ used in model training @ experiment @ @ @ promising perfor-mance of @ method @ achieved @ f-score of @ on @ bionlp st corpus @ different @ @ @ similar system @ valid protein @ participate in @ event @ identified @ is helpful to detect @ trigger @ @ believe @ work is a positive contribution to @ biomedical text mining community by easing biomedical event extraction @ 
2695,Cost-sensitive structured perceptron incorporating category hierarchy for named entity recognition,"Named Entity Recognition (NER) is a fundamental natural language processing task for the identifi cation and classifi cation of expressions into predefi ned categories, such as person and organization. Existing NER systems usually target about 10 categories and do not incorporate analysis of category relations. However, categories often belong naturally to some predefi ned hierarchy. In such cases, the distance between categories in the hierarchy becomes a rich source of information that can be exploited. This is intuitively useful particularly when the categories are numerous. On that account, this paper proposes an NER approach that can leverage category hierarchy information by introducing, in the structured perceptron framework, a cost function more strongly penalizing category predictions that are more distant from the correct category in the hierarchy. Experimental results on the GENIA biomedical text corpus indicate the effectiveness of the proposed approach as compared with the case where no cost function is utilized. In addition, the proposed approach demonstrates the superior performance over a representative work using multi-class support vector machines on the same corpus. A possible direction to further improve the proposed approach is to investigate more elaborate cost functions than a simple additive cost adopted in this work.",2015,Journal of Information and Communication Technology,0,named entity recognition @ ner @ is a fundamental natural language processing task @ @ identifi cation and classifi cation of expression @ predefi ned category @ a person and organization @ existing ner system usually target @ category and @ not incorporate analysis of category relation @ however category often belong naturally to some predefi ned hierarchy @ in @ case @ distance @ category in @ hierarchy becomes a rich source of information @ @ @ exploited @ @ is intuitively useful particularly @ @ category @ numerous @ on @ account @ @ proposes @ ner approach @ @ leverage category hierarchy information by introducing in @ structured perceptron framework a cost function more strongly penalizing category prediction @ @ more distant @ @ correct category in @ hierarchy @ experimental @ on @ genia biomedical text corpus indicate @ effectiveness of @ proposed approach a compared @ @ case @ no cost function is utilized @ in addition @ proposed approach demonstrates @ superior performance @ a representative work @ multi-class support vector machine on @ @ corpus @ a possible direction to @ improve @ proposed approach is to investigate more elaborate cost function @ a simple additive cost adopted in @ work @ 
2696,Introduction,"The future of information search is not browsing through tons of web pages or documents. In times of big data and the information overload of the internet, experts in the field agree that both everyday and enterprise search will gradually shift from only retrieving large numbers of texts that potentially contain relevant information to directly mining relevant information in these texts (Etzioni 2011; Kelly and Hamm 2013; Ananiadou et al. 2013). In this chapter, we first motivate the benefit of such large-scale text mining for today’s web search and big data analytics applications (Sect. 1.1). Then, we reveal the task specificity and the process complexity of analyzing natural language text as the main problems that prevent applications from performing text mining ad-hoc, i.e., immediately in response to a user query (Sect. 1.2). Section 1.3 points out how we propose to tackle these problems by improving the design, efficiency, and domain robustness of the pipelines of algorithms used for text analysis with artificial intelligence techniques. This leads to the contributions of the book at hand (Sect. 1.4). © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ future of information search is not browsing @ ton of web page @ document @ in time of big data and @ information overload of @ internet expert in @ field agree @ @ everyday and enterprise search @ gradually shift @ only retrieving @ number of text @ potentially contain relevant information to directly mining relevant information in @ text @ etzioni @ kelly and hamm @ ananiadou et al @ @ @ in @ chapter @ first motivate @ benefit of @ large-scale text mining @ today s web search and big data analytics application @ sect @ @ @ @ @ @ reveal @ task specificity and @ process complexity of analyzing natural language text a @ main problem @ prevent application @ performing text mining ad-hoc i @ e @ immediately in response to a user query @ sect @ @ @ @ section @ point @ @ @ propose to tackle @ problem by improving @ design efficiency and domain robustness of @ pipeline of algorithm used @ text analysis @ artificial intelligence technique @ @ lead to @ contribution of @ book at hand @ sect @ @ @ @ @ international publishing switzerland @ 
2702,"Deciphering review comments: Identifying suggestions, appreciations and complaints","The problem of classifying sentences into various categories, arises frequently in text mining applications. One of the most important categorization of sentences observed in product reviews, movie reviews, blogs, customer feedbacks is - Suggestions, Appreciations and Complaints. We observed that the document classification techniques do not perform well for these three non-topical sentence classes. We propose to solve this problem using a supervised approach based on Dependencybased Word Subsequence Kernel and its variations. We compare the performance of our approach with the state-of-the-art short text classification techniques on 2 different datasets - Performance Appraisal comments and Product Reviews. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ problem of classifying sentence @ various category arises frequently in text mining application @ @ of @ @ important categorization of sentence observed in product review movie review blog customer feedback is suggestion appreciation and complaint @ @ observed @ @ document classification technique @ not perform well @ @ three non-topical sentence class @ @ propose to solve @ problem @ a supervised approach based on dependencybased word subsequence kernel and @ variation @ @ compare @ performance of @ approach @ @ state-of-the-art short text classification technique on different datasets performance appraisal comment and product review @ @ international publishing switzerland @ 
2706,SU@PAN'2015: Experiments in author verification,"We describe the submission of the Sofia University team for the Author Identification Task, part of the PAN 2015 Challenge. Given a small set of documents by a single person and a ""questioned"" document, possibly of a different genre and/or topic, the task is to determine whether the questioned document was written by the same person who wrote the known document set. This is a hard but realistic formulation of the task, also known as author verification. We experimented with an SVM classifier using variety of features extracted from publicly available resources. Our solution was among the fastest, and running time was an official evaluation metric; however, our results were not so strong on AUC and C1.",2015,CEUR Workshop Proceedings,0,@ describe @ submission of @ sofia university team @ @ author identification task part of @ pan challenge @ given a small set of document by a single person and a @ questioned @ document possibly of a different genre and @ topic @ task is to determine whether @ questioned document wa written by @ @ person @ wrote @ known document set @ @ is a hard @ realistic formulation of @ task @ known a author verification @ @ experimented @ @ svm classifier @ variety of feature extracted @ publicly available resource @ @ solution wa among @ fastest and running time wa @ official evaluation metric @ however @ @ @ not @ strong on auc and c @ 
2717,Named entity recognition from financial press releases,"This paper explores a previous model’s use of named entity recognition to predict the changes in stock prices from financial news. Detecting company mentions in the articles is crucial for this task, and we modified these methods to gain additional mentions. We first expanded upon the rules of the named entity recognition from the original model. We also incorporated coreference resolution and modified an existing toolkit to be compatible with our specific domain. After these two adjustments, the number of instances captured increased significantly. Although this did not necessarily improve the overall prediction performance, the results give us an opportunity to explore reasons why the scores stayed around the same, and a full analysis will allow us to achieve our goals. © Springer International Publishing Switzerland 2015.",2015,Communications in Computer and Information Science,0,@ @ explores a previous model s use of named entity recognition to predict @ change in stock price @ financial news @ detecting company mention in @ article is crucial @ @ task and @ modified @ method to gain additional mention @ @ first expanded upon @ rule of @ named entity recognition @ @ original model @ @ @ incorporated coreference resolution and modified @ existing toolkit to @ compatible @ @ specific domain @ @ @ @ adjustment @ number of instance captured increased significantly @ although @ @ not necessarily improve @ overall prediction performance @ @ give u @ opportunity to explore reason @ @ score stayed around @ @ and a full analysis @ allow u to achieve @ goal @ @ international publishing switzerland @ 
2718,Exploring the relatedness of gene sets,"A key activity for life scientists is the exploration of the relatedness of a set of genes in order to differentiate genes performing coherently related functions from random grouped genes. This paper considers exploring the relatedness within two popular bio-organizations, namely gene families and pathways. This exploration is carried out by integrating different resources (ontologies, texts, expert classifications) and aims to suggest patterns that facilitate the biologists in obtaining a more comprehensive vision of differences in gene behaviour. Our approach is based on the annotation of a specialized corpus of texts (the gene summaries) that condense the description of functions/processes in which genes are involved. By annotating these summaries with different ontologies a set of descriptor terms is derived and compared in order to obtain a measure of relatedness within the bio-organizations we considered. Finally, the most important annotations within each family are extracted using a text categorization method. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,a key activity @ life scientist is @ exploration of @ relatedness of a set of gene in order to differentiate gene performing coherently related function @ random grouped gene @ @ @ considers exploring @ relatedness within @ popular bio-organizations namely gene family and pathway @ @ exploration is carried @ by integrating different resource @ ontology text expert classification @ and aim to suggest pattern @ facilitate @ biologist in obtaining a more comprehensive vision of difference in gene behaviour @ @ approach is based on @ annotation of a specialized corpus of text @ @ gene summary @ @ condense @ description of function process in @ gene @ involved @ by annotating @ summary @ different ontology a set of descriptor term is derived and compared in order to obtain a measure of relatedness within @ bio-organizations @ considered @ finally @ @ important annotation within @ family @ extracted @ a text categorization method @ @ international publishing switzerland @ 
2720,Domain specific document retrieval framework for real-time social health data,"With the advent of the web search and microblogging, the percentage of Online Health Information Seekers (OHIS) using these online services to share and seek health real-time information has increased exponentially. OHIS use web search engines or microblogging search services to seek out latest, relevant as well as reliable health information. When OHIS turn to microblogging search services to search real-time content, trends and breaking news, etc. the search results are not promising. Two major challenges exist in the current microblogging search engines are keyword based techniques and results do not contain real-time information. To address these challenges, we developed an approach to search near real-time and reliable content from Twitter, based on triple-pattern mining, near real-time retrieval, and ranking considering popularity and relevancy of the results. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ advent of @ web search and microblogging @ percentage of online health information seeker @ ohis @ @ @ online service to share and seek health real-time information ha increased exponentially @ ohis use web search engine @ microblogging search service to seek @ latest relevant a well a reliable health information @ @ ohis turn to microblogging search service to search real-time content trend and breaking news etc @ @ search @ @ not promising @ @ major challenge exist in @ current microblogging search engine @ keyword based technique and @ @ not contain real-time information @ to address @ challenge @ developed @ approach to search near real-time and reliable content @ twitter based on triple-pattern mining near real-time retrieval and ranking considering popularity and relevancy of @ @ @ @ international publishing switzerland @ 
2723,Building Normalized SentiMI to enhance semi-supervised sentiment analysis,"Sentiment analysis and polarity detection is a type of text classification where natural language opinion is analyzed in order to classify it into either positive or negative categories. Classification of text into sentiment labels is a very difficult task as opinions expressed in natural language may contain abbreviations, slangs, sarcasm, irony and/or idioms. The proposed research focuses on the use of SentiWordNet3.0 as a labeled corpus for training purposes. We present a complete framework based on a dictionary named Normalized SentiMI (nSentiMI) which is created by calculating point-wise mutual information for each term/part-of-speech pair extracted from SentiWordNet. The proposed framework is applied on a dataset of 50,000 movie reviews to identify the value of a weight factor α and then evaluated on an unseen test dataset of 2000 movie reviews. Comparison with state of art techniques also confirms the superiority of proposed approach. © 2015 - IOS Press and the authors. All rights reserved.",2015,Journal of Intelligent and Fuzzy Systems,8,sentiment analysis and polarity detection is a type of text classification @ natural language opinion is analyzed in order to classify @ @ either positive @ negative category @ classification of text @ sentiment label is a @ difficult task a opinion expressed in natural language may contain abbreviation slang sarcasm irony and @ idiom @ @ proposed research focus on @ use of sentiwordnet @ a a labeled corpus @ training purpose @ @ @ a complete framework based on a dictionary named normalized sentimi @ nsentimi @ @ is created by calculating point-wise mutual information @ @ term part-of-speech pair extracted @ sentiwordnet @ @ proposed framework is applied on a dataset of movie review to identify @ value of a weight factor α and @ evaluated on @ unseen test dataset of movie review @ comparison @ state of art technique @ confirms @ superiority of proposed approach @ io @ and @ author @ @ right reserved @ 
2724,Legal question answering using ranking SVM and syntactic/semantic similarity,"We describe a legal question answering system which combines legal information retrieval and textual entailment. We have evaluated our system using the data from the first competition on legal information extraction/ entailment (COLIEE) 2014. The competition focuses on two aspects of legal information processing related to answering yes/no questions from Japanese legal bar exams. The shared task consists of two phases: legal ad hoc information retrieval and textual entailment. The first phase requires the identification of Japan civil law articles relevant to a legal bar exam query. We have implemented two unsupervised baseline models (tf-idf and Latent Dirichlet Allocation (LDA)-based Information Retrieval (IR)), and a supervised model, Ranking SVM, for the task. The features of the model are a set of words, and scores of an article based on the corresponding baseline models. The results show that the Ranking SVM model nearly doubles the Mean Average Precision compared with both baseline models. The second phase is to answer “Yes” or “No” to previously unseen queries, by comparing the meanings of queries with relevant articles. The features used for phase two are syntactic/semantic similarities and identification of negation/antonym relations. The results show that our method, combined with rule-based model and the unsupervised model, outperforms the SVM-based supervised model. © Springer-Verlag Berlin Heidelberg 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11,@ describe a legal question answering system @ combine legal information retrieval and textual entailment @ @ @ evaluated @ system @ @ data @ @ first competition on legal information extraction entailment @ coliee @ @ @ competition focus on @ aspect of legal information processing related to answering yes no question @ japanese legal bar exam @ @ shared task consists of @ phase @ legal ad hoc information retrieval and textual entailment @ @ first phase requires @ identification of japan civil law article relevant to a legal bar exam query @ @ @ implemented @ unsupervised baseline model @ tf-idf and latent dirichlet allocation @ lda @ based information retrieval @ ir @ @ and a supervised model ranking svm @ @ task @ @ feature of @ model @ a set of word and score of @ article based on @ corresponding baseline model @ @ @ @ @ @ ranking svm model nearly double @ mean average precision compared @ @ baseline model @ @ second phase is to answer yes @ no to @ unseen query by comparing @ meaning of query @ relevant article @ @ feature used @ phase @ @ syntactic semantic similarity and identification of negation antonym relation @ @ @ @ @ @ method combined @ rule-based model and @ unsupervised model outperforms @ svm-based supervised model @ springer-verlag @ @ @ 
2725,Detecting sentiment embedded in Arabic social media - A lexicon-based approach,"Sentiment analysis aims at extracting sentiment embedded mainly in text reviews. The prevalence of semantic web technologies has encouraged users of the web to become authors as well as readers. People write on a wide range of topics. These writings embed valuable information for organizations and industries. This paper introduces a novel framework for sentiment detection in Arabic tweets. The heart of this framework is a sentiment lexicon. This lexicon was built by translating the SentiStrength English sentiment lexicon into Arabic and afterwards the lexicon was expanded using Arabic thesauri. To assess the viability of the suggested framework, the authors have collected and manually annotated a set of 4400 Arabic tweets. These tweets were classified according to their sentiment into positive or negative tweets using the proposed framework. The results reveal that lexicons are helpful for sentiment detection. The overall results are encouraging and open venues for future research. © 2015 - IOS Press and the authors. All rights reserved.",2015,Journal of Intelligent and Fuzzy Systems,31,sentiment analysis aim at extracting sentiment embedded mainly in text review @ @ prevalence of semantic web technology ha encouraged user of @ web to become author a well a reader @ people write on a wide range of topic @ @ writing embed valuable information @ organization and industry @ @ @ introduces a novel framework @ sentiment detection in arabic tweet @ @ heart of @ framework is a sentiment lexicon @ @ lexicon wa built by translating @ sentistrength english sentiment lexicon @ arabic and afterwards @ lexicon wa expanded @ arabic thesaurus @ to ass @ viability of @ suggested framework @ author @ collected and manually annotated a set of arabic tweet @ @ tweet @ classified according to @ sentiment @ positive @ negative tweet @ @ proposed framework @ @ @ reveal @ lexicon @ helpful @ sentiment detection @ @ overall @ @ encouraging and open venue @ future research @ io @ and @ author @ @ right reserved @ 
2730,Towards visualization recommendation-a semi-automated domain-specific learning approach,"Information visualization is important in science as it helps scientists in exploring, analysing, and presenting both the obvious and less obvious features of their datasets. However, scientists are not typically visualization experts. It is therefore difficult and time-consuming for them to choose the optimal visualization to convey the desired message. To provide a solution for this problem of visualization selection, we propose a semi-automated, context aware visualization recommendation model. In the model, information will be extracted from data and metadata, the latter providing relevant context. This information will be annotated with suitable domain specific operations (like rank abundance), which will be mapped to the relevant visualizations. We also propose an interactive learning workflow for visualization recommendation that will enrich the model from the knowledge gathered from the interaction with the user. We will use biodiversity research as the application domain to guide the concrete instantiation of our approach and its evaluation. Copyright is held by the author/owner(s).",2015,CEUR Workshop Proceedings,1,information visualization is important in science a @ help scientist in exploring analysing and presenting @ @ obvious and le obvious feature of @ datasets @ however scientist @ not typically visualization expert @ @ is therefore difficult and time-consuming @ @ to choose @ optimal visualization to convey @ desired message @ to provide a solution @ @ problem of visualization selection @ propose a semi-automated context aware visualization recommendation model @ in @ model information @ @ extracted @ data and metadata @ latter providing relevant context @ @ information @ @ annotated @ suitable domain specific operation @ like rank abundance @ @ @ @ mapped to @ relevant visualization @ @ @ propose @ interactive learning workflow @ visualization recommendation @ @ enrich @ model @ @ knowledge gathered @ @ interaction @ @ user @ @ @ use biodiversity research a @ application domain to guide @ concrete instantiation of @ approach and @ evaluation @ @ is held by @ author owner @ s @ @ 
2734,Discovering and querying hybrid linked data,"In this paper, we present a unified framework for discovering and que-rying hybrid linked data. We describe our approach to developing a natural lan-guage query interface for a hybrid knowledge base Wikitology, and present that as a case study for accessing hybrid information sources with structured and un-structured data through natural language queries. We evaluate our system on a publicly available dataset and demonstrate improvements over a baseline system. We describe limitations of our approach and also discuss cases where our system can complement other structured data querying systems by retrieving additional answers not available in structured sources. Copyright © 2015 for the individual papers by the papers' authors.",2015,CEUR Workshop Proceedings,0,in @ @ @ @ a unified framework @ discovering and que-rying hybrid linked data @ @ describe @ approach to developing a natural lan-guage query interface @ a hybrid knowledge base wikitology and @ @ a a case study @ accessing hybrid information source @ structured and un-structured data @ natural language query @ @ evaluate @ system on a publicly available dataset and demonstrate improvement @ a baseline system @ @ describe limitation of @ approach and @ discus case @ @ system @ complement @ structured data querying system by retrieving additional answer not available in structured source @ @ @ @ individual @ by @ @ @ author @ 
2735,From terminology extraction to terminology validation: An approach adapted to log files,"Log files generated by computational systems contain relevant and essential information. In some application areas like the design of integrated circuits, log files generated by design tools contain information which can be used in management information systems to evaluate the final products. However, the complexity of such textual data raises some challenges concerning the extraction of information from log files. Log files are usually multi-source, multi-format, and have a heterogeneous and evolving structure. Moreover, they usually do not respect natural language grammar and structures even though they are written in English. Classical methods of information extraction such as terminology extraction methods are particularly irrelevant to this context. In this paper, we introduce our approach Exterlog to extract terminology from log files. We detail how it deals with the specific features of such textual data. The performance is emphasized by favoring the most relevant terms of the domain based on a scoring function which uses a Web and context based measure. The experiments show that Exterlog is a well-adapted approach for terminology extraction from log files. © J.UCS.",2015,Journal of Universal Computer Science,4,log file generated by computational system contain relevant and essential information @ in some application area like @ design of integrated circuit log file generated by design tool contain information @ @ @ used in management information system to evaluate @ final product @ however @ complexity of @ textual data raise some challenge concerning @ extraction of information @ log file @ log file @ usually multi-source multi-format and @ a heterogeneous and evolving structure @ moreover @ usually @ not respect natural language grammar and structure even though @ @ written in english @ classical method of information extraction @ a terminology extraction method @ particularly irrelevant to @ context @ in @ @ @ introduce @ approach exterlog to extract terminology @ log file @ @ detail @ @ deal @ @ specific feature of @ textual data @ @ performance is emphasized by favoring @ @ relevant term of @ domain based on a scoring function @ us a web and context based measure @ @ experiment @ @ exterlog is a well-adapted approach @ terminology extraction @ log file @ j @ ucs @ 
2736,Identifying the stances of topic persons using a model-based expectation-maximization method,"Identifying persons with the same stance in topic documents that contain competing viewpoints can help readers construct the background of a topic and facilitate topic reading. In this paper, we propose an unsupervised method for identifying topic persons with the same stance. Specifically, we employ a model-based Expectation-Maximization (EM) method to cluster individuals into positively correlated groups. In addition, we utilize an off-topic block elimination technique and a weighted correlation coefficient to remove off-topic text blocks and alleviate the text sparseness problem. We also present an effective initialization algorithm that generates appropriate EM initializations. Our experiment results demonstrate that the proposed method clusters topic persons with the same stance correctly and outperforms many well-known clustering methods. Moreover, the initialization algorithm yields accurate and stable stance identification results.",2015,Journal of Information Science and Engineering,2,identifying person @ @ @ stance in topic document @ contain competing viewpoint @ help reader construct @ background of a topic and facilitate topic reading @ in @ @ @ propose @ unsupervised method @ identifying topic person @ @ @ stance @ specifically @ employ a model-based expectation-maximization @ em @ method to cluster individual @ positively correlated group @ in addition @ utilize @ off-topic block elimination technique and a weighted correlation coefficient to remove off-topic text block and alleviate @ text sparseness problem @ @ @ @ @ effective initialization algorithm @ generates appropriate em initialization @ @ experiment @ demonstrate @ @ proposed method cluster topic person @ @ @ stance correctly and outperforms many well-known clustering method @ moreover @ initialization algorithm yield accurate and stable stance identification @ @ 
2737,Dynamic non-parametric joint sentiment topic mixture model,"Abstract The reviews in social media are produced continuously by a large and uncontrolled number of users. To capture the mixture of sentiment and topics simultaneously in reviews is still a challenging task. In this paper, we present a novel probabilistic model framework based on the non-parametric hierarchical Dirichlet process (HDP) topic model, called non-parametric joint sentiment topic mixture model (NJST), which adds a sentiment level to the HDP topic model and detects sentiment and topics simultaneously from reviews. Then considered the dynamic nature of social media data, we propose dynamic NJST (dNJST) which adds time decay dependencies of historical epochs to the current epochs. Compared with the existing sentiment topic mixture models which are based on latent Dirichlet allocation (LDA), the biggest difference of NJST and dNJST is that they can determine topic number automatically. We implement NJST and dNJST with online variational inference algorithms, and incorporate the sentiment priors of words into NJST and dNJST with HowNet lexicon. The experiment results in some Chinese social media dataset show that dNJST can effectively detect and track dynamic sentiment and topics. © 2015 Elsevier B.V. All rights reserved.",2015,Knowledge-Based Systems,31,abstract @ review in social medium @ produced continuously by a @ and uncontrolled number of user @ to capture @ mixture of sentiment and topic simultaneously in review is still a challenging task @ in @ @ @ @ a novel probabilistic model framework based on @ non-parametric hierarchical dirichlet process @ hdp @ topic model called non-parametric joint sentiment topic mixture model @ njst @ @ add a sentiment level to @ hdp topic model and detects sentiment and topic simultaneously @ review @ @ considered @ dynamic nature of social medium data @ propose dynamic njst @ dnjst @ @ add time decay dependency of historical epoch to @ current epoch @ compared @ @ existing sentiment topic mixture model @ @ based on latent dirichlet allocation @ lda @ @ biggest difference of njst and dnjst is @ @ @ determine topic number automatically @ @ implement njst and dnjst @ online variational inference algorithm and incorporate @ sentiment prior of word @ njst and dnjst @ hownet lexicon @ @ experiment @ in some chinese social medium dataset @ @ dnjst @ effectively detect and track dynamic sentiment and topic @ @ b @ v @ @ right reserved @ 
2739,A similarity assessment technique for effective grouping of documents,"Document clustering refers to the task of grouping similar documents and segregating dissimilar documents. It is very useful to find meaningful categories from a large corpus. In practice, the task to categorize a corpus is not so easy, since it generally contains huge documents and the document vectors are high dimensional. This paper introduces a hybrid document clustering technique by combining a new hierarchical and the traditional k-means clustering techniques. A distance function is proposed to find the distance between the hierarchical clusters. Initially the algorithm constructs some clusters by the hierarchical clustering technique using the new distance function. Then k-means algorithm is performed by using the centroids of the hierarchical clusters to group the documents that are not included in the hierarchical clusters. The major advantage of the proposed distance function is that it is able to find the nature of the corpora by varying a similarity threshold. Thus the proposed clustering technique does not require the number of clusters prior to executing the algorithm. In this way the initial random selection of k centroids for k-means algorithm is not needed for the proposed method. The experimental evaluation using Reuter, Ohsumed and various TREC data sets shows that the proposed method performs significantly better than several other document clustering techniques. F-measure and normalized mutual information are used to show that the proposed method is effectively grouping the text data sets. © 2015 Elsevier Inc. All rights reserved.",2015,Information Sciences,22,document clustering refers to @ task of grouping similar document and segregating dissimilar document @ @ is @ useful to find meaningful category @ a @ corpus @ in practice @ task to categorize a corpus is not @ easy since @ generally contains huge document and @ document vector @ high dimensional @ @ @ introduces a hybrid document clustering technique by combining a @ hierarchical and @ traditional k-means clustering technique @ a distance function is proposed to find @ distance @ @ hierarchical cluster @ initially @ algorithm construct some cluster by @ hierarchical clustering technique @ @ @ distance function @ @ k-means algorithm is performed by @ @ centroid of @ hierarchical cluster to group @ document @ @ not included in @ hierarchical cluster @ @ major advantage of @ proposed distance function is @ @ is able to find @ nature of @ corpus by varying a similarity threshold @ thus @ proposed clustering technique doe not require @ number of cluster prior to executing @ algorithm @ in @ way @ initial random selection of k centroid @ k-means algorithm is not needed @ @ proposed method @ @ experimental evaluation @ reuter ohsumed and various trec data set @ @ @ proposed method performs significantly better @ several @ document clustering technique @ f-measure and normalized mutual information @ used to @ @ @ proposed method is effectively grouping @ text data set @ @ inc @ @ right reserved @ 
2743,Hybrid Multilingual Key Terms Extraction System for Hindi and Punjabi Text,"Key terms represent overall theme of any text document. By looking these key terms we can easily predict the nature of text documents. A number of statistical and linguistic based approaches are present for finding the key terms. This paper describes a hybrid approach for finding key terms from multilingual Hindi and Punjabi documents. The technique used is the hybrid of both the statistical and linguistic based features for finding the key terms from Hindi and Punjabi text. Regarding statistical features two features are used: i) Entropy based metric for finding relevant terms and ii) TermFreq-InverseLineFreq metric for finding key terms. Regarding linguistic features 03 features are used i) treating nouns and verbs as key terms ii) treat terms belonging to lines in quotation marks, bold/italics/underlined fonts and with more font size as key terms iii) treating title words with maximum coverage as key terms. It is first time in history that this hybrid system has been proposed for Hindi and Punjabi languages. For Hindi and Punjabi very less number of language resources are existing. This multi lingual key terms extraction system for Hindi and Punjabi will be very much helpful for developing other NLP resources in Hindi and Punjabi like: automatic text summarization, document classification, document clustering, question answering and topic tracking etc. The overall Precision and Recall for this hybrid multilingual key terms extraction system are 90.78 % and 87.43 % respectively which are reasonably good. © Springer International Publishing Switzerland 2015.",2015,Advances in Intelligent Systems and Computing,2,key term represent overall theme of @ text document @ by looking @ key term @ @ easily predict @ nature of text document @ a number of statistical and linguistic based approach @ @ @ finding @ key term @ @ @ describes a hybrid approach @ finding key term @ multilingual hindi and punjabi document @ @ technique used is @ hybrid of @ @ statistical and linguistic based feature @ finding @ key term @ hindi and punjabi text @ regarding statistical feature @ feature @ used @ i @ entropy based metric @ finding relevant term and ii @ termfreq-inverselinefreq metric @ finding key term @ regarding linguistic feature feature @ used i @ treating noun and verb a key term ii @ treat term belonging to line in quotation mark bold italic underlined font and @ more font size a key term iii @ treating title word @ maximum coverage a key term @ @ is first time in history @ @ hybrid system ha @ proposed @ hindi and punjabi language @ @ hindi and punjabi @ le number of language resource @ existing @ @ multi lingual key term extraction system @ hindi and punjabi @ @ @ much helpful @ developing @ nlp resource in hindi and punjabi like @ automatic text summarization document classification document clustering question answering and topic tracking etc @ @ overall precision and recall @ @ hybrid multilingual key term extraction system @ @ and @ respectively @ @ reasonably good @ @ international publishing switzerland @ 
2744,A corpus-based semantic kernel for text classification by using meaning values of terms,"Text categorization plays a crucial role in both academic and commercial platforms due to the growing demand for automatic organization of documents. Kernel-based classification algorithms such as Support Vector Machines (SVM) have become highly popular in the task of text mining. This is mainly due to their relatively high classification accuracy on several application domains as well as their ability to handle high dimensional and sparse data which is the prohibitive characteristics of textual data representation. Recently, there is an increased interest in the exploitation of background knowledge such as ontologies and corpus-based statistical knowledge in text categorization. It has been shown that, by replacing the standard kernel functions such as linear kernel with customized kernel functions which take advantage of this background knowledge, it is possible to increase the performance of SVM in the text classification domain. Based on this, we propose a novel semantic smoothing kernel for SVM. The suggested approach is based on a meaning measure, which calculates the meaningfulness of the terms in the context of classes. The documents vectors are smoothed based on these meaning values of the terms in the context of classes. Since we efficiently make use of the class information in the smoothing process, it can be considered a supervised smoothing kernel. The meaning measure is based on the Helmholtz principle from Gestalt theory and has previously been applied to several text mining applications such as document summarization and feature extraction. However, to the best of our knowledge, ours is the first study to use meaning measure in a supervised setting to build a semantic kernel for SVM. We evaluated the proposed approach by conducting a large number of experiments on well-known textual datasets and present results with respect to different experimental conditions. We compare our results with traditional kernels used in SVM such as linear kernel as well as with several corpus-based semantic kernels. Our results show that classification performance of the proposed approach outperforms other kernels. © 2015 Elsevier Ltd.",2015,Engineering Applications of Artificial Intelligence,43,text categorization play a crucial role in @ @ and commercial platform due to @ growing demand @ automatic organization of document @ kernel-based classification algorithm @ a support vector machine @ svm @ @ become highly popular in @ task of text mining @ @ is mainly due to @ relatively high classification accuracy on several application domain a well a @ ability to handle high dimensional and sparse data @ is @ prohibitive characteristic of textual data representation @ recently @ is @ increased interest in @ exploitation of background knowledge @ a ontology and corpus-based statistical knowledge in text categorization @ @ ha @ @ @ by replacing @ standard kernel function @ a linear kernel @ customized kernel function @ take advantage of @ background knowledge @ is possible to increase @ performance of svm in @ text classification domain @ based on @ @ propose a novel semantic smoothing kernel @ svm @ @ suggested approach is based on a meaning measure @ calculates @ meaningfulness of @ term in @ context of class @ @ document vector @ smoothed based on @ meaning value of @ term in @ context of class @ since @ efficiently make use of @ class information in @ smoothing process @ @ @ considered a supervised smoothing kernel @ @ meaning measure is based on @ helmholtz principle @ gestalt theory and ha @ @ applied to several text mining application @ a document summarization and feature extraction @ however to @ best of @ knowledge @ is @ first study to use meaning measure in a supervised setting to build a semantic kernel @ svm @ @ evaluated @ proposed approach by conducting a @ number of experiment on well-known textual datasets and @ @ @ respect to different experimental condition @ @ compare @ @ @ traditional kernel used in svm @ a linear kernel a well a @ several corpus-based semantic kernel @ @ @ @ @ classification performance of @ proposed approach outperforms @ kernel @ @ ltd @ 
2745,A novel root based Arabic stemmer,"Stemming algorithms are used in information retrieval systems, indexers, text mining, text classifiers etc., to extract stems or roots of different words, so that words derived from the same stem or root are grouped together. Many stemming algorithms were built in different natural languages. Khoja stemmer is one of the known and widely used Arabic stemmers. In this paper, we introduced a new light and heavy Arabic stemmer. This new stemmer is presented in this study and compared with two well-known Arabic stemmers. Results showed that accuracy of our stemmer is slightly better than the accuracy yielded by each one of those two well-known Arabic stemmers used for evaluation and comparison. Evaluation tests on our novel stemmer yield 75.03% accuracy, while the other two Arabic stemmers yield slightly lower accuracy. © 2015 The Authors.",2015,Journal of King Saud University - Computer and Information Sciences,36,stemming algorithm @ used in information retrieval system indexer text mining text classifier etc @ to extract stem @ root of different word @ @ word derived @ @ @ stem @ root @ grouped together @ many stemming algorithm @ built in different natural language @ khoja stemmer is @ of @ known and widely used arabic stemmer @ in @ @ @ introduced a @ light and heavy arabic stemmer @ @ @ stemmer is presented in @ study and compared @ @ well-known arabic stemmer @ @ showed @ accuracy of @ stemmer is slightly better @ @ accuracy yielded by @ @ of @ @ well-known arabic stemmer used @ evaluation and comparison @ evaluation test on @ novel stemmer yield @ accuracy @ @ @ @ arabic stemmer yield slightly lower accuracy @ @ author @ 
2746,Automatic tag recommendation for journal abstracts using statistical topic modeling,"Topic modeling is a powerful technique for unsupervised analysis of large document collections. Topic models conceive latent topics in text using hidden random variables, and discover that structure with posterior inference. Topic models have a wide range of applications like tag recommendation, text categorization, keyword extraction and similarity search in the broad fields of text mining, information retrieval, statistical language modeling. In this work, a dataset with 200 abstracts fall under four topics are collected from two different domain journals for tagging journal abstracts. The document model is built using LDA (Latent Dirichlet Allocation) with Collapsed Variational Bayes (CVB0) and Gibbs sampling. Then the built model is used to find appropriate tag for a given abstract. An interface is designed to extract and recommend the tag for a given abstract. © 2015, Springer International Publishing Switzerland.",2015,Advances in Intelligent Systems and Computing,0,topic modeling is a powerful technique @ unsupervised analysis of @ document collection @ topic model conceive latent topic in text @ hidden random variable and discover @ structure @ posterior inference @ topic model @ a wide range of application like tag recommendation text categorization keyword extraction and similarity search in @ broad field of text mining information retrieval statistical language modeling @ in @ work a dataset @ abstract fall @ four topic @ collected @ @ different domain journal @ tagging journal abstract @ @ document model is built @ lda @ latent dirichlet allocation @ @ collapsed variational bayes @ cvb @ and gibbs sampling @ @ @ built model is used to find appropriate tag @ a given abstract @ @ interface is designed to extract and recommend @ tag @ a given abstract @ @ international publishing switzerland @ 
2750,Detecting and disambiguating locations mentioned in twitter messages,"Detecting the location entities mentioned in Twitter messages is useful in text mining for business, marketing or defence applications. Therefore, techniques for extracting the location entities from the Twitter textual content are needed. In this work, we approach this task in a similar manner to the Named Entity Recognition (NER) task focused only on locations, but we address a deeper task: classifying the detected locations into names of cities, provinces/states, and countries. We approach the task in a novel way, consisting in two stages. In the first stage, we train Conditional Random Fields (CRF) models with various sets of features; we collected and annotated our own dataset or training and testing. In the second stage, we resolve cases when there exist more than one place with the same name. We propose a set of heuristics for choosing the correct physical location in these cases. We report good evaluation results for both tasks. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),14,detecting @ location entity mentioned in twitter message is useful in text mining @ @ marketing @ defence application @ therefore technique @ extracting @ location entity @ @ twitter textual content @ needed @ in @ work @ approach @ task in a similar manner to @ named entity recognition @ ner @ task focused only on location @ @ address a deeper task @ classifying @ detected location @ name of city province state and country @ @ approach @ task in a novel way consisting in @ stage @ in @ first stage @ train conditional random field @ crf @ model @ various set of feature @ @ collected and annotated @ @ dataset @ training and testing @ in @ second stage @ resolve case @ @ exist more @ @ place @ @ @ name @ @ propose a set of heuristic @ choosing @ correct physical location in @ case @ @ report good evaluation @ @ @ task @ @ international publishing switzerland @ 
2752,Graph-based approach for cross domain text linking,"Comprehensive analysis of multi-domain texts has generated an important effect on text mining. Although the objects described by these multi-domain texts belong to different fields, they sometimes are overlapped partially; and linking these texts fragments which are overlapped or complementary is a necessary step for many tasks, such as entity resolution, information retrieval and text clustering. Previous works for computing text similarity mainly focus on string-based, corpus-based and knowledge-based approaches. However cross-domain texts exhibit very special features compared to texts in the same domain: (1) entity ambiguity, texts from different domains may contain various references to the same entity; (2) content skewness, cross domain texts are overlapped partially. In this paper, we propose a novel fine-grained approach based on text graph for evaluating the semantic similarity of cross-domain texts to link the similar parts. The experiment results show that our approach gives an effective solution to discover the semantic relationship between cross domain text fragments. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,comprehensive analysis of multi-domain text ha generated @ important effect on text mining @ although @ object described by @ multi-domain text belong to different field @ sometimes @ overlapped partially @ and linking @ text fragment @ @ overlapped @ complementary is a necessary step @ many task @ a entity resolution information retrieval and text clustering @ previous work @ computing text similarity mainly focus on string-based corpus-based and knowledge-based approach @ however cross-domain text exhibit @ special feature compared to text in @ @ domain @ @ @ entity ambiguity text @ different domain may contain various reference to @ @ entity @ @ @ content skewness cross domain text @ overlapped partially @ in @ @ @ propose a novel fine-grained approach based on text graph @ evaluating @ semantic similarity of cross-domain text to link @ similar part @ @ experiment @ @ @ @ approach give @ effective solution to discover @ semantic relationship @ cross domain text fragment @ @ international publishing switzerland @ 
2754,Entity relation mining in large-scale data,"Currently, the web-based Named-Entity relationship extraction has been a new research field with a tremendous potential. The goal of web-based entity relationship extraction is to explore the relationship between a set of realistic entities. It’s a challenging research field and has a widely application value in the related fields of text mining. In this paper, we propose a newly defined framework called Snowball++based on the traditional entity relationship extraction frameworks. In our Snowball++ framework, we focus on the many-to-many relations more than one-to-one relations. The system is also implemented in the many-to-many manner and it improves the precision and recall. It’s worth to notice that Snowball++ will assign a specific relation type to each entity-relationship pair and the whole training process only need a few manual labor. For the sake of building a efficient and scalable system, we implement the Snowball++ framework on the Hadoop platform which is a totally distributed computing system. Eventually, the experiments show that our framework and implementation are efficient and effective. © Springer International Publishing Switzerland 2015.",2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,currently @ web-based named-entity relationship extraction ha @ a @ research field @ a tremendous potential @ @ goal of web-based entity relationship extraction is to explore @ relationship @ a set of realistic entity @ @ s a challenging research field and ha a widely application value in @ related field of text mining @ in @ @ @ propose a newly defined framework called snowball based on @ traditional entity relationship extraction framework @ in @ snowball framework @ focus on @ many-to-many relation more @ one-to-one relation @ @ system is @ implemented in @ many-to-many manner and @ improves @ precision and recall @ @ s worth to notice @ snowball @ assign a specific relation type to @ entity-relationship pair and @ whole training process only need a @ manual labor @ @ @ sake of building a efficient and scalable system @ implement @ snowball framework on @ hadoop platform @ is a totally distributed computing system @ eventually @ experiment @ @ @ framework and implementation @ efficient and effective @ @ international publishing switzerland @ 
2757,Automatic construction of a semantic knowledge base from CEUR workshop proceedings,"We present an automatic workflow that performs text segmentation and entity extraction from scientific literature to primarily address Task 2 of the Semantic Publishing Challenge 2015. The goal of Task 2 is to extract various information from full-text papers to represent the context in which a document is written, such as the affiliation of its authors and the corresponding funding bodies. Our proposed solution is composed of two subsystems: (i) A text mining pipeline, developed based on the GATE framework, which extracts structural and semantic entities, such as authors’ information and references, and produces semantic (typed) annotations; and (ii) a flexible exporting module, the LODeXporter, which translates the document annotations into RDF triples according to custom mapping rules. Additionally, we leverage existing Named Entity Recognition (NER) tools to extract named entities from text and ground them to their corresponding resources on the Linked Open Data cloud, thus, briefly covering Task 3 objectives, which involves linking of detected entities to resources in existing open datasets. The output of our system is an RDF graph stored in a scalable TDB-based storage with a public SPARQL endpoint for the task’s queries. © Springer International Publishing Switzerland 2015.",2015,Communications in Computer and Information Science,6,@ @ @ automatic workflow @ performs text segmentation and entity extraction @ scientific literature to primarily address task of @ semantic publishing challenge @ @ goal of task is to extract various information @ full-text @ to represent @ context in @ a document is written @ a @ affiliation of @ author and @ corresponding funding body @ @ proposed solution is composed of @ subsystem @ @ i @ a text mining pipeline developed based on @ gate framework @ extract structural and semantic entity @ a author information and reference and produce semantic @ typed @ annotation @ and @ ii @ a flexible exporting module @ lodexporter @ translates @ document annotation @ rdf triple according to custom mapping rule @ additionally @ leverage existing named entity recognition @ ner @ tool to extract named entity @ text and ground @ to @ corresponding resource on @ linked open data cloud thus briefly covering task objective @ involves linking of detected entity to resource in existing open datasets @ @ output of @ system is @ rdf graph stored in a scalable tdb-based storage @ a public sparql endpoint @ @ task s query @ @ international publishing switzerland @ 
2760,A compositional perspective in convolution kernels,"Kernel-based learning has been largely adopted in many semantic textual inference tasks. In particular, Tree Kernels (TKs) have been successfully applied in the modeling of syntactic similarity between linguistic instances in Question Answering or Information Extraction tasks. At the same time, lexical semantic information has been studied through the adoption of the so-called Distributional Semantics (DS) paradigm, where lexical vectors are acquired automatically from large-scale corpora. Recently, Compositional Semantics phenomena arising in complex linguistic structures have been studied in an extended paradigm called Distributional Compositional Semantics (DCS), where, for example, algebraic operators on lexical vectors have been defined to account for grammatically typed bi-grams or complex verb or noun phrases. In this paper, a novel kernel called Compositionally Smoothed Partial Tree Kernel is presented to integrate DCS operators into the tree kernel evaluation by also considering complex compositional nodes. Empirical results on well-known NLP tasks show that state-of-the-art performances can be achieved, without resorting to manual feature engineering, thus suggesting that a large set of Web and text mining tasks can be handled successfully by this kernel.",2015,CEUR Workshop Proceedings,0,kernel-based learning ha @ largely adopted in many semantic textual inference task @ in particular tree kernel @ tks @ @ @ successfully applied in @ modeling of syntactic similarity @ linguistic instance in question answering @ information extraction task @ at @ @ time lexical semantic information ha @ studied @ @ adoption of @ so-called distributional semantics @ @ @ paradigm @ lexical vector @ acquired automatically @ large-scale corpus @ recently compositional semantics phenomenon arising in complex linguistic structure @ @ studied in @ extended paradigm called distributional compositional semantics @ dc @ @ @ example algebraic operator on lexical vector @ @ defined to account @ grammatically typed bi-grams @ complex verb @ noun phrase @ in @ @ a novel kernel called compositionally smoothed partial tree kernel is presented to integrate dc operator @ @ tree kernel evaluation by @ considering complex compositional node @ empirical @ on well-known nlp task @ @ state-of-the-art performance @ @ achieved without resorting to manual feature engineering thus suggesting @ a @ set of web and text mining task @ @ handled successfully by @ kernel @ 
2762,Detecting events and sentiment on twitter for improving urban mobility,"The streams of tweets from and to the Twitter account of urban transport operators have been considered. A computational module has been designed and developed in order to collect tweets and, on the fly, analyze them to detect some relevant event (e.g. accidents, sudden traffic jams, service interruption, etc.) and/or evaluate possible sentiments and opinions about the quality of service. Events are recognized through a simple word matching while sentiment analysis is performed via supervised learning (Support Vector Machine). The text mining solutions have been developed to work with Italian language; however they could be easily extended to other languages in the case tweets in other languages would be available. This approach has been tested for the urban transportation in Milan (Azienda Trasporti Milano, ATM) in the framework of the TAMTAM project which has developed a technological platform for improving urban mobility by exploiting the large amount of information shared by the users of transportation services through Twitter. Events detected are used by other software modules of the TAM-TAM platform in order to support a more effective travel planning, while sentiment inferred may be used by the transport provider in order to tune the mobility supply to the commuter needs.",2015,CEUR Workshop Proceedings,3,@ stream of tweet @ and to @ twitter account of urban transport operator @ @ considered @ a computational module ha @ designed and developed in order to collect tweet and on @ fly analyze @ to detect some relevant event @ e @ g @ accident sudden traffic jam service interruption etc @ @ and @ evaluate possible sentiment and opinion @ @ quality of service @ event @ recognized @ a simple word matching @ sentiment analysis is performed via supervised learning @ support vector machine @ @ @ text mining solution @ @ developed to work @ italian language @ however @ could @ easily extended to @ language in @ case tweet in @ language would @ available @ @ approach ha @ tested @ @ urban transportation in milan @ azienda trasporti milano atm @ in @ framework of @ tamtam project @ ha developed a technological platform @ improving urban mobility by exploiting @ @ amount of information shared by @ user of transportation service @ twitter @ event detected @ used by @ software module of @ tam-tam platform in order to support a more effective travel planning @ sentiment inferred may @ used by @ transport provider in order to tune @ mobility supply to @ commuter need @ 
2764,Topic modeling of E-news in Punjabi,"Topic Modeling refers to the act of discovering the theme of a document. Theme of a document provides an abstract view of the set of subjects (topics) addressed in the document. So, documents can be classified, arranged and searched according to their subjects using Topic Modeling. Topic Modeling has been the area of interest of most of the researchers from the fields of Text Mining, Natural Language Processing, and Machine Learning etc. Literature shows some techniques for generating theme out of a document. Most of the suggested Topic Models have been designed for English language. For Indian languages, particularly in Punjabi Language, such topic modeling is lacking in the literature. Although some Topic Summarization, Topic Tracking and Keyword Extraction systems has been developed for Punjabi Language, yet the technique of Topic Modeling is quite different from them. The paper presents a topic model for E-news in Punjabi Language. The idea of this topic model has been taken from the simplest and most basic probabilistic topic model; named LDA (Latent Dirichlet Allocation). This topic model finds the topics and their respective proportions present in the news text given as input to it. The theme generation process needs a Topic List Corpus at the backend of Topic Model. Such Corpus has been built containing Punjabi words commonly occurring in news articles, classified under different topic lists. The topic model has been tested on more than 1000 news articles for verification of its exactness. The values of various parameters attesting the quality of outputs given by topic model are quite satisfactory.",2015,Indian Journal of Science and Technology,2,topic modeling refers to @ act of discovering @ theme of a document @ theme of a document provides @ abstract view of @ set of subject @ topic @ addressed in @ document @ @ document @ @ classified arranged and searched according to @ subject @ topic modeling @ topic modeling ha @ @ area of interest of @ of @ researcher @ @ field of text mining natural language processing and machine learning etc @ literature @ some technique @ generating theme @ of a document @ @ of @ suggested topic model @ @ designed @ english language @ @ indian language particularly in punjabi language @ topic modeling is lacking in @ literature @ although some topic summarization topic tracking and keyword extraction system ha @ developed @ punjabi language yet @ technique of topic modeling is quite different @ @ @ @ @ @ a topic model @ e-news in punjabi language @ @ idea of @ topic model ha @ taken @ @ simplest and @ basic probabilistic topic model @ named lda @ latent dirichlet allocation @ @ @ topic model find @ topic and @ respective proportion @ in @ news text given a input to @ @ @ theme generation process need a topic list corpus at @ backend of topic model @ @ corpus ha @ built containing punjabi word commonly occurring in news article classified @ different topic list @ @ topic model ha @ tested on more @ news article @ verification of @ exactness @ @ value of various parameter attesting @ quality of output given by topic model @ quite satisfactory @ 
2767,Insights from hashtag #supplychain and Twitter analytics: Considering Twitter and Twitter data for supply chain practice and research,"Abstract Recently, businesses and research communities have paid a lot of attention to social media and big data. However, the field of supply chain management (SCM) has been relatively slow in studying social media and big data for research and practice. In these contexts, this research contributes to the SCM community by proposing a novel, analytical framework (Twitter Analytics) for analyzing supply chain tweets, highlighting the current use of Twitter in supply chain contexts, and further developing insights into the potential role of Twitter for supply chain practice and research. The proposed framework combines three methodologies - descriptive analytics (DA), content analytics (CA) integrating text mining and sentiment analysis, and network analytics (NA) relying on network visualization and metrics - for extracting intelligence from 22,399 #supplychain tweets. Some of the findings are: supply chain tweets are used by different groups of supply chain professionals and organizations (e.g., news services, IT companies, logistic providers, manufacturers) for information sharing, hiring professionals, and communicating with stakeholders, among others; diverse topics are being discussed, ranging from logistics and corporate social responsibility, to risk, manufacturing, SCM IT and even human rights; some tweets carry strong sentiments about companies' delivery services, sales performance, and environmental standards, and risk and disruption in supply chains. Based on these findings, this research presents insights into the use and potential role of Twitter for supply chain practices (e.g., professional networking, stakeholder engagement, demand shaping, new product/service development, supply chain risk management) and the implications for research. Finally, the limitations of the current study and suggestions for future research are presented. © 2015 Elsevier B.V.",2015,International Journal of Production Economics,208,abstract recently @ and research community @ paid a lot of attention to social medium and big data @ however @ field of supply chain management @ scm @ ha @ relatively slow in studying social medium and big data @ research and practice @ in @ context @ research contributes to @ scm community by proposing a novel analytical framework @ twitter analytics @ @ analyzing supply chain tweet highlighting @ current use of twitter in supply chain context and @ developing insight @ @ potential role of twitter @ supply chain practice and research @ @ proposed framework combine three methodology descriptive analytics @ da @ content analytics @ ca @ integrating text mining and sentiment analysis and network analytics @ na @ relying on network visualization and metric @ extracting intelligence @ supplychain tweet @ some of @ finding @ @ supply chain tweet @ used by different group of supply chain professional and organization @ e @ g @ news service @ company logistic provider manufacturer @ @ information sharing hiring professional and communicating @ stakeholder among others @ diverse topic @ @ discussed ranging @ logistics and corporate social responsibility to risk manufacturing scm @ and even human right @ some tweet carry strong sentiment @ company @ delivery service sale performance and environmental standard and risk and disruption in supply chain @ based on @ finding @ research @ insight @ @ use and potential role of twitter @ supply chain practice @ e @ g @ professional networking stakeholder engagement demand shaping @ product service development supply chain risk management @ and @ implication @ research @ finally @ limitation of @ current study and suggestion @ future research @ presented @ @ b @ v @ 
2775,Mining Semantics Structures from Syntactic Structures in Web Document Corpora,"The Web is making possible many advanced text-mining applications, such as news summarization, essay grading, question answering, semantic search and structured queries on corpora of Web documents. For many of such applications, statistical text-mining techniques are of limited effectiveness since they do not utilize the morphological structure of the text. On the other hand, many approaches use NLP-based techniques that parse the text into parse trees, and then use patterns to mine and analyze parse trees which are often unnecessarily complex. To reduce this complexity and ease the entire process of text mining, we propose a weighted-graph representation of text, called TextGraphs, which captures the grammatical and semantic relations between words and terms in the text. TextGraphs are generated using a new text mining framework which is the main focus of this paper. Our framework, SemScape, uses a statistical parser to generate few of the most probable parse trees for each sentence and employs a novel two-step pattern-based technique to extract from parse trees candidate terms and their grammatical relations. Moreover, SemScape resolves coreferences by a novel technique, generates domain-specific TextGraphs by consulting ontologies, and provides a SPARQL-like query language and an optimized engine for semantically querying and mining TextGraphs. © 2014 World Scientific Publishing Company.",2014,International Journal of Semantic Computing,1,@ web is making possible many advanced text-mining application @ a news summarization essay grading question answering semantic search and structured query on corpus of web document @ @ many of @ application statistical text-mining technique @ of limited effectiveness since @ @ not utilize @ morphological structure of @ text @ on @ @ hand many approach use nlp-based technique @ parse @ text @ parse tree and @ use pattern to mine and analyze parse tree @ @ often unnecessarily complex @ to reduce @ complexity and ease @ entire process of text mining @ propose a weighted-graph representation of text called textgraphs @ capture @ grammatical and semantic relation @ word and term in @ text @ textgraphs @ generated @ a @ text mining framework @ is @ main focus of @ @ @ @ framework semscape us a statistical parser to generate @ of @ @ probable parse tree @ @ sentence and employ a novel two-step pattern-based technique to extract @ parse tree candidate term and @ grammatical relation @ moreover semscape resolve coreference by a novel technique generates domain-specific textgraphs by consulting ontology and provides a sparql-like query language and @ optimized engine @ semantically querying and mining textgraphs @ world scientific publishing company @ 
2781,Multi level causal relation identification using extended features,"Extracting causal relation underlying natural language is an important issue in knowledge discovery. Most previous studies of casual relation extraction focus on simple cases like causal relations between two noun phrases indicated by fixed verbs or prepositions. For more complicated causal relations, such as causal relations between clauses, the previously developed algorithm may not work. To solve this problem, this paper develops a system that is able to extract causal relations in multi-level language expressions such as, words, phrases and clauses without fixed relators. The information extraction system is composed of a multi-level relation extractor and an ensemble-based relation classifier. It may extract more subtypes of causal relations than previous work because extracting domain is expanded in terms of both syntactic expressions and semantic meanings. In addition, the proposed method outperforms previously developed methods because extended features based on lexical semantic resources are explored. Experiments show that our system achieves an accuracy of 88.69% and F-score of 0.6637 in a dataset with 300 sentences. © 2014 Elsevier Ltd. All rights reserved.",2014,Expert Systems with Applications,12,extracting causal relation underlying natural language is @ important issue in knowledge discovery @ @ previous study of casual relation extraction focus on simple case like causal relation @ @ noun phrase indicated by fixed verb @ preposition @ @ more complicated causal relation @ a causal relation @ clause @ @ developed algorithm may not work @ to solve @ problem @ @ develops a system @ is able to extract causal relation in multi-level language expression @ a word phrase and clause without fixed relators @ @ information extraction system is composed of a multi-level relation extractor and @ ensemble-based relation classifier @ @ may extract more subtypes of causal relation @ previous work @ extracting domain is expanded in term of @ syntactic expression and semantic meaning @ in addition @ proposed method outperforms @ developed method @ extended feature based on lexical semantic resource @ explored @ experiment @ @ @ system achieves @ accuracy of @ and f-score of @ in a dataset @ sentence @ @ ltd @ @ right reserved @ 
2785,Rhetorical Structure Theory for polarity estimation: An experimental study,"Sentiment analysis tools often rely on counts of sentiment-carrying words, ignoring structural aspects of content. Natural Language Processing has been fruitfully exploited in text mining, but advanced discourse processing is still nonpervasive for mining opinions. Some studies, however, extracted opinions based on the discursive role of text segments. The merits of such computationally intensive analyses have thus far been assessed in very specific, small-scale scenarios. In this paper, we investigate the usefulness of Rhetorical Structure Theory in various sentiment analysis tasks on different types of information sources. First, we demonstrate how to perform a large-scale ranking of individual blog posts in terms of their overall polarity, by exploiting the rhetorical structure of a few key evaluative sentences. In order to further validate our findings, we additionally explore the potential of Rhetorical Structure Theory in sentence-level polarity classification of news and product reviews. Our most valuable polarity classification features turn out to capture the way in which polar terms are used, rather than the sentiment-carrying words per se. © 2014 Elsevier B.V. All rights reserved.",2014,Data and Knowledge Engineering,12,sentiment analysis tool often rely on count of sentiment-carrying word ignoring structural aspect of content @ natural language processing ha @ fruitfully exploited in text mining @ advanced discourse processing is still nonpervasive @ mining opinion @ some study however extracted opinion based on @ discursive role of text segment @ @ merit of @ computationally intensive analysis @ thus far @ assessed in @ specific small-scale scenario @ in @ @ @ investigate @ usefulness of rhetorical structure theory in various sentiment analysis task on different type of information source @ first @ demonstrate @ to perform a large-scale ranking of individual blog post in term of @ overall polarity by exploiting @ rhetorical structure of a @ key evaluative sentence @ in order to @ validate @ finding @ additionally explore @ potential of rhetorical structure theory in sentence-level polarity classification of news and product review @ @ @ valuable polarity classification feature turn @ to capture @ way in @ polar term @ used rather @ @ sentiment-carrying word per se @ @ b @ v @ @ right reserved @ 
2791,Feature-based opinion mining through ontologies,"The idiosyncrasy of the Web has, in the last few years, been altered by Web 2.0 technologies and applications and the advent of the so-called Social Web. While users were merely information consumers in the traditional Web, they play a much more active role in the Social Web since they are now also data providers. The mass involved in the process of creating Web content has led many public and private organizations to focus their attention on analyzing this content in order to ascertain the general public's opinions as regards a number of topics. Given the current Web size and growth rate, automated techniques are essential if practical and scalable solutions are to be obtained. Opinion mining is a highly active research field that comprises natural language processing, computational linguistics and text analysis techniques with the aim of extracting various kinds of added-value and informational elements from users' opinions. However, current opinion mining approaches are hampered by a number of drawbacks such as the absence of semantic relations between concepts in feature search processes or the lack of advanced mathematical methods in sentiment analysis processes. In this paper we propose an innovative opinion mining methodology that takes advantage of new Semantic Web-guided solutions to enhance the results obtained with traditional natural language processing techniques and sentiment analysis processes. The main goals of the proposed methodology are: (1) to improve feature-based opinion mining by using ontologies at the feature selection stage, and (2) to provide a new vector analysis-based method for sentiment analysis. The methodology has been implemented and thoroughly tested in a real-world movie review-themed scenario, yielding very promising results when compared with other conventional approaches.© 2014 Elsevier Ltd. All rights reserved.",2014,Expert Systems with Applications,113,@ idiosyncrasy of @ web ha in @ last @ year @ altered by web @ technology and application and @ advent of @ so-called social web @ @ user @ merely information consumer in @ traditional web @ play a much more active role in @ social web since @ @ now @ data provider @ @ mass involved in @ process of creating web content ha led many public and private organization to focus @ attention on analyzing @ content in order to ascertain @ general public @ s opinion a regard a number of topic @ given @ current web size and growth rate automated technique @ essential if practical and scalable solution @ to @ obtained @ opinion mining is a highly active research field @ comprises natural language processing computational linguistics and text analysis technique @ @ aim of extracting various kind of added-value and informational element @ user @ opinion @ however current opinion mining approach @ hampered by a number of drawback @ a @ absence of semantic relation @ concept in feature search process @ @ lack of advanced mathematical method in sentiment analysis process @ in @ @ @ propose @ innovative opinion mining methodology @ take advantage of @ semantic web-guided solution to enhance @ @ obtained @ traditional natural language processing technique and sentiment analysis process @ @ main goal of @ proposed methodology @ @ @ @ to improve feature-based opinion mining by @ ontology at @ feature selection stage and @ @ to provide a @ vector analysis-based method @ sentiment analysis @ @ methodology ha @ implemented and thoroughly tested in a real-world movie review-themed scenario yielding @ promising @ @ compared @ @ conventional approach @ @ ltd @ @ right reserved @ 
2793,NLP-KAOS for systems goal elicitation: Smart metering system case study,"This paper presents a computational method that employs Natural Language Processing (NLP) and text mining techniques to support requirements engineers in extracting and modeling goals from textual documents. We developed a NLP-based goal elicitation approach within the context of KAOS goal-oriented requirements engineering method. The hierarchical relationships among goals are inferred by automatically building taxonomies from extracted goals. We use smart metering system as a case study to investigate the proposed approach. Smart metering system is an important subsystem of the next generation of power systems (smart grids). Goals are extracted by semantically parsing the grammar of goal-related phrases in abstracts of research publications. The results of this case study show that the developed approach is an effective way to model goals for complex systems, and in particular, for the research-intensive complex systems. © 1976-2012 IEEE.",2014,IEEE Transactions on Software Engineering,24,@ @ @ a computational method @ employ natural language processing @ nlp @ and text mining technique to support requirement engineer in extracting and modeling goal @ textual document @ @ developed a nlp-based goal elicitation approach within @ context of kaos goal-oriented requirement engineering method @ @ hierarchical relationship among goal @ inferred by automatically building taxonomy @ extracted goal @ @ use smart metering system a a case study to investigate @ proposed approach @ smart metering system is @ important subsystem of @ next generation of power system @ smart grid @ @ goal @ extracted by semantically parsing @ grammar of goal-related phrase in abstract of research publication @ @ @ of @ case study @ @ @ developed approach is @ effective way to model goal @ complex system and in particular @ @ research-intensive complex system @ @ @ 
2794,Exploring Co-training strategies for opinion detection,"For the last decade or so, sentiment analysis, which aims to automatically identify opinions, polarities, or emotions from user-generated content (e.g., blogs, tweets), has attracted interest from both academic and industrial communities. Most sentiment analysis strategies fall into 2 categories: lexicon-based and corpus-based approaches. While the latter often requires sentiment-labeled data to build a machine learning model, both approaches need sentiment-labeled data for evaluation. Unfortunately, most data domains lack sufficient quantities of labeled data, especially at the subdocument level. Semisupervised learning (SSL), a machine learning technique that requires only a few labeled examples and can automatically label unlabeled data, is a promising strategy to deal with the issue of insufficient labeled data. Although previous studies have shown promising results of applying various SSL algorithms to solve a sentiment-analysis problem, co-training, an SSL algorithm, has not attracted much attention for sentiment analysis largely due to its restricted assumptions. Therefore, this study focuses on revisiting co-training in depth and discusses several co-training strategies for sentiment analysis following a loose assumption. Results suggest that co-training can be more effective than can other currently adopted SSL methods for sentiment analysis. © 2014 ASIS&T.",2014,Journal of the Association for Information Science and Technology,12,@ @ last decade @ @ sentiment analysis @ aim to automatically identify opinion polarity @ emotion @ user-generated content @ e @ g @ blog tweet @ ha attracted interest @ @ @ and industrial community @ @ sentiment analysis strategy fall @ category @ lexicon-based and corpus-based approach @ @ @ latter often requires sentiment-labeled data to build a machine learning model @ approach need sentiment-labeled data @ evaluation @ unfortunately @ data domain lack sufficient quantity of labeled data especially at @ subdocument level @ semisupervised learning @ ssl @ a machine learning technique @ requires only a @ labeled example and @ automatically label unlabeled data is a promising strategy to deal @ @ issue of insufficient labeled data @ although previous study @ @ promising @ of applying various ssl algorithm to solve a sentiment-analysis problem co-training @ ssl algorithm ha not attracted much attention @ sentiment analysis largely due to @ restricted assumption @ therefore @ study focus on revisiting co-training in depth and discus several co-training strategy @ sentiment analysis following a loose assumption @ @ suggest @ co-training @ @ more effective @ @ @ currently adopted ssl method @ sentiment analysis @ asis t @ 
2800,The effect of news and public mood on stock movements,"With technological advancements that cultivate vibrant creation, sharing, and collaboration among Web users, investors can rapidly obtain more valuable and timely information. Meanwhile, the adaption of user engagement in media effectively magnifies the information in the news. With such rapid information influx, investor decisions tend to be influenced by peer and public emotions. An effective methodology to quantitatively analyze the mechanism of information percolation and its degree of impact on stock markets has yet to be explored. In this article, we propose a quantitative media-aware trading strategy to investigate the media impact on stock markets. Our main findings are that (1) fundamental information of firm-specific news articles can enrich the knowledge of investors and affect their trading activities; (2) public sentiments cause emotional fluctuations in investors and intervene in their decision making; and (3) the media impact on firms varies according to firm characteristics and article content. © 2014 Elsevier Inc. All rights reserved.",2014,Information Sciences,103,@ technological advancement @ cultivate vibrant creation sharing and collaboration among web user investor @ rapidly obtain more valuable and timely information @ meanwhile @ adaption of user engagement in medium effectively magnifies @ information in @ news @ @ @ rapid information influx investor decision tend to @ influenced by peer and public emotion @ @ effective methodology to quantitatively analyze @ mechanism of information percolation and @ degree of impact on stock market ha yet to @ explored @ in @ article @ propose a quantitative media-aware trading strategy to investigate @ medium impact on stock market @ @ main finding @ @ @ @ fundamental information of firm-specific news article @ enrich @ knowledge of investor and affect @ trading activity @ @ @ public sentiment cause emotional fluctuation in investor and intervene in @ decision making @ and @ @ @ medium impact on firm varies according to firm characteristic and article content @ @ inc @ @ right reserved @ 
2801,Negotiating a text mining license for faculty researchers,"This case study examines strategies used to leverage the library's existing journal licenses to obtain a large collection of full-text journal articles in XML format, the right to text mine the collection, and the right to use the collection and the data mined from it for grant-funded research to develop biomedical natural language processing (BNLP) tools. Researchers attempted to obtain content directly from PubMed Central (PMC). This attempt failed because of limits on use of content in PMC. Next, researchers and their library liaison attempted to obtain content from contacts in the technical divisions of the publishing industry. This resulted in an incomplete research data set. Researchers, the library liaison, and the acquisitions librarian then collaborated with the sales and technical staff of a major science, technology, engineering, and medical (STEM) publisher to successfully create a method for obtaining XML content as an extension of the library's typical acquisition process for electronic resources. Our experience led us to realize that text-mining rights of full-text articles in XML format should routinely be included in the negotiation of the library's licenses. © 2014. American Library Association. All rights reserved.",2014,Information Technology and Libraries,5,@ case study examines strategy used to leverage @ library @ s existing journal license to obtain a @ collection of full-text journal article in xml format @ right to text mine @ collection and @ right to use @ collection and @ data mined @ @ @ grant-funded research to develop biomedical natural language processing @ bnlp @ tool @ researcher attempted to obtain content directly @ pubmed central @ pmc @ @ @ attempt failed @ of limit on use of content in pmc @ next researcher and @ library liaison attempted to obtain content @ contact in @ technical division of @ publishing industry @ @ resulted in @ incomplete research data set @ researcher @ library liaison and @ acquisition librarian @ collaborated @ @ sale and technical staff of a major science technology engineering and medical @ stem @ publisher to successfully create a method @ obtaining xml content a @ extension of @ library @ s typical acquisition process @ electronic resource @ @ experience led u to realize @ text-mining right of full-text article in xml format @ routinely @ included in @ negotiation of @ library @ s license @ @ american library association @ @ right reserved @ 
2802,"Text-mining, structured queries, and knowledge management on web document corpora","Wikipedia's InfoBoxes play a crucial role in advanced applications and provide the main knowledge source for DBpedia and the powerful structured queries it supports. However, InfoBoxes, which were created by crowdsourcing for human rather than computer consumption, suffer from incompleteness, inconsistencies, and inaccuracies. To overcome these problems, we have developed (i) the IBminer system that extracts InfoBox information by text-mining Wikipedia pages, (ii) the IKBStore system that integrates the information derived by IBminer with that of DBpedia, YAGO2, WikiData, WordNet, and other sources, and (iii) SWiPE and InfoBox Editor (IBE) that provide a user-friendly interfaces for querying and revising the knowledge base. Thus, IBminer uses a deep NLP-based approach to extract from text a semantic representation structure called TextGraph from which the system detects patterns and derives subject-attribute-value relations, as well as domain-specific synonyms for the knowledge base. IKBStore and IBE complement the powerful, user-friendly, by-example structured queries of SWiPE by supporting the validation and provenance history for the information contained in the knowledge base, along with the ability of upgrading its knowledge when this is found incomplete, incorrect, or outdated.",2014,SIGMOD Record,8,wikipedia @ s infoboxes play a crucial role in advanced application and provide @ main knowledge source @ dbpedia and @ powerful structured query @ support @ however infoboxes @ @ created by crowdsourcing @ human rather @ computer consumption suffer @ incompleteness inconsistency and inaccuracy @ to overcome @ problem @ @ developed @ i @ @ ibminer system @ extract infobox information by text-mining wikipedia page @ ii @ @ ikbstore system @ integrates @ information derived by ibminer @ @ of dbpedia yago wikidata wordnet and @ source and @ iii @ swipe and infobox editor @ ibe @ @ provide a user-friendly interface @ querying and revising @ knowledge base @ thus ibminer us a deep nlp-based approach to extract @ text a semantic representation structure called textgraph @ @ @ system detects pattern and derives subject-attribute-value relation a well a domain-specific synonym @ @ knowledge base @ ikbstore and ibe complement @ powerful user-friendly by-example structured query of swipe by supporting @ validation and provenance history @ @ information contained in @ knowledge base along @ @ ability of upgrading @ knowledge @ @ is found incomplete incorrect @ outdated @ 
2803,A random walk-based model for identifying semantic orientation,"Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification, text filtering, product review analysis, survey response analysis, and on-line discussion mining. We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph, and produces a polarity estimate for any given word. The model can accurately and quickly assign a polarity sign and magnitude to any word. It can be used both in a semi-supervised setting where a training set of labeled words is used, and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes. The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon. We also show how our method can be used for three-way classification which identifies neutral words in addition to positive and negative words. Our experiments show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting and is comparable to the best reported values in the weakly supervised setting. In addition, the proposed method is faster and does not need a large corpus. We also present extensions of our methods for identifying the polarity of foreign words and out-of-vocabulary words. © 2014 Association for Computational Linguistics.",2014,Computational Linguistics,1,automatically identifying @ sentiment polarity of word is a @ important task @ ha @ used a @ essential building block of many natural language processing system @ a text classification text filtering product review analysis survey response analysis and on-line discussion mining @ @ propose a method @ identifying @ sentiment polarity of word @ applies a markov random walk model to a @ word relatedness graph and produce a polarity estimate @ @ given word @ @ model @ accurately and quickly assign a polarity sign and magnitude to @ word @ @ @ @ used @ in a semi-supervised setting @ a training set of labeled word is used and in a weakly supervised setting @ only a handful of seed word is used to define @ @ polarity class @ @ method is experimentally tested @ a gold standard set of positive and negative word @ @ general inquirer lexicon @ @ @ @ @ @ method @ @ used @ three-way classification @ identifies neutral word in addition to positive and negative word @ @ experiment @ @ @ proposed method outperforms @ state-of-the-art method in @ semi-supervised setting and is comparable to @ best reported value in @ weakly supervised setting @ in addition @ proposed method is faster and doe not need a @ corpus @ @ @ @ extension of @ method @ identifying @ polarity of foreign word and out-of-vocabulary word @ association @ computational linguistics @ 
2804,A keyword extraction method from twitter messages represented as graphs,"Twitter is a microblog service that generates a huge amount of textual content daily. All this content needs to be explored by means of text mining, natural language processing, information retrieval, and other techniques. In this context, automatic keyword extraction is a task of great usefulness. A fundamental step in text mining techniques consists of building a model for text representation. The model known as vector space model, VSM, is the most well-known and used among these techniques. However, some difficulties and limitations of VSM, such as scalability and sparsity, motivate the proposal of alternative approaches. This paper proposes a keyword extraction method for tweet collections that represents texts as graphs and applies centrality measures for finding the relevant vertices (keywords). To assess the performance of the proposed approach, three different sets of experiments are performed. The first experiment applies TKG to a text from the Time magazine and compares its performance with that of the literature. The second set of experiments takes tweets from three different TV shows, applies TKG and compares it with TFIDF and KEA, having human classifications as benchmarks. Finally, these three algorithms are applied to tweets sets of increasing size and their computational running time is measured and compared. Altogether, these experiments provide a general overview of how TKG can be used in practice, its performance when compared with other standard approaches, and how it scales to larger data instances. The results show that TKG is a novel and robust proposal to extract keywords from texts, particularly from short messages, such as tweets. © 2014 Elsevier Inc. All rights reserved.",2014,Applied Mathematics and Computation,64,twitter is a microblog service @ generates a huge amount of textual content daily @ @ @ content need to @ explored by mean of text mining natural language processing information retrieval and @ technique @ in @ context automatic keyword extraction is a task of great usefulness @ a fundamental step in text mining technique consists of building a model @ text representation @ @ model known a vector space model vsm is @ @ well-known and used among @ technique @ however some difficulty and limitation of vsm @ a scalability and sparsity motivate @ proposal of alternative approach @ @ @ proposes a keyword extraction method @ tweet collection @ represents text a graph and applies centrality measure @ finding @ relevant vertex @ keywords @ @ to ass @ performance of @ proposed approach three different set of experiment @ performed @ @ first experiment applies tkg to a text @ @ time magazine and compare @ performance @ @ of @ literature @ @ second set of experiment take tweet @ three different tv @ applies tkg and compare @ @ tfidf and kea @ human classification a benchmark @ finally @ three algorithm @ applied to tweet set of increasing size and @ computational running time is measured and compared @ altogether @ experiment provide a general overview of @ tkg @ @ used in practice @ performance @ compared @ @ standard approach and @ @ scale to larger data instance @ @ @ @ @ tkg is a novel and robust proposal to extract keywords @ text particularly @ short message @ a tweet @ @ inc @ @ right reserved @ 
2808,Experimental analysis based on feature engineering techniques for effective polarity identification on E-commerce user reviews,"The Web has dramatically changed the way that people express their views and opinions. They can now post reviews of products at merchant sites and express their views on almost anything in Internet forums, discussion groups, and blogs, which are collectively called the user-generated content. There are a large number of diverse sources, and each source may also have a huge volume of opinionated text (text with opinions or sentiments). In many cases, opinions are bidden in long forum posts and blogs. It is difficult for a human reader to find relevant sources, extract related sentences with opinions, read them, summarize them, and organize them into usable forms. Thus, automated opinion discovery and summarization systems are needed. Sentiment analysis, also known as opinion mining, grows out of this need. It is a challenging natural language processing or text mining problem. In this paper, the objective is to study the pros-cons dataset and perform a statement level sentiment classification. Also, we analyze the effects of extracting different kinds of text based features and their role in the building of machine learning models for text classification. Then, a detailed error analysis is performed to highlight the features that are strong, weak indicators of predicted classes, along with displaying the most confusing features that causes examples in the dataset to be misclassified. We further aim to improve classification accuracy using stretchy patterns and regular expressions. ©2014 International Information Institute.",2014,Information (Japan),0,@ web ha dramatically changed @ way @ people express @ view and opinion @ @ @ now post review of product at merchant site and express @ view on almost anything in internet forum discussion group and blog @ @ collectively called @ user-generated content @ @ @ a @ number of diverse source and @ source may @ @ a huge volume of opinionated text @ text @ opinion @ sentiment @ @ in many case opinion @ bidden in long forum post and blog @ @ is difficult @ a human reader to find relevant source extract related sentence @ opinion read @ summarize @ and organize @ @ usable form @ thus automated opinion discovery and summarization system @ needed @ sentiment analysis @ known a opinion mining grows @ of @ need @ @ is a challenging natural language processing @ text mining problem @ in @ @ @ objective is to study @ pros-cons dataset and perform a statement level sentiment classification @ @ @ analyze @ effect of extracting different kind of text based feature and @ role in @ building of machine learning model @ text classification @ @ a detailed error analysis is performed to highlight @ feature @ @ strong weak indicator of predicted class along @ displaying @ @ confusing feature @ cause example in @ dataset to @ misclassified @ @ @ aim to improve classification accuracy @ stretchy pattern and regular expression @ international information institute @ 
2811,A study of supervised term weighting scheme for sentiment analysis,"Term weighting is a strategy that assigns weights to terms to improve the performance of sentiment analysis and other text mining tasks. In this paper, we propose a supervised term weighting scheme based on two basic factors: Importance of a term in a document (ITD) and importance of a term for expressing sentiment (ITS), to improve the performance of analysis. For ITD, we explore three definitions based on term frequency. Then, seven statistical functions are employed to learn the ITS of each term from training documents with category labels. Compared with the previous unsupervised term weighting schemes originated from information retrieval, our scheme can make full use of the available labeling information to assign appropriate weights to terms. We have experimentally evaluated the proposed method against the state-of-the-art method. The experimental results show that our method outperforms the method and produce the best accuracy on two of three data sets. © 2013 Elsevier B.V. All rights reserved.",2014,Expert Systems with Applications,108,term weighting is a strategy @ assigns weight to term to improve @ performance of sentiment analysis and @ text mining task @ in @ @ @ propose a supervised term weighting scheme based on @ basic factor @ importance of a term in a document @ itd @ and importance of a term @ expressing sentiment @ @ @ to improve @ performance of analysis @ @ itd @ explore three definition based on term frequency @ @ seven statistical function @ employed to learn @ @ of @ term @ training document @ category label @ compared @ @ previous unsupervised term weighting scheme originated @ information retrieval @ scheme @ make full use of @ available labeling information to assign appropriate weight to term @ @ @ experimentally evaluated @ proposed method @ @ state-of-the-art method @ @ experimental @ @ @ @ method outperforms @ method and produce @ best accuracy on @ of three data set @ @ b @ v @ @ right reserved @ 
2812,Document clustering method using dimension reduction and support vector clustering to overcome sparseness,"Many studies on developing technologies have been published as articles, papers, or patents. We use and analyze these documents to find scientific and technological trends. In this paper, we consider document clustering as a method of document data analysis. In general, we have trouble analyzing documents directly because document data are not suitable for statistical and machine learning methods of analysis. Therefore, we have to transform document data into structured data for analytical purposes. For this process, we use text mining techniques. The structured data are very sparse, and hence, it is difficult to analyze them. This study proposes a new method to overcome the sparsity problem of document clustering. We build a combined clustering method using dimension reduction and K-means clustering based on support vector clustering and Silhouette measure. In particular, we attempt to overcome the sparseness in patent document clustering. To verify the efficacy of our work, we first conduct an experiment using news data from the machine learning repository of the University of California at Irvine. Second, using patent documents retrieved from the United States Patent and Trademark Office, we carry out patent clustering for technology forecasting. © 2013 Elsevier Ltd. All rights reserved.",2014,Expert Systems with Applications,68,many study on developing technology @ @ published a article @ @ patent @ @ use and analyze @ document to find scientific and technological trend @ in @ @ @ consider document clustering a a method of document data analysis @ in general @ @ trouble analyzing document directly @ document data @ not suitable @ statistical and machine learning method of analysis @ therefore @ @ to transform document data @ structured data @ analytical purpose @ @ @ process @ use text mining technique @ @ structured data @ @ sparse and hence @ is difficult to analyze @ @ @ study proposes a @ method to overcome @ sparsity problem of document clustering @ @ build a combined clustering method @ dimension reduction and k-means clustering based on support vector clustering and silhouette measure @ in particular @ attempt to overcome @ sparseness in patent document clustering @ to verify @ efficacy of @ work @ first conduct @ experiment @ news data @ @ machine learning repository of @ university of california at irvine @ second @ patent document retrieved @ @ united state patent and trademark office @ carry @ patent clustering @ technology forecasting @ @ ltd @ @ right reserved @ 
2813,Text mining self-disclosing health information for public health service,"Understanding specific patterns or knowledge of selfdisclosing health information could support public health surveillance and healthcare. This study aimed to develop an analytical framework to identify selfdisclosing health information with unusual messages on web forums by leveraging advanced text-mining techniques. To demonstrate the performance of the proposed analytical framework, we conducted an experimental study on 2 major human immunodeficiency virus (HIV)/acquired immune deficiency syndrome (AIDS) forums in Taiwan. The experimental results show that the classification accuracy increased significantly (up to 83.83%) when using features selected by the information gain technique. The results also show the importance of adopting domain-specific features in analyzing unusual messages on web forums. This study has practical implications for the prevention and support of HIV/AIDS healthcare. For example, public health agencies can re-allocate resources and deliver services to people who need help via social media sites. In addition, individuals can also join a social media site to get better suggestions and support from each other. © 2014 ASIS&T.",2014,Journal of the Association for Information Science and Technology,8,understanding specific pattern @ knowledge of selfdisclosing health information could support public health surveillance and healthcare @ @ study aimed to develop @ analytical framework to identify selfdisclosing health information @ unusual message on web forum by leveraging advanced text-mining technique @ to demonstrate @ performance of @ proposed analytical framework @ conducted @ experimental study on major human immunodeficiency virus @ hiv @ acquired immune deficiency syndrome @ aid @ forum in taiwan @ @ experimental @ @ @ @ classification accuracy increased significantly @ up to @ @ @ @ feature selected by @ information gain technique @ @ @ @ @ @ importance of adopting domain-specific feature in analyzing unusual message on web forum @ @ study ha practical implication @ @ prevention and support of hiv aid healthcare @ @ example public health agency @ re-allocate resource and deliver service to people @ need help via social medium site @ in addition individual @ @ join a social medium site to get better suggestion and support @ @ @ @ asis t @ 
2814,A graph-based multi-level linguistic representation for document understanding,"Document understanding goal requires discovery of meaningful patterns in text, which in turn requires analyzing documents and extracting information useful for a purpose. The documents to be analyzed are expected to be represented in some way. It is true that different representations of the same piece of text might have different information extraction outcomes. Therefore, it is very important to propose a reliable text representation schema that may incorporate as many features as possible, and at the same time provides use of efficient document understanding algorithms. In this paper, we propose a graph-based representation of textual documents that employs different levels of formal representation of natural language. This schema takes into account different linguistic levels, such as lexical, morphological, syntactical and semantics. The representation schema proposed is accompanied with a proposal for a technique which allows to extract useful text patterns based on the idea of minimum paths in the graph. The efficiency of the representation schema proposed has been tested in one case of study (Question-Answering for machine Reading Evaluation - QA4MRE), and the results of experiments carried in it, are described. The results obtained show that the proposed graph-based multi-level linguistic representation schema may be successfully used in the broader framework of document understanding. © 2013 Elsevier B.V. All rights reserved.",2014,Pattern Recognition Letters,13,document understanding goal requires discovery of meaningful pattern in text @ in turn requires analyzing document and extracting information useful @ a purpose @ @ document to @ analyzed @ expected to @ represented in some way @ @ is true @ different representation of @ @ piece of text might @ different information extraction outcome @ therefore @ is @ important to propose a reliable text representation schema @ may incorporate a many feature a possible and at @ @ time provides use of efficient document understanding algorithm @ in @ @ @ propose a graph-based representation of textual document @ employ different level of formal representation of natural language @ @ schema take @ account different linguistic level @ a lexical morphological syntactical and semantics @ @ representation schema proposed is accompanied @ a proposal @ a technique @ allows to extract useful text pattern based on @ idea of minimum path in @ graph @ @ efficiency of @ representation schema proposed ha @ tested in @ case of study @ question-answering @ machine reading evaluation qa mre @ and @ @ of experiment carried in @ @ described @ @ @ obtained @ @ @ proposed graph-based multi-level linguistic representation schema may @ successfully used in @ broader framework of document understanding @ @ b @ v @ @ right reserved @ 
2817,Coordinate relationship extraction based on ontology from Chinese corpus,"An important problem in text mining is the automatic extraction of relations. The study of extracting semantic relations between event semantic chunks from sentences is rarely involved in the current research. This paper presents a metric theory on identifying semantic relations based on matching the degree of the deictic word and the similarity of event semantic chunk, and discusses the calculation of the similarities of concept and event semantic tree. Finally, a novel method based on ontology is proposed to extract the coordinate relation from sentences. This method explores the effect of extracting coordinate relations for deictic words of the coordinate relation and none of deictic word. This paper provides a contrast with the experimental comparison between Ontology-based, Naïve Bayesian (NB) and Support Vector Machine (SVM). The overall performance of Ontology-based method is tested by selecting randomly 100 sentences of coordinate relation and 600 sentences of non-coordinate relation from data corpus. This Experimental data shows that the method is effective. © 2014 Binary Information Press.",2014,Journal of Computational Information Systems,0,@ important problem in text mining is @ automatic extraction of relation @ @ study of extracting semantic relation @ event semantic chunk @ sentence is rarely involved in @ current research @ @ @ @ a metric theory on identifying semantic relation based on matching @ degree of @ deictic word and @ similarity of event semantic chunk and discus @ calculation of @ similarity of concept and event semantic tree @ finally a novel method based on ontology is proposed to extract @ coordinate relation @ sentence @ @ method explores @ effect of extracting coordinate relation @ deictic word of @ coordinate relation and none of deictic word @ @ @ provides a contrast @ @ experimental comparison @ ontology-based naïve bayesian @ nb @ and support vector machine @ svm @ @ @ overall performance of ontology-based method is tested by selecting randomly sentence of coordinate relation and sentence of non-coordinate relation @ data corpus @ @ experimental data @ @ @ method is effective @ binary information @ @ 
2820,Predicting self-monitoring skills using textual posts on Facebook,"The popularity of the social networking site Facebook (FB) has grown unprecedented during the past five years. The research question investigated is whether posts on FB would also be applicable for the prediction of users' psychological traits such as self-monitoring (SM) skill that is supposed to be linked with users' expression behavior in the online environment. We present a model to evaluate the relationship between the posts and SM skills. The aim of this study is twofold: first, to evaluate the quality of responses to the Snyder's Self-Monitoring Questionnaire (1974) collected via the Internet; and secondly, to explore the textual features of the posts in different SM-level groups. The prediction of posts resulted in an approximate 60% accuracy compared with the classification made by Snyder's SM scale. The variable ""family"" was found the most significant predictor in structured textual analysis via Linguistic Inquiry and Word Count (LIWC). The emoticons and Internet slangs were extracted as the most robust classifiers in the unstructured textual analysis. We concluded that the textual posts on the FB Wall could partially predict the users' SM skills. Besides, we recommend that researchers always check the validity of Internet data using the methodology presented here to ensure the data is valid before being processed. © 2014 Elsevier B.V. All rights reserved.",2014,Computers in Human Behavior,31,@ popularity of @ social networking site facebook @ fb @ ha grown unprecedented @ @ past five year @ @ research question investigated is whether post on fb would @ @ applicable @ @ prediction of user @ psychological trait @ a self-monitoring @ sm @ skill @ is supposed to @ linked @ user @ expression behavior in @ online environment @ @ @ a model to evaluate @ relationship @ @ post and sm skill @ @ aim of @ study is twofold @ first to evaluate @ quality of response to @ snyder @ s self-monitoring questionnaire @ @ collected via @ internet @ and secondly to explore @ textual feature of @ post in different sm-level group @ @ prediction of post resulted in @ approximate accuracy compared @ @ classification made by snyder @ s sm scale @ @ variable @ family @ wa found @ @ significant predictor in structured textual analysis via linguistic inquiry and word count @ liwc @ @ @ emoticon and internet slang @ extracted a @ @ robust classifier in @ unstructured textual analysis @ @ concluded @ @ textual post on @ fb wall could partially predict @ user @ sm skill @ besides @ recommend @ researcher always check @ validity of internet data @ @ methodology presented @ to ensure @ data is valid @ @ processed @ @ b @ v @ @ right reserved @ 
2828,Pinpointing needles in giant haystacks: Use of text mining to reduce impractical screening workload in extremely large scoping reviews,"In scoping reviews, boundaries of relevant evidence may be initially fuzzy, with refined conceptual understanding of interventions and their proposed mechanisms of action an intended output of the scoping process rather than its starting point. Electronic searches are therefore sensitive, often retrieving very large record sets that are impractical to screen in their entirety. This paper describes methods for applying and evaluating the use of text mining (TM) technologies to reduce impractical screening workload in reviews, using examples of two extremely large-scale scoping reviews of public health evidence (choice architecture (CA) and economic environment (EE)). Electronic searches retrieved >800,000 (CA) and >1 million (EE) records. TM technologies were used to prioritise records for manual screening. TM performance was measured prospectively. TM reduced manual screening workload by 90% (CA) and 88% (EE) compared with conventional screening (absolute reductions of ≈430000 (CA) and ≈378000 (EE) records). This study expands an emerging corpus of empirical evidence for the use of TM to expedite study selection in reviews. By reducing screening workload to manageable levels, TM made it possible to assemble and configure large, complex evidence bases that crossed research discipline boundaries. These methods are transferable to other scoping and systematic reviews incorporating conceptual development or explanatory dimensions. © 2013 The Authors. Research Synthesis Methods published by John Wiley & Sons, Ltd. Copyright © 2014 John Wiley & Sons, Ltd.",2014,Research Synthesis Methods,74,in scoping review boundary of relevant evidence may @ initially fuzzy @ refined conceptual understanding of intervention and @ proposed mechanism of action @ intended output of @ scoping process rather @ @ starting point @ electronic search @ therefore sensitive often retrieving @ @ record set @ @ impractical to screen in @ entirety @ @ @ describes method @ applying and evaluating @ use of text mining @ tm @ technology to reduce impractical screening workload in review @ example of @ extremely large-scale scoping review of public health evidence @ choice architecture @ ca @ and economic environment @ ee @ @ @ electronic search retrieved @ ca @ and million @ ee @ record @ tm technology @ used to prioritise record @ manual screening @ tm performance wa measured prospectively @ tm reduced manual screening workload by @ ca @ and @ ee @ compared @ conventional screening @ absolute reduction of @ ca @ and @ ee @ record @ @ @ study expands @ emerging corpus of empirical evidence @ @ use of tm to expedite study selection in review @ by reducing screening workload to manageable level tm made @ possible to assemble and configure @ complex evidence base @ crossed research discipline boundary @ @ method @ transferable to @ scoping and systematic review incorporating conceptual development @ explanatory dimension @ @ author @ research synthesis method published by john wiley son ltd @ @ john wiley son ltd @ 
2829,Techniques for identifying cross-disciplinary and 'hard-to-detect' evidence for systematic review,"Driven by necessity in our own complex review, we developed alternative systematic ways of identifying relevant evidence where the key concepts are generally not focal to the primary studies' aims and are found across multiple disciplines-that is, hard-to-detect evidence. Specifically, we sought to identify evidence on community engagement in public health interventions that aim to reduce health inequalities. Our initial search strategy used text mining to identify synonyms for the concept 'community engagement'. We conducted a systematic search for reviews on public health interventions, supplemented by searches of trials databases. We then used information in the reviews' evidence tables to gather more information about the included studies than was evident in the primary studies' own titles or abstracts. We identified 319 primary studies cited in reviews after full-text screening. In this paper, we retrospectively reflect on the challenges and benefits of the approach taken. We estimate that more than a quarter of the studies that were identified would have been missed by typical searching and screening methods. This identification strategy was highly effective and could be useful for reviews of broad research questions, or where the key concepts are unlikely to be the main focus of primary research. Copyright © 2014 John Wiley & Sons, Ltd.",2014,Research Synthesis Methods,9,driven by necessity in @ @ complex review @ developed alternative systematic way of identifying relevant evidence @ @ key concept @ generally not focal to @ primary study @ aim and @ found across multiple disciplines-that is hard-to-detect evidence @ specifically @ sought to identify evidence on community engagement in public health intervention @ aim to reduce health inequality @ @ initial search strategy used text mining to identify synonym @ @ concept @ community engagement @ @ @ conducted a systematic search @ review on public health intervention supplemented by search of trial database @ @ @ used information in @ review @ evidence table to gather more information @ @ included study @ wa evident in @ primary study @ @ title @ abstract @ @ identified primary study cited in review @ full-text screening @ in @ @ @ retrospectively reflect on @ challenge and benefit of @ approach taken @ @ estimate @ more @ a quarter of @ study @ @ identified would @ @ missed by typical searching and screening method @ @ identification strategy wa highly effective and could @ useful @ review of broad research question @ @ @ key concept @ unlikely to @ @ main focus of primary research @ @ john wiley son ltd @ 
2834,New formats and interfaces for multi-document news summarization and its evaluation,"News production, delivery, and consumption are increasing in ubiquity and speed, spreading over more software and hardware platforms, in particular mobile devices. This has led to an increasing interest in automated methods for multi-document summarization. The authors start this chapter with discussing several new alternatives for automated news summarization, with a particular focus on temporal text mining, graph-based methods, and graphical interfaces. Then they present automated and user-centric frameworks for cross-evaluating summarization methods that output different summary formats and describe the challenges associated with each evaluation framework. Based on the results of the user studies, the authors argue that it is crucial for effective summarization to integrate the user into sensemaking through usable, entertaining, and ultimately useful interactive summarization-plus-documentsearch interfaces. In particular, graph-based methods and interfaces may be a better preparation for people to concentrate on what is essential in a collection of texts, and thus may be a key to enhancing the summary evaluation process by replacing the ""one gold standard fits all"" approach with carefully designed user studies built upon a variety of summary representation formats. © 2014, IGI Global. All rights reserved.",2014,Innovative Document Summarization Techniques: Revolutionizing Knowledge Understanding,1,news production delivery and consumption @ increasing in ubiquity and speed spreading @ more software and hardware platform in particular mobile device @ @ ha led to @ increasing interest in automated method @ multi-document summarization @ @ author start @ chapter @ discussing several @ alternative @ automated news summarization @ a particular focus on temporal text mining graph-based method and graphical interface @ @ @ @ automated and user-centric framework @ cross-evaluating summarization method @ output different summary format and describe @ challenge associated @ @ evaluation framework @ based on @ @ of @ user study @ author argue @ @ is crucial @ effective summarization to integrate @ user @ sensemaking @ usable entertaining and ultimately useful interactive summarization-plus-documentsearch interface @ in particular graph-based method and interface may @ a better preparation @ people to concentrate on @ is essential in a collection of text and thus may @ a key to enhancing @ summary evaluation process by replacing @ @ @ gold standard fit @ @ approach @ carefully designed user study built upon a variety of summary representation format @ igi global @ @ right reserved @ 
2835,Interactive summaries by multi- pole information extraction for the archaeological domain,"Understanding and describing past or present societies is a complex task, as it involves a multi-faceted analysis of the norms, interactions, and evolutions that characterize them. This serves as the motivation for developing a tool, named Herodotus, aiming at supporting domain experts, such as historians or archaeologists, in the reasoning tasks over complex interactions characterizing a society in order to explain why some event took place and, possibly, to predict what could happen when some factors change. An important part of Herodotus is the text mining module that is responsible for the extraction of knowledge from written sources, such as books and scientific papers. Machines cannot always help users in dealing with natural language, because of the variety, ambiguity and non-rigidity that language shows in its use; they can only try to process information in a meaningful way for users. Information Extraction (IE) is the technology that pulls specific information from large volumes of unstructured texts and stores this information in structured forms. Users can then consult, compose, and analyze them. Domain-based IE should focus on an analysis of a specific state of affairs and, in this way, it can obtain more precise and detailed results. This helps domain experts to deal with the complexity of their everyday objects and environments. This chapter is centered on the Interactive Summary Extractor tool, whose scope is to organize, in a partially automated but substantially interactive way, text summaries for archaeological and historical documental sources. The texts so analyzed will help domain experts to collect data, viewing a synthesized version of it, compose such summaries in units of sense for the particular archaeological study or research that is in place, and so on. Summaries can then be modified, stored, retrieved and managed for later elaboration. © 2014, IGI Global. All rights reserved.",2014,Innovative Document Summarization Techniques: Revolutionizing Knowledge Understanding,0,understanding and describing past @ @ society is a complex task a @ involves a multi-faceted analysis of @ norm interaction and evolution @ characterize @ @ @ serf a @ motivation @ developing a tool named herodotus aiming at supporting domain expert @ a historian @ archaeologist in @ reasoning task @ complex interaction characterizing a society in order to explain @ some event took place and possibly to predict @ could happen @ some factor change @ @ important part of herodotus is @ text mining module @ is responsible @ @ extraction of knowledge @ written source @ a book and scientific @ @ machine cannot always help user in dealing @ natural language @ of @ variety ambiguity and non-rigidity @ language @ in @ use @ @ @ only try to process information in a meaningful way @ user @ information extraction @ ie @ is @ technology @ pull specific information @ @ volume of unstructured text and store @ information in structured form @ user @ @ consult compose and analyze @ @ domain-based ie @ focus on @ analysis of a specific state of affair and in @ way @ @ obtain more precise and detailed @ @ @ help domain expert to deal @ @ complexity of @ everyday object and environment @ @ chapter is centered on @ interactive summary extractor tool whose scope is to organize in a partially automated @ substantially interactive way text summary @ archaeological and historical documental source @ @ text @ analyzed @ help domain expert to collect data viewing a synthesized version of @ compose @ summary in unit of sense @ @ particular archaeological study @ research @ is in place and @ on @ summary @ @ @ modified stored retrieved and managed @ later elaboration @ igi global @ @ right reserved @ 
2840,Introducing a new scalable data-as-a-service cloud platform for enriching traditional text mining techniques by integrating ontology modelling and natural language processing,"A good deal of digital data produced in academia, commerce and industry is made up of a raw, unstructured text, such as Word documents, Excel tables, emails, web pages, etc., which are also often represented in a natural language. An important analytical task in a number of scientific and technological domains is to retrieve information from text data, aiming to get a deeper insight into the content represented by the data in order to obtain some useful, often not explicitly stated knowledge and facts, related to a particular domain of interest. The major challenge is the size, structural complexity, and frequency of the analysed text sets’ updates (i.e., the ‘big data’ aspect), which makes the use of traditional analysis techniques and tools impossible. We introduce an innovative approach to analyse unstructured text data. This allows for improving traditional data mining techniques by adopting algorithms from ontological domain modelling, natural language processing, and machine learning. The technique is inherently designed with parallelism in mind, which allows for high performance on large-scale Cloud computing infrastructures. © Springer-Verlag Berlin Heidelberg 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,a good deal of digital data produced in academia commerce and industry is made up of a raw unstructured text @ a word document excel table email web page etc @ @ @ @ often represented in a natural language @ @ important analytical task in a number of scientific and technological domain is to retrieve information @ text data aiming to get a deeper insight @ @ content represented by @ data in order to obtain some useful often not explicitly stated knowledge and fact related to a particular domain of interest @ @ major challenge is @ size structural complexity and frequency of @ analysed text set update @ i @ e @ @ big data aspect @ @ make @ use of traditional analysis technique and tool impossible @ @ introduce @ innovative approach to analyse unstructured text data @ @ allows @ improving traditional data mining technique by adopting algorithm @ ontological domain modelling natural language processing and machine learning @ @ technique is inherently designed @ parallelism in mind @ allows @ high performance on large-scale cloud computing infrastructure @ springer-verlag @ @ @ 
2841,Natural language processing and semantic technologies. The application on Brand Rain and Anpro21,This paper presents the application and results on research about natural language processing and semantic technologies in Brand Rain and Anpro21. The related projects are explained and the obtained benefits from the research on this new technologies developed are presented. All this research have been applied on the monitoring and reputation system of Brand Rain. © 2014 Sociedad Española para el Procesamiento del Lenguaje Natural.,2014,Procesamiento de Lenguaje Natural,0,@ @ @ @ application and @ on research @ natural language processing and semantic technology in brand rain and anpro @ @ related project @ explained and @ obtained benefit @ @ research on @ @ technology developed @ presented @ @ @ research @ @ applied on @ monitoring and reputation system of brand rain @ sociedad española para el procesamiento del lenguaje natural @ 
2844,Natural language processing pipelines to annotate BioC collections with an application to the NCBI disease corpus,"BioC is a new format and associated code libraries for sharing text and annotations. We have implemented BioC natural language preprocessing pipelines in two popular programming languages: C++ and Java. The current implementations interface with the well-known MedPost and Stanford natural language processing tool sets. The pipeline functionality includes sentence segmentation, tokenization, part-of-speech tagging, lemmatization and sentence parsing. These pipelines can be easily integrated along with other BioC programs into any BioC compliant text mining systems. As an application, we converted the NCBI disease corpus to BioC format, and the pipelines have successfully run on this corpus to demonstrate their functionality. Code and data can be downloaded from http://bioc.sourceforge.net.",2014,Database,3,bioc is a @ format and associated code library @ sharing text and annotation @ @ @ implemented bioc natural language preprocessing pipeline in @ popular programming language @ c and java @ @ current implementation interface @ @ well-known medpost and stanford natural language processing tool set @ @ pipeline functionality includes sentence segmentation tokenization part-of-speech tagging lemmatization and sentence parsing @ @ pipeline @ @ easily integrated along @ @ bioc program @ @ bioc compliant text mining system @ a @ application @ converted @ ncbi disease corpus to bioc format and @ pipeline @ successfully run on @ corpus to demonstrate @ functionality @ code and data @ @ downloaded @ http @ bioc @ sourceforge @ net @ 
2847,TKG: A graph-based approach to extract keywords from tweets,"Twitter is a microblog service that generates a huge amount of textual content daily. All this content needs to be explored by means of text mining, natural language processing, information retrieval, and other techniques. In this context, automatic keyword extraction is a task of great usefulness. A fundamental step in text mining techniques consists of building a model for text representation. This paper proposes a keyword extraction method for tweet collections that represents texts as graphs and applies centrality measures for finding the relevant vertices (keywords). The proposal is applied to two tweet collections of Brazilian TV shows and its results are compared to those of TFIDF and KEA. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,3,twitter is a microblog service @ generates a huge amount of textual content daily @ @ @ content need to @ explored by mean of text mining natural language processing information retrieval and @ technique @ in @ context automatic keyword extraction is a task of great usefulness @ a fundamental step in text mining technique consists of building a model @ text representation @ @ @ proposes a keyword extraction method @ tweet collection @ represents text a graph and applies centrality measure @ finding @ relevant vertex @ keywords @ @ @ proposal is applied to @ tweet collection of brazilian tv @ and @ @ @ compared to @ of tfidf and kea @ @ international publishing switzerland @ 
2849,"Biomedical text mining: State-of-the-art, open problems and future challenges","Text is a very important type of data within the biomedical domain. For example, patient records contain large amounts of text which has been entered in a non-standardized format, consequently posing a lot of challenges to processing of such data. For the clinical doctor the written text in the medical findings is still the basis for decision making-neither images nor multimedia data. However, the steadily increasing volumes of unstructured information need machine learning approaches for data mining, i.e. text mining. This paper provides a short, concise overview of some selected text mining methods, focusing on statistical methods, i.e. Latent Semantic Analysis, Probabilistic Latent Semantic Analysis, Latent Dirichlet Allocation, Hierarchical Latent Dirichlet Allocation, Principal Component Analysis, and Support Vector Machines, along with some examples from the biomedical domain. Finally, we provide some open problems and future challenges, particularly from the clinical domain, that we expect to stimulate future research. © Springer-Verlag Berlin Heidelberg 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),47,text is a @ important type of data within @ biomedical domain @ @ example patient record contain @ amount of text @ ha @ entered in a non-standardized format consequently posing a lot of challenge to processing of @ data @ @ @ clinical doctor @ written text in @ medical finding is still @ basis @ decision making-neither image @ multimedia data @ however @ steadily increasing volume of unstructured information need machine learning approach @ data mining i @ e @ text mining @ @ @ provides a short concise overview of some selected text mining method focusing on statistical method i @ e @ latent semantic analysis probabilistic latent semantic analysis latent dirichlet allocation hierarchical latent dirichlet allocation principal component analysis and support vector machine along @ some example @ @ biomedical domain @ finally @ provide some open problem and future challenge particularly @ @ clinical domain @ @ expect to stimulate future research @ springer-verlag @ @ @ 
2853,"Retrieval and discovery of cell cycle literature and proteins by means of machine learning, text mining and network analysis","The cell cycle is one of the most important biological processes, being studied intensely by experimental as well as bioinformatics means. A considerable amount of literature provides relevant descriptions of proteins involved in this complex process. These proteins are often key to understand cellular alterations encountered in pathological conditions such as abnormal cell growth. The authors explored the use of text mining strategies to improve the retrieval of relevant articles and individual sentences for this topic. Moreover information extraction and text mining was used to detect and rank automatically Arabidopsis proteins important for the cell cycle. The obtained results were evaluated using independent data collections and compared to keyword-based strategies. The obtained results indicate that the use of machine learning methods can improve the sensitivity compared to term-co-occurrence, although with considerable differences when using abstracts and full text articles as input. At the level of document triage the recall ranges for abstracts from around 16% for keyword indexing, 37% for a sentence SVM classifier to 57% for SVM abstract classifier. In case of full text data, keyword and cell cycle phrase indexing obtained a recall of 42% and 55% respectively compared to 94% reached by a sentence classifier. In case of the cell cycle protein detection, the cell cycle keyword-protein co-occurrence strategy had a recall of 52% for abstracts and 70% for full text while a protein mentioning sentence classifier obtained a recall of over 83% for abstracts and 79% for full text. The generated cell cycle term co-occurrence statistics and SVM confidence scores for each protein were explored to rank proteins and filter a protein network in order to derive a topic specific subnetwork. All the generated protein cell cycle scores together with a global protein interaction and gene regulation network for Arabidopsis are available at: http://zope.bioinfo.cnio.es/cellcyle addmaterial. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,2,@ cell cycle is @ of @ @ important biological process @ studied intensely by experimental a well a bioinformatics mean @ a considerable amount of literature provides relevant description of protein involved in @ complex process @ @ protein @ often key to understand cellular alteration encountered in pathological condition @ a abnormal cell growth @ @ author explored @ use of text mining strategy to improve @ retrieval of relevant article and individual sentence @ @ topic @ moreover information extraction and text mining wa used to detect and rank automatically arabidopsis protein important @ @ cell cycle @ @ obtained @ @ evaluated @ independent data collection and compared to keyword-based strategy @ @ obtained @ indicate @ @ use of machine learning method @ improve @ sensitivity compared to term-co-occurrence although @ considerable difference @ @ abstract and full text article a input @ at @ level of document triage @ recall range @ abstract @ around @ keyword indexing @ a sentence svm classifier to @ svm abstract classifier @ in case of full text data keyword and cell cycle phrase indexing obtained a recall of and respectively compared to reached by a sentence classifier @ in case of @ cell cycle protein detection @ cell cycle keyword-protein co-occurrence strategy @ a recall of @ abstract and @ full text @ a protein mentioning sentence classifier obtained a recall of @ @ abstract and @ full text @ @ generated cell cycle term co-occurrence statistic and svm confidence score @ @ protein @ explored to rank protein and filter a protein network in order to derive a topic specific subnetwork @ @ @ generated protein cell cycle score together @ a global protein interaction and gene regulation network @ arabidopsis @ available at @ http @ zope @ bioinfo @ cnio @ e cellcyle addmaterial @ @ international publishing switzerland @ 
2856,Insight to hyponymy lexical relation extraction in the patent genre versus other text genres,"Due to the large amount of available patent data, it is no longer feasible for industry actors to manually create their own terminology lists and ontologies. Furthermore, domain specific the-sauruses are rarely accessible to the research community. In this paper we present extraction of hyponymy lexical relations conducted on patent text using lexico-syntactic patterns. We explore the lexico-syntactic patterns. Since this kind of extraction involves Natural Language Processing we also compare the extractions made with and without domain adaptation of the extraction pipeline. We also deployed our modified extraction method to other text genres in order to demonstrate the method's portability to other text domains. From our study we conclude that the lexico-syntactic patterns are portable to domain specific text genre such as the patent genre. We observed that general Natural Language Processing tools, when not adapted to the patent genre, reduce the amount of correct hyponymy lexical relation extractions and increase the number of incomplete extractions. This was also observed in other domain specific text genres. Copyright © 2014 for the individual papers by the papers' authors.",2014,CEUR Workshop Proceedings,0,due to @ @ amount of available patent data @ is no longer feasible @ industry actor to manually create @ @ terminology list and ontology @ furthermore domain specific the-sauruses @ rarely accessible to @ research community @ in @ @ @ @ extraction of hyponymy lexical relation conducted on patent text @ lexico-syntactic pattern @ @ explore @ lexico-syntactic pattern @ since @ kind of extraction involves natural language processing @ @ compare @ extraction made @ and without domain adaptation of @ extraction pipeline @ @ @ deployed @ modified extraction method to @ text genre in order to demonstrate @ method @ s portability to @ text domain @ @ @ study @ conclude @ @ lexico-syntactic pattern @ portable to domain specific text genre @ a @ patent genre @ @ observed @ general natural language processing tool @ not adapted to @ patent genre reduce @ amount of correct hyponymy lexical relation extraction and increase @ number of incomplete extraction @ @ wa @ observed in @ domain specific text genre @ @ @ @ individual @ by @ @ @ author @ 
2857,Augmented vector space model for passage intention classification in Chinese agricultural prescription documents,"Passage-level natural language processing has received increasing attentions in recent years. This paper addresses the problem of passage intention classification for agricultural prescription documents. The proposed approach is based on supervised learning. In order to give a simple yet effective representation for passages in agricultural prescription documents, we augment the traditional Vector Space Model (VSM) widely used in text mining by 2-gram features. Besides, a set of third-party features with priori high class-discriminability are extracted using a third-party resource. We give comprehensive experimental comparisons of various learning algorithms and feature combinations using real world data set. The experimental results indicate that our approach offers promising classification performance. Copyright © 2014 Binary Information Press.",2014,Journal of Computational Information Systems,0,passage-level natural language processing ha received increasing attention in recent year @ @ @ address @ problem of passage intention classification @ agricultural prescription document @ @ proposed approach is based on supervised learning @ in order to give a simple yet effective representation @ passage in agricultural prescription document @ augment @ traditional vector space model @ vsm @ widely used in text mining by gram feature @ besides a set of third-party feature @ priori high class-discriminability @ extracted @ a third-party resource @ @ give comprehensive experimental comparison of various learning algorithm and feature combination @ real world data set @ @ experimental @ indicate @ @ approach offer promising classification performance @ @ binary information @ @ 
2870,Text mining for open domain semi-supervised Semantic Role Labeling,"The identification and classification of some circumstance semantic roles like Location, Time, Manner and Direction, a task of Semantic Role Labeling (SRL), plays a very important role in building text understanding applications. However, the performance of the current SRL systems on those roles is often very poor, especially when the systems are applied on domains other than the ones they are trained on. We present a method to build open domain SRL system, in which the training data is expanded by replacing its predicates by words in the testing domain. A language model, which is considered as a text mining technique, and some linguistic resources are used to select from the vocabulary of the testing domain the best words for the replacement. We apply our method on the case study of transferring a semantic role labeler trained on the news domain to the children story domain. It gives us valuable improvements over the four circumstance semantic roles Location, Time, Manner and Direction. Copyright © by the paper's authors.",2014,CEUR Workshop Proceedings,2,@ identification and classification of some circumstance semantic role like location time manner and direction a task of semantic role labeling @ srl @ play a @ important role in building text understanding application @ however @ performance of @ current srl system on @ role is often @ poor especially @ @ system @ applied on domain @ @ @ @ @ @ trained on @ @ @ a method to build open domain srl system in @ @ training data is expanded by replacing @ predicate by word in @ testing domain @ a language model @ is considered a a text mining technique and some linguistic resource @ used to select @ @ vocabulary of @ testing domain @ best word @ @ replacement @ @ apply @ method on @ case study of transferring a semantic role labeler trained on @ news domain to @ child story domain @ @ give u valuable improvement @ @ four circumstance semantic role location time manner and direction @ @ by @ @ @ s author @ 
2871,Text mining approach for knowledge extraction in Sahîh Al-Bukhari,"The areas of information retrieval (IR) and information extraction (IE) are the subject of active research for several years in the community of Artificial Intelligence and Text Mining. With the appearance of large textual corpora in the recent years, we felt the need to integrate modules for information extraction in the existing information retrieval systems. The processing of large textual corpora leads needs that are situated at the border of information extraction and information retrieval areas. Our work in this paper, focus on the extraction of the surface information, i.e. information that not requires complex linguistic processing to be categorized. The goal is to detect and extract passages or sequences of words containing relevant information from the prophetic narrations texts. We propose Finite state transducers-based system that solves successively the problem of texts comprehension. Experimental evaluation results demonstrated that our approach is feasible. Our system achieved encouraging precision and recall rates, the overall precision and recall are 71% and 39% respectively. © 2013 Elsevier Ltd. All rights reserved.",2014,Computers in Human Behavior,21,@ area of information retrieval @ ir @ and information extraction @ ie @ @ @ subject of active research @ several year in @ community of artificial intelligence and text mining @ @ @ appearance of @ textual corpus in @ recent year @ felt @ need to integrate module @ information extraction in @ existing information retrieval system @ @ processing of @ textual corpus lead need @ @ situated at @ border of information extraction and information retrieval area @ @ work in @ @ focus on @ extraction of @ surface information i @ e @ information @ not requires complex linguistic processing to @ categorized @ @ goal is to detect and extract passage @ sequence of word containing relevant information @ @ prophetic narration text @ @ propose finite state transducers-based system @ solves successively @ problem of text comprehension @ experimental evaluation @ demonstrated @ @ approach is feasible @ @ system achieved encouraging precision and recall rate @ overall precision and recall @ and respectively @ @ ltd @ @ right reserved @ 
2873,An integrated and efficient approach to measure semantic similarity between short sentences and paragraphs,"Sentence similarity measures play an important role in text-related research and applications in areas such as Text Mining, Natural Language processing and Information Retrieval system. Similarity calculation for the short texts and paragraphs improve the retrieval effectiveness of the system. It is a complex concept which has been widely discussed in various domains. The target approach is to find that how the given text is semantically similar to another text. The proposed technique provides an efficient method to measure similarity for sentences and two short paragraphs based on the Similarity score. The scores indicate the Similarity at semantic level between two input text segments.",2014,Advances in Modelling and Analysis B,0,sentence similarity measure play @ important role in text-related research and application in area @ a text mining natural language processing and information retrieval system @ similarity calculation @ @ short text and paragraph improve @ retrieval effectiveness of @ system @ @ is a complex concept @ ha @ widely discussed in various domain @ @ target approach is to find @ @ @ given text is semantically similar to another text @ @ proposed technique provides @ efficient method to measure similarity @ sentence and @ short paragraph based on @ similarity score @ @ score indicate @ similarity at semantic level @ @ input text segment @ 
2874,Gaining customer knowledge in low cost airlines through text mining,"Purpose-The purpose of this paper is to study the consumer opinion towards the low-cost airlines or low-cost carriers (LCCs) (these two terms are used interchangeably) industry in Malaysia to better understand consumers' needs and to provide better services. Sentiment analysis is undertaken in revealing current customers' satisfaction level towards low-cost airlines. Design/methodology/approach-About 10,895 tweets (data collected for two and a half months) are analysed. Text mining techniques are used during data pre-processing and a mixture of statistical techniques are used to segment the customers' opinion. Findings-The results with two different sentiment algorithms show that there is more positive than negative polarity across the different algorithms. Clustering results show that both K-Means and spherical K-Means algorithms delivered similar results and the four main topics that are discussed by the consumers on Twitter are customer service, LCCs tickets promotions, flight cancellations and delays and post-booking management. Practical implications-Gaining knowledge of customer sentiments as well as improvements on the four main topics discussed in this study, i.e. customer service, LCCs tickets promotions, flight cancellations or delays and post-booking management will help LCCs to attract more customers and generate more profits. Originality/value-This paper provides useful insights on customers' sentiments and opinions towards LCCs by utilizing social media information. © Emerald Group Publishing Limited.",2014,Industrial Management and Data Systems,68,purpose-the purpose of @ @ is to study @ consumer opinion towards @ low-cost airline @ low-cost carrier @ lccs @ @ @ @ term @ used interchangeably @ industry in malaysia to better understand consumer @ need and to provide better service @ sentiment analysis is undertaken in revealing current customer @ satisfaction level towards low-cost airline @ design methodology approach-about tweet @ data collected @ @ and a half month @ @ analysed @ text mining technique @ used @ data pre-processing and a mixture of statistical technique @ used to segment @ customer @ opinion @ findings-the @ @ @ different sentiment algorithm @ @ @ is more positive @ negative polarity across @ different algorithm @ clustering @ @ @ @ k-means and spherical k-means algorithm delivered similar @ and @ four main topic @ @ discussed by @ consumer on twitter @ customer service lccs ticket promotion flight cancellation and delay and post-booking management @ practical implications-gaining knowledge of customer sentiment a well a improvement on @ four main topic discussed in @ study i @ e @ customer service lccs ticket promotion flight cancellation @ delay and post-booking management @ help lccs to attract more customer and generate more profit @ originality value-this @ provides useful insight on customer @ sentiment and opinion towards lccs by utilizing social medium information @ emerald group publishing limited @ 
2877,Large scale text mining approaches for information retrieval and extraction,"The issues for Natural Language Processing and Information Retrieval have been studied for long time but the recent availability of very large resources (Web pages, digital documents.) and the development of statistical machine learning methods exploiting annotated texts (manual encoding by crowdsourcing is a new major way) have transformed these fields. This allows not limiting these approaches to highly specialized domains and reducing the cost of their implementation. For this chapter, our aim is to present some popular text-mining statistical approaches for information retrieval and information extraction and to discuss the practical limits of actual systems that introduce challenges for future. © 2014 Springer International Publishing Switzerland.",2014,Studies in Computational Intelligence,4,@ issue @ natural language processing and information retrieval @ @ studied @ long time @ @ recent availability of @ @ resource @ web page digital document @ @ and @ development of statistical machine learning method exploiting annotated text @ manual encoding by crowdsourcing is a @ major way @ @ transformed @ field @ @ allows not limiting @ approach to highly specialized domain and reducing @ cost of @ implementation @ @ @ chapter @ aim is to @ some popular text-mining statistical approach @ information retrieval and information extraction and to discus @ practical limit of actual system @ introduce challenge @ future @ @ international publishing switzerland @ 
2881,Word vector modeling for sentiment analysis of product reviews,"Recent years, an amount of product reviews on the internet have become an important source of information for potential customers. These reviews do help to research products or services before making purchase decisions. Thus, sentiment analysis of product reviews has become a hot issue in the field of natural language processing and text mining. Considering good performances of unsupervised neural network language models in a wide range of natural language processing tasks, a semi-supervised deep learning model has been proposed for sentiment analysis. The model introduces supervised sentiment labels into traditional neural network language models. It enhances expression ability of sentiment information as well as semantic information in word vectors. Experiments on NLPCC2014 product review datasets demonstrate that our method outperforms the traditional methods and methods of other teams. © Springer-Verlag Berlin Heidelberg 2014.",2014,Communications in Computer and Information Science,13,recent year @ amount of product review on @ internet @ become @ important source of information @ potential customer @ @ review @ help to research product @ service @ making purchase decision @ thus sentiment analysis of product review ha become a hot issue in @ field of natural language processing and text mining @ considering good performance of unsupervised neural network language model in a wide range of natural language processing task a semi-supervised deep learning model ha @ proposed @ sentiment analysis @ @ model introduces supervised sentiment label @ traditional neural network language model @ @ enhances expression ability of sentiment information a well a semantic information in word vector @ experiment on nlpcc product review datasets demonstrate @ @ method outperforms @ traditional method and method of @ team @ springer-verlag @ @ @ 
2885,How to combine text-mining methods to validate induced verb-object relations?,"This paper describes methods using Natural Language Processing ap- proaches to extract and validate induced syntactic relations (here restricted to the Verb-Object relation). These methods use a syntactic parser and a semantic close- ness measure to extract such relations. Then, their validation is based on two dif- ferent techniques: A Web Validation system on one part, then a Semantic-Vector- based approach, and finally different combinations of both techniques in order to rank induced Verb-Object relations. The Semantic Vector approach is a Roget-based method which computes a syntactic relation as a vector. Web Validation uses a search engine to determine the relevance of a syntactic relation according to its popularity. An experimental protocol is set up to judge automatically the relevance of the sorted induced relations. We finally apply our approach on a French corpus of news by using ROC Curves to evaluate the results.",2014,Computer Science and Information Systems,4,@ @ describes method @ natural language processing ap proaches to extract and validate induced syntactic relation @ @ restricted to @ verb-object relation @ @ @ method use a syntactic parser and a semantic close ness measure to extract @ relation @ @ @ validation is based on @ dif ferent technique @ a web validation system on @ part @ a semantic-vector based approach and finally different combination of @ technique in order to rank induced verb-object relation @ @ semantic vector approach is a roget-based method @ computes a syntactic relation a a vector @ web validation us a search engine to determine @ relevance of a syntactic relation according to @ popularity @ @ experimental protocol is set up to judge automatically @ relevance of @ sorted induced relation @ @ finally apply @ approach on a french corpus of news by @ roc curve to evaluate @ @ @ 
2894,Commtrust: Computing multi-dimensional trust by mining e-commerce feedback comments,"Reputation-based trust models are widely used in e-commerce applications, and feedback ratings are aggregated to compute sellers' reputation trust scores. The 'all good reputation' problem, however, is prevalent in current reputation systems-reputation scores are universally high for sellers and it is difficult for potential buyers to select trustworthy sellers. In this paper, based on the observation that buyers often express opinions openly in free text feedback comments, we propose CommTrust for trust evaluation by mining feedback comments. Our main contributions include: 1) we propose a multidimensional trust model for computing reputation scores from user feedback comments; and 2) we propose an algorithm for mining feedback comments for dimension ratings and weights, combining techniques of natural language processing, opinion mining, and topic modeling. Extensive experiments on eBay and Amazon data demonstrate that CommTrust can effectively address the 'all good reputation' issue and rank sellers effectively. To the best of our knowledge, our research is the first piece of work on trust evaluation by mining feedback comments. © 2013 IEEE.",2014,IEEE Transactions on Knowledge and Data Engineering,39,reputation-based trust model @ widely used in e-commerce application and feedback rating @ aggregated to compute seller @ reputation trust score @ @ @ @ good reputation @ problem however is prevalent in current reputation systems-reputation score @ universally high @ seller and @ is difficult @ potential buyer to select trustworthy seller @ in @ @ based on @ observation @ buyer often express opinion openly in free text feedback comment @ propose commtrust @ trust evaluation by mining feedback comment @ @ main contribution include @ @ @ propose a multidimensional trust model @ computing reputation score @ user feedback comment @ and @ @ propose @ algorithm @ mining feedback comment @ dimension rating and weight combining technique of natural language processing opinion mining and topic modeling @ extensive experiment on ebay and amazon data demonstrate @ commtrust @ effectively address @ @ @ good reputation @ issue and rank seller effectively @ to @ best of @ knowledge @ research is @ first piece of work on trust evaluation by mining feedback comment @ @ @ 
2895,Dream sentiment analysis using second order soft co-occurrences (SOSCO) and time course representations,"We describe a project undertaken by an interdisciplinary team combining researchers in sleep psychology and in Natural Language Processing/Machine Learning. The goal is sentiment analysis on a corpus containing short textual descriptions of dreams. Dreams are categorized in a four-level scale of positive and negative sentiments. We chose a four scale annotation to reflect the sentiment strength and simplicity at the same time. The approach is based on a novel representation, taking into account the leading themes of the dream and the sequential unfolding of associated sentiments during the dream. The dream representation is based on three combined parts, two of which are automatically produced from the description of the dream. The first part consists of co-occurrence vector representation of dreams in order to detect sentiment levels in the dream texts. Those vectors unlike the standard Bag-of-words model capture non-local relationships between meanings of word in a corpus. The second part introduces the dynamic representation that captures the sentimental changes throughout the progress of the dream. The third part is the self-reported assessment of the dream by the dreamer according to eight given attributes (self-assessment is different in many respects from the dream's sentiment classification). The three representations are subject to aggressive feature selection. Using an ensemble of classifiers on the combined 3-partite representation, the agreement between machine rating and the human judge scores on the four scales was 64 % which is in the range of human experts' consensus in that domain. The accuracy of the system was 14 % more than previous results on the same task. © Springer Science+Business Media New York 2013.",2014,Journal of Intelligent Information Systems,19,@ describe a project undertaken by @ interdisciplinary team combining researcher in sleep psychology and in natural language processing machine learning @ @ goal is sentiment analysis on a corpus containing short textual description of dream @ dream @ categorized in a four-level scale of positive and negative sentiment @ @ chose a four scale annotation to reflect @ sentiment strength and simplicity at @ @ time @ @ approach is based on a novel representation taking @ account @ leading theme of @ dream and @ sequential unfolding of associated sentiment @ @ dream @ @ dream representation is based on three combined part @ of @ @ automatically produced @ @ description of @ dream @ @ first part consists of co-occurrence vector representation of dream in order to detect sentiment level in @ dream text @ @ vector unlike @ standard bag-of-words model capture non-local relationship @ meaning of word in a corpus @ @ second part introduces @ dynamic representation @ capture @ sentimental change throughout @ progress of @ dream @ @ third part is @ self-reported assessment of @ dream by @ dreamer according to eight given attribute @ self-assessment is different in many respect @ @ dream @ s sentiment classification @ @ @ three representation @ subject to aggressive feature selection @ @ @ ensemble of classifier on @ combined partite representation @ agreement @ machine rating and @ human judge score on @ four scale wa @ is in @ range of human expert @ consensus in @ domain @ @ accuracy of @ system wa more @ previous @ on @ @ task @ @ science @ medium @ york @ 
2896,The role of linked data in content selection,"This paper explores the appropriateness of utilizing Linked Data as a knowledge source for content selection. Content Selection is a crucial subtask in Natural Language Generation which has the function of determining the relevancy of contents from a knowledge source based on a communicative goal. The recent online era has enabled us to accumulate extensive amounts of generic online knowledge some of which has been made available as structured knowledge sources for computational natural language processing purposes. This paper proposes a model for content selection by utilizing a generic structured knowledge source, DBpedia, which is a replica of the unstructured counterpart, Wikipedia. The proposed model uses log likelihood to rank the contents from DBpedia Linked Data for relevance to a communicative goal. We performed experiments using DBpedia as the Linked Data resource using two keyword datasets as communicative goals. To optimize parameters we used keywords extracted from QALD-2 training dataset and QALD-2 testing dataset is used for the testing. The results was evaluated against the verbatim based selection strategy. The results showed that our model can perform 18.03% better than verbatim selection. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11,@ @ explores @ appropriateness of utilizing linked data a a knowledge source @ content selection @ content selection is a crucial subtask in natural language generation @ ha @ function of determining @ relevancy of content @ a knowledge source based on a communicative goal @ @ recent online era ha enabled u to accumulate extensive amount of generic online knowledge some of @ ha @ made available a structured knowledge source @ computational natural language processing purpose @ @ @ proposes a model @ content selection by utilizing a generic structured knowledge source dbpedia @ is a replica of @ unstructured counterpart wikipedia @ @ proposed model us log likelihood to rank @ content @ dbpedia linked data @ relevance to a communicative goal @ @ performed experiment @ dbpedia a @ linked data resource @ @ keyword datasets a communicative goal @ to optimize parameter @ used keywords extracted @ qald training dataset and qald testing dataset is used @ @ testing @ @ @ wa evaluated @ @ verbatim based selection strategy @ @ @ showed @ @ model @ perform @ better @ verbatim selection @ @ international publishing switzerland @ 
2902,The NASA automated requirements measurement tool: A reconstruction,"In the late 1990s the National Aeronautics and Space Administration (NASA) Software Assurance Technology Center (SATC) developed a tool to automatically analyze a requirements document and produce a detailed quality report. The report was based on statistical analysis of word frequencies at various structural levels of the document. The Automated Requirements Measurement (ARM) tool was further enhanced to include additional functionality such as custom definitions of quality indicators inputs for document analysis. By 2011 work on the ARM tool was discontinued. This paper describes the reverse-engineering and reproduction of the functionality of ARM. Recreating the functionality of this tool yielded valuable insight into certain quality metrics and provides a benchmark tool for future research. In addition to recreating and working with the ARM tool, this paper explores both existing and potential definitions of quality metrics in requirements specifications. Automated requirements analysis is a convergence of various fields of research, including text mining, quality analysis, and natural language processing. Informed by tangential areas of research in document understanding and data mining, recommendations are made for future areas of research and development in automated requirements analysis. © 2013 Springer-Verlag London.",2014,Innovations in Systems and Software Engineering,18,in @ late s @ national aeronautics and space administration @ nasa @ software assurance technology center @ satc @ developed a tool to automatically analyze a requirement document and produce a detailed quality report @ @ report wa based on statistical analysis of word frequency at various structural level of @ document @ @ automated requirement measurement @ arm @ tool wa @ enhanced to include additional functionality @ a custom definition of quality indicator input @ document analysis @ by work on @ arm tool wa discontinued @ @ @ describes @ reverse-engineering and reproduction of @ functionality of arm @ recreating @ functionality of @ tool yielded valuable insight @ certain quality metric and provides a benchmark tool @ future research @ in addition to recreating and working @ @ arm tool @ @ explores @ existing and potential definition of quality metric in requirement specification @ automated requirement analysis is a convergence of various field of research including text mining quality analysis and natural language processing @ informed by tangential area of research in document understanding and data mining recommendation @ made @ future area of research and development in automated requirement analysis @ springer-verlag london @ 
2903,Answering yes/no questions in legal bar exams,"The development of Question Answering (QA) systems has become important because it reveals research issues that require insight from a variety of disciplines, including Artificial Intelligence, Information Extraction, Natural Language Processing, and Psychology. Our goal here is to develop a QA approach to answer yes/no questions relevant to civil laws in legal bar exams. A bar examination is intended to determine whether a candidate is qualified to practice law in a given jurisdiction. We have found that the development of a QA system for this task provides insight into the challenges of formalizing reasoning about legal text, and about how to exploit advances in computational linguistics. We separate our QA approach into two steps. The first step is to identify legal documents relevant to the exam questions; the second step is to answer the questions by analyzing the relevant documents. In our initial approach described here, the first step has been already solved for us: the appropriate articles for each question have been identified by legal experts. So here, we focus on the second task, which can be considered as a form of Recognizing Textual Entailment (RTE), where input to the system is a question sentence and its corresponding civil law article(s), and the output is a binary answer: whether the question sentence is entailed from the article(s). We propose a hybrid method, which combines simple rules and an unsupervised learning model using deep linguistic features. We first construct a knowledge base for negation and antonym words for the legal domain. We then identify potential premise and conclusion components of input questions and documents, based on text patterns and separating commas. We further classify the questions into easy and difficult ones, and develop a two-phase method for answering yes/ no questions. We answer easy questions by negation/antonym detection. For more difficult questions, we adapt an unsupervised machine learning method based on morphological, syntactic, and lexical semantic analysis on identified premises and conclusions. This provides the basis to compare the semantic correlation between a question and a legal article. Our experimental results show reasonable performance, which improves the baseline system, and outperforms an SVM-based supervised machine learning model. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ development of question answering @ qa @ system ha become important @ @ reveals research issue @ require insight @ a variety of discipline including artificial intelligence information extraction natural language processing and psychology @ @ goal @ is to develop a qa approach to answer yes no question relevant to civil law in legal bar exam @ a bar examination is intended to determine whether a candidate is qualified to practice law in a given jurisdiction @ @ @ found @ @ development of a qa system @ @ task provides insight @ @ challenge of formalizing reasoning @ legal text and @ @ to exploit advance in computational linguistics @ @ separate @ qa approach @ @ step @ @ first step is to identify legal document relevant to @ exam question @ @ second step is to answer @ question by analyzing @ relevant document @ in @ initial approach described @ @ first step ha @ already solved @ u @ @ appropriate article @ @ question @ @ identified by legal expert @ @ @ @ focus on @ second task @ @ @ considered a a form of recognizing textual entailment @ rte @ @ input to @ system is a question sentence and @ corresponding civil law article @ s @ and @ output is a binary answer @ whether @ question sentence is entailed @ @ article @ s @ @ @ propose a hybrid method @ combine simple rule and @ unsupervised learning model @ deep linguistic feature @ @ first construct a knowledge base @ negation and antonym word @ @ legal domain @ @ @ identify potential premise and conclusion component of input question and document based on text pattern and separating comma @ @ @ classify @ question @ easy and difficult @ and develop a two-phase method @ answering yes no question @ @ answer easy question by negation antonym detection @ @ more difficult question @ adapt @ unsupervised machine learning method based on morphological syntactic and lexical semantic analysis on identified premise and conclusion @ @ provides @ basis to compare @ semantic correlation @ a question and a legal article @ @ experimental @ @ reasonable performance @ improves @ baseline system and outperforms @ svm-based supervised machine learning model @ @ international publishing switzerland @ 
2904,Assessing box office performance using movie scripts: A kernel-based approach,"We develop a methodology to predict box office performance of a movie at the point of green-lighting, when only its script and estimated production budget are available. We extract three levels of textual features (genre and content, semantics, and bag-of-words) from scripts using screenwriting domain knowledge, human input, and natural language processing techniques. These textual variables define a distance metric across scripts, which is then used as an input for a kernel-based approach to assess box office performance. We show that our proposed methodology predicts box office revenues more accurately (29 percent lower mean squared error (MSE)) compared to benchmark methods. © 1989-2012 IEEE.",2014,IEEE Transactions on Knowledge and Data Engineering,29,@ develop a methodology to predict box office performance of a movie at @ point of green-lighting @ only @ script and estimated production budget @ available @ @ extract three level of textual feature @ genre and content semantics and bag-of-words @ @ script @ screenwriting domain knowledge human input and natural language processing technique @ @ textual variable define a distance metric across script @ is @ used a @ input @ a kernel-based approach to ass box office performance @ @ @ @ @ proposed methodology predicts box office revenue more accurately @ percent lower mean squared error @ mse @ @ compared to benchmark method @ @ @ 
2911,A text based indexing system for mammographic image retrieval and classification,"In modern medical systems huge amount of text, words, images and videos are produced and stored in ad hoc databases. Medical community needs to extract precise information from that large amount of data. Currently ICT approaches do not provide a methodology for content-based medical images retrieval and classification. On the other hand, from the Internet of Things (IoT) perspective, the ICT medical data can be produced by several devices. Produced data complies with all Big Data features and constraints. The IoT guidelines put at the center of the system a new smart software to manage and transform Big Data in a new understanding form. This paper describes a text based indexing system for mammographic images retrieval and classification. The system deals with text (structured reports) and images (mammograms) mining and classification in a typical Department of Radiology. DICOM structured reports, containing free text for medical diagnosis, have been analyzed and labeled in order to classify the corresponding mammographic images. Information Retrieval process is based on some text manipulation techniques, such as light semantic analysis, stop-word removing, and light medical natural language processing. The system includes also a Search Engine module, based on a Bayes Naive Classifier. The experimental results provide interesting performance in terms of Specificity and Sensibility. Two more indexes have been computed in order to assess the system robustness: the Az (Area under ROC Curve) index and the σAz (Az standard error) index. The dataset is composed of healthy and pathological DICOM structured reports. Two use case scenarios are presented and described to prove the effectiveness of the proposed approach. © 2014 Elsevier B.V. All rights reserved.",2014,Future Generation Computer Systems,18,in modern medical system huge amount of text word image and video @ produced and stored in ad hoc database @ medical community need to extract precise information @ @ @ amount of data @ currently ict approach @ not provide a methodology @ content-based medical image retrieval and classification @ on @ @ hand @ @ internet of thing @ iot @ perspective @ ict medical data @ @ produced by several device @ produced data complies @ @ big data feature and constraint @ @ iot guideline put at @ center of @ system a @ smart software to manage and transform big data in a @ understanding form @ @ @ describes a text based indexing system @ mammographic image retrieval and classification @ @ system deal @ text @ structured report @ and image @ mammogram @ mining and classification in a typical department of radiology @ dicom structured report containing free text @ medical diagnosis @ @ analyzed and labeled in order to classify @ corresponding mammographic image @ information retrieval process is based on some text manipulation technique @ a light semantic analysis stop-word removing and light medical natural language processing @ @ system includes @ a search engine module based on a bayes naive classifier @ @ experimental @ provide interesting performance in term of specificity and sensibility @ @ more index @ @ computed in order to ass @ system robustness @ @ az @ area @ roc curve @ index and @ σaz @ az standard error @ index @ @ dataset is composed of healthy and pathological dicom structured report @ @ use case scenario @ presented and described to prove @ effectiveness of @ proposed approach @ @ b @ v @ @ right reserved @ 
2912,Text mining of web-based medical content,"Focus on data extraction methods for mining biomedical literature, electronic health records, and social media including online forums, query search terms, video sharing and tweets. Aims and Scope •Includes Text Mining and Natural Language Processing Methods for extracting information from electronic health records and biomedical literature. •Analyzes text analytic tools for new media such as online forums, social media posts, tweets and video sharing. •Demonstrates how to use speech and audio technologies for improving access to online content for the visually impaired. Text Mining of Web-Based Medical Content examines various approaches to deriving high quality information from online biomedical literature, electronic health records, query search terms, social media posts and tweets. Using some of the latest empirical methods of knowledge extraction, the authors show how online content, generated by both professionals and laypersons, can be mined for valuable information about disease processes, adverse drug reactions not captured during clinical trials, and tropical fever outbreaks. Additionally, the authors show how to perform infromation extraction on a hospital intranet, how to build a social media search engine to glean information about patients' own experiences interacting with healthcare professionals, and how to improve access to online health information. This volume provides a wealth of timely material for health informatic professionals and machine learning, data mining, and natural language researchers. Topics in this book include: •Mining Biomedical Literature and Clinical Narratives •Medication Information Extraction •Machine Learning Techniques for Mining Medical Search Queries •Detecting the Level of Personal Health Information Revealed in Social Media •Curating Layperson’s Personal Experiences with Health Care from Social Media and Twitter •Health Dialogue Systems for Improving Access to Online Content •Crowd-based Audio Clips to Improve Online Video Access for the Visually Impaired •Semantic-based Visual Information Retrieval for Mining Radiographic Image Data •Evaluating the Importance of Medical Terminology in YouTube Video Titles and Descriptions. © 2014 Walter de Gruyter Inc., Boston/Berlin.",2014,Text Mining of Web-Based Medical Content,4,focus on data extraction method @ mining biomedical literature electronic health record and social medium including online forum query search term video sharing and tweet @ aim and scope includes text mining and natural language processing method @ extracting information @ electronic health record and biomedical literature @ analyzes text analytic tool @ @ medium @ a online forum social medium post tweet and video sharing @ demonstrates @ to use speech and audio technology @ improving access to online content @ @ visually impaired @ text mining of web-based medical content examines various approach to deriving high quality information @ online biomedical literature electronic health record query search term social medium post and tweet @ @ some of @ latest empirical method of knowledge extraction @ author @ @ online content generated by @ professional and layperson @ @ mined @ valuable information @ disease process adverse drug reaction not captured @ clinical trial and tropical fever outbreak @ additionally @ author @ @ to perform infromation extraction on a hospital intranet @ to build a social medium search engine to glean information @ patient @ @ experience @ @ healthcare professional and @ to improve access to online health information @ @ volume provides a wealth of timely material @ health informatic professional and machine learning data mining and natural language researcher @ topic in @ book include @ mining biomedical literature and clinical narrative medication information extraction machine learning technique @ mining medical search query detecting @ level of personal health information revealed in social medium curating layperson s personal experience @ health care @ social medium and twitter health dialogue system @ improving access to online content crowd-based audio clip to improve online video access @ @ visually impaired semantic-based visual information retrieval @ mining radiographic image data evaluating @ importance of medical terminology in youtube video title and description @ walter de gruyter inc @ boston @ @ 
2915,"Five-Dimensional Sentiment Analysis of Corpora, Documents and Words","Sentiment analysis has become a widely used approach to assess the emotional content of written documents such as customer feedback. In positive psychology research, the typical one-dimensional analysis framework has been extended to include five dimensions. This five-dimensional model, PERMA, enables a fine-grained analysis of written texts. We propose an approach in which this model, statistical analysis and the self-organizing map are used. We analyze corpora from various genres. A hybrid methodology that uses the self-organizing maps algorithm and human judgment is suggested for expanding the PERMA lexicon. This vocabulary expansion can be useful for English but it is potentially even more crucial in the case of other languages for which the lexicon is not readily available. The challenges and solutions related to the text mining of texts written in a morphologically complex language such as Finnish are also considered. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,3,sentiment analysis ha become a widely used approach to ass @ emotional content of written document @ a customer feedback @ in positive psychology research @ typical one-dimensional analysis framework ha @ extended to include five dimension @ @ five-dimensional model perma enables a fine-grained analysis of written text @ @ propose @ approach in @ @ model statistical analysis and @ self-organizing map @ used @ @ analyze corpus @ various genre @ a hybrid methodology @ us @ self-organizing map algorithm and human judgment is suggested @ expanding @ perma lexicon @ @ vocabulary expansion @ @ useful @ english @ @ is potentially even more crucial in @ case of @ language @ @ @ lexicon is not readily available @ @ challenge and solution related to @ text mining of text written in a morphologically complex language @ a finnish @ @ considered @ @ international publishing switzerland @ 
2918,A new text mining approach in search technology,"Text-Mining (TM) refers generally to the practice of extracting attractive and non-trivial information and facts from unstructured text. TM includes several Computer Science (CS) regulations with a strong direction towards Artificial Intelligence (AI) in general including but not limited to Pattern Recognition (PR), Neural Networks (NN), Natural Language Processing (NLP), Information Retrieval (IR) and Machine Learning (ML). A significant variation with search is that search requires a user to identify what he or she is looking for while TM attempts to realize information in a model that is not known earlier. TM is mainly motivating in domains where users have to invent new information. This is the case for, e.g., in criminal enquiries and legal findings. Such examinations require 100% evoke, i.e., users can not meet the expense of missing relevant data. In distinction, a user searching the internet for background information using a benchmark Search Engine (SE) simply requires any data as long as it is reliable. Increasing evoke almost positively will decrease accuracy involving that users have to browse huge collections of documents that that may or may not be relevant. Standard procedures use language expertise to increase accuracy but when text collections are not in one language are not domain specific and or contain variable size and type documents either these schemes fail or are so complicated that the user does not understand what is happening and loses control. A different technique is to combine standard significance ranking with Adaptive Filtering (AF) and Interactive Visualization (IV) that is based on characteristics that have been mined earlier. © Medwell Journals, 2014.",2014,Asian Journal of Information Technology,0,text-mining @ tm @ refers generally to @ practice of extracting attractive and non-trivial information and fact @ unstructured text @ tm includes several computer science @ c @ regulation @ a strong direction towards artificial intelligence @ ai @ in general including @ not limited to pattern recognition @ pr @ neural network @ nn @ natural language processing @ nlp @ information retrieval @ ir @ and machine learning @ ml @ @ a significant variation @ search is @ search requires a user to identify @ he @ @ is looking @ @ tm attempt to realize information in a model @ is not known earlier @ tm is mainly motivating in domain @ user @ to invent @ information @ @ is @ case @ e @ g @ in criminal enquiry and legal finding @ @ examination require evoke i @ e @ user @ not meet @ expense of missing relevant data @ in distinction a user searching @ internet @ background information @ a benchmark search engine @ se @ simply requires @ data a long a @ is reliable @ increasing evoke almost positively @ decrease accuracy involving @ user @ to browse huge collection of document @ @ may @ may not @ relevant @ standard procedure use language expertise to increase accuracy @ @ text collection @ not in @ language @ not domain specific and @ contain variable size and type document either @ scheme fail @ @ @ complicated @ @ user doe not understand @ is happening and loses control @ a different technique is to combine standard significance ranking @ adaptive filtering @ af @ and interactive visualization @ @ @ @ is based on characteristic @ @ @ mined earlier @ medwell journal @ 
2922,Practical application in development and use of mining tools with total environment for text data mining,"In this challenge, we develop and distribute an integrated environment to flexibly combine multiple text mining techniques. Text mining techniques include numerous tasks such as salient sentence extraction, keyword extraction, topic extraction, textual coherence evaluation, multi-document summarization, and text clustering. Although tools that individually perform one or more of the above-mentioned tasks exist, it is difficult to integrate and activate multiple tools for a particular task. We attempt to provide the flexibility to integrate numerous tools that exist in the community in our proposed text mining environment. Users can use a customized version of the proposed text mining environment for their specific tasks, thereby concentrating solely on their creative work.",2014,Transactions of the Japanese Society for Artificial Intelligence,5,in @ challenge @ develop and distribute @ integrated environment to flexibly combine multiple text mining technique @ text mining technique include numerous task @ a salient sentence extraction keyword extraction topic extraction textual coherence evaluation multi-document summarization and text clustering @ although tool @ individually perform @ @ more of @ above-mentioned task exist @ is difficult to integrate and activate multiple tool @ a particular task @ @ attempt to provide @ flexibility to integrate numerous tool @ exist in @ community in @ proposed text mining environment @ user @ use a customized version of @ proposed text mining environment @ @ specific task thereby concentrating solely on @ creative work @ 
2927,Techniques for extracting named entities,"Text mining has significant potential, as a substantial amount of the information available in organizations is in the form of unstructured text documents. One of the basic tasks in text mining is named-entity recognition (NER). This paper describes some of the main approaches to this task and applies them to a specific problem, namely information extraction from an 8000-document corpus of university administrative decisions. The experiments compare various approaches y show that conditional random fields (CRFs) are the best technique for the problem. The paper also describes the framework of this task, the unstructured information management architecture of which it is a component. © IBERAMIA and the authors.",2014,Inteligencia Artificial,1,text mining ha significant potential a a substantial amount of @ information available in organization is in @ form of unstructured text document @ @ of @ basic task in text mining is named-entity recognition @ ner @ @ @ @ describes some of @ main approach to @ task and applies @ to a specific problem namely information extraction @ @ document corpus of university administrative decision @ @ experiment compare various approach @ @ @ conditional random field @ crfs @ @ @ best technique @ @ problem @ @ @ @ describes @ framework of @ task @ unstructured information management architecture of @ @ is a component @ iberamia and @ author @ 
2930,Clinical text retrieval-an overview of basic building blocks and applications,"This article describes information retrieval, natural language processing and text mining of electronic patient record text, also called clinical text. Clinical text is written by physicians and nurses to document the health care process of the patient. First we describe some characteristics of clinical text, followed by the automatic preprocessing of the text that is necessary for making it usable for some applications. We also describe some applications for clinicians including spelling and grammar checking, ICD-10 diagnosis code assignment, as well as other applications for hospital management such as ICD-10 diagnosis code validation and detection of adverse events such as hospital acquired infections. Part of the preprocessing makes the clinical text useful for faceted search, although clinical text already has some keys for performing faceted search such as gender, age, ICD-10 diagnosis codes, ATC drug codes, etc. Preprocessing makes use of ICD-10 codes and the SNOMED-CT textual descriptions. ICD-10 codes and SNOMED-CT are available in several languages and can be considered the modern Greek or Latin of medical language. The basic research presented here has its roots in the challenges described by the health care sector. These challenges have been partially solved in academia, and we believe the solutions will be adapted to the health care sector in real world applications. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,@ article describes information retrieval natural language processing and text mining of electronic patient record text @ called clinical text @ clinical text is written by physician and nurse to document @ health care process of @ patient @ first @ describe some characteristic of clinical text followed by @ automatic preprocessing of @ text @ is necessary @ making @ usable @ some application @ @ @ describe some application @ clinician including spelling and grammar checking icd diagnosis code assignment a well a @ application @ hospital management @ a icd diagnosis code validation and detection of adverse event @ a hospital acquired infection @ part of @ preprocessing make @ clinical text useful @ faceted search although clinical text already ha some key @ performing faceted search @ a gender age icd diagnosis code atc drug code etc @ preprocessing make use of icd code and @ snomed-ct textual description @ icd code and snomed-ct @ available in several language and @ @ considered @ modern greek @ latin of medical language @ @ basic research presented @ ha @ root in @ challenge described by @ health care sector @ @ challenge @ @ partially solved in academia and @ believe @ solution @ @ adapted to @ health care sector in real world application @ @ international publishing switzerland @ 
2938,Analyzing information flow and context for Facebook fan pages,"As the recent growth of online social network services such as Facebook and Twitter, people are able to easily share information with each other by writing posts or commenting for another's posts. In this paper, we firstly suggest a method of discovering information flows of posts on Facebook and their underlying contexts by incorporating process mining and text mining techniques. Based on comments collected from Facebook, the experiment results illustrate how the proposed method can be applied to analyze information flows and contexts of posts on social network services. Copyright © 2014 The Institute of Electronics, Information and Communication Engineers.",2014,IEICE Transactions on Information and Systems,5,a @ recent growth of online social network service @ a facebook and twitter people @ able to easily share information @ @ @ by writing post @ commenting @ another @ s post @ in @ @ @ firstly suggest a method of discovering information flow of post on facebook and @ underlying context by incorporating process mining and text mining technique @ based on comment collected @ facebook @ experiment @ illustrate @ @ proposed method @ @ applied to analyze information flow and context of post on social network service @ @ @ institute of electronics information and communication engineer @ 
2939,"BioTex: A system for biomedical terminology extraction, ranking, and validation","Term extraction is an essential task in domain knowledge acquisition. Although hundreds of terminologies and ontologies exist in the biomedical domain, the language evolves faster than our ability to formalize and catalog it. We may be interested in the terms and words explicitly used in our corpus in order to index or mine this corpus or just to enrich currently available terminologies and ontologies. Automatic term recognition and keyword extraction measures are widely used in biomedical text mining applications. We present BIOTEX, a Web application that implements state-of-the-art measures for automatic extraction of biomedical terms from free text in English and French.",2014,CEUR Workshop Proceedings,13,term extraction is @ essential task in domain knowledge acquisition @ although hundred of terminology and ontology exist in @ biomedical domain @ language evolves faster @ @ ability to formalize and catalog @ @ @ may @ interested in @ term and word explicitly used in @ corpus in order to index @ mine @ corpus @ @ to enrich currently available terminology and ontology @ automatic term recognition and keyword extraction measure @ widely used in biomedical text mining application @ @ @ biotex a web application @ implement state-of-the-art measure @ automatic extraction of biomedical term @ free text in english and french @ 
2940,Negation and speculation target identification,"Negation and speculation are common in natural language text. Many applications, such as biomedical text mining and clinical information extraction, seek to distinguish positive/factual objects from negative/speculative ones (i.e., to determine what is negated or speculated) in biomedical texts. This paper proposes a novel task, called negation and speculation target identification, to identify the target of a negative or speculative expression. For this purpose, a new layer of the target information is incorporated over the BioScope corpus and a machine learning algorithm is proposed to automatically identify this new information. Evaluation justifies the effectiveness of our proposed approach on negation and speculation target identification in biomedical texts. © Springer-Verlag Berlin Heidelberg 2014.",2014,Communications in Computer and Information Science,0,negation and speculation @ common in natural language text @ many application @ a biomedical text mining and clinical information extraction seek to distinguish positive factual object @ negative speculative @ @ i @ e @ to determine @ is negated @ speculated @ in biomedical text @ @ @ proposes a novel task called negation and speculation target identification to identify @ target of a negative @ speculative expression @ @ @ purpose a @ layer of @ target information is incorporated @ @ bioscope corpus and a machine learning algorithm is proposed to automatically identify @ @ information @ evaluation justifies @ effectiveness of @ proposed approach on negation and speculation target identification in biomedical text @ springer-verlag @ @ @ 
2943,Dependency-based semantic parsing for concept-level text analysis,"Concept-level text analysis is superior to word-level analysis as it preserves the semantics associated with multi-word expressions. It offers a better understanding of text and helps to significantly increase the accuracy of many text mining tasks. Concept extraction from text is a key step in concept-level text analysis. In this paper, we propose a ConceptNet-based semantic parser that deconstructs natural language text into concepts based on the dependency relation between clauses. Our approach is domain-independent and is able to extract concepts from heterogeneous text. Through this parsing technique, 92.21% accuracy was obtained on a dataset of 3,204 concepts. We also show experimental results on three different text analysis tasks, on which the proposed framework outperformed state-of-the-art parsing techniques. © 2014 Springer-Verlag Berlin Heidelberg.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),39,concept-level text analysis is superior to word-level analysis a @ preserve @ semantics associated @ multi-word expression @ @ offer a better understanding of text and help to significantly increase @ accuracy of many text mining task @ concept extraction @ text is a key step in concept-level text analysis @ in @ @ @ propose a conceptnet-based semantic parser @ deconstructs natural language text @ concept based on @ dependency relation @ clause @ @ approach is domain-independent and is able to extract concept @ heterogeneous text @ @ @ parsing technique @ accuracy wa obtained on a dataset of concept @ @ @ @ experimental @ on three different text analysis task on @ @ proposed framework outperformed state-of-the-art parsing technique @ springer-verlag @ @ @ 
2945,Bringing named entity recognition on drupal content management system,"Content management systems and frameworks (CMS/F) play a key role in Web development. They support common Web operations and provide for a number of optional modules to implement customized functionalities. Given the increasing demand for text mining (TM) applications, it seems logical that CMS/F extend their offer of TM modules. In this regard, this work contributes to Drupal CMS/F with modules that support customized named entity recognition and enable the construction of domain-specific document search engines. Implementation relies on well-recognized Apache Information Retrieval and TM initiatives, namely Apache Lucene, Apache Solr and Apache Unstructured Information Management Architecture (UIMA). As proof of concept, we present here the development of a Drupal CMS/F that retrieves biomedical articles and performs automatic recognition of organism names to enable further organism-driven document screening. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,1,content management system and framework @ cm f @ play a key role in web development @ @ support common web operation and provide @ a number of optional module to implement customized functionality @ given @ increasing demand @ text mining @ tm @ application @ seems logical @ cm f extend @ offer of tm module @ in @ regard @ work contributes to drupal cm f @ module @ support customized named entity recognition and enable @ construction of domain-specific document search engine @ implementation relies on well-recognized apache information retrieval and tm initiative namely apache lucene apache solr and apache unstructured information management architecture @ uima @ @ a proof of concept @ @ @ @ development of a drupal cm f @ retrieves biomedical article and performs automatic recognition of organism name to enable @ organism-driven document screening @ @ international publishing switzerland @ 
2952,Sentiment analysis and the impact of employee satisfaction on firm earnings,"Prior text mining studies of corporate reputational sentiment based on newswires, blogs and Twitter feeds have mostly captured reputation from the perspective of two groups of stakeholders - the media and consumers. In this study we examine the sentiment of a potentially overlooked stakeholder group, namely, the firm's employees. First, we present a novel dataset that uses online employee reviews to capture employee satisfaction. We employ LDA to identify salient aspects in employees' reviews, and manually infer one latent topic that appears to be associated with the firm's outlook. Second, we create a composite document by aggregating employee reviews for each firm and measure employee sentiment as the polarity of the composite document using the General Inquirer dictionary to count positive and negative terms. Finally, we define employee satisfaction as a weighted combination of the firm outlook topic cluster and employee sentiment. The results of our joint aspect-polarity model suggest that it may be beneficial for investors to incorporate a measure of employee satisfaction into their method for forecasting firm earnings. © 2014 Springer International Publishing Switzerland.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,prior text mining study of corporate reputational sentiment based on newswires blog and twitter feed @ mostly captured reputation @ @ perspective of @ group of stakeholder @ medium and consumer @ in @ study @ examine @ sentiment of a potentially overlooked stakeholder group namely @ firm @ s employee @ first @ @ a novel dataset @ us online employee review to capture employee satisfaction @ @ employ lda to identify salient aspect in employee @ review and manually infer @ latent topic @ appears to @ associated @ @ firm @ s outlook @ second @ create a composite document by aggregating employee review @ @ firm and measure employee sentiment a @ polarity of @ composite document @ @ general inquirer dictionary to count positive and negative term @ finally @ define employee satisfaction a a weighted combination of @ firm outlook topic cluster and employee sentiment @ @ @ of @ joint aspect-polarity model suggest @ @ may @ beneficial @ investor to incorporate a measure of employee satisfaction @ @ method @ forecasting firm earnings @ @ international publishing switzerland @ 
2959,"BioC implementations in Go, Perl, Python and Ruby","As part of a communitywide effort for evaluating text mining and information extraction systems applied to the biomedical domain, BioC is focused on the goal of interoperability, currently a major barrier to wide-scale adoption of text mining tools. BioC is a simple XML format, specified by DTD, for exchanging data for biomedical natural language processing. With initial implementations in C"" and Java, BioC provides libraries of code for reading and writing BioC text documents and annotations. We extend BioC to Perl, Python, Go and Ruby. We used SWIG to extend the C"" implementation for Perl and one Python implementation. A second Python implementation and the Ruby implementation use native data structures and libraries. BioC is also implemented in the Google language Go. BioC modules are functional in all of these languages, which can facilitate text mining tasks. BioC implementations are freely available through the BioC site: http://bioc.sourceforge.net. Published by Oxford University Press 2014.",2014,Database,2,a part of a communitywide effort @ evaluating text mining and information extraction system applied to @ biomedical domain bioc is focused on @ goal of interoperability currently a major barrier to wide-scale adoption of text mining tool @ bioc is a simple xml format specified by dtd @ exchanging data @ biomedical natural language processing @ @ initial implementation in c @ and java bioc provides library of code @ reading and writing bioc text document and annotation @ @ extend bioc to perl python go and ruby @ @ used swig to extend @ c @ implementation @ perl and @ python implementation @ a second python implementation and @ ruby implementation use native data structure and library @ bioc is @ implemented in @ google language go @ bioc module @ functional in @ of @ language @ @ facilitate text mining task @ bioc implementation @ freely available @ @ bioc site @ http @ bioc @ sourceforge @ net @ published by oxford university @ @ 
2960,Hybrid approach for Punjabi Question Answering system,"In this paper a hybrid algorithm for Punjabi Question Answering system has been implemented. A hybrid system that works on various kinds of question types using the concepts of pattern matching as well as mathematical expression for developing a scoring system that can help differentiate best answer among available set of multiple answers found by the algorithm and is also domain specific like sports. The proposed system is designed and built in such a way that it increases the accuracy of question answering system in terms of recall and precision and is working for factoid questions and answers text in Punjabi. The system constructs a novel mathematical scoring system to identify most accurate probable answer out of the multiple answer patterns.The answers are extracted for various types of Punjabi questions. The experimental results are evaluated on the basis of Precision, Recall, F-score and Mean Reciprocal Rank (MRR). The average value of precision, recall, f-score and Mean Reciprocal Rank is 85.66%, 65.28%, 74.06%, 0.43 (normalised value) respectively. MRR values are Optimal. These values are act as discrimination factor values between one relevant answer to the other relevant answer. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,2,in @ @ a hybrid algorithm @ punjabi question answering system ha @ implemented @ a hybrid system @ work on various kind of question type @ @ concept of pattern matching a well a mathematical expression @ developing a scoring system @ @ help differentiate best answer among available set of multiple answer found by @ algorithm and is @ domain specific like sport @ @ proposed system is designed and built in @ a way @ @ increase @ accuracy of question answering system in term of recall and precision and is working @ factoid question and answer text in punjabi @ @ system construct a novel mathematical scoring system to identify @ accurate probable answer @ of @ multiple answer pattern @ @ answer @ extracted @ various type of punjabi question @ @ experimental @ @ evaluated on @ basis of precision recall f-score and mean reciprocal rank @ mrr @ @ @ average value of precision recall f-score and mean reciprocal rank is @ @ @ @ @ normalised value @ respectively @ mrr value @ optimal @ @ value @ act a discrimination factor value @ @ relevant answer to @ @ relevant answer @ @ international publishing switzerland @ 
2961,Modelling of language processing dependence on morphological features,"The order, association and variability of the advertising language is different in every language and culture, because it is based on different rules in the given culture. Therefore, the study is focused on comparative linguistic data analysis of advertisements written in Slovak and English randomly collected from online sources. The transaction/sequence model for text representation was used and an association rules analysis was applied as the research method. The results are significant mainly in terms of the differences in the incidence of parts of speech in English and Slovak written advertisements. Based on the morphological features of the examined languages, different models of language of advertising were being created. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,5,@ order association and variability of @ advertising language is different in every language and culture @ @ is based on different rule in @ given culture @ therefore @ study is focused on comparative linguistic data analysis of advertisement written in slovak and english randomly collected @ online source @ @ transaction sequence model @ text representation wa used and @ association rule analysis wa applied a @ research method @ @ @ @ significant mainly in term of @ difference in @ incidence of part of speech in english and slovak written advertisement @ based on @ morphological feature of @ examined language different model of language of advertising @ @ created @ @ international publishing switzerland @ 
2962,Influence of stop-words removal on sequence patterns identification within comparable corpora,"Short texts like advertisements are characterised by a number of slogans, phrases, words, symbols etc. To improve the quality of textual data, it is necessary to filter out noise textual data from important data. The aim of this work is to determine to what extent it is necessary to carry out the time consuming data pre-processing in the process of discovering sequential patterns in English and Slovak advertisement corpora. For this purpose, an experiment was conducted focusing on data pre-processing in these two comparable corpora. We try to find out to what extent removing the stop words has an influence on a quantity and quality of extracted rules. Stop words removal has no impact on the quantity and quality of extracted rules in English as well as in Slovak advertisement corpora. Only language has a significant impact on the quantity and quality of extracted rules. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,12,short text like advertisement @ characterised by a number of slogan phrase word symbol etc @ to improve @ quality of textual data @ is necessary to filter @ noise textual data @ important data @ @ aim of @ work is to determine to @ extent @ is necessary to carry @ @ time consuming data pre-processing in @ process of discovering sequential pattern in english and slovak advertisement corpus @ @ @ purpose @ experiment wa conducted focusing on data pre-processing in @ @ comparable corpus @ @ try to find @ to @ extent removing @ stop word ha @ influence on a quantity and quality of extracted rule @ stop word removal ha no impact on @ quantity and quality of extracted rule in english a well a in slovak advertisement corpus @ only language ha a significant impact on @ quantity and quality of extracted rule @ @ international publishing switzerland @ 
2964,How can catchy titles be generated without loss of informativeness?,"Automatic titling of text documents is an essential task for several applications (automatic heading of e-mails, summarization, and so forth). This paper describes a system facilitating information retrieval in a set of textual documents by tackling the automatic titling and subtitling issue. Automatic titling here involves providing both informative and catchy titles. We thus propose two different approaches based on NLP, text mining, and Web Mining techniques. The first one (POSTIT) consists of extracting relevant noun phrases from texts as candidate titles. An original approach combining statistical criteria and noun phrase positions in the text helps in collecting informative titles and subtitles. The second approach (NOMIT) is based on various assumptions made on POSTIT and aims to generate both informative and catchy titles. Both approaches are applied to a corpus of news articles, then evaluated according to two criteria, i.e. informativeness and catchiness. © 2013 Elsevier Ltd. All rights reserved.",2014,Expert Systems with Applications,9,automatic titling of text document is @ essential task @ several application @ automatic heading of e-mail summarization and @ forth @ @ @ @ describes a system facilitating information retrieval in a set of textual document by tackling @ automatic titling and subtitling issue @ automatic titling @ involves providing @ informative and catchy title @ @ thus propose @ different approach based on nlp text mining and web mining technique @ @ first @ @ postit @ consists of extracting relevant noun phrase @ text a candidate title @ @ original approach combining statistical criterion and noun phrase position in @ text help in collecting informative title and subtitle @ @ second approach @ nomit @ is based on various assumption made on postit and aim to generate @ informative and catchy title @ @ approach @ applied to a corpus of news article @ evaluated according to @ criterion i @ e @ informativeness and catchiness @ @ ltd @ @ right reserved @ 
2965,Section heading recognition in electronic health records using conditional random fields,"Electronic health records (EHRs) contain a wealth of information, such as discharge diagnoses, laboratory results, and pharmacy orders, which can be used to support clinical decision support systems and enable clinical and translational research. Unfortunately, the information is represented in a highly heterogeneous semi-structured or unstructured format with author- and domainspecific idiosyncrasies, acronyms and abbreviations. To take full advantage of health data, text-mining techniques have been applied by researchers to recognize named entities (NEs) mentioned in EHRs. However, the judgment of clinical data cannot be known solely from the NE level. For instance, a disease mention in the section of past medical history has different clinical significance when mentioned in the family medical history section. To obtain high-quality information and improve the understanding of clinical records, this work developed a machine learning-based section heading recognition system and evaluated its performance on a manually annotated corpus. The experiment results showed that the machine learning-based system achieved a satisfactory F-score of 0.939, which outperformed a dictionary-based system by 0.321. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,electronic health record @ ehrs @ contain a wealth of information @ a discharge diagnosis laboratory @ and pharmacy order @ @ @ used to support clinical decision support system and enable clinical and translational research @ unfortunately @ information is represented in a highly heterogeneous semi-structured @ unstructured format @ author and domainspecific idiosyncrasy acronym and abbreviation @ to take full advantage of health data text-mining technique @ @ applied by researcher to recognize named entity @ ne @ mentioned in ehrs @ however @ judgment of clinical data cannot @ known solely @ @ ne level @ @ instance a disease mention in @ section of past medical history ha different clinical significance @ mentioned in @ family medical history section @ to obtain high-quality information and improve @ understanding of clinical record @ work developed a machine learning-based section heading recognition system and evaluated @ performance on a manually annotated corpus @ @ experiment @ showed @ @ machine learning-based system achieved a satisfactory f-score of @ @ outperformed a dictionary-based system by @ @ @ international publishing switzerland @ 
2966,Constraint logic programming for resolution of relative time expressions,"Translating time expression into absolute time points or durations is a challenge for natural languages processing such as text mining and text understanding in general. We present a constraint logic language CLP(Time) tailored to text usages concerned with time and calendar. It provides a simple and flexible formalism to express relationships between different time expressions in a text, thereby giving a recipe for resolving them into absolute time. A constraint solver is developed which, as opposed to some earlier approaches, is independent of the order in which temporal information is introduced, and it can give meaningful output also when no exact reference time is available. © 2014 Springer International Publishing.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,translating time expression @ absolute time point @ duration is a challenge @ natural language processing @ a text mining and text understanding in general @ @ @ a constraint logic language clp @ time @ tailored to text usage concerned @ time and calendar @ @ provides a simple and flexible formalism to express relationship @ different time expression in a text thereby giving a recipe @ resolving @ @ absolute time @ a constraint solver is developed @ a opposed to some earlier approach is independent of @ order in @ temporal information is introduced and @ @ give meaningful output @ @ no exact reference time is available @ @ international publishing @ 
2967,Patent key component extraction with the application of patent similarity analysis,"Patents are crucial for a company to protect its intellectual properties. Effective text mining in patent portfolios provides companies with valuable insights to develop marketing strategies. In this paper, we study the problem of patent key component extraction, and demonstrate its application value by patent similarity analysis. We propose a syntactic-based approach to identify the key components in an invention, and integrate them with topic models to measure patent similarity. Experimental results on real-world patent portfolios show the effectiveness of the proposed method. © 2014 by Binary Information Press",2014,Journal of Computational Information Systems,0,patent @ crucial @ a company to protect @ intellectual property @ effective text mining in patent portfolio provides company @ valuable insight to develop marketing strategy @ in @ @ @ study @ problem of patent key component extraction and demonstrate @ application value by patent similarity analysis @ @ propose a syntactic-based approach to identify @ key component in @ invention and integrate @ @ topic model to measure patent similarity @ experimental @ on real-world patent portfolio @ @ effectiveness of @ proposed method @ by binary information @
2969,Sentiment analysis on Twitter data set: Survey,"The growth of social website, microblogging services and electronic media contributes vast amount of user generated contents such as customer reviews, comments and opinions. Twitter is a most popular microblogging service. Sentiment Analysis term is referred to the extraction of others (speaker or writer) opinion in given source material (text) by using NLP, Linguistic Computation and Text mining. Sentiment analysis is emerged as the broad range of domains such as natural language processing, computational linguistics and text mining. Sentiment classification of product reviews, service reviews and comments has emerged as the most useful application in the area of sentiment analysis. This paper focuses on the comparative study of different sentiment classification techniques, performed on twitter data set. The most popular approaches are Bag of words and feature extraction used by researchers to deal with sentiment analysis of opinion related to social issues, movies, electronics, cars, music etc. The sentiment analysis is used by manufacturers, politicians, news groups, and some organization to know the opinions of customer, people, and social website users. © Research India Publications.",2014,International Journal of Applied Engineering Research,3,@ growth of social website microblogging service and electronic medium contributes vast amount of user generated content @ a customer review comment and opinion @ twitter is a @ popular microblogging service @ sentiment analysis term is referred to @ extraction of others @ speaker @ writer @ opinion in given source material @ text @ by @ nlp linguistic computation and text mining @ sentiment analysis is emerged a @ broad range of domain @ a natural language processing computational linguistics and text mining @ sentiment classification of product review service review and comment ha emerged a @ @ useful application in @ area of sentiment analysis @ @ @ focus on @ comparative study of different sentiment classification technique performed on twitter data set @ @ @ popular approach @ bag of word and feature extraction used by researcher to deal @ sentiment analysis of opinion related to social issue movie electronics car music etc @ @ sentiment analysis is used by manufacturer politician news group and some organization to know @ opinion of customer people and social website user @ research india publication @ 
2974,Bioclass: A tool for biomedical text classification,"Traditional search engines are not efficient enough to extract useful information from scientific text databases. Therefore, it is necessary to develop advanced information retrieval software tools that allow for further classification of the scientific texts. The aim of this work is to present BioClass, a freely available graphic tool for biomedical text classification. With BioClass an user can parameterize, train and test different text classifiers to determine which technique performs better according to the document corpus. The framework includes data balancing and attribute reduction techniques to prepare the input data and improve the classification efficiency. Classification methods analyze documents by content and differentiate those that are best suited to the user requeriments. BioClass also offers graphical interfaces to get conclusions simply and easily. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,4,traditional search engine @ not efficient enough to extract useful information @ scientific text database @ therefore @ is necessary to develop advanced information retrieval software tool @ allow @ @ classification of @ scientific text @ @ aim of @ work is to @ bioclass a freely available graphic tool @ biomedical text classification @ @ bioclass @ user @ parameterize train and test different text classifier to determine @ technique performs better according to @ document corpus @ @ framework includes data balancing and attribute reduction technique to prepare @ input data and improve @ classification efficiency @ classification method analyze document by content and differentiate @ @ @ best suited to @ user requeriments @ bioclass @ offer graphical interface to get conclusion simply and easily @ @ international publishing switzerland @ 
2975,Understanding online social networks' users –a twitter approach,"Twitter messages, also known as tweets, are increasingly used by marketers worldwide to determine consumer sentiments towards brands, products or events. Currently, most existing approaches used for social networks sentiment analysis only extract simple feedbacks in terms of positive and negative perception. In this paper, TweetOntoSense is proposed - a semantic based approach that uses ontologies in order to infer the actual user’s emotions. The extracted sentiments are described using a WordNet enriched emotional categories ontology. Thus, feelings such as happiness, affection, surprise, anger, sadness, etc. are put forth. Moreover, compared to existing approaches, TweetOntoSense also takes into consideration the fact that a single tweet message might express several, rather than a single emotion. A case study on Twitter is performed, also showing this approach’s practical applicability. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,twitter message @ known a tweet @ increasingly used by marketer worldwide to determine consumer sentiment towards brand product @ event @ currently @ existing approach used @ social network sentiment analysis only extract simple feedback in term of positive and negative perception @ in @ @ tweetontosense is proposed a semantic based approach @ us ontology in order to infer @ actual user s emotion @ @ extracted sentiment @ described @ a wordnet enriched emotional category ontology @ thus feeling @ a happiness affection surprise anger sadness etc @ @ put forth @ moreover compared to existing approach tweetontosense @ take @ consideration @ fact @ a single tweet message might express several rather @ a single emotion @ a case study on twitter is performed @ showing @ approach s practical applicability @ @ international publishing switzerland @ 
2976,Retrieving attitudes: Sentiment analysis from clinical narratives,"Physicians and nurses express their judgments and observations towards a patient's health status in clinical narratives. Thus, their judgments are explicitly or implicitly included in patient records. To get impressions on the current health situation of a patient or on changes in the status, analysis and retrieval of this subjective content is crucial. In this paper, we approach this question as sentiment analysis problem and analyze the feasibility of assessing these judgments in clinical text by means of general sentiment analysis methods. Specifically, the word usage in clinical narratives and in a general text corpus is compared. The linguistic characteristics of judgments in clinical narratives are collected. Besides, the requirements for sentiment analysis and retrieval from clinical narratives are derived.",2014,CEUR Workshop Proceedings,11,physician and nurse express @ judgment and observation towards a patient @ s health status in clinical narrative @ thus @ judgment @ explicitly @ implicitly included in patient record @ to get impression on @ current health situation of a patient @ on change in @ status analysis and retrieval of @ subjective content is crucial @ in @ @ @ approach @ question a sentiment analysis problem and analyze @ feasibility of assessing @ judgment in clinical text by mean of general sentiment analysis method @ specifically @ word usage in clinical narrative and in a general text corpus is compared @ @ linguistic characteristic of judgment in clinical narrative @ collected @ besides @ requirement @ sentiment analysis and retrieval @ clinical narrative @ derived @ 
2982,Arabic light stemmer (ARS),"Stemming is a main step used to process textual data. It is usually used in several types of applications such as: text mining, information retrieval (IR), and natural language processing (NLP). A major task in stemming is to standardize words; which can be achieved by reducing each word to its base (root or stem). Arabic stemming is not an easy task. Unlike other languages, Arabic language is a highly inflected language, since it uses many inflectional forms. Researchers are divided on the benefit of using stemming in fields of IR, NLP...etc, since in Arabic the morphological variants of a certain word are not always semantically related. The aim of this paper is to design and implement a new Arabic light stemmer (ARS) which is not based on Arabic root patterns. Instead, it depends on well defined mathematical rules and several relations between letters. A series of tests were conducted on ARS stemmer to compare its effectiveness with the effectiveness of two other Arabic stemmers. Test shows clearly the effectiveness superiority of ARS compared to effectiveness of these two Arabic stemmers. © School of Engineering, Taylor’s University.",2014,Journal of Engineering Science and Technology,17,stemming is a main step used to process textual data @ @ is usually used in several type of application @ a @ text mining information retrieval @ ir @ and natural language processing @ nlp @ @ a major task in stemming is to standardize word @ @ @ @ achieved by reducing @ word to @ base @ root @ stem @ @ arabic stemming is not @ easy task @ unlike @ language arabic language is a highly inflected language since @ us many inflectional form @ researcher @ divided on @ benefit of @ stemming in field of ir nlp @ @ @ etc since in arabic @ morphological variant of a certain word @ not always semantically related @ @ aim of @ @ is to design and implement a @ arabic light stemmer @ ar @ @ is not based on arabic root pattern @ instead @ depends on well defined mathematical rule and several relation @ letter @ a series of test @ conducted on ar stemmer to compare @ effectiveness @ @ effectiveness of @ @ arabic stemmer @ test @ clearly @ effectiveness superiority of ar compared to effectiveness of @ @ arabic stemmer @ school of engineering taylor s university @ 
2983,Data mining cultural aspects of social media marketing,"or marketing to function in a globalized world it must respect a diverse set of local cultures. With marketing efforts extending to social media platforms, the crossing of cultural boundaries can happen in an instant. In this paper we examine how culture influences the popularity of marketing messages in social media platforms. Text mining, automated translation and sentiment analysis contribute largely to our research. From our analysis of 400 posts on the localized Google+ pages of German car brands in Germany and the US, we conclude that posting time and emotions are important predictors for reshare counts. © 2014 Springer International Publishing Switzerland.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ marketing to function in a globalized world @ must respect a diverse set of local culture @ @ marketing effort extending to social medium platform @ crossing of cultural boundary @ happen in @ instant @ in @ @ @ examine @ culture influence @ popularity of marketing message in social medium platform @ text mining automated translation and sentiment analysis contribute largely to @ research @ @ @ analysis of post on @ localized google page of german car brand in germany and @ u @ conclude @ posting time and emotion @ important predictor @ reshare count @ @ international publishing switzerland @ 
2984,"14th International Workshops on Web Information Systems Engineering, WISE 2013 with Big WebData, MBC, PCS, STeH, QUAT, SCEH,and STSC","The proceedings contain 42 papers. The special focus in this conference is on Challenging the Big Data Problem on the Web, Mobile Business and Personalization in Cloud and Service Computing. The topics include: Large-scale complex reasoning with semantics; using semantic techology for consistency checking of road signs; hg bitmap join index; semantic and qualitative spatial reasoning based road network modeling; a system to generate mobile data based on real user behavior; introducing a new scalable data-as-a service cloud platform for enriching traditional text mining techniques by integrating ontology modelling and natural language processing; towards fine-grained verification of application mobility; matrix factorization for user behavior analysis of location-based social network; understanding user behavior through url analysis in sina tweets; towards evaluating the performance of higher education; dynamically predicting the deadlines in time-constrained workflows; a task selecting algorithm for personal schedules in workflow systems; entity identification in deep web; situation-aware data service composition based on service hyperlinks; research on service content-based web service selection method; a Petri net based execution engine for web service composition; a method of optimizing multi-tenant database query access and cloud-based massive electricity data mining and consumption pattern discovery.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proceeding contain @ @ @ special focus in @ conference is on challenging @ big data problem on @ web mobile @ and personalization in cloud and service computing @ @ topic include @ large-scale complex reasoning @ semantics @ @ semantic techology @ consistency checking of road sign @ hg bitmap join index @ semantic and qualitative spatial reasoning based road network modeling @ a system to generate mobile data based on real user behavior @ introducing a @ scalable data-as-a service cloud platform @ enriching traditional text mining technique by integrating ontology modelling and natural language processing @ towards fine-grained verification of application mobility @ matrix factorization @ user behavior analysis of location-based social network @ understanding user behavior @ url analysis in sina tweet @ towards evaluating @ performance of higher education @ dynamically predicting @ deadline in time-constrained workflow @ a task selecting algorithm @ personal schedule in workflow system @ entity identification in deep web @ situation-aware data service composition based on service hyperlink @ research on service content-based web service selection method @ a petri net based execution engine @ web service composition @ a method of optimizing multi-tenant database query access and cloud-based massive electricity data mining and consumption pattern discovery @ 
2986,Mathematical language processing project,"In natural language, words and phrases themselves imply the semantics. In contrast, the meaning of identifiers in mathematical formulae is undefined. Thus scientists must study the context to decode the meaning. The Mathematical Language Processing (MLP) project aims to support that process. In this paper, we compare two approaches to discover identifier-definition tuples. At first we use a simple pattern matching approach. Second, we present the MLP approach that uses part-of-speech tag based distances as well as sentence positions to calculate identifier-definition probabilities. The evaluation of our prototypical system, applied on the Wikipedia text corpus, shows that our approach augments the user experience substantially. While hovering the identifiers in the formula, tool-tips with the most probable definitions occur. Tests with random samples show that the displayed definitions provide a good match with the actual meaning of the identifiers.",2014,CEUR Workshop Proceedings,6,in natural language word and phrase @ imply @ semantics @ in contrast @ meaning of identifier in mathematical formula is undefined @ thus scientist must study @ context to decode @ meaning @ @ mathematical language processing @ mlp @ project aim to support @ process @ in @ @ @ compare @ approach to discover identifier-definition tuples @ at first @ use a simple pattern matching approach @ second @ @ @ mlp approach @ us part-of-speech tag based distance a well a sentence position to calculate identifier-definition probability @ @ evaluation of @ prototypical system applied on @ wikipedia text corpus @ @ @ approach augments @ user experience substantially @ @ hovering @ identifier in @ formula tool-tips @ @ @ probable definition occur @ test @ random sample @ @ @ displayed definition provide a good match @ @ actual meaning of @ identifier @ 
2988,Building an Arabic Sentiment Lexicon Using Semi-supervised Learning,"Sentiment analysis is the process of determining a predefined sentiment from text written in a natural language with respect to the entity to which it is referring. A number of lexical resources are available to facilitate this task in English. One such resource is the SentiWordNet, which assigns sentiment scores to words found in the English WordNet. In this paper, we present an Arabic sentiment lexicon that assigns sentiment scores to the words found in the Arabic WordNet. Starting from a small seed list of positive and negative words, we used semi-supervised learning to propagate the scores in the Arabic WordNet by exploiting the synset relations. Our algorithm assigned a positive sentiment score to more than 800, a negative score to more than 600 and a neutral score to more than 6000 words in the Arabic WordNet. The lexicon was evaluated by incorporating it into a machine learning-based classifier. The experiments were conducted on several Arabic sentiment corpora, and we were able to achieve a 96% classification accuracy. © 2014 King Saud University.",2014,Journal of King Saud University - Computer and Information Sciences,52,sentiment analysis is @ process of determining a predefined sentiment @ text written in a natural language @ respect to @ entity to @ @ is referring @ a number of lexical resource @ available to facilitate @ task in english @ @ @ resource is @ sentiwordnet @ assigns sentiment score to word found in @ english wordnet @ in @ @ @ @ @ arabic sentiment lexicon @ assigns sentiment score to @ word found in @ arabic wordnet @ starting @ a small seed list of positive and negative word @ used semi-supervised learning to propagate @ score in @ arabic wordnet by exploiting @ synset relation @ @ algorithm assigned a positive sentiment score to more @ a negative score to more @ and a neutral score to more @ word in @ arabic wordnet @ @ lexicon wa evaluated by incorporating @ @ a machine learning-based classifier @ @ experiment @ conducted on several arabic sentiment corpus and @ @ able to achieve a classification accuracy @ king saud university @ 
2989,A case study of utilising concept knowledge in a topic specific document collection,"The use of 'topic' concepts has shown improved search performance, given a query, by bringing together relevant documents which use different terms to describe a higher level concept. In this paper, we propose a method for discovering and utilizing concepts in indexing and search for a domain specific document collection being utilized in industry. This approach differs from others in that we only collect focused concepts to build the concept space and that instead of turning a user's query into a concept based query, we experiment with different techniques of combining the original query with a concept query. We apply the proposed approach to a real-world document collection and the results show that in this scenario the use of concept knowledge at index and search can improve the relevancy of results. © 2014, Australian Computer Society, Inc.",2014,Conferences in Research and Practice in Information Technology Series,0,@ use of @ topic @ concept ha @ improved search performance given a query by bringing together relevant document @ use different term to describe a higher level concept @ in @ @ @ propose a method @ discovering and utilizing concept in indexing and search @ a domain specific document collection @ utilized in industry @ @ approach differs @ others in @ @ only collect focused concept to build @ concept space and @ instead of turning a user @ s query @ a concept based query @ experiment @ different technique of combining @ original query @ a concept query @ @ apply @ proposed approach to a real-world document collection and @ @ @ @ in @ scenario @ use of concept knowledge at index and search @ improve @ relevancy of @ @ australian computer society inc @ 
2991,Identification of multi-focal questions in question and answer reports,"A significant amount of business and scientific data is collected via question and answer reports. However, these reports often suffer from various data quality issues. In many cases, questionnaires contain a number of questions that require multiple answers, which we argue can be a potential source of problems that may lead to poor-quality answers. This paper introduces multi-focal questions and proposes a model for identifying them. The model consists of three phases: question pre-processing, feature engineering and question classification. We use six types of features: lexical/surface features, Part-of-Speech, readability, question structure, wording and placement features, question response type and format features and question focus. A comparative study of three different machine learning algorithms (Bayes Net, Decision Tree and Support Vector Machine) is performed on a dataset of 150 questions obtained from the Carbon Disclosure Project, achieving the accuracy of 91%. © Springer International Publishing Switzerland 2014.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,a significant amount of @ and scientific data is collected via question and answer report @ however @ report often suffer @ various data quality issue @ in many case questionnaire contain a number of question @ require multiple answer @ @ argue @ @ a potential source of problem @ may lead to poor-quality answer @ @ @ introduces multi-focal question and proposes a model @ identifying @ @ @ model consists of three phase @ question pre-processing feature engineering and question classification @ @ use six type of feature @ lexical surface feature part-of-speech readability question structure wording and placement feature question response type and format feature and question focus @ a comparative study of three different machine learning algorithm @ bayes net decision tree and support vector machine @ is performed on a dataset of question obtained @ @ carbon disclosure project achieving @ accuracy of @ @ international publishing switzerland @ 
2992,Discovering relations between indirectly connected biomedical concepts,"The complexity and scale of the knowledge in the biomedical domain has motivated research work towards mining heterogeneous data from structured and unstructured knowledge bases. Towards this direction, it is necessary to combine facts in order to formulate hypotheses or draw conclusions about the domain concepts. In this work we attempt to address this problem by using indirect knowledge connecting two concepts in a graph to identify hidden relations between them. The graph represents concepts as vertices and relations as edges, stemming from structured (ontologies) and unstructured (text) data. In this graph we attempt to mine path patterns which potentially characterize a biomedical relation. For our experimental evaluation we focus on two frequent relations, namely ""has target"", and ""may treat"". Our results suggest that relation discovery using indirect knowledge is possible, with an AUC that can reach up to 0.8. Finally, analysis of the results indicates that the models can successfully learn expressive path patterns for the examined relations. © 2014 Springer International Publishing Switzerland.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ complexity and scale of @ knowledge in @ biomedical domain ha motivated research work towards mining heterogeneous data @ structured and unstructured knowledge base @ towards @ direction @ is necessary to combine fact in order to formulate hypothesis @ draw conclusion @ @ domain concept @ in @ work @ attempt to address @ problem by @ indirect knowledge connecting @ concept in a graph to identify hidden relation @ @ @ @ graph represents concept a vertex and relation a edge stemming @ structured @ ontology @ and unstructured @ text @ data @ in @ graph @ attempt to mine path pattern @ potentially characterize a biomedical relation @ @ @ experimental evaluation @ focus on @ frequent relation namely @ ha target @ and @ may treat @ @ @ @ suggest @ relation discovery @ indirect knowledge is possible @ @ auc @ @ reach up to @ @ finally analysis of @ @ indicates @ @ model @ successfully learn expressive path pattern @ @ examined relation @ @ international publishing switzerland @ 
2997,Discovering sentiment of social messages by mining message correlations,"With explosive growth of the Internet, the amount of information in text form is growing rapidly and the demand for data analysis is also increases. We can perform sentiment analysis on a large set of text messages to discover valuable knowledge and obtain enormous benefits in national security, business, politics, economics, etc. However, text messages from the social networks are rather different from those of traditional text documents. Therefore, it is difficult but essential to develop an effective method of sentiment exploration in social networks. In this paper we first applied a neural network model, namely the selforganizing maps, to cluster similar messages and sentiment keywords, respectively. We then developed an association discovery process to find the associations between a message and some sentiment keywords. The sentiment of a message is then determined according to such associations. We performed experiments on Twitter messages and obtained promising results. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,0,@ explosive growth of @ internet @ amount of information in text form is growing rapidly and @ demand @ data analysis is @ increase @ @ @ perform sentiment analysis on a @ set of text message to discover valuable knowledge and obtain enormous benefit in national security @ politics economics etc @ however text message @ @ social network @ rather different @ @ of traditional text document @ therefore @ is difficult @ essential to develop @ effective method of sentiment exploration in social network @ in @ @ @ first applied a neural network model namely @ selforganizing map to cluster similar message and sentiment keywords respectively @ @ @ developed @ association discovery process to find @ association @ a message and some sentiment keywords @ @ sentiment of a message is @ determined according to @ association @ @ performed experiment on twitter message and obtained promising @ @ @ international publishing switzerland @ 
2999,Sentiment analysis and city branding,"The Web is a huge virtual space where to express and share individual opinions, influencing any aspect of life, with implications for marketing and communication alike. Social Media are already an important marketing arena. This paper describes, on one hand, the characteristics of Sentiment Analysis and, on the other hand, the results of its application to an empirical research on the city of Bologna and on its brand perception on the Web. In the international scenario a growing number of cities compete with each other in order to attract: investors and foreign companies; different types of tourists, and new residents. City branding can be considered the starting point for developing effective policy of city marketing. The Bologna City Branding Project aims at increasing the effectiveness of territorial marketing policies carried out by the municipality of Bologna. This study partially confirms and partially rejects what many sectors of the city would have expected from the perception of Bologna on the Web. From the point of view of academic research, it has shown the potential of Sentiment Analysis in the study of perception of the city brand. Further investigations should be made to integrate this approach with the more qualitative and quantitative techniques. From the point of view of the place marketing of cities, the results of this research have shown that place marketing is a complex activity and that, in order to be more effective, an integrated plurality of approaches have to be promoted and used. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,3,@ web is a huge virtual space @ to express and share individual opinion influencing @ aspect of life @ implication @ marketing and communication alike @ social medium @ already @ important marketing arena @ @ @ describes on @ hand @ characteristic of sentiment analysis and on @ @ hand @ @ of @ application to @ empirical research on @ city of bologna and on @ brand perception on @ web @ in @ international scenario a growing number of city compete @ @ @ in order to attract @ investor and foreign company @ different type of tourist and @ resident @ city branding @ @ considered @ starting point @ developing effective policy of city marketing @ @ bologna city branding project aim at increasing @ effectiveness of territorial marketing policy carried @ by @ municipality of bologna @ @ study partially confirms and partially reject @ many sector of @ city would @ expected @ @ perception of bologna on @ web @ @ @ point of view of @ research @ ha @ @ potential of sentiment analysis in @ study of perception of @ city brand @ @ investigation @ @ made to integrate @ approach @ @ more qualitative and quantitative technique @ @ @ point of view of @ place marketing of city @ @ of @ research @ @ @ place marketing is a complex activity and @ in order to @ more effective @ integrated plurality of approach @ to @ promoted and used @ @ international publishing switzerland @ 
3000,Opinion-driven communities' detection,"Purpose - The purpose of this paper is to address the challenge of opinion mining in text documents to perform further analysis such as community detection and consistency control. More specifically, we aim to identify and extract opinions from natural language documents and to represent them in a structured manner to identify communities of opinion holders based on their common opinions. Another goal is to rapidly identify similar or contradictory opinions on a target issued by different holders. Design/methodology/approach - For the opinion extraction problem we opted for a supervised approach focusing on the feature selection problem to improve our classification results. On the community detection problem, we rely on the Infomap community detection algorithm and the multi-scale community detection framework used on a graph representation based on the available opinions and social data. Findings - The classification performance in terms of precision and recall was significantly improved by adding a set of ""meta-features"" based on grouping rules of certain part of speech (POS) instead of the actual words. Concerning the evaluation of the community detection feature, we have used two quality metrics: the network modularity and the normalized mutual information (NMI). We evaluated seven one-target similarity functions and ten multi-target aggregation functions and concluded that linear functions perform poorly for data sets with multiple targets, while functions that calculate the average similarity have greater resilience to noise. Originality/value - Although our solution relies on existing approaches, we managed to adapt and integrate them in an efficient manner. Based on the initial experimental results obtained, we managed to integrate original enhancements to improve the performance of the obtained results. © Emerald Group Publishing Limited.",2014,International Journal of Web Information Systems,2,purpose @ purpose of @ @ is to address @ challenge of opinion mining in text document to perform @ analysis @ a community detection and consistency control @ more specifically @ aim to identify and extract opinion @ natural language document and to represent @ in a structured manner to identify community of opinion holder based on @ common opinion @ another goal is to rapidly identify similar @ contradictory opinion on a target issued by different holder @ design methodology approach @ @ opinion extraction problem @ opted @ a supervised approach focusing on @ feature selection problem to improve @ classification @ @ on @ community detection problem @ rely on @ infomap community detection algorithm and @ multi-scale community detection framework used on a graph representation based on @ available opinion and social data @ finding @ classification performance in term of precision and recall wa significantly improved by adding a set of @ meta-features @ based on grouping rule of certain part of speech @ po @ instead of @ actual word @ concerning @ evaluation of @ community detection feature @ @ used @ quality metric @ @ network modularity and @ normalized mutual information @ nmi @ @ @ evaluated seven one-target similarity function and ten multi-target aggregation function and concluded @ linear function perform poorly @ data set @ multiple target @ function @ calculate @ average similarity @ greater resilience to noise @ originality value although @ solution relies on existing approach @ managed to adapt and integrate @ in @ efficient manner @ based on @ initial experimental @ obtained @ managed to integrate original enhancement to improve @ performance of @ obtained @ @ emerald group publishing limited @ 
3001,Automatic selection of verbs-markers for segmentation task of process descriptions in natural language texts,"The paper presents the intermediate results of the research, the final goal of which is to develop the universal algorithm for process diagrams automatic visualization by text description of these processes. The purpose of this study is to check the use of verbs as markers for the semantic labeling of long fragments in scientific texts.",2014,CEUR Workshop Proceedings,0,@ @ @ @ intermediate @ of @ research @ final goal of @ is to develop @ universal algorithm @ process diagram automatic visualization by text description of @ process @ @ purpose of @ study is to check @ use of verb a marker @ @ semantic labeling of long fragment in scientific text @ 
3002,Predicting the impact of central bank communications on financial market investors' interest rate expectations,"In this paper, we design an automated system that predicts the impact of central bank communications on investors' interest rate expectations. Our corpus is the Bank of England's 'Monetary Policy Committee Minutes'. Prior studies suggest that effective communications can mitigate a financial crisis; ineffective communications may exacerbate one. The system described here works in four phases. First, the system detects salient aspects associated with economic growth, prices, interest rates and bank lending using information from Wikipedia. These economic aspects are detected using the TextRank link analysis algorithm. A multinomial Naive Bayesian model then classifies document sentences to these aspects. The second phase measures sentiment using a count of terms from the General Inquirer dictionary. The third phase employs Latent Dirichlet Allocation (LDA) to infer topic clusters that may acts as intensifiers/ diminishers for the economic aspects. Finally, an ensemble tree combines the phases to predict the impact of the communications on financial market interest rates.",2014,CEUR Workshop Proceedings,0,in @ @ @ design @ automated system @ predicts @ impact of central bank communication on investor @ interest rate expectation @ @ corpus is @ bank of england @ s @ monetary policy committee minute @ @ prior study suggest @ effective communication @ mitigate a financial crisis @ ineffective communication may exacerbate @ @ @ system described @ work in four phase @ first @ system detects salient aspect associated @ economic growth price interest rate and bank lending @ information @ wikipedia @ @ economic aspect @ detected @ @ textrank link analysis algorithm @ a multinomial naive bayesian model @ classifies document sentence to @ aspect @ @ second phase measure sentiment @ a count of term @ @ general inquirer dictionary @ @ third phase employ latent dirichlet allocation @ lda @ to infer topic cluster @ may act a intensifier diminishers @ @ economic aspect @ finally @ ensemble tree combine @ phase to predict @ impact of @ communication on financial market interest rate @ 
3007,Chemical named entity recognition: Improving recall using a comprehensive list of lexical features,"As the number of published scientific papers grows everyday, there is also an increasing necessity for automated named entity recognition (NER) systems capable of identifying relevant entities mentioned in a given text, such as chemical entities. Since high precision values are crucial to deliver useful results, we developed a NER method, Identifying Chemical Entities (ICE), which was tuned for precision. Thus, ICE achieved the second highest precision value in the BioCreative IV CHEMDNER task, but with significant low recall values. However, this paper shows how the use of simple lexical features was able to improve the recall of ICE while maintaining high levels of precision. Using a selection of the best features tested, ICE obtained a best recall of 27.2% for a precision of 92.4%. © Springer International Publishing Switzerland 2014.",2014,Advances in Intelligent Systems and Computing,0,a @ number of published scientific @ grows everyday @ is @ @ increasing necessity @ automated named entity recognition @ ner @ system capable of identifying relevant entity mentioned in a given text @ a chemical entity @ since high precision value @ crucial to deliver useful @ @ developed a ner method identifying chemical entity @ ice @ @ wa tuned @ precision @ thus ice achieved @ second highest precision value in @ biocreative @ chemdner task @ @ significant low recall value @ however @ @ @ @ @ use of simple lexical feature wa able to improve @ recall of ice @ maintaining high level of precision @ @ a selection of @ best feature tested ice obtained a best recall of @ @ a precision of @ @ @ international publishing switzerland @ 
3008,Automatically detecting and rating product aspects from textual customer reviews,"This paper proposes a new approach to aspect-based sentiment analysis. The goal of our algorithm is to obtain a summary of the most positive and the most negative aspects of a specific product, given a collection of free-text customer reviews. Our approach starts by matching handcrafted dependency paths in individual sentences to find opinions expressed towards candidate aspects. Then, it clusters together different mentions of the same aspect by using a WordNet-based similarity measure. Finally, it computes a sentiment score for each aspect, which represents the overall emerging opinion of a group of customers towards a specific aspect of the product. Our approach does not require any seed word or domain-specific knowledge, as it only employs an off-the-shelf sentiment lexicon. We discuss encouraging preliminary results in detecting and rating aspects from on-line reviews of movies and MP3 players. Copyright © by the paper's authors.",2014,CEUR Workshop Proceedings,27,@ @ proposes a @ approach to aspect-based sentiment analysis @ @ goal of @ algorithm is to obtain a summary of @ @ positive and @ @ negative aspect of a specific product given a collection of free-text customer review @ @ approach start by matching handcrafted dependency path in individual sentence to find opinion expressed towards candidate aspect @ @ @ cluster together different mention of @ @ aspect by @ a wordnet-based similarity measure @ finally @ computes a sentiment score @ @ aspect @ represents @ overall emerging opinion of a group of customer towards a specific aspect of @ product @ @ approach doe not require @ seed word @ domain-specific knowledge a @ only employ @ off-the-shelf sentiment lexicon @ @ discus encouraging preliminary @ in detecting and rating aspect @ on-line review of movie and mp player @ @ by @ @ @ s author @ 
3011,"OCMiner: Text processing, annotation and relation extraction for the life sciences","We present OCMiner, a high-performance text processing system for large document collections of scientific publications. Several linguistic options allow adjusting the quality of annotation results which can be specialized and fine-tuned for the recognition of Life Science terms. Recognized terms are mapped to semantic concepts which are ontologically located within their respective domain taxonomies. Relying on a correct identification and semantic interpretation of mentions of domain concepts, relations between entities are extracted. The annotated text, as well as extracted knowledge triples, can be visualized on a web-based front-end at http://www.ocminer.com/, permitting an explorative information retrieval.",2014,CEUR Workshop Proceedings,0,@ @ ocminer a high-performance text processing system @ @ document collection of scientific publication @ several linguistic option allow adjusting @ quality of annotation @ @ @ @ specialized and fine-tuned @ @ recognition of life science term @ recognized term @ mapped to semantic concept @ @ ontologically located within @ respective domain taxonomy @ relying on a correct identification and semantic interpretation of mention of domain concept relation @ entity @ extracted @ @ annotated text a well a extracted knowledge triple @ @ visualized on a web-based front-end at http @ www @ ocminer @ com permitting @ explorative information retrieval @ 
3025,Fuzzy ontology-based approach for automatic construction of user profiles,"This paper shows a fuzzy ontology based approach to automatically build user profiles from a collection of user interest documents. The ontological representation of the user profile enhances the performance in tasks such as filtering, categorization and information retrieval. The proposed technique takes advantage of relevance measures to generate semantic representations of user context. The proposed work also presents a strategy for automatic generation of fuzzy ontologies to support user profile modeling. The experiments performed confirm that the automatically obtained fuzzy ontologies are good representation of the user's preferences. In order to test the applicability of the obtained ontologies, a text categorization experiment has been proposed and the obtained results indicate that the approach can be applied with satisfactory results and warrants further research. © 2014 Springer International Publishing.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ @ @ a fuzzy ontology based approach to automatically build user profile @ a collection of user interest document @ @ ontological representation of @ user profile enhances @ performance in task @ a filtering categorization and information retrieval @ @ proposed technique take advantage of relevance measure to generate semantic representation of user context @ @ proposed work @ @ a strategy @ automatic generation of fuzzy ontology to support user profile modeling @ @ experiment performed confirm @ @ automatically obtained fuzzy ontology @ good representation of @ user @ s preference @ in order to test @ applicability of @ obtained ontology a text categorization experiment ha @ proposed and @ obtained @ indicate @ @ approach @ @ applied @ satisfactory @ and warrant @ research @ @ international publishing @ 
3026,Adapting Gloss Vector semantic relatedness measure for semantic similarity estimation: An evaluation in the biomedical domain,"Automatic methods of ontology alignment are essential for establishing interoperability across web services. These methods are needed to measure semantic similarity between two ontologies' entities to discover reliable correspondences. While existing similarity measures suffer from some difficulties, semantic relatedness measures tend to yield better results; even though they are not completely appropriate for the 'equivalence' relationship (e.g. ""blood"" and ""bleeding"" related but not similar). We attempt to adapt Gloss Vector relatedness measure for similarity estimation. Generally, Gloss Vector uses angles between entities' gloss vectors for relatedness calculation. After employing Pearson's chi-squared test for statistical elimination of insignificant features to optimize entities' gloss vectors, by considering concepts' taxonomy, we enrich them for better similarity measurement. Discussed measures get evaluated in the biomedical domain using MeSH, MEDLINE and dataset of 301 concept pairs. We conclude Adapted Gloss Vector similarity results are more correlated with human judgment of similarity compared to other measures. © 2014 Springer International Publishing.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,automatic method of ontology alignment @ essential @ establishing interoperability across web service @ @ method @ needed to measure semantic similarity @ @ ontology @ entity to discover reliable correspondence @ @ existing similarity measure suffer @ some difficulty semantic relatedness measure tend to yield better @ @ even though @ @ not completely appropriate @ @ @ equivalence @ relationship @ e @ g @ @ blood @ and @ bleeding @ related @ not similar @ @ @ attempt to adapt gloss vector relatedness measure @ similarity estimation @ generally gloss vector us angle @ entity @ gloss vector @ relatedness calculation @ @ employing pearson @ s chi-squared test @ statistical elimination of insignificant feature to optimize entity @ gloss vector by considering concept @ taxonomy @ enrich @ @ better similarity measurement @ discussed measure get evaluated in @ biomedical domain @ mesh medline and dataset of concept pair @ @ conclude adapted gloss vector similarity @ @ more correlated @ human judgment of similarity compared to @ measure @ @ international publishing @ 
3038,Spatially aware term selection for geotagging,"The task of assigning geographic coordinates to textual resources plays an increasingly central role in geographic information retrieval. The ability to select those terms from a given collection that are most indicative of geographic location is of key importance in successfully addressing this task. However, this process of selecting spatially relevant terms is at present not well understood, and the majority of current systems are based on standard term selection techniques, such as χ2 or information gain, and thus fail to exploit the spatial nature of the domain. In this paper, we propose two classes of term selection techniques based on standard geostatistical methods. First, to implement the idea of spatial smoothing of term occurrences, we investigate the use of kernel density estimation (KDE) to model each term as a two-dimensional probability distribution over the surface of the Earth. The second class of term selection methods we consider is based on Ripley's K statistic, which measures the deviation of a point set from spatial homogeneity. We provide experimental results which compare these classes of methods against existing baseline techniques on the tasks of assigning coordinates to Flickr photos and to Wikipedia articles, revealing marked improvements in cases where only a relatively small number of terms can be selected. © 1989-2012 IEEE.",2014,IEEE Transactions on Knowledge and Data Engineering,28,@ task of assigning geographic coordinate to textual resource play @ increasingly central role in geographic information retrieval @ @ ability to select @ term @ a given collection @ @ @ indicative of geographic location is of key importance in successfully addressing @ task @ however @ process of selecting spatially relevant term is at @ not well understood and @ majority of current system @ based on standard term selection technique @ a χ @ information gain and thus fail to exploit @ spatial nature of @ domain @ in @ @ @ propose @ class of term selection technique based on standard geostatistical method @ first to implement @ idea of spatial smoothing of term occurrence @ investigate @ use of kernel density estimation @ kde @ to model @ term a a two-dimensional probability distribution @ @ surface of @ earth @ @ second class of term selection method @ consider is based on ripley @ s k statistic @ measure @ deviation of a point set @ spatial homogeneity @ @ provide experimental @ @ compare @ class of method @ existing baseline technique on @ task of assigning coordinate to flickr photo and to wikipedia article revealing marked improvement in case @ only a relatively small number of term @ @ selected @ @ @ 
3040,Boosting text classification through stemming of composite words,"Text mining is a knowledge intensive process with the main purpose of effectively and efficiently processing large amounts of unstructured data. Due to the rapidly growing amount of raw text available there is a strong need for methods that are capable of dealing with this in terms of automatic classification or indexing. In this context, an essential task is the semantic processing of natural language in order to provide a sound input to the text classification or categorization task. One of the important tasks is stemming which is the process of reducing a certain word to its root (or stem). When a text is pre-processed for mining purposes, stemming is applied in order to bring words from their current variation to their original root in order to better process the natural language with subsequent steps. A challenging task is that of stemming composite words which in many languages form a large part of the daily used vocabulary. In this paper we develop a novel rule-based algorithm for stemming composite words and we show through extensive experiments that the text classification accuracy greatly improves by stemming composite words. © Springer International Publishing Switzerland 2014",2014,Advances in Intelligent Systems and Computing,7,text mining is a knowledge intensive process @ @ main purpose of effectively and efficiently processing @ amount of unstructured data @ due to @ rapidly growing amount of raw text available @ is a strong need @ method @ @ capable of dealing @ @ in term of automatic classification @ indexing @ in @ context @ essential task is @ semantic processing of natural language in order to provide a sound input to @ text classification @ categorization task @ @ of @ important task is stemming @ is @ process of reducing a certain word to @ root @ @ stem @ @ @ a text is pre-processed @ mining purpose stemming is applied in order to bring word @ @ current variation to @ original root in order to better process @ natural language @ subsequent step @ a challenging task is @ of stemming composite word @ in many language form a @ part of @ daily used vocabulary @ in @ @ @ develop a novel rule-based algorithm @ stemming composite word and @ @ @ extensive experiment @ @ text classification accuracy greatly improves by stemming composite word @ @ international publishing switzerland 
3042,Finding abbreviations in biomedical literature: three BioC-compatible modules and four BioC-formatted corpora,"BioC is a recently created XML format to share text data and annotations, and an accompanying input/output library to promote interoperability of data and tools for natural language processing of biomedical text. This article reports the use of BioC to address a common challenge in processing biomedical text information that of frequent entity name abbreviation. We selected three different abbreviation definition identification modules, and used the publicly available BioC code to convert these independent modules into BioC-compatible components that interact seamlessly with BioC-formatted data, and other BioC-compatible modules. In addition, we consider four manually annotated corpora of abbreviations in biomedical text: the Ab3P corpus of 1250 PubMed abstracts, the BIOADI corpus of 1201 PubMed abstracts, the old MEDSTRACT corpus of 199 PubMedVR citations and the Schwartz and Hearst corpus of 1000 PubMed abstracts. Annotations in these corpora have been re-evaluated by four annotators and their consistency and quality levels have been improved. We converted them to BioC-format and described the representation of the annotations. These corpora are used to measure the three abbreviation-finding algorithms and the results are given. The BioC-compatible modules, when compared with their original form, have no difference in their efficiency, running time or any other comparable aspects. They can be conveniently used as a common pre-processing step for larger multi-layered text-mining endeavors.",2014,Database,9,bioc is a recently created xml format to share text data and annotation and @ accompanying input output library to promote interoperability of data and tool @ natural language processing of biomedical text @ @ article report @ use of bioc to address a common challenge in processing biomedical text information @ of frequent entity name abbreviation @ @ selected three different abbreviation definition identification module and used @ publicly available bioc code to convert @ independent module @ bioc-compatible component @ interact seamlessly @ bioc-formatted data and @ bioc-compatible module @ in addition @ consider four manually annotated corpus of abbreviation in biomedical text @ @ ab p corpus of pubmed abstract @ bioadi corpus of pubmed abstract @ old medstract corpus of pubmedvr citation and @ schwartz and hearst corpus of pubmed abstract @ annotation in @ corpus @ @ re-evaluated by four annotator and @ consistency and quality level @ @ improved @ @ converted @ to bioc-format and described @ representation of @ annotation @ @ corpus @ used to measure @ three abbreviation-finding algorithm and @ @ @ given @ @ bioc-compatible module @ compared @ @ original form @ no difference in @ efficiency running time @ @ @ comparable aspect @ @ @ @ conveniently used a a common pre-processing step @ larger multi-layered text-mining endeavor @ 
3048,Graph-based domain-specific semantic relatedness from Wikipedia,"Human made ontologies and lexicons are promising resources for many text mining tasks in domain specific applications, but they do not exist for most domains. We study the suitability of Wikipedia as an alternative resource for ontologies regarding the Semantic Relatedness problem. We focus on the biomedical domain because (1) high quality manually curated ontologies are available and (2) successful graph based methods have been proposed for semantic relatedness in this domain. Because Wikipedia is not hierarchical and links do not convey defined semantic relationships, the same methods used on lexical resources (such as WordNet) cannot be applied here straightforwardly. Our contributions are (1) Demonstrating that Wikipedia based methods outperform state of the art ontology based methods on most of the existing ontologies in the biomedical domain (2) Adapting and evaluating the effectiveness of a group of bibliometric methods of various degrees of sophistication on Wikipedia for the first time (3) Proposing a new graph-based method that is outperforming existing methods by considering some specific features of Wikipedia structure. © 2014 Springer International Publishing.",2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,human made ontology and lexicon @ promising resource @ many text mining task in domain specific application @ @ @ not exist @ @ domain @ @ study @ suitability of wikipedia a @ alternative resource @ ontology regarding @ semantic relatedness problem @ @ focus on @ biomedical domain @ @ @ high quality manually curated ontology @ available and @ @ successful graph based method @ @ proposed @ semantic relatedness in @ domain @ @ wikipedia is not hierarchical and link @ not convey defined semantic relationship @ @ method used on lexical resource @ @ a wordnet @ cannot @ applied @ straightforwardly @ @ contribution @ @ @ demonstrating @ wikipedia based method outperform state of @ art ontology based method on @ of @ existing ontology in @ biomedical domain @ @ adapting and evaluating @ effectiveness of a group of bibliometric method of various degree of sophistication on wikipedia @ @ first time @ @ proposing a @ graph-based method @ is outperforming existing method by considering some specific feature of wikipedia structure @ @ international publishing @ 
3052,Discovering relations by entity search in lightweight semantic text graphs,"Entity search is becoming a popular alternative for full text search. Recently Google released its entity search based on confirmed, human-generated data such as Wikipedia. In spite of these developments, the task of entity discovery, search, or relation search in unstructured text remains a major challenge in the fields of information retrieval and information extraction. This paper tries to address that challenge, focusing specifically on entity relation discovery. This is achieved by processing unstructured text using simple information extraction methods, building lightweight semantic graphs and reusing them for entity relation discovery by applying algorithms from graph theory. An important part is also user interaction with semantic graphs, which can significantly improve information extraction results and entity relation search. Entity relations can be discovered by various text mining methods, but the advantage of the presented method lies in the similarity between the lightweight semantics extracted from a text and the information networks available as structured data. Both graph structures have similar properties and similar relation discovery algorithms can be applied. In addition, we can benefit from the integration of such graph data. We provide both a relevance and performance evaluations of the approach and showcase it in several use case applications.",2014,Computing and Informatics,2,entity search is becoming a popular alternative @ full text search @ recently google released @ entity search based on confirmed human-generated data @ a wikipedia @ in spite of @ development @ task of entity discovery search @ relation search in unstructured text remains a major challenge in @ field of information retrieval and information extraction @ @ @ try to address @ challenge focusing specifically on entity relation discovery @ @ is achieved by processing unstructured text @ simple information extraction method building lightweight semantic graph and reusing @ @ entity relation discovery by applying algorithm @ graph theory @ @ important part is @ user interaction @ semantic graph @ @ significantly improve information extraction @ and entity relation search @ entity relation @ @ discovered by various text mining method @ @ advantage of @ presented method lie in @ similarity @ @ lightweight semantics extracted @ a text and @ information network available a structured data @ @ graph structure @ similar property and similar relation discovery algorithm @ @ applied @ in addition @ @ benefit @ @ integration of @ graph data @ @ provide @ a relevance and performance evaluation of @ approach and showcase @ in several use case application @ 
3054,Acknowledgments in scientific publications: Presence in spanish science and text patterns across disciplines,"The acknowledgments in scientific publications are an important feature in the scholarly communication process. This research analyzes funding acknowledgment presence in scientific publications and introduces a novel approach for discovering text patterns by discipline in the acknowledgment section of papers. First, the presence of acknowledgments in 38,257 Englishlanguage papers published by Spanish researchers in 2010 is studied by subject area on the basis of the funding acknowledgment information available in the Webof Science database. Funding acknowledgments are present in two thirds of Spanish articles, with significant differences by subject area, number of authors, impact factor of journals, and, in one specific area, basic/applied nature of research. Second, the existence of specific acknowledgment patterns in English-language papers of Spanish researchers in 4 selected subject categories (cardiac and cardiovascular systems, economics, evolutionary biology, and statistics and probability) is explored through a combination of text mining and multivariate analyses. ""Peer interactive communication"" predominates in the more theoretical or social-oriented fields (statistics and probability, economics), whereas the recognition of technical assistance is more common in experimental research (evolutionary biology), and the mention of potential conflicts of interest emerges forcefully in the clinical field (cardiac and cardiovascular systems). The systematic inclusion of structured data about acknowledgments in journal articles and bibliographic databases would have a positive impact on the study of collaboration practices in science. © 2014 ASIS&T.",2014,Journal of the Association for Information Science and Technology,26,@ acknowledgment in scientific publication @ @ important feature in @ scholarly communication process @ @ research analyzes funding acknowledgment presence in scientific publication and introduces a novel approach @ discovering text pattern by discipline in @ acknowledgment section of @ @ first @ presence of acknowledgment in englishlanguage @ published by spanish researcher in is studied by subject area on @ basis of @ funding acknowledgment information available in @ webof science database @ funding acknowledgment @ @ in @ third of spanish article @ significant difference by subject area number of author impact factor of journal and in @ specific area basic applied nature of research @ second @ existence of specific acknowledgment pattern in english-language @ of spanish researcher in selected subject category @ cardiac and cardiovascular system economics evolutionary biology and statistic and probability @ is explored @ a combination of text mining and multivariate analysis @ @ peer interactive communication @ predominates in @ more theoretical @ social-oriented field @ statistic and probability economics @ whereas @ recognition of technical assistance is more common in experimental research @ evolutionary biology @ and @ mention of potential conflict of interest emerges forcefully in @ clinical field @ cardiac and cardiovascular system @ @ @ systematic inclusion of structured data @ acknowledgment in journal article and bibliographic database would @ a positive impact on @ study of collaboration practice in science @ asis t @ 
3056,Multi-transfer: Transfer learning with multiple views and multiple sources,"Transfer learning, which aims to help learning tasks in a target domain by leveraging knowledge from auxiliary domains, has been demonstrated to be effective in different applications such as text mining, sentiment analysis, and so on. In addition, in many real-world applications, auxiliary data are described from multiple perspectives and usually carried by multiple sources. For example, to help classify videos on Youtube, which include three perspectives: image, voice and subtitles, one may borrow data from Flickr, Last.FM and Google News. Although any single instance in these domains can only cover a part of the views available on Youtube, the piece of information carried by them may compensate one another. If we can exploit these auxiliary domains in a collective manner, and transfer the knowledge to the target domain, we can improve the target model building from multiple perspectives. In this article, we consider this transfer learning problem as Transfer Learning with Multiple Views and Multiple Sources. As different sources may have different probability distributions and different views may compensate or be inconsistent with each other, merging all data in a simplistic manner will not give an optimal result. Thus, we propose a novel algorithm to leverage knowledge from different views and sources collaboratively, by letting different views from different sources complement each other through a co-training style framework, at the same time, it revises the distribution differences in different domains. We conduct empirical studies on several real-world datasets to show that the proposed approach can improve the classification accuracy by up to 8% against different kinds of state-of-the-art baselines. © 2014 Wiley Periodicals, Inc.",2014,Statistical Analysis and Data Mining,20,transfer learning @ aim to help learning task in a target domain by leveraging knowledge @ auxiliary domain ha @ demonstrated to @ effective in different application @ a text mining sentiment analysis and @ on @ in addition in many real-world application auxiliary data @ described @ multiple perspective and usually carried by multiple source @ @ example to help classify video on youtube @ include three perspective @ image voice and subtitle @ may borrow data @ flickr last @ fm and google news @ although @ single instance in @ domain @ only cover a part of @ view available on youtube @ piece of information carried by @ may compensate @ another @ if @ @ exploit @ auxiliary domain in a collective manner and transfer @ knowledge to @ target domain @ @ improve @ target model building @ multiple perspective @ in @ article @ consider @ transfer learning problem a transfer learning @ multiple view and multiple source @ a different source may @ different probability distribution and different view may compensate @ @ inconsistent @ @ @ merging @ data in a simplistic manner @ not give @ optimal @ @ thus @ propose a novel algorithm to leverage knowledge @ different view and source collaboratively by letting different view @ different source complement @ @ @ a co-training style framework at @ @ time @ revise @ distribution difference in different domain @ @ conduct empirical study on several real-world datasets to @ @ @ proposed approach @ improve @ classification accuracy by up to @ different kind of state-of-the-art baseline @ wiley periodical inc @ 
3060,Distance weighted cosine similarity measure for text classification,"In Vector Space Model, Cosine is widely used to measure the similarity between two vectors. Its calculation is very efficient, especially for sparse vectors, as only the non-zero dimensions need to be considered. As a fundamental component, cosine similarity has been applied in solving different text mining problems, such as text classification, text summarization, information retrieval, question answering, and so on. Although it is popular, the cosine similarity does have some problems. Starting with a few synthetic samples, we demonstrate some problems of cosine similarity: it is overly biased by features of higher values and does not care much about how many features two vectors share. A distance weighted cosine similarity metric is thus proposed. Extensive experiments on text classification exhibit the effectiveness of the proposed metric. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),49,in vector space model cosine is widely used to measure @ similarity @ @ vector @ @ calculation is @ efficient especially @ sparse vector a only @ non-zero dimension need to @ considered @ a a fundamental component cosine similarity ha @ applied in solving different text mining problem @ a text classification text summarization information retrieval question answering and @ on @ although @ is popular @ cosine similarity doe @ some problem @ starting @ a @ synthetic sample @ demonstrate some problem of cosine similarity @ @ is overly biased by feature of higher value and doe not care much @ @ many feature @ vector share @ a distance weighted cosine similarity metric is thus proposed @ extensive experiment on text classification exhibit @ effectiveness of @ proposed metric @ springer-verlag @ 
3067,Text mining and data mining in knowledge organization and discovery: The making of knowledge-based products,"Discusses the importance of knowledge organization in the context of the information overload caused by the vast quantities of data and information accessible on internal and external networks of an organization. Defines the characteristics of a knowledge-based product. Elaborates on the techniques and applications of text mining in developing knowledge products. Presents two approaches, as case studies, to the making of knowledge products: (1) steps and processes in the planning,designing and development of a composite multilingual multimedia CD product, with the potential international, inter-cultural end users in view, and (2) application of natural language processing software in text mining. Using a text mining software, it is possible to link concept terms from a processed text to a related thesaurus, glossary, schedules of a classification scheme, and facet structured subject representations. Concludes that the products of text mining and data mining could be made more useful if the features of a faceted scheme for subject classification are incorporated into text mining techniques and products. © 2003 by The Haworth Press, Inc. All rights reserved.",2013,Knowledge Organization and Classification in International Information Retrieval,5,discus @ importance of knowledge organization in @ context of @ information overload caused by @ vast quantity of data and information accessible on internal and external network of @ organization @ defines @ characteristic of a knowledge-based product @ elaborates on @ technique and application of text mining in developing knowledge product @ @ @ approach a case study to @ making of knowledge product @ @ @ step and process in @ planning designing and development of a composite multilingual multimedia cd product @ @ potential international inter-cultural end user in view and @ @ application of natural language processing software in text mining @ @ a text mining software @ is possible to link concept term @ a processed text to a related thesaurus glossary schedule of a classification scheme and facet structured subject representation @ concludes @ @ product of text mining and data mining could @ made more useful if @ feature of a faceted scheme @ subject classification @ incorporated @ text mining technique and product @ by @ haworth @ inc @ @ right reserved @ 
3072,BioC: A minimalist approach to interoperability for biomedical text processing,"A vast amount of scientific information is encoded in natural language text, and the quantity of such text has become so great that it is no longer economically feasible to have a human as the first step in the search process. Natural language processing and text mining tools have become essential to facilitate the search for and extraction of information from text. This has led to vigorous research efforts to create useful tools and to create humanly labeled text corpora, which can be used to improve such tools. To encourage combining these efforts into larger, more powerful and more capable systems, a common interchange format to represent, store and exchange the data in a simple manner between different language processing systems and text mining tools is highly desirable. Here we propose a simple extensible mark-up language format to share text documents and annotations. The proposed annotation approach allows a large number of different annotations to be represented including sentences, tokens, parts of speech, named entities such as genes or diseases and relationships between named entities. In addition, we provide simple code to hold this data, read it from and write it back to extensible mark-up language files and perform some sample processing. We also describe completed as well as ongoing work to apply the approach in several directions. Code and data are available at http://bioc.sourceforge.net/.",2013,Database,78,a vast amount of scientific information is encoded in natural language text and @ quantity of @ text ha become @ great @ @ is no longer economically feasible to @ a human a @ first step in @ search process @ natural language processing and text mining tool @ become essential to facilitate @ search @ and extraction of information @ text @ @ ha led to vigorous research effort to create useful tool and to create humanly labeled text corpus @ @ @ used to improve @ tool @ to encourage combining @ effort @ larger more powerful and more capable system a common interchange format to represent store and exchange @ data in a simple manner @ different language processing system and text mining tool is highly desirable @ @ @ propose a simple extensible mark-up language format to share text document and annotation @ @ proposed annotation approach allows a @ number of different annotation to @ represented including sentence token part of speech named entity @ a gene @ disease and relationship @ named entity @ in addition @ provide simple code to hold @ data read @ @ and write @ back to extensible mark-up language file and perform some sample processing @ @ @ describe completed a well a ongoing work to apply @ approach in several direction @ code and data @ available at http @ bioc @ sourceforge @ net @ 
3073,Applying latent semantic analysis to optimize second-order co-occurrence vectors for semantic relatedness measurement,"Measures of semantic relatedness are largely applicable in intelligent tasks of NLP and Bioinformatics. By taking these automated measures into account, this paper attempts to improve Second-order Co-occurrence Vector semantic relatedness measure for more effective estimation of relatedness between two given concepts. Typically, this measure, after constructing concepts definitions (Glosses) from a thesaurus, considers the cosine of the angle between the concepts' gloss vectors as the degree of relatedness. Nonetheless, these computed gloss vectors of concepts are impure and rather large in size which would hinder the expected performance of the measure. By employing latent semantic analysis (LSA), we try to conduct some level of insignificant feature elimination to generate economic gloss vectors. Applying both approaches to the biomedical domain, using MEDLINE as corpus, UMLS as thesaurus, and reference standard of biomedical concept-pairs manually rated for relatedness, we show LSA implementation enforces positive impact in terms of performance and efficiency. © 2013 Springer International Publishing.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,measure of semantic relatedness @ largely applicable in intelligent task of nlp and bioinformatics @ by taking @ automated measure @ account @ @ attempt to improve second-order co-occurrence vector semantic relatedness measure @ more effective estimation of relatedness @ @ given concept @ typically @ measure @ constructing concept definition @ gloss @ @ a thesaurus considers @ cosine of @ angle @ @ concept @ gloss vector a @ degree of relatedness @ nonetheless @ computed gloss vector of concept @ impure and rather @ in size @ would hinder @ expected performance of @ measure @ by employing latent semantic analysis @ lsa @ @ try to conduct some level of insignificant feature elimination to generate economic gloss vector @ applying @ approach to @ biomedical domain @ medline a corpus umls a thesaurus and reference standard of biomedical concept-pairs manually rated @ relatedness @ @ lsa implementation enforces positive impact in term of performance and efficiency @ @ international publishing @ 
3092,Webpage mining for inflation emergency early warning,"Serious inflation turbulence is a signal of potential financial crisis and social economic emergencies. Macroeconomic early warning of inflation and other major economic indicators is critical to discover potential crisis in advance. Traditional early warning methods are based on official economic statistics which are usually either released at least one month later than the economic activities actually occur or lack of flexibility to cope with fast business changes. With proper extraction and aggregation, huge amount of Internet data can serve as a new complementarity source to facilitate more timely and accurate emergency early warning. This research innovatively adopts web data processing techniques and text mining methods to extract useful information from huge amount of Internet news reports. Based on the extracted information, a price sentiment index is proposed to detect turning points of inflation efficiently. Empirical evaluation proved that the price sentiment index is efficient in inflation emergency early warning. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,serious inflation turbulence is a signal of potential financial crisis and social economic emergency @ macroeconomic early warning of inflation and @ major economic indicator is critical to discover potential crisis in advance @ traditional early warning method @ based on official economic statistic @ @ usually either released at least @ month later @ @ economic activity actually occur @ lack of flexibility to cope @ fast @ change @ @ proper extraction and aggregation huge amount of internet data @ serve a a @ complementarity source to facilitate more timely and accurate emergency early warning @ @ research innovatively adopts web data processing technique and text mining method to extract useful information @ huge amount of internet news report @ based on @ extracted information a price sentiment index is proposed to detect turning point of inflation efficiently @ empirical evaluation proved @ @ price sentiment index is efficient in inflation emergency early warning @ springer-verlag @ 
3096,Identifying authoritative and reliable contents in community question answering with domain knowledge,"Community Question Answering (CQA) has emerged as a popular forum for users to ask and answer questions. Over the last few years, CQA portals such as Yahoo answers and Baidu Zhidao have exploded in popularity, and now provide a viable alternative to general purpose Web search. A number of answers submitted to address questions on CQA sites compose a valuable knowledge repository, which could be a gold mine for information retrieval as well as text mining. Two important questions in CQA research are focused on the quality of contents and the reputation of the answerers. Previous approaches for retrieving relevant and high quality content have been proposed, but not much work has been done on providing an integrated framework to solve these two problems. Besides, no research work has used both text and link information in their methods via leveraging existing ratings of answers and questions. In this paper, we present a novel approach to analyze questions and answers based on the topic modeling framework with Dirichlet forest priors (LDA-DF)[8]. We utilize information obtained from LDA-DF to construct a joint topical and link model to identify authorities and reliable answers on a CQA site. We evaluate our methods in a dataset obtained from Yahoo! Answers. With the new representation of topical structures on CQA datasets, using a limited amount of web resource, we show significant improvements over the state-of-art methods LDA-DF, LDA, and HLDA on performance of authority identification and answer ranking. © Springer-Verlag 2013.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,community question answering @ cqa @ ha emerged a a popular forum @ user to ask and answer question @ @ @ last @ year cqa portal @ a yahoo answer and baidu zhidao @ exploded in popularity and now provide a viable alternative to general purpose web search @ a number of answer submitted to address question on cqa site compose a valuable knowledge repository @ could @ a gold mine @ information retrieval a well a text mining @ @ important question in cqa research @ focused on @ quality of content and @ reputation of @ answerer @ previous approach @ retrieving relevant and high quality content @ @ proposed @ not much work ha @ done on providing @ integrated framework to solve @ @ problem @ besides no research work ha used @ text and link information in @ method via leveraging existing rating of answer and question @ in @ @ @ @ a novel approach to analyze question and answer based on @ topic modeling framework @ dirichlet forest prior @ lda-df @ @ @ utilize information obtained @ lda-df to construct a joint topical and link model to identify authority and reliable answer on a cqa site @ @ evaluate @ method in a dataset obtained @ yahoo @ answer @ @ @ @ representation of topical structure on cqa datasets @ a limited amount of web resource @ @ significant improvement @ @ state-of-art method lda-df lda and hlda on performance of authority identification and answer ranking @ springer-verlag @ 
3105,Processing of spatial and temporal information in the text,"The paper presents a mining methodology, which is based on processing of spatial and temporal information in natural language text. It is based on methodology capable of recognizing the relationship between events and objects in the text, using spatial and temporal relationships between these events to enhance the semantic coherence of natural language text. Heterogeneous semantic network with extended family relationships is used to implement relational-situational analysis between the words in the sentence. For correlation lining between spatial and temporal categories and events in the text the mechanism of Markov-logic networks is used, allowing to operate in the peaks of Markov-logic networks with the normal logical formulas representing the combined spatial-temporal logic. The described system can be used in tasks of semantic search. © IDOSI Publications, 2013.",2013,World Applied Sciences Journal,4,@ @ @ a mining methodology @ is based on processing of spatial and temporal information in natural language text @ @ is based on methodology capable of recognizing @ relationship @ event and object in @ text @ spatial and temporal relationship @ @ event to enhance @ semantic coherence of natural language text @ heterogeneous semantic network @ extended family relationship is used to implement relational-situational analysis @ @ word in @ sentence @ @ correlation lining @ spatial and temporal category and event in @ text @ mechanism of markov-logic network is used allowing to operate in @ peak of markov-logic network @ @ normal logical formula representing @ combined spatial-temporal logic @ @ described system @ @ used in task of semantic search @ idosi publication @ 
3106,Extraction of functions of a job position using text mining in electronic mail,"A methodology to extract, from a group of electronic mails belonging to workers with defined responsibilities, a sample of electronic mails that are representative of the tasks assigned to the workers. This is done through natural language processing tools and data mining. The results showed that the methodology represents a good approach for extracting the functions of the workers, allowing 65% of the work descriptions in the evaluation stage. Thus, the proposed methodology can be used with relative confidence to gathered information about workers that suddenly quit their job positions and also for position changes and promotions.",2013,Informacion Tecnologica,1,a methodology to extract @ a group of electronic mail belonging to worker @ defined responsibility a sample of electronic mail @ @ representative of @ task assigned to @ worker @ @ is done @ natural language processing tool and data mining @ @ @ showed @ @ methodology represents a good approach @ extracting @ function of @ worker allowing of @ work description in @ evaluation stage @ thus @ proposed methodology @ @ used @ relative confidence to gathered information @ worker @ suddenly quit @ job position and @ @ position change and promotion @ 
3107,Document analytics through entity resolution,"We present a prototype system for resolving named entities, mentioned in textual documents, into the corresponding Wikipedia entities. This prototype can aid in document analysis, by using the disambiguated references to provide useful information in context. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ a prototype system @ resolving named entity mentioned in textual document @ @ corresponding wikipedia entity @ @ prototype @ aid in document analysis by @ @ disambiguated reference to provide useful information in context @ springer-verlag @ 
3108,GraphSum: Discovering correlations among multiple terms for graph-based summarization,"Graph-based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph-based model to represent the correlations between pairs of document terms. However, since the high-order correlations among multiple terms are disregarded during graph evaluation, the summarization performance could be limited unless integrating ad hoc language-dependent or semantics-based analysis. This paper presents a novel and general-purpose graph-based summarizer, namely GraphSum (Graph-based Summarizer). It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. The graph nodes, which represent combinations of two or more terms, are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations. Then, the produced node ranking is used to drive the sentence selection process. The experiments performed on benchmark and real-life documents demonstrate the effectiveness of the proposed approach compared to many state-of-the-art summarizers. © 2013 Elsevier Inc. All rights reserved.",2013,Information Sciences,55,graph-based summarization entail extracting a worthwhile subset of sentence @ a collection of textual document by @ a graph-based model to represent @ correlation @ pair of document term @ however since @ high-order correlation among multiple term @ disregarded @ graph evaluation @ summarization performance could @ limited unless integrating ad hoc language-dependent @ semantics-based analysis @ @ @ @ a novel and general-purpose graph-based summarizer namely graphsum @ graph-based summarizer @ @ @ discovers and exploit association rule to represent @ correlation among multiple term @ @ @ neglected by previous approach @ @ graph node @ represent combination of @ @ more term @ first ranked by mean of a pagerank strategy @ discriminates @ positive and negative term correlation @ @ @ produced node ranking is used to drive @ sentence selection process @ @ experiment performed on benchmark and real-life document demonstrate @ effectiveness of @ proposed approach compared to many state-of-the-art summarizers @ @ inc @ @ right reserved @ 
3109,SREC: Discourse-level semantic relation extraction from text,"Semantic relation extraction is a significant topic in semantic web and natural language processing with various important applications such as knowledge acquisition, web and text mining, information retrieval and search engine, text classification and summarization. Many approaches such rule base, machine learning and statistical methods have been applied, targeting different types of relation ranging from hyponymy, hypernymy, meronymy, holonymy to domain-specific relation. In this paper, we present a computational method for extraction of explicit and implicit semantic relation from text, by applying statistic and linear algebraic approaches besides syntactic and semantic processing of text. © 2012 Springer-Verlag London Limited.",2013,Neural Computing and Applications,7,semantic relation extraction is a significant topic in semantic web and natural language processing @ various important application @ a knowledge acquisition web and text mining information retrieval and search engine text classification and summarization @ many approach @ rule base machine learning and statistical method @ @ applied targeting different type of relation ranging @ hyponymy hypernymy meronymy holonymy to domain-specific relation @ in @ @ @ @ a computational method @ extraction of explicit and implicit semantic relation @ text by applying statistic and linear algebraic approach besides syntactic and semantic processing of text @ springer-verlag london limited @ 
3110,Minimally-supervised learning of domain-specific causal relations using an open-domain corpus as knowledge base,"We propose a novel framework for overcoming the challenges in extracting causal relations from domain-specific texts. Our technique is minimally-supervised, alleviating the need for manually-annotated, expensive training data. As our main contribution, we show that open-domain corpora can be exploited as knowledge bases to overcome data sparsity issues posed by domain-specific relation extraction, and that they enable substantial performance gains. We also address longstanding challenges of extant minimally-supervised approaches. To suppress the negative impact of semantic drift, we propose a technique based on the Latent Relational Hypothesis. In addition, our approach discovers both explicit (e.g. ""to cause"") and implicit (e.g. ""to destroy"") causal patterns/relations. Unlike existing minimally-supervised techniques, we adopt a principled seed selection strategy, which enables us to discover a more diverse set of causal patterns/relations. Our experiments reveal that our approach outperforms a state-of-the-art baseline in discovering causal relations from a real-life, domain-specific corpus. © 2013 Elsevier B.V.",2013,Data and Knowledge Engineering,7,@ propose a novel framework @ overcoming @ challenge in extracting causal relation @ domain-specific text @ @ technique is minimally-supervised alleviating @ need @ manually-annotated expensive training data @ a @ main contribution @ @ @ open-domain corpus @ @ exploited a knowledge base to overcome data sparsity issue posed by domain-specific relation extraction and @ @ enable substantial performance gain @ @ @ address longstanding challenge of extant minimally-supervised approach @ to suppress @ negative impact of semantic drift @ propose a technique based on @ latent relational hypothesis @ in addition @ approach discovers @ explicit @ e @ g @ @ to cause @ @ and implicit @ e @ g @ @ to destroy @ @ causal pattern relation @ unlike existing minimally-supervised technique @ adopt a principled seed selection strategy @ enables u to discover a more diverse set of causal pattern relation @ @ experiment reveal @ @ approach outperforms a state-of-the-art baseline in discovering causal relation @ a real-life domain-specific corpus @ @ b @ v @ 
3111,A generic unsupervised method for decomposing multi-author documents,"Given an unsegmented multi-author text, we wish to automatically separate out distinct authorial threads. We present a novel, entirely unsupervised, method that achieves strong results on multiple testbeds, including those for which authorial threads are topically identical. Unlike previous work, our method requires no specialized linguistic tools and can be easily applied to any text. © 2013 ASIS&T.",2013,Journal of the American Society for Information Science and Technology,14,given @ unsegmented multi-author text @ wish to automatically separate @ distinct authorial thread @ @ @ a novel entirely unsupervised method @ achieves strong @ on multiple testbeds including @ @ @ authorial thread @ topically identical @ unlike previous work @ method requires no specialized linguistic tool and @ @ easily applied to @ text @ asis t @ 
3113,Entity recognition in parallel multi-lingual biomedical corpora: The CLEF-ER laboratory overview,"The identification and normalisation of biomedical entities from the scientific literature has a long tradition and a number of challenges have contributed to the development of reliable solutions. Increasingly patient records are processed to align their content with other biomedical data resources, but this approach requires analysing documents in different languages across Europe [1,2]. The CLEF-ER challenge has been organized by the Mantra project partners to improve entity recognition (ER) in multilingual documents. Several corpora in different languages, i.e. Medline titles, EMEA documents and patent claims, have been prepared to enable ER in parallel documents. The participants have been ask to annotate entity mentions with concept unique identifiers (CUIs) in the documents of their preferred non-English language. The evaluation determines the number of correctly identified entity mentions against a silver standard (Task A) and the performance measures for the identification of CUIs in the non-English corpora. The participants could make use of the prepared terminological resources for entity normalisation and of the English silver standard corpora (SSCs) as input for concept candidates in the non-English documents. The participants used different approaches including translation techniques and word or phrase alignments apart from lexical lookup and other text mining techniques. The performances for task A and B was lower for the patent corpus in comparison to Medline titles and EMEA documents. In the patent documents, chemical entities were identified at higher performance, whereas the other two document types cover a higher portion of medical terms. The number of novel terms provided from all corpora is currently under investigation. Altogether, the CLEF-ER challenge demonstrates the performances of annotation solutions in different languages against an SSC. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),16,@ identification and normalisation of biomedical entity @ @ scientific literature ha a long tradition and a number of challenge @ contributed to @ development of reliable solution @ increasingly patient record @ processed to align @ content @ @ biomedical data resource @ @ approach requires analysing document in different language across europe @ @ clef-er challenge ha @ organized by @ mantra project partner to improve entity recognition @ er @ in multilingual document @ several corpus in different language i @ e @ medline title emea document and patent claim @ @ prepared to enable er in parallel document @ @ participant @ @ ask to annotate entity mention @ concept unique identifier @ cuis @ in @ document of @ preferred non-english language @ @ evaluation determines @ number of correctly identified entity mention @ a silver standard @ task a @ and @ performance measure @ @ identification of cuis in @ non-english corpus @ @ participant could make use of @ prepared terminological resource @ entity normalisation and of @ english silver standard corpus @ sscs @ a input @ concept candidate in @ non-english document @ @ participant used different approach including translation technique and word @ phrase alignment apart @ lexical lookup and @ text mining technique @ @ performance @ task a and b wa lower @ @ patent corpus in comparison to medline title and emea document @ in @ patent document chemical entity @ identified at higher performance whereas @ @ @ document type cover a higher portion of medical term @ @ number of novel term provided @ @ corpus is currently @ investigation @ altogether @ clef-er challenge demonstrates @ performance of annotation solution in different language @ @ ssc @ springer-verlag @ 
3115,Multilingual and cross-lingual news analysis in the Europe Media Monitor (EMM) (extended abstract),"We give an overview of the highly multilingual news analysis systemEurope Media Monitor (EMM), which gathers an average of 175,000 online news articles per day in tens of languages, categorises the news items and extracts named entities and various other information from them. We explain how users benefit from media monitoring and why it is so important to monitor the news in many different languages. We also describe the challenge of developing text mining tools for tens of languages and in particular that of dealing with highly inflected languages, such as those of the Balto-Slavonic and Finno-Ugric language families. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,@ give @ overview of @ highly multilingual news analysis systemeurope medium monitor @ emm @ @ gather @ average of online news article per day in ten of language categorises @ news item and extract named entity and various @ information @ @ @ @ explain @ user benefit @ medium monitoring and @ @ is @ important to monitor @ news in many different language @ @ @ describe @ challenge of developing text mining tool @ ten of language and in particular @ of dealing @ highly inflected language @ a @ of @ balto-slavonic and finno-ugric language family @ springer-verlag @ 
3118,Multilingual media monitoring and text analysis - Challenges for highly inflected languages,"We present the highly multilingual news analysis system Europe Media Monitor (EMM), which gathers an average of 175,000 online news articles per day in tens of languages, categorises the news items and extracts named entities and various other information from them. We also give an overview of EMM's text mining tool set, focusing on the issue of how the software deals with highly inflected languages such as those of the Slavic and Finno-Ugric language families. The questions we ask are: How to adapt extraction patterns to such languages? How to de-inflect extracted named entities? And: Will document categorisation benefit from lemmatising the texts? © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7,@ @ @ highly multilingual news analysis system europe medium monitor @ emm @ @ gather @ average of online news article per day in ten of language categorises @ news item and extract named entity and various @ information @ @ @ @ @ give @ overview of emm @ s text mining tool set focusing on @ issue of @ @ software deal @ highly inflected language @ a @ of @ slavic and finno-ugric language family @ @ question @ ask @ @ @ to adapt extraction pattern to @ language @ @ to de-inflect extracted named entity @ and @ @ document categorisation benefit @ lemmatising @ text @ springer-verlag @ 
3119,A new approach for improving cross-document knowledge discovery using wikipedia,"In this paper, we present a new model that incorporates the extensive knowledge derived from Wikipedia for cross-document knowledge discovery. The model proposed here is based on our previously introduced Concept Chain Queries (CCQ) which is a special case of text mining focusing on detecting semantic relationships between two concepts across multiple documents. We attempt to overcome the limitations of CCQ by building a semantic kernel for concept closeness computing to complement existing knowledge in text corpus. The experimental evaluation demonstrates that the kernel-based approach outperforms in ranking important chains retrieved in the search results. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,in @ @ @ @ a @ model @ incorporates @ extensive knowledge derived @ wikipedia @ cross-document knowledge discovery @ @ model proposed @ is based on @ @ introduced concept chain query @ ccq @ @ is a special case of text mining focusing on detecting semantic relationship @ @ concept across multiple document @ @ attempt to overcome @ limitation of ccq by building a semantic kernel @ concept closeness computing to complement existing knowledge in text corpus @ @ experimental evaluation demonstrates @ @ kernel-based approach outperforms in ranking important chain retrieved in @ search @ @ springer-verlag @ @ @ 
3120,SuDoC: Semi-unsupervised classification of text document opinions using a few labeled examples and clustering,"The presented novel procedure named SuDoC - or Semi-unsupervised Document Classification - provides an alternative method to standard clustering techniques when it is necessary to separate a very large set of textual instances into groups that represent the text-document semantics. Unlike the conventional clustering, SuDoC proceeds from an initial small set of typical specimen that can be created manually and which provides the necessary bias for generating appropriate classes. SuDoC starts with a higher number of generated clusters and - to avoid over-fitting - reiteratively decreases their quantity, increasing the resulting classification generality. The unlabeled instances are automatically labeled according to their similarity to the defined labeled samples, thus reaching higher classification accuracy in the future. The results of the presented strengthened clustering procedure are demonstrated using a real-world data set represented by hotel guests' unstructured reviews written in natural language. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ presented novel procedure named sudoc @ semi-unsupervised document classification provides @ alternative method to standard clustering technique @ @ is necessary to separate a @ @ set of textual instance @ group @ represent @ text-document semantics @ unlike @ conventional clustering sudoc proceeds @ @ initial small set of typical specimen @ @ @ created manually and @ provides @ necessary bias @ generating appropriate class @ sudoc start @ a higher number of generated cluster and to avoid over-fitting reiteratively decrease @ quantity increasing @ resulting classification generality @ @ unlabeled instance @ automatically labeled according to @ similarity to @ defined labeled sample thus reaching higher classification accuracy in @ future @ @ @ of @ presented strengthened clustering procedure @ demonstrated @ a real-world data set represented by hotel guest @ unstructured review written in natural language @ springer-verlag @ @ @ 
3121,Automatic detection of arabic causal relations,"The work described in this paper is about the automatic detection and extraction of causal relations that are explicitly expressed in Modern Standard Arabic (MSA) texts. In this initial study, a set of linguistic patterns was derived to indicate the presence of cause-effect information in sentences from open domain texts. The patterns were constructed based on a set of syntactic features which was acquired by analyzing a large untagged Arabic corpus so that parts of the sentence representing the cause and those representing the effect can be distinguished. To the best of researchers knowledge, no previous studies have dealt this type of relation for the Arabic language. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ work described in @ @ is @ @ automatic detection and extraction of causal relation @ @ explicitly expressed in modern standard arabic @ msa @ text @ in @ initial study a set of linguistic pattern wa derived to indicate @ presence of cause-effect information in sentence @ open domain text @ @ pattern @ constructed based on a set of syntactic feature @ wa acquired by analyzing a @ untagged arabic corpus @ @ part of @ sentence representing @ cause and @ representing @ effect @ @ distinguished @ to @ best of researcher knowledge no previous study @ dealt @ type of relation @ @ arabic language @ springer-verlag @ @ @ 
3125,An ontology-based semantic clustering algorithm for accounting text,"The feature selection and semantic similarity computing between texts are essential components of accounting text clustering. In the past, several approaches for generic text feature selection and similarity computing by exploiting different measures (vector space model, words frequency, thesauri, domain corpora, etc.) have been proposed. However, accounting field is different from general field. Accounting has its own concepts and rules. These generic methods are not so suitable for accounting text clustering. In this paper, a novel accounting ontology-based feature selection and similarity computing algorithm for accounting text is proposed. Firstly, characterizing the accounting texts, we get a terms vector. Secondly, terms vector is mapped into concept of accounting ontology and converted into concept vector. Based on the structure of concept, the semantic similarity between texts is computed. Then, trough an improved clustering method, accounting texts are clustered effectively. The experiments results imply that our proposal outperforms most of the previous measures as well as eliminates some of their limitations. © 2013 by CESER Publications.",2013,International Journal of Applied Mathematics and Statistics,3,@ feature selection and semantic similarity computing @ text @ essential component of accounting text clustering @ in @ past several approach @ generic text feature selection and similarity computing by exploiting different measure @ vector space model word frequency thesaurus domain corpus etc @ @ @ @ proposed @ however accounting field is different @ general field @ accounting ha @ @ concept and rule @ @ generic method @ not @ suitable @ accounting text clustering @ in @ @ a novel accounting ontology-based feature selection and similarity computing algorithm @ accounting text is proposed @ firstly characterizing @ accounting text @ get a term vector @ secondly term vector is mapped @ concept of accounting ontology and converted @ concept vector @ based on @ structure of concept @ semantic similarity @ text is computed @ @ trough @ improved clustering method accounting text @ clustered effectively @ @ experiment @ imply @ @ proposal outperforms @ of @ previous measure a well a eliminates some of @ limitation @ by ceser publication @ 
3131,'Clustering' documents automatically to support scoping reviews of research: a case study,"BACKGROUND: Scoping reviews of research help determine the feasibility and the resource requirements of conducting a systematic review, and the potential to generate a description of the literature quickly is attractive. AIMS: To test the utility and applicability of an automated clustering tool to describe and group research studies to improve the efficiency of scoping reviews. METHODS: A retrospective study of two completed scoping reviews was conducted. This compared the groups and descriptive categories obtained by automatically clustering titles and abstracts with those that had originally been derived using traditional researcher-driven techniques. RESULTS: The clustering tool rapidly categorised research into themes, which were useful in some instances, but not in others. This provided a dynamic means to view each dataset. Interpretation was challenging where there were potentially multiple meanings of terms. Where relevant clusters were unambiguous, there was a high precision of relevant studies, although recall varied widely. CONCLUSIONS: Policy-relevant scoping reviews are often undertaken rapidly, and this could potentially be enhanced by automation depending on the nature of the dataset and information sought. However, it is not a replacement for researcher-developed classification. The possibilities of further applications and potential for use in other types of review are discussed. Copyright © 2013 John Wiley & Sons, Ltd.",2013,Research synthesis methods,7,background @ scoping review of research help determine @ feasibility and @ resource requirement of conducting a systematic review and @ potential to generate a description of @ literature quickly is attractive @ aim @ to test @ utility and applicability of @ automated clustering tool to describe and group research study to improve @ efficiency of scoping review @ method @ a retrospective study of @ completed scoping review wa conducted @ @ compared @ group and descriptive category obtained by automatically clustering title and abstract @ @ @ @ originally @ derived @ traditional researcher-driven technique @ @ @ @ clustering tool rapidly categorised research @ theme @ @ useful in some instance @ not in others @ @ provided a dynamic mean to view @ dataset @ interpretation wa challenging @ @ @ potentially multiple meaning of term @ @ relevant cluster @ unambiguous @ wa a high precision of relevant study although recall varied widely @ conclusion @ policy-relevant scoping review @ often undertaken rapidly and @ could potentially @ enhanced by automation depending on @ nature of @ dataset and information sought @ however @ is not a replacement @ researcher-developed classification @ @ possibility of @ application and potential @ use in @ type of review @ discussed @ @ john wiley son ltd @ 
3133,Assessing the quality factors found in in-line documentation written in natural language: The JavadocMiner,"An important software engineering artifact used by developers and maintainers to assist in software comprehension and maintenance is source code documentation. It provides the insight needed by software engineers when performing a task, and therefore ensuring the quality of this documentation is extremely important. In-line documentation is at the forefront of explaining a programmer's original intentions for a given implementation. Since this documentation is written in natural language, ensuring its quality so far needed to be performed manually. In this paper, we present an effective and automated approach for assessing the quality of in-line documentation using a set of heuristics, targeting both the quality of language and consistency between the source code and its comments. Our evaluation is made up of two parts: We first apply the JavadocMiner tool to the different modules of two open source applications (ArgoUML and Eclipse) in order to automatically assess their intrinsic comment quality. In the second part of our evaluation, we correlate the results returned by the analysis with bug defects reported for the individual modules in order to examine connections between natural language documentation and source code quality. © 2013 Elsevier B.V. All rights reserved.",2013,Data and Knowledge Engineering,6,@ important software engineering artifact used by developer and maintainer to assist in software comprehension and maintenance is source code documentation @ @ provides @ insight needed by software engineer @ performing a task and therefore ensuring @ quality of @ documentation is extremely important @ in-line documentation is at @ forefront of explaining a programmer @ s original intention @ a given implementation @ since @ documentation is written in natural language ensuring @ quality @ far needed to @ performed manually @ in @ @ @ @ @ effective and automated approach @ assessing @ quality of in-line documentation @ a set of heuristic targeting @ @ quality of language and consistency @ @ source code and @ comment @ @ evaluation is made up of @ part @ @ first apply @ javadocminer tool to @ different module of @ open source application @ argouml and eclipse @ in order to automatically ass @ intrinsic comment quality @ in @ second part of @ evaluation @ correlate @ @ returned by @ analysis @ bug defect reported @ @ individual module in order to examine connection @ natural language documentation and source code quality @ @ b @ v @ @ right reserved @ 
3140,Mining semantic relationships between concepts across documents incorporating Wikipedia knowledge,"The ongoing astounding growth of text data has created an enormous need for fast and efficient text mining algorithms. Traditional approaches for document representation are mostly based on the Bag of Words (BOW) model which takes a document as an unordered collection of words. However, when applied in fine-grained information discovery tasks, such as mining semantic relationships between concepts, sorely relying on the BOW representation may not be sufficient to identify all potential relationships since the resulting associations based on the BOW approach are limited to the concepts that appear in the document collection literally. In this paper, we attempt to complement existing information in the corpus by proposing a new hybrid approach, which mines semantic associations between concepts across multiple text units through incorporating extensive knowledge from Wikipedia. The experimental evaluation demonstrates that search performance has been significantly enhanced in terms of accuracy and coverage compared with a purely BOW-based approach and alternative solutions where only the article contents of Wikipedia or category information are considered. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,@ ongoing astounding growth of text data ha created @ enormous need @ fast and efficient text mining algorithm @ traditional approach @ document representation @ mostly based on @ bag of word @ bow @ model @ take a document a @ unordered collection of word @ however @ applied in fine-grained information discovery task @ a mining semantic relationship @ concept sorely relying on @ bow representation may not @ sufficient to identify @ potential relationship since @ resulting association based on @ bow approach @ limited to @ concept @ appear in @ document collection literally @ in @ @ @ attempt to complement existing information in @ corpus by proposing a @ hybrid approach @ mine semantic association @ concept across multiple text unit @ incorporating extensive knowledge @ wikipedia @ @ experimental evaluation demonstrates @ search performance ha @ significantly enhanced in term of accuracy and coverage compared @ a purely bow-based approach and alternative solution @ only @ article content of wikipedia @ category information @ considered @ springer-verlag @ 
3144,An automatic classification of product review into given viewpoints,"Product reviews on the web sites help not only consumers to purchase products but also developers to analyze consumers' needs. Because huge amount of the reviews are presented on the various sites, however, it is a hard task for them to read and to find only the reviews which match their viewpoint that they focus on. Though, to overcome this issue, many researchers in the field of the natural language processing tried to find review sites, classification of reviews according to their viewpoints does not have been succeeded because the corpus for classification must be needed and building it takes a lot of cost. In this paper, we propose a method to build the corpus for each type of products automatically and also propose a method for automatic classification of method of the review. In our method of classification, we focused on the property of review by extending the Tf-Idf. As the classification results contained many errors of classifications in the similar viewpoints, we built the improved method. In this method, we divided the classification process into two-stage. As the result, we could classify reviews by over 80 point. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,product review on @ web site help not only consumer to purchase product @ @ developer to analyze consumer @ need @ @ huge amount of @ review @ presented on @ various site however @ is a hard task @ @ to read and to find only @ review @ match @ viewpoint @ @ focus on @ though to overcome @ issue many researcher in @ field of @ natural language processing tried to find review site classification of review according to @ viewpoint doe not @ @ succeeded @ @ corpus @ classification must @ needed and building @ take a lot of cost @ in @ @ @ propose a method to build @ corpus @ @ type of product automatically and @ propose a method @ automatic classification of method of @ review @ in @ method of classification @ focused on @ property of review by extending @ tf-idf @ a @ classification @ contained many error of classification in @ similar viewpoint @ built @ improved method @ in @ method @ divided @ classification process @ two-stage @ a @ @ @ could classify review by @ point @ springer-verlag @ @ @ 
3146,Information extraction from text,"Information extraction is the task of finding structured information from unstructured or semi-structured text. It is an important task in text mining and has been extensively studied in various research communities including natural language processing, information retrieval and Web mining. It has a wide range of applications in domains such as biomedical literature mining and business intelligence. Two fundamental tasks of information extraction are named entity recognition and relation extraction. The former refers to finding names of entities such as people, organizations and locations. The latter refers to finding the semantic relations such as FounderOf and HeadquarteredIn between entities. In this chapter we provide a survey of the major work on named entity recognition and relation extraction in the past few decades, with a focus on work from the natural language processing community. © 2012 Springer Science+Business Media, LLC. All rights reserved.",2013,Mining Text Data,66,information extraction is @ task of finding structured information @ unstructured @ semi-structured text @ @ is @ important task in text mining and ha @ extensively studied in various research community including natural language processing information retrieval and web mining @ @ ha a wide range of application in domain @ a biomedical literature mining and @ intelligence @ @ fundamental task of information extraction @ named entity recognition and relation extraction @ @ former refers to finding name of entity @ a people organization and location @ @ latter refers to finding @ semantic relation @ a founderof and headquarteredin @ entity @ in @ chapter @ provide a survey of @ major work on named entity recognition and relation extraction in @ past @ decade @ a focus on work @ @ natural language processing community @ @ science @ medium llc @ @ right reserved @ 
3147,More than words: Social networks' text mining for consumer brand sentiments,"Blogs and social networks have recently become a valuable resource for mining sentiments in fields as diverse as customer relationship management, public opinion tracking and text filtering. In fact knowledge obtained from social networks such as Twitter and Facebook has been shown to be extremely valuable to marketing research companies, public opinion organizations and other text mining entities. However, Web texts have been classified as noisy as they represent considerable problems both at the lexical and the syntactic levels. In this research we used a random sample of 3516 tweets to evaluate consumers' sentiment towards well-known brands such as Nokia, T-Mobile, IBM, KLM and DHL. We used an expert-predefined lexicon including around 6800 seed adjectives with known orientation to conduct the analysis. Our results indicate a generally positive consumer sentiment towards several famous brands. By using both a qualitative and quantitative methodology to analyze brands' tweets, this study adds breadth and depth to the debate over attitudes towards cosmopolitan brands. © 2012 Elsevier Ltd. All rights reserved.",2013,Expert Systems with Applications,335,blog and social network @ recently become a valuable resource @ mining sentiment in field a diverse a customer relationship management public opinion tracking and text filtering @ in fact knowledge obtained @ social network @ a twitter and facebook ha @ @ to @ extremely valuable to marketing research company public opinion organization and @ text mining entity @ however web text @ @ classified a noisy a @ represent considerable problem @ at @ lexical and @ syntactic level @ in @ research @ used a random sample of tweet to evaluate consumer @ sentiment towards well-known brand @ a nokia t-mobile ibm klm and dhl @ @ used @ expert-predefined lexicon including around seed adjective @ known orientation to conduct @ analysis @ @ @ indicate a generally positive consumer sentiment towards several famous brand @ by @ @ a qualitative and quantitative methodology to analyze brand @ tweet @ study add breadth and depth to @ debate @ attitude towards cosmopolitan brand @ @ ltd @ @ right reserved @ 
3148,Biomedical text mining: A survey of recent progress,"The biomedical community makes extensive use of text mining technology. In the past several years, enormous progress has been made in developing tools and methods, and the community has been witness to some exciting developments. Although the state of the community is regularly reviewed, the sheer volume of work related to biomedical text mining and the rapid pace in which progress continues to be made make this a worthwhile, if not necessary, endeavor. This chapter provides a brief overview of the current state of text mining in the biomedical domain. Emphasis is placed on the resources and tools available to biomedical researchers and practitioners, as well as the major text mining tasks of interest to the community. These tasks include the recognition of explicit facts from biomedical literature, the discovery of previously unknown or implicit facts, document summarization, and question answering. For each topic, its basic challenges and methods are outlined and recent and influential work is reviewed. © 2012 Springer Science+Business Media, LLC. All rights reserved.",2013,Mining Text Data,65,@ biomedical community make extensive use of text mining technology @ in @ past several year enormous progress ha @ made in developing tool and method and @ community ha @ witness to some exciting development @ although @ state of @ community is regularly reviewed @ sheer volume of work related to biomedical text mining and @ rapid pace in @ progress continues to @ made make @ a worthwhile if not necessary endeavor @ @ chapter provides a brief overview of @ current state of text mining in @ biomedical domain @ emphasis is placed on @ resource and tool available to biomedical researcher and practitioner a well a @ major text mining task of interest to @ community @ @ task include @ recognition of explicit fact @ biomedical literature @ discovery of @ unknown @ implicit fact document summarization and question answering @ @ @ topic @ basic challenge and method @ outlined and recent and influential work is reviewed @ @ science @ medium llc @ @ right reserved @ 
3149,Discovering latent commercial networks from online financial news articles,"Unlike most online social networks where explicit links among individual users are defined, the relations among commercial entities (e.g. firms) may not be explicitly declared in commercial Web sites. One main contribution of this article is the development of a novel computational model for the discovery of the latent relations among commercial entities from online financial news. More specifically, a CRF model which can exploit both structural and contextual features is applied to commercial entity recognition. In addition, a point-wise mutual information (PMI)-based unsupervised learning method is developed for commercial relation identification. To evaluate the effectiveness of the proposed computational methods, a prototype system called CoNet has been developed. Based on the financial news articles crawled from Google finance, the CoNet system achieves average F-scores of 0.681 and 0.754 in commercial entity recognition and commercial relation identification, respectively. Our experimental results confirm that the proposed shallow natural language processing methods are effective for the discovery of latent commercial networks from online financial news. © 2013 Copyright Taylor and Francis Group, LLC.",2013,Enterprise Information Systems,15,unlike @ online social network @ explicit link among individual user @ defined @ relation among commercial entity @ e @ g @ firm @ may not @ explicitly declared in commercial web site @ @ main contribution of @ article is @ development of a novel computational model @ @ discovery of @ latent relation among commercial entity @ online financial news @ more specifically a crf model @ @ exploit @ structural and contextual feature is applied to commercial entity recognition @ in addition a point-wise mutual information @ pmi @ based unsupervised learning method is developed @ commercial relation identification @ to evaluate @ effectiveness of @ proposed computational method a prototype system called conet ha @ developed @ based on @ financial news article crawled @ google finance @ conet system achieves average f-scores of @ and @ in commercial entity recognition and commercial relation identification respectively @ @ experimental @ confirm @ @ proposed shallow natural language processing method @ effective @ @ discovery of latent commercial network @ online financial news @ @ taylor and francis group llc @ 
3151,"An emotion-based model of negation, intensifiers, and modality for polarity and intensity classification","Negation, intensifiers, and modality are common linguistic constructions that may modify the emotional meaning of the text and therefore need to be taken into consideration in sentiment analysis. Negation is usually considered as a polarity shifter, whereas intensifiers are regarded as amplifiers or diminishers of the strength of such polarity. Modality, in turn, has only been addressed in a very naïve fashion, so that modal forms are treated as polarity blockers. However, processing these constructions as mere polarity modifiers may be adequate for polarity classification, but it is not enough for more complex tasks (e.g., intensity classification), for which a more fine-grained model based on emotions is needed. In this work, we study the effect of modifiers on the emotions affected by them and propose a model of negation, intensifiers, and modality especially conceived for sentiment analysis tasks. We compare our emotion-based strategy with two traditional approaches based on polar expressions and find that representing the text as a set of emotions increases accuracy in different classification tasks and that this representation allows for a more accurate modeling of modifiers that results in further classification improvements. We also study the most common uses of modifiers in opinionated texts and quantify their impact in polarity and intensity classification. Finally, we analyze the joint effect of emotional modifiers and find that interesting synergies exist between them. © 2013 ASIS&T.",2013,Journal of the American Society for Information Science and Technology,25,negation intensifier and modality @ common linguistic construction @ may modify @ emotional meaning of @ text and therefore need to @ taken @ consideration in sentiment analysis @ negation is usually considered a a polarity shifter whereas intensifier @ regarded a amplifier @ diminishers of @ strength of @ polarity @ modality in turn ha only @ addressed in a @ naïve fashion @ @ modal form @ treated a polarity blocker @ however processing @ construction a mere polarity modifier may @ adequate @ polarity classification @ @ is not enough @ more complex task @ e @ g @ intensity classification @ @ @ a more fine-grained model based on emotion is needed @ in @ work @ study @ effect of modifier on @ emotion affected by @ and propose a model of negation intensifier and modality especially conceived @ sentiment analysis task @ @ compare @ emotion-based strategy @ @ traditional approach based on polar expression and find @ representing @ text a a set of emotion increase accuracy in different classification task and @ @ representation allows @ a more accurate modeling of modifier @ @ in @ classification improvement @ @ @ study @ @ common us of modifier in opinionated text and quantify @ impact in polarity and intensity classification @ finally @ analyze @ joint effect of emotional modifier and find @ interesting synergy exist @ @ @ asis t @ 
3152,Efficient semisupervised MEDLINE document clustering with MeSH-semantic and global-content constraints,"For clustering biomedical documents, we can consider three different types of information: the local-content (LC) information from documents, the global-content (GC) information from the whole MEDLINE collections, and the medical subject heading (MeSH)-semantic (MS) information. Previous methods for clustering biomedical documents are not necessarily effective for integrating different types of information, by which only one or two types of information have been used. Recently, the performance of MEDLINE document clustering has been enhanced by linearly combining both the LC and MS information. However, the simple linear combination could be ineffective because of the limitation of the representation space for combining different types of information (similarities) with different reliability. To overcome the limitation, we propose a new semisupervised spectral clustering method, i.e., SSNCut, for clustering over the LC similarities, with two types of constraints: must-link (ML) constraints on document pairs with high MS (or GC) similarities and cannot-link (CL) constraints on those with low similarities. We empirically demonstrate the performance of SSNCut on MEDLINE document clustering, by using 100 data sets of MEDLINE records. Experimental results show that SSNCut outperformed a linear combination method and several well-known semisupervised clustering methods, being statistically significant. Furthermore, the performance of SSNCut with constraints from both MS and GC similarities outperformed that from only one type of similarities. Another interesting finding was that ML constraints more effectively worked than CL constraints, since CL constraints include around 10% incorrect ones, whereas this number was only 1% for ML constraints. © 2012 IEEE.",2013,IEEE Transactions on Cybernetics,37,@ clustering biomedical document @ @ consider three different type of information @ @ local-content @ lc @ information @ document @ global-content @ gc @ information @ @ whole medline collection and @ medical subject heading @ mesh @ semantic @ @ @ information @ previous method @ clustering biomedical document @ not necessarily effective @ integrating different type of information by @ only @ @ @ type of information @ @ used @ recently @ performance of medline document clustering ha @ enhanced by linearly combining @ @ lc and @ information @ however @ simple linear combination could @ ineffective @ of @ limitation of @ representation space @ combining different type of information @ similarity @ @ different reliability @ to overcome @ limitation @ propose a @ semisupervised spectral clustering method i @ e @ ssncut @ clustering @ @ lc similarity @ @ type of constraint @ must-link @ ml @ constraint on document pair @ high @ @ @ gc @ similarity and cannot-link @ cl @ constraint on @ @ low similarity @ @ empirically demonstrate @ performance of ssncut on medline document clustering by @ data set of medline record @ experimental @ @ @ ssncut outperformed a linear combination method and several well-known semisupervised clustering method @ statistically significant @ furthermore @ performance of ssncut @ constraint @ @ @ and gc similarity outperformed @ @ only @ type of similarity @ another interesting finding wa @ ml constraint more effectively worked @ cl constraint since cl constraint include around incorrect @ whereas @ number wa only @ ml constraint @ @ @ 
3153,Entity translation mining from comparable corpora: Combining graph mapping with corpus latent features,"This paper addresses the problem of mining named entity translations from comparable corpora, specifically, mining English and Chinese named entity translation. We first observe that existing approaches use one or more of the following named entity similarity metrics: entity, entity context, and relationship. Motivated by this observation, we propose a new holistic approach by 1) combining all similarity types used and 2) additionally considering relationship context similarity between pairs of named entities, a missing quadrant in the taxonomy of similarity metrics. We abstract the named entity translation problem as the matching of two named entity graphs extracted from the comparable corpora. Specifically, named entity graphs are first constructed from comparable corpora to extract relationship between named entities. Entity similarity and entity context similarity are then calculated from every pair of bilingual named entities. A reinforcing method is utilized to reflect relationship similarity and relationship context similarity between named entities. We also discover ""latent"" features lost in the graph extraction process and integrate this into our framework. According to our experimental results, our holistic graph-based approach and its enhancement using corpus latent features are highly effective and our framework significantly outperforms previous approaches. © 2012 IEEE.",2013,IEEE Transactions on Knowledge and Data Engineering,6,@ @ address @ problem of mining named entity translation @ comparable corpus specifically mining english and chinese named entity translation @ @ first observe @ existing approach use @ @ more of @ following named entity similarity metric @ entity entity context and relationship @ motivated by @ observation @ propose a @ holistic approach by @ combining @ similarity type used and @ additionally considering relationship context similarity @ pair of named entity a missing quadrant in @ taxonomy of similarity metric @ @ abstract @ named entity translation problem a @ matching of @ named entity graph extracted @ @ comparable corpus @ specifically named entity graph @ first constructed @ comparable corpus to extract relationship @ named entity @ entity similarity and entity context similarity @ @ calculated @ every pair of bilingual named entity @ a reinforcing method is utilized to reflect relationship similarity and relationship context similarity @ named entity @ @ @ discover @ latent @ feature lost in @ graph extraction process and integrate @ @ @ framework @ according to @ experimental @ @ holistic graph-based approach and @ enhancement @ corpus latent feature @ highly effective and @ framework significantly outperforms previous approach @ @ @ 
3157,Multi-document summarization based on the Yago ontology,"Sentence-based multi-document summarization is the task of generating a succinct summary of a document collection, which consists of the most salient document sentences. In recent years, the increasing availability of semantics-based models (e.g., ontologies and taxonomies) has prompted researchers to investigate their usefulness for improving summarizer performance. However, semantics-based document analysis is often applied as a preprocessing step, rather than integrating the discovered knowledge into the summarization process. This paper proposes a novel summarizer, namely Yago-based Summarizer, that relies on an ontology-based evaluation and selection of the document sentences. To capture the actual meaning and context of the document sentences and generate sound document summaries, an established entity recognition and disambiguation step based on the Yago ontology is integrated into the summarization process. The experimental results, which were achieved on the DUC'04 benchmark collections, demonstrate the effectiveness of the proposed approach compared to a large number of competitors as well as the qualitative soundness of the generated summaries. © 2013 Elsevier Ltd. All rights reserved.",2013,Expert Systems with Applications,32,sentence-based multi-document summarization is @ task of generating a succinct summary of a document collection @ consists of @ @ salient document sentence @ in recent year @ increasing availability of semantics-based model @ e @ g @ ontology and taxonomy @ ha prompted researcher to investigate @ usefulness @ improving summarizer performance @ however semantics-based document analysis is often applied a a preprocessing step rather @ integrating @ discovered knowledge @ @ summarization process @ @ @ proposes a novel summarizer namely yago-based summarizer @ relies on @ ontology-based evaluation and selection of @ document sentence @ to capture @ actual meaning and context of @ document sentence and generate sound document summary @ established entity recognition and disambiguation step based on @ yago ontology is integrated @ @ summarization process @ @ experimental @ @ @ achieved on @ duc @ benchmark collection demonstrate @ effectiveness of @ proposed approach compared to a @ number of competitor a well a @ qualitative soundness of @ generated summary @ @ ltd @ @ right reserved @ 
3159,Georeferencing Flickr resources based on textual meta-data,"The task of automatically estimating the location of web resources is of central importance in location-based services on the Web. Much attention has been focused on Flickr photos and videos, for which it was found that language modeling approaches are particularly suitable. In particular, state-of-the art systems for georeferencing Flickr photos tend to cluster the locations on Earth in a relatively small set of disjoint regions, apply feature selection to identify location-relevant tags, then use a form of text classification to identify which area is most likely to contain the true location of the resource, and finally attempt to find an appropriate location within the identified area. In this paper, we present a systematic discussion of each of the aforementioned components, based on the lessons we have learned from participating in the 2010 and 2011 editions of MediaEval's Placing Task. Extensive experimental results allow us to analyze why certain methods work well on this task and show that a median error of just over 1 km can be achieved on a standard benchmark test set. © 2013 Elsevier Inc. All rights reserved.",2013,Information Sciences,26,@ task of automatically estimating @ location of web resource is of central importance in location-based service on @ web @ much attention ha @ focused on flickr photo and video @ @ @ wa found @ language modeling approach @ particularly suitable @ in particular state-of-the art system @ georeferencing flickr photo tend to cluster @ location on earth in a relatively small set of disjoint region apply feature selection to identify location-relevant tag @ use a form of text classification to identify @ area is @ likely to contain @ true location of @ resource and finally attempt to find @ appropriate location within @ identified area @ in @ @ @ @ a systematic discussion of @ of @ aforementioned component based on @ lesson @ @ learned @ participating in @ and edition of mediaeval @ s placing task @ extensive experimental @ allow u to analyze @ certain method work well on @ task and @ @ a median error of @ @ km @ @ achieved on a standard benchmark test set @ @ inc @ @ right reserved @ 
3160,"Combining HCI, natural language processing, and knowledge discovery - Potential of IBM content analytics as an assistive technology in the biomedical field","Medical professionals are confronted with a flood of big data most of it containing unstructured information. Such unstructured information is the subset of information, where the information itself describes parts of what constitutes as significant within it, or in other words - structure and information are not completely separable. The best example for such unstructured information is text. For many years, text mining has been an essential area of medical informatics. Although text can easily be created by medical professionals, the support of automatic analyses for knowledge discovery is extremely difficult. We follow the definition that knowledge consists of a set of hypotheses, and knowledge discovery is the process of finding or generating new hypotheses by medical professionals with the aim of getting insight into the data. In this paper we present some lessons learned of ICA for dermatological knowledge discovery, for the first time. We follow the HCI-KDD approach, i.e. with the human expert in the loop matching the best of two worlds: human intelligence with computational intelligence. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),37,medical professional @ confronted @ a flood of big data @ of @ containing unstructured information @ @ unstructured information is @ subset of information @ @ information @ describes part of @ constitutes a significant within @ @ in @ word structure and information @ not completely separable @ @ best example @ @ unstructured information is text @ @ many year text mining ha @ @ essential area of medical informatics @ although text @ easily @ created by medical professional @ support of automatic analysis @ knowledge discovery is extremely difficult @ @ follow @ definition @ knowledge consists of a set of hypothesis and knowledge discovery is @ process of finding @ generating @ hypothesis by medical professional @ @ aim of getting insight @ @ data @ in @ @ @ @ some lesson learned of ica @ dermatological knowledge discovery @ @ first time @ @ follow @ hci-kdd approach i @ e @ @ @ human expert in @ loop matching @ best of @ world @ human intelligence @ computational intelligence @ springer-verlag @ 
3162,Measuring semantic relatedness using wikipedia signed network,"Identifying the semantic relatedness of two words is an important task for the information retrieval, natural language processing, and text mining. However, due to the diversity of meaning for a word, the semantic relatedness of two words is still hard to precisely evaluate under the limited corpora. Nowadays, Wikipedia is now a huge and wiki-based encyclopedia on the internet that has become a valuable resource for research work. Wikipedia articles, written by a live collaboration of user editors, contain a high volume of reference links, URL identification for concepts and a complete revision history. Moreover, each Wikipedia article represents an individual concept that simultaneously contains other concepts that are hyperlinks of other articles embedded in its content. Through this, we believe that the semantic relatedness between two words can be found through the semantic relatedness between two Wikipedia articles. Therefore, we propose an Editor-Contribution-based Rank (ECR) algorithm for ranking the concepts in the article's content through all revisions and take the ranked concepts as a vector representing the article. We classify four types of relationship in which the behavior of addition and deletion maps appropriate and inappropriate concepts. ECR also extend the concept semantics by the editor-concept network. ECR ranks those concepts depending on the mutual signed-reinforcement relationship between the concepts and the editors. The results reveal that our method leads to prominent performance improvement and increases the correlation coefficient by a factor ranging from 4% to 23% over previous methods that calculate the relatedness between two articles.",2013,Journal of Information Science and Engineering,1,identifying @ semantic relatedness of @ word is @ important task @ @ information retrieval natural language processing and text mining @ however due to @ diversity of meaning @ a word @ semantic relatedness of @ word is still hard to precisely evaluate @ @ limited corpus @ nowadays wikipedia is now a huge and wiki-based encyclopedia on @ internet @ ha become a valuable resource @ research work @ wikipedia article written by a live collaboration of user editor contain a high volume of reference link url identification @ concept and a complete revision history @ moreover @ wikipedia article represents @ individual concept @ simultaneously contains @ concept @ @ hyperlink of @ article embedded in @ content @ @ @ @ believe @ @ semantic relatedness @ @ word @ @ found @ @ semantic relatedness @ @ wikipedia article @ therefore @ propose @ editor-contribution-based rank @ ecr @ algorithm @ ranking @ concept in @ article @ s content @ @ revision and take @ ranked concept a a vector representing @ article @ @ classify four type of relationship in @ @ behavior of addition and deletion map appropriate and inappropriate concept @ ecr @ extend @ concept semantics by @ editor-concept network @ ecr rank @ concept depending on @ mutual signed-reinforcement relationship @ @ concept and @ editor @ @ @ reveal @ @ method lead to prominent performance improvement and increase @ correlation coefficient by a factor ranging @ to @ previous method @ calculate @ relatedness @ @ article @ 
3163,Opinion mining for relating subjective expressions and annual earnings in US financial statements,"Financial statements contain quantitative information and manager's subjective evaluation of firm's financial status. Using information released in U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for quality financial decisions. To extract such opinioned statements from the reports, we built tagging models based on the conditional random field (CRF) techniques, considering a variety of combinations of linguistic factors including morphology, orthography, predicate-argument structure, syntax, and simple semantics. Our results show that the CRF models are reasonably effective to find opinion holders in experiments when we adopted the popular MPQA corpus for training and testing. The contribution of our paper is to identify opinion patterns in multiword expressions (MWEs) forms rather than in single word forms. We find that the managers of corporations attempt to use more optimistic words to obfuscate negative financial performance and to accentuate the positive financial performance. Our results also show that decreasing earnings were often accompanied by ambiguous and mild statements in the reporting year and that increasing earnings were stated in assertive and positive way.",2013,Journal of Information Science and Engineering,12,financial statement contain quantitative information and manager @ s subjective evaluation of firm @ s financial status @ @ information released in u @ s @ k filing @ @ qualitative and quantitative appraisal @ crucial @ quality financial decision @ to extract @ opinioned statement @ @ report @ built tagging model based on @ conditional random field @ crf @ technique considering a variety of combination of linguistic factor including morphology orthography predicate-argument structure syntax and simple semantics @ @ @ @ @ @ crf model @ reasonably effective to find opinion holder in experiment @ @ adopted @ popular mpqa corpus @ training and testing @ @ contribution of @ @ is to identify opinion pattern in multiword expression @ mwes @ form rather @ in single word form @ @ find @ @ manager of corporation attempt to use more optimistic word to obfuscate negative financial performance and to accentuate @ positive financial performance @ @ @ @ @ @ decreasing earnings @ often accompanied by ambiguous and mild statement in @ reporting year and @ increasing earnings @ stated in assertive and positive way @ 
3165,A feature-free and parameter-light multi-task clustering framework,"The two last decades have witnessed extensive research on multi-task learning algorithms in diverse domains such as bioinformatics, text mining, natural language processing as well as image and video content analysis. However, all existing multi-task learning methods require either domain-specific knowledge to extract features or a careful setting of many input parameters. There are many disadvantages associated with prior knowledge requirements for feature extraction or parameter-laden approaches. One of the most obvious problems is that we may find a wrong or non-existent pattern because of poorly extracted features or incorrectly set parameters. In this work, we propose a feature-free and parameter-light multi-task clustering framework to overcome these disadvantages. Our proposal is motivated by the recent successes of Kolmogorov-based methods on various applications. However, such methods are only defined for single-task problems because they lack a mechanism to share knowledge between different tasks. To address this problem, we create a novel dictionary-based compression dissimilarity measure that allows us to share knowledge across different tasks effectively. Experimental results with extensive comparisons demonstrate the generality and the effectiveness of our proposal. © 2012 Springer-Verlag London Limited.",2013,Knowledge and Information Systems,13,@ @ last decade @ witnessed extensive research on multi-task learning algorithm in diverse domain @ a bioinformatics text mining natural language processing a well a image and video content analysis @ however @ existing multi-task learning method require either domain-specific knowledge to extract feature @ a careful setting of many input parameter @ @ @ many disadvantage associated @ prior knowledge requirement @ feature extraction @ parameter-laden approach @ @ of @ @ obvious problem is @ @ may find a wrong @ non-existent pattern @ of poorly extracted feature @ incorrectly set parameter @ in @ work @ propose a feature-free and parameter-light multi-task clustering framework to overcome @ disadvantage @ @ proposal is motivated by @ recent success of kolmogorov-based method on various application @ however @ method @ only defined @ single-task problem @ @ lack a mechanism to share knowledge @ different task @ to address @ problem @ create a novel dictionary-based compression dissimilarity measure @ allows u to share knowledge across different task effectively @ experimental @ @ extensive comparison demonstrate @ generality and @ effectiveness of @ proposal @ springer-verlag london limited @ 
3167,DegExt: A language-independent keyphrase extractor,"In this paper, we introduce DegExt, a graph-based language-independent keyphrase extractor, which extends the keyword extraction method described in Litvak and Last (Graph-based keyword extraction for single-document summarization. In: Proceedings of the workshop on multi-source multilingual information extraction and summarization, pp 17-24, 2008). We compare DegExt with two state-of-the-art approaches to keyphrase extraction: GenEx (Turney in Inf Retr 2:303-336, 2000) and TextRank (Mihalcea and Tarau in Textrank-bringing order into texts. In: Proceedings of the conference on empirical methods in natural language processing. Barcelona, Spain, 2004). We evaluated DegExt on collections of benchmark summaries in two different languages: English and Hebrew. Our experiments on the English corpus show that DegExt significantly outperforms TextRank and GenEx in terms of precision and area under curve for summaries of 15 keyphrases or more at the expense of a mostly non-significant decrease in recall and F-measure, when the extracted phrases are matched against gold standard collection. Due to DegExt's tendency to extract bigger phrases than GenEx and TextRank, when the single extracted words are considered, DegExt outperforms them both in terms of recall and F-measure. In the Hebrew corpus, DegExt performs the same as TextRank disregarding the number of keyphrases. An additional experiment shows that DegExt applied to the TextRank representation graphs outperforms the other systems in the text classification task. For documents in both languages, DegExt surpasses both GenEx and TextRank in terms of implementation simplicity and computational complexity. © 2012 Springer-Verlag.",2013,Journal of Ambient Intelligence and Humanized Computing,10,in @ @ @ introduce degext a graph-based language-independent keyphrase extractor @ extends @ keyword extraction method described in litvak and last @ graph-based keyword extraction @ single-document summarization @ in @ proceeding of @ workshop on multi-source multilingual information extraction and summarization pp @ @ @ compare degext @ @ state-of-the-art approach to keyphrase extraction @ genex @ turney in inf retr @ @ and textrank @ mihalcea and tarau in textrank-bringing order @ text @ in @ proceeding of @ conference on empirical method in natural language processing @ barcelona spain @ @ @ evaluated degext on collection of benchmark summary in @ different language @ english and hebrew @ @ experiment on @ english corpus @ @ degext significantly outperforms textrank and genex in term of precision and area @ curve @ summary of keyphrases @ more at @ expense of a mostly non-significant decrease in recall and f-measure @ @ extracted phrase @ matched @ gold standard collection @ due to degext @ s tendency to extract bigger phrase @ genex and textrank @ @ single extracted word @ considered degext outperforms @ @ in term of recall and f-measure @ in @ hebrew corpus degext performs @ @ a textrank disregarding @ number of keyphrases @ @ additional experiment @ @ degext applied to @ textrank representation graph outperforms @ @ system in @ text classification task @ @ document in @ language degext surpasses @ genex and textrank in term of implementation simplicity and computational complexity @ springer-verlag @ 
3169,TSDW: Two-stage word sense disambiguation using Wikipedia,"The semantic knowledge of Wikipedia has proved to be useful for many tasks, for example, named entity disambiguation. Among these applications, the task of identifying the word sense based on Wikipedia is a crucial component because the output of this component is often used in subsequent tasks. In this article, we present a two-stage framework (called TSDW) for word sense disambiguation using knowledge latent in Wikipedia. The disambiguation of a given phrase is applied through a two-stage disambiguation process: (a) The first-stage disambiguation explores the contextual semantic information, where the noisy information is pruned for better effectiveness and efficiency; and (b) the second-stage disambiguation explores the disambiguated phrases of high confidence from the first stage to achieve better redisambiguation decisions for the phrases that are difficult to disambiguate in the first stage. Moreover, existing studies have addressed the disambiguation problem for English text only. Considering the popular usage of Wikipedia in different languages, we study the performance of TSDW and the existing state-of-the-art approaches over both English and Traditional Chinese articles. The experimental results show that TSDW generalizes well to different semantic relatedness measures and text in different languages. More important, TSDW significantly outperforms the state-of-the-art approaches with both better effectiveness and efficiency. © 2013 ASIS&T.",2013,Journal of the American Society for Information Science and Technology,14,@ semantic knowledge of wikipedia ha proved to @ useful @ many task @ example named entity disambiguation @ among @ application @ task of identifying @ word sense based on wikipedia is a crucial component @ @ output of @ component is often used in subsequent task @ in @ article @ @ a two-stage framework @ called tsdw @ @ word sense disambiguation @ knowledge latent in wikipedia @ @ disambiguation of a given phrase is applied @ a two-stage disambiguation process @ @ a @ @ first-stage disambiguation explores @ contextual semantic information @ @ noisy information is pruned @ better effectiveness and efficiency @ and @ b @ @ second-stage disambiguation explores @ disambiguated phrase of high confidence @ @ first stage to achieve better redisambiguation decision @ @ phrase @ @ difficult to disambiguate in @ first stage @ moreover existing study @ addressed @ disambiguation problem @ english text only @ considering @ popular usage of wikipedia in different language @ study @ performance of tsdw and @ existing state-of-the-art approach @ @ english and traditional chinese article @ @ experimental @ @ @ tsdw generalizes well to different semantic relatedness measure and text in different language @ more important tsdw significantly outperforms @ state-of-the-art approach @ @ better effectiveness and efficiency @ asis t @ 
3170,"Term extraction from sparse, ungrammatical domain-specific documents","Existing term extraction systems have predominantly targeted large and well-written document collections, which provide reliable statistical and linguistic evidence to support term extraction. In this article, we address the term extraction challenges posed by sparse, ungrammatical texts with domain-specific contents, such as customer complaint emails and engineers' repair notes. To this aim, we present ExtTerm, a novel term extraction system. Specifically, as our core innovations, we accurately detect rare (low frequency) terms, overcoming the issue of data sparsity. These rare terms may denote critical events, but they are often missed by extant TE systems. ExtTerm also precisely detects multi-word terms of arbitrarily lengths, e.g. with more than 2 words. This is achieved by exploiting fundamental theoretical notions underlying term formation, and by developing a technique to compute the collocation strength between any number of words. Thus, we address the limitation of existing TE systems, which are primarily designed to identify terms with 2 words. Furthermore, we show that open-domain (general) resources, such as Wikipedia, can be exploited to support domain-specific term extraction. Thus, they can be used to compensate for the unavailability of domain-specific knowledge resources. Our experimental evaluations reveal that ExtTerm outperforms a state-of-the-art baseline in extracting terms from a domain-specific, sparse and ungrammatical real-life text collection. © 2012 Elsevier B.V. All rights reserved.",2013,Expert Systems with Applications,18,existing term extraction system @ predominantly targeted @ and well-written document collection @ provide reliable statistical and linguistic evidence to support term extraction @ in @ article @ address @ term extraction challenge posed by sparse ungrammatical text @ domain-specific content @ a customer complaint email and engineer @ repair note @ to @ aim @ @ extterm a novel term extraction system @ specifically a @ core innovation @ accurately detect rare @ low frequency @ term overcoming @ issue of data sparsity @ @ rare term may denote critical event @ @ @ often missed by extant te system @ extterm @ precisely detects multi-word term of arbitrarily length e @ g @ @ more @ word @ @ is achieved by exploiting fundamental theoretical notion underlying term formation and by developing a technique to compute @ collocation strength @ @ number of word @ thus @ address @ limitation of existing te system @ @ primarily designed to identify term @ word @ furthermore @ @ @ open-domain @ general @ resource @ a wikipedia @ @ exploited to support domain-specific term extraction @ thus @ @ @ used to compensate @ @ unavailability of domain-specific knowledge resource @ @ experimental evaluation reveal @ extterm outperforms a state-of-the-art baseline in extracting term @ a domain-specific sparse and ungrammatical real-life text collection @ @ b @ v @ @ right reserved @ 
3173,Facilitating the analysis of discourse phenomena in an interoperable NLP platform,"The analysis of discourse phenomena is essential in many natural language processing (NLP) applications. The growing diversity of available corpora and NLP tools brings a multitude of representation formats. In order to alleviate the problem of incompatible formats when constructing complex text mining pipelines, the Unstructured Information Management Architecture (UIMA) provides a standard means of communication between tools and resources. U-Compare, a text mining workflow construction platform based on UIMA, further enhances interoperability through a shared system of data types, allowing free combination of compliant components into workflows. Although U-Compare and its type system already support syntactic and semantic analyses, support for the analysis of discourse phenomena was previously lacking. In response, we have extended the U-Compare type system with new discourse-level types. We illustrate processing and visualisation of discourse information in U-Compare by providing several new deserialisation components for corpora containing discourse annotations. The new U-Compare is downloadable from http://nactem.ac.uk/ ucompare. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),27,@ analysis of discourse phenomenon is essential in many natural language processing @ nlp @ application @ @ growing diversity of available corpus and nlp tool brings a multitude of representation format @ in order to alleviate @ problem of incompatible format @ constructing complex text mining pipeline @ unstructured information management architecture @ uima @ provides a standard mean of communication @ tool and resource @ u-compare a text mining workflow construction platform based on uima @ enhances interoperability @ a shared system of data type allowing free combination of compliant component @ workflow @ although u-compare and @ type system already support syntactic and semantic analysis support @ @ analysis of discourse phenomenon wa @ lacking @ in response @ @ extended @ u-compare type system @ @ discourse-level type @ @ illustrate processing and visualisation of discourse information in u-compare by providing several @ deserialisation component @ corpus containing discourse annotation @ @ @ u-compare is downloadable @ http @ nactem @ ac @ uk ucompare @ springer-verlag @ 
3174,Enhancing search: Events and their discourse context,"Event-based search systems have become of increasing interest. This paper provides an overview of recent advances in event-based text mining, with an emphasis on biomedical text. We focus particularly on the enrichment of events with information relating to their interpretation according to surrounding textual and discourse contexts. We describe our annotation scheme used to capture this information at the event level, report on the corpora that have so far been enriched according to this scheme and provide details of our experiments to recognise this information automatically. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),28,event-based search system @ become of increasing interest @ @ @ provides @ overview of recent advance in event-based text mining @ @ emphasis on biomedical text @ @ focus particularly on @ enrichment of event @ information relating to @ interpretation according to surrounding textual and discourse context @ @ describe @ annotation scheme used to capture @ information at @ event level report on @ corpus @ @ @ far @ enriched according to @ scheme and provide detail of @ experiment to recognise @ information automatically @ springer-verlag @ 
3175,SIAM: Social interaction analysis for multimedia,"This paper describes the SIAM demonstrator, a system that illustrates the usefulness of indexing multimedia segments thanks to associated microblog posts. From a socialized multimedia content (i.e. video and associated microblog posts on Twitter), the system applies text mining techniques and derives a topic model to index socialized multimedia segments. That result may then be used inside many multimedia applications, such as in-media social navigation, multimedia summarization or composition, or exploration of multimedia collections according to various socially-based viewpoints. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ describes @ siam demonstrator a system @ illustrates @ usefulness of indexing multimedia segment thanks to associated microblog post @ @ a socialized multimedia content @ i @ e @ video and associated microblog post on twitter @ @ system applies text mining technique and derives a topic model to index socialized multimedia segment @ @ @ may @ @ used inside many multimedia application @ a in-media social navigation multimedia summarization @ composition @ exploration of multimedia collection according to various socially-based viewpoint @ springer-verlag @ 
3176,Monolingual and cross-lingual probabilistic topic models and their applications in information retrieval,"Probabilistic topic models are a group of unsupervised generative machine learning models that can be effectively trained on large text collections. They model document content as a two-step generation process, i.e., documents are observed as mixtures of latent topics, while topics are probability distributions over vocabulary words. Recently, a significant research effort has been invested into transferring the probabilistic topic modeling concept from monolingual to multilingual settings. Novel topic models have been designed to work with parallel and comparable multilingual data (e.g., Wikipedia or news data discussing the same events). Probabilistic topics models offer an elegant way to represent content across different languages. Their probabilistic framework allows for their easy integration into a language modeling framework for monolingual and cross-lingual information retrieval. Moreover, we present how to use the knowledge from the topic models in the tasks of cross-lingual event clustering, cross-lingual document classification and the detection of cross-lingual semantic similarity of words. The tutorial also demonstrates how semantically similar words across languages are integrated as useful additional evidences in cross-lingual information retrieval models. © 2013 Springer-Verlag.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,probabilistic topic model @ a group of unsupervised generative machine learning model @ @ @ effectively trained on @ text collection @ @ model document content a a two-step generation process i @ e @ document @ observed a mixture of latent topic @ topic @ probability distribution @ vocabulary word @ recently a significant research effort ha @ invested @ transferring @ probabilistic topic modeling concept @ monolingual to multilingual setting @ novel topic model @ @ designed to work @ parallel and comparable multilingual data @ e @ g @ wikipedia @ news data discussing @ @ event @ @ probabilistic topic model offer @ elegant way to represent content across different language @ @ probabilistic framework allows @ @ easy integration @ a language modeling framework @ monolingual and cross-lingual information retrieval @ moreover @ @ @ to use @ knowledge @ @ topic model in @ task of cross-lingual event clustering cross-lingual document classification and @ detection of cross-lingual semantic similarity of word @ @ tutorial @ demonstrates @ semantically similar word across language @ integrated a useful additional evidence in cross-lingual information retrieval model @ springer-verlag @ 
3180,Lightly supervised acquisition of named entities and linguistic patterns for multilingual text mining,"Named Entity Recognition and Classification (NERC) is an important component of applications like Opinion Tracking, Information Extraction, or Question Answering. When these applications require to work in several languages, NERC becomes a bottleneck because its development requires language-specific tools and resources like lists of names or annotated corpora. This paper presents a lightly supervised system that acquires lists of names and linguistic patterns from large raw text collections in western languages and starting with only a few seeds per class selected by a human expert. Experiments have been carried out with English and Spanish news collections and with the Spanish Wikipedia. Evaluation of NE classification on standard datasets shows that NE lists achieve high precision and reveals that contextual patterns increase recall significantly. Therefore, it would be helpful for applications where annotated NERC data are not available such as those that have to deal with several western languages or information from different domains. © 2012 Springer-Verlag London Limited.",2013,Knowledge and Information Systems,5,named entity recognition and classification @ nerc @ is @ important component of application like opinion tracking information extraction @ question answering @ @ @ application require to work in several language nerc becomes a bottleneck @ @ development requires language-specific tool and resource like list of name @ annotated corpus @ @ @ @ a lightly supervised system @ acquires list of name and linguistic pattern @ @ raw text collection in western language and starting @ only a @ seed per class selected by a human expert @ experiment @ @ carried @ @ english and spanish news collection and @ @ spanish wikipedia @ evaluation of ne classification on standard datasets @ @ ne list achieve high precision and reveals @ contextual pattern increase recall significantly @ therefore @ would @ helpful @ application @ annotated nerc data @ not available @ a @ @ @ to deal @ several western language @ information @ different domain @ springer-verlag london limited @ 
3182,Big data analytics: A framework for unstructured data analysis,"Nowadays, most of information saved in companies are unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web areas. Many of these requirements will be depend on the unstructured data analysis. More than 80% of all potentially useful business information is unstructured data, in kind of sensor readings, console logs and so on. The large number and complexity of unstructured data opens up many new possibilities for the analyst. Text mining and natural language processing are two techniques with their methods for knowledge discovery from textual context in documents. This is an approach to organize a complex unstructured data and to retrieve necessary information. The paper is to find an efficient way of storing unstructured data and appropriate approach of fetching data. Unstructured data targeted in this work to organize, is the public tweets of Twitter. Building an Big Data application that gets stream of public tweets from twitter which is latter stored in the HBase using Hadoop cluster and followed by data analysis for data retrieved from HBase by REST calls is the pragmatic approach of this project.",2013,International Journal of Engineering and Technology,49,nowadays @ of information saved in company @ unstructured model @ retrieval and extraction of @ information is essential work and importance in semantic web area @ many of @ requirement @ @ depend on @ unstructured data analysis @ more @ of @ potentially useful @ information is unstructured data in kind of sensor reading console log and @ on @ @ @ number and complexity of unstructured data open up many @ possibility @ @ analyst @ text mining and natural language processing @ @ technique @ @ method @ knowledge discovery @ textual context in document @ @ is @ approach to organize a complex unstructured data and to retrieve necessary information @ @ @ is to find @ efficient way of storing unstructured data and appropriate approach of fetching data @ unstructured data targeted in @ work to organize is @ public tweet of twitter @ building @ big data application @ get stream of public tweet @ twitter @ is latter stored in @ hbase @ hadoop cluster and followed by data analysis @ data retrieved @ hbase by rest call is @ pragmatic approach of @ project @ 
3186,Automatic text analysis by artificial intelligence,"Text is one of the traditional ways of communication between people. With the growing availability of text data in electronic form, handling and analysis of text by means of computers gained popularity. Handling text data with machine learning methods brought interesting challenges to the area that got further extended by incorporation of some natural language specifics. As the methods were capable of addressing more complex problems related to text data, the expectations become bigger calling for more sophisticated methods, in particular a combination of methods from different research areas including information retrieval, machine learning, statistical data analysis, data mining, natural language processing, semantic technologies. Automatic text analysis become an integral part of many systems, pushing boundaries of research capabilities towards what one can refer to as an artificial intelligence dream - never ending learning from text aiming at mimicking ways of human learning. The paper presents development of text analysis research in Slovenian that we have been personally involved in, pointing out interesting research problems that have been and are still addressed by the research, example tasks that have been addressed and some challenges on the way.",2013,Informatica (Slovenia),0,text is @ of @ traditional way of communication @ people @ @ @ growing availability of text data in electronic form handling and analysis of text by mean of computer gained popularity @ handling text data @ machine learning method brought interesting challenge to @ area @ got @ extended by incorporation of some natural language specific @ a @ method @ capable of addressing more complex problem related to text data @ expectation become bigger calling @ more sophisticated method in particular a combination of method @ different research area including information retrieval machine learning statistical data analysis data mining natural language processing semantic technology @ automatic text analysis become @ integral part of many system pushing boundary of research capability towards @ @ @ refer to a @ artificial intelligence dream never ending learning @ text aiming at mimicking way of human learning @ @ @ @ development of text analysis research in slovenian @ @ @ @ personally involved in pointing @ interesting research problem @ @ @ and @ still addressed by @ research example task @ @ @ addressed and some challenge on @ way @ 
3187,Approximation of COSMIC functional size to support early effort estimation in Agile,"The demands in the software industry of estimating development effort in the early phases of development are met by measuring software size from user requirements. A large number of companies have adapted themselves with Agile processes, which, although, promise rapid software development, pose a huge burden on the development teams for continual decision making and expert judgement, when estimating the size of the software components to be developed at each iteration. COSMIC, on the other hand, is an ISO/IEC international standard that presents an objective method of measuring the functional size of the software from user requirements. However, its measurement process is not compatible with Agile processes, as COSMIC requires user requirements to be formalised and decomposed at a level of granularity where external interactions with the system are visible to the human measurer. This time-consuming task is avoided by agile processes, leaving it with the only option of quick subjective judgement by human measurers for size measurement that often tends to be erroneous. In this article, we address these issues by presenting an approach to approximate COSMIC functional size from informally written textual requirements demonstrating its applicability in popular agile processes. We also discuss the results of a preliminary experiment studying the feasibility of automating our approach using supervised text mining. © 2012 Elsevier B.V. All rights reserved.",2013,Data and Knowledge Engineering,31,@ demand in @ software industry of estimating development effort in @ early phase of development @ met by measuring software size @ user requirement @ a @ number of company @ adapted @ @ agile process @ although promise rapid software development pose a huge burden on @ development team @ continual decision making and expert judgement @ estimating @ size of @ software component to @ developed at @ iteration @ cosmic on @ @ hand is @ iso iec international standard @ @ @ objective method of measuring @ functional size of @ software @ user requirement @ however @ measurement process is not compatible @ agile process a cosmic requires user requirement to @ formalised and decomposed at a level of granularity @ external interaction @ @ system @ visible to @ human measurer @ @ time-consuming task is avoided by agile process leaving @ @ @ only option of quick subjective judgement by human measurer @ size measurement @ often tends to @ erroneous @ in @ article @ address @ issue by presenting @ approach to approximate cosmic functional size @ informally written textual requirement demonstrating @ applicability in popular agile process @ @ @ discus @ @ of a preliminary experiment studying @ feasibility of automating @ approach @ supervised text mining @ @ b @ v @ @ right reserved @ 
3188,Semantic computing and business intelligence,"With rapidly expanding data collections becoming increasingly available, the application of Semantic Computing has become imperative to leverage this resource for industrial applications. This paper presents a survey of Semantic Computing in the area of Business Intelligence. We examine semantic analytical techniques and tools as applied for prediction analysis and decision support. We also define the role of Semantic Computing as applied in the context of Data Mining, Text Mining and Big Data Analytics. Additionally, we describe how business data is queried with Structured Natural Language as well as the use of On-Line Analytic Processing. © 2013 World Scientific Publishing Company.",2013,International Journal of Semantic Computing,11,@ rapidly expanding data collection becoming increasingly available @ application of semantic computing ha become imperative to leverage @ resource @ industrial application @ @ @ @ a survey of semantic computing in @ area of @ intelligence @ @ examine semantic analytical technique and tool a applied @ prediction analysis and decision support @ @ @ define @ role of semantic computing a applied in @ context of data mining text mining and big data analytics @ additionally @ describe @ @ data is queried @ structured natural language a well a @ use of on-line analytic processing @ world scientific publishing company @ 
3189,Integrating statistical and lexical information for recognizing textual entailments in text,"Recognizing textual entailment is to infer that a given text span follows from the meaning of a given hypothesis. To have better recognition capability, it is necessary to employ deep text processing units such as syntactic parsers and semantic taggers. However, these resources are not usually available in other non-English languages. In this paper, we present a light-weight Chinese textual entailment recognition system using part-of-speech information only. We designed two different feature models from training data and employed the well-known kernel method to learn to predict testing data. One feature set abstracts the generic statistics between the text pairs, while the other set directly models lexical features based on the traditional bag-of-words model. The ability of the proposed feature models not only brings additional statistical information from their datasets but also helps to enhance the prediction capability. To validate this, we conducted the experiments on the novel benchmark corpus-NTCIR-RITE-2011. The empirical results demonstrate that our method achieves the best results in comparison to the other competitors. In terms of accuracy, our method achieves 54.77% for the NTCIR RITE MC task. © 2012 Elsevier B.V. All rights reserved.",2013,Knowledge-Based Systems,4,recognizing textual entailment is to infer @ a given text span follows @ @ meaning of a given hypothesis @ to @ better recognition capability @ is necessary to employ deep text processing unit @ a syntactic parser and semantic tagger @ however @ resource @ not usually available in @ non-english language @ in @ @ @ @ a light-weight chinese textual entailment recognition system @ part-of-speech information only @ @ designed @ different feature model @ training data and employed @ well-known kernel method to learn to predict testing data @ @ feature set abstract @ generic statistic @ @ text pair @ @ @ set directly model lexical feature based on @ traditional bag-of-words model @ @ ability of @ proposed feature model not only brings additional statistical information @ @ datasets @ @ help to enhance @ prediction capability @ to validate @ @ conducted @ experiment on @ novel benchmark corpus-ntcir-rite @ @ empirical @ demonstrate @ @ method achieves @ best @ in comparison to @ @ competitor @ in term of accuracy @ method achieves @ @ @ ntcir rite mc task @ @ b @ v @ @ right reserved @ 
3190,Minimally-supervised extraction of domain-specific part-whole relations using Wikipedia as knowledge-base,"We present a minimally-supervised approach for learning part-whole relations from texts. Unlike previous techniques, we focused on sparse, domain-specific texts. The novelty in our approach lies in the use of Wikipedia as a knowledge-base, from which we first acquire a set of reliable patterns that express part-whole relations. This is achieved by a minimally-supervised algorithm. We then use the patterns acquired to extract part-whole relation triples from a collection of sparse, domain-specific texts. Our strategy, of learning in one domain and applying the knowledge in another domain is based upon the notion of domain-adaption. It allows us to overcome the challenges of learning the relations directly from the sparse, domain-specific corpus. Our experimental evaluations reveal that, despite its general-purpose nature, Wikipedia can be exploited as a source of knowledge for improving the performance of domain-specific part-whole relation extraction. As our other contributions, we propose a mechanism that mitigates the negative impact of semantic-drift on minimally-supervised algorithms. Also, we represent the patterns in the extracted relations using sophisticated syntactic structures that avoid the limitations of traditional surface string representations. In addition, we show that domain-specific part-whole relations cannot be conclusively classified in existing taxonomies. © 2012 Elsevier B.V. All rights reserved.",2013,Data and Knowledge Engineering,25,@ @ a minimally-supervised approach @ learning part-whole relation @ text @ unlike previous technique @ focused on sparse domain-specific text @ @ novelty in @ approach lie in @ use of wikipedia a a knowledge-base @ @ @ first acquire a set of reliable pattern @ express part-whole relation @ @ is achieved by a minimally-supervised algorithm @ @ @ use @ pattern acquired to extract part-whole relation triple @ a collection of sparse domain-specific text @ @ strategy of learning in @ domain and applying @ knowledge in another domain is based upon @ notion of domain-adaption @ @ allows u to overcome @ challenge of learning @ relation directly @ @ sparse domain-specific corpus @ @ experimental evaluation reveal @ despite @ general-purpose nature wikipedia @ @ exploited a a source of knowledge @ improving @ performance of domain-specific part-whole relation extraction @ a @ @ contribution @ propose a mechanism @ mitigates @ negative impact of semantic-drift on minimally-supervised algorithm @ @ @ represent @ pattern in @ extracted relation @ sophisticated syntactic structure @ avoid @ limitation of traditional surface string representation @ in addition @ @ @ domain-specific part-whole relation cannot @ conclusively classified in existing taxonomy @ @ b @ v @ @ right reserved @ 
3203,Document clustering for forensic analysis: An approach for improving computer inspection,"In computer forensic analysis, hundreds of thousands of files are usually examined. Much of the data in those files consists of unstructured text, whose analysis by computer examiners is difficult to be performed. In this context, automated methods of analysis are of great interest. In particular, algorithms for clustering documents can facilitate the discovery of new and useful knowledge from the documents under analysis. We present an approach that applies document clustering algorithms to forensic analysis of computers seized in police investigations. We illustrate the proposed approach by carrying out extensive experimentation with six well-known clustering algorithms (K-means, K-medoids, Single Link, Complete Link, Average Link, and CSPA) applied to five real-world datasets obtained from computers seized in real-world investigations. Experiments have been performed with different combinations of parameters, resulting in 16 different instantiations of algorithms. In addition, two relative validity indexes were used to automatically estimate the number of clusters. Related studies in the literature are significantly more limited than our study. Our experiments show that the Average Link and Complete Link algorithms provide the best results for our application domain. If suitably initialized, partitional algorithms (K-means and K-medoids) can also yield to very good results. Finally, we also present and discuss several practical results that can be useful for researchers and practitioners of forensic computing. © 2005-2012 IEEE.",2013,IEEE Transactions on Information Forensics and Security,47,in computer forensic analysis hundred of thousand of file @ usually examined @ much of @ data in @ file consists of unstructured text whose analysis by computer examiner is difficult to @ performed @ in @ context automated method of analysis @ of great interest @ in particular algorithm @ clustering document @ facilitate @ discovery of @ and useful knowledge @ @ document @ analysis @ @ @ @ approach @ applies document clustering algorithm to forensic analysis of computer seized in police investigation @ @ illustrate @ proposed approach by carrying @ extensive experimentation @ six well-known clustering algorithm @ k-means k-medoids single link complete link average link and cspa @ applied to five real-world datasets obtained @ computer seized in real-world investigation @ experiment @ @ performed @ different combination of parameter resulting in different instantiation of algorithm @ in addition @ relative validity index @ used to automatically estimate @ number of cluster @ related study in @ literature @ significantly more limited @ @ study @ @ experiment @ @ @ average link and complete link algorithm provide @ best @ @ @ application domain @ if suitably initialized partitional algorithm @ k-means and k-medoids @ @ @ yield to @ good @ @ finally @ @ @ and discus several practical @ @ @ @ useful @ researcher and practitioner of forensic computing @ @ @ 
3204,Assessment of Software Testing and Quality Assurance in Natural Language Processing Applications and a Linguistically Inspired Approach to Improving It,"Significant progress has been made in addressing the scientific challenges of biomedical text mining. However, the transition from a demonstration of scientific progress to the production of tools on which a broader community can rely requires that fundamental software engineering requirements be addressed. In this paper we characterize the state of biomedical text mining software with respect to software testing and quality assurance. Biomedical natural language processing software was chosen because it frequently specifically claims to offer production-quality services, rather than just research prototypes. We examined twenty web sites offering a variety of text mining services. On each web site, we performed the most basic software test known to us and classified the results. Seven out of twenty web sites returned either bad results or the worst class of results in response to this simple test. We conclude that biomedical natural language processing tools require greater attention to software quality. We suggest a linguistically motivated approach to granular evaluation of natural language processing applications, and show how it can be used to detect performance errors of several systems and to predict overall performance on specific equivalence classes of inputs. We also assess the ability of linguistically-motivated test suites to provide good software testing, as compared to large corpora of naturally-occurring data. We measure code coverage and find that it is considerably higher when even small structured test suites are utilized than when large corpora are used. © Springer-Verlag Berlin Heidelberg 2013.",2013,Communications in Computer and Information Science,0,significant progress ha @ made in addressing @ scientific challenge of biomedical text mining @ however @ transition @ a demonstration of scientific progress to @ production of tool on @ a broader community @ rely requires @ fundamental software engineering requirement @ addressed @ in @ @ @ characterize @ state of biomedical text mining software @ respect to software testing and quality assurance @ biomedical natural language processing software wa chosen @ @ frequently specifically claim to offer production-quality service rather @ @ research prototype @ @ examined twenty web site offering a variety of text mining service @ on @ web site @ performed @ @ basic software test known to u and classified @ @ @ seven @ of twenty web site returned either bad @ @ @ worst class of @ in response to @ simple test @ @ conclude @ biomedical natural language processing tool require greater attention to software quality @ @ suggest a linguistically motivated approach to granular evaluation of natural language processing application and @ @ @ @ @ used to detect performance error of several system and to predict overall performance on specific equivalence class of input @ @ @ ass @ ability of linguistically-motivated test suite to provide good software testing a compared to @ corpus of naturally-occurring data @ @ measure code coverage and find @ @ is considerably higher @ even small structured test suite @ utilized @ @ @ corpus @ used @ springer-verlag @ @ @ 
3206,MOETA: A novel text-mining model for collecting and analysing competitive intelligence,"The internet constitutes a vast repository of textual information, and its emergence has dramatically changed the environment in which businesses operate. Its development has had a great influence on the current business models. The goal of this work is to outline a novel text-mining-based decision-support model, Mining for Opinion, Event and Timeline Analysis (MOETA), which aims to explore competitive intelligence from the internet and the internal textual data sources of a company in depth. MOETA integrates novel Natural Language Processing (NLP) technologies for event detection and opinion mining to locate events and opinions on a timeline. The aim is to distil unstructured textual data into knowledge and intelligence that are useful to business decision-makers. An overview of the model is given and the architecture of a system based on the model is introduced. Moreover, we provide a practical example to explain how MOETA can support decision making. © 2013 Inderscience Enterprises Ltd.",2013,International Journal of Advanced Media and Communication,4,@ internet constitutes a vast repository of textual information and @ emergence ha dramatically changed @ environment in @ @ operate @ @ development ha @ a great influence on @ current @ model @ @ goal of @ work is to outline a novel text-mining-based decision-support model mining @ opinion event and timeline analysis @ moeta @ @ aim to explore competitive intelligence @ @ internet and @ internal textual data source of a company in depth @ moeta integrates novel natural language processing @ nlp @ technology @ event detection and opinion mining to locate event and opinion on a timeline @ @ aim is to distil unstructured textual data @ knowledge and intelligence @ @ useful to @ decision-makers @ @ overview of @ model is given and @ architecture of a system based on @ model is introduced @ moreover @ provide a practical example to explain @ moeta @ support decision making @ inderscience enterprise ltd @ 
3213,World Wide Web in the service of schooling: Semantic Web as a solution for language teaching in Cypriot secondary education,"This paper examines some suitability aspects of existing web search engines in relation to the content and the stated learning objectives of language teaching in Cypriot secondary education, focusing on the language course of the third high school grade (G9). The end goal is to put the internet in the service of schooling; specifically to categorize the results returned by the search engine in-to genres in order to facilitate user (teacher or student) in choosing the most ap-propriate texts for their learning purposes. The tools for categorizing texts are being sought in the field of Semantic Web technology, such as metadata, ontol-ogies, software agents, and, the techniques in the fields of Natural Language Processing (NLP), Information Retrieval (IR), Information Extraction (IE) and Text Mining. The paper proposes the categorization of texts into six major ge-nre categories according to their external (structural) and internal (linguistic, stylistic) characteristics. For the purpose of this research, the MeDa13 metadata model was designed on the basis of the standard metadata model Dublin Core, and the Textual Genres Ontology (TeGO) was developed for describing the concepts mentioned in genres. In this work, we present the theoretical back-ground for the development of the proposed models (MeDa13 and TeGO), and also the methodological plan to achieve the research objective, which is the ca-tegorization of texts into genres considering the content and learning objectives for language teaching.",2013,CEUR Workshop Proceedings,0,@ @ examines some suitability aspect of existing web search engine in relation to @ content and @ stated learning objective of language teaching in cypriot secondary education focusing on @ language course of @ third high school grade @ g @ @ @ end goal is to put @ internet in @ service of schooling @ specifically to categorize @ @ returned by @ search engine in-to genre in order to facilitate user @ teacher @ student @ in choosing @ @ ap-propriate text @ @ learning purpose @ @ tool @ categorizing text @ @ sought in @ field of semantic web technology @ a metadata ontol-ogies software agent and @ technique in @ field of natural language processing @ nlp @ information retrieval @ ir @ information extraction @ ie @ and text mining @ @ @ proposes @ categorization of text @ six major ge-nre category according to @ external @ structural @ and internal @ linguistic stylistic @ characteristic @ @ @ purpose of @ research @ meda metadata model wa designed on @ basis of @ standard metadata model dublin core and @ textual genre ontology @ tego @ wa developed @ describing @ concept mentioned in genre @ in @ work @ @ @ theoretical back-ground @ @ development of @ proposed model @ meda and tego @ and @ @ methodological plan to achieve @ research objective @ is @ ca-tegorization of text @ genre considering @ content and learning objective @ language teaching @ 
3219,TermWatch II: Unsupervised Terminology Graph Extraction and Decomposition,"We present a symbolic and graph-based approach for mapping knowledge domains. The symbolic component relies on shallow linguistic processing of texts to extract multi-word terms and cluster them based on lexico-syntactic relations. The clusters are subjected to graph decomposition based on inherent graph theoretic properties of association graphs of items (multi-word terms and authors). This includes the search for complete minimal separators that can decompose the graphs into central (core topics) and peripheral atoms. The methodology is implemented in the TermWatch system and can be used for several text mining tasks. In this paper, we apply our methodology to map the dynamics of terrorism research between 1990-2006. We also mined for frequent itemsets as a mean of revealing dependencies between formal concepts in the corpus. A comparison of the extracted frequent itemsets and the structure of the central atom shows an interesting overlap. The main features of our approach lie in the combination of state-of-the-art techniques from Natural Language Processing (NLP), Clustering and Graph Theory to develop a system and a methodology adapted to uncovering hidden sub-structures from texts. © Springer-Verlag Berlin Heidelberg 2013.",2013,Communications in Computer and Information Science,0,@ @ a symbolic and graph-based approach @ mapping knowledge domain @ @ symbolic component relies on shallow linguistic processing of text to extract multi-word term and cluster @ based on lexico-syntactic relation @ @ cluster @ subjected to graph decomposition based on inherent graph theoretic property of association graph of item @ multi-word term and author @ @ @ includes @ search @ complete minimal separator @ @ decompose @ graph @ central @ core topic @ and peripheral atom @ @ methodology is implemented in @ termwatch system and @ @ used @ several text mining task @ in @ @ @ apply @ methodology to map @ dynamic of terrorism research @ @ @ @ mined @ frequent itemsets a a mean of revealing dependency @ formal concept in @ corpus @ a comparison of @ extracted frequent itemsets and @ structure of @ central atom @ @ interesting overlap @ @ main feature of @ approach lie in @ combination of state-of-the-art technique @ natural language processing @ nlp @ clustering and graph theory to develop a system and a methodology adapted to uncovering hidden sub-structures @ text @ springer-verlag @ @ @ 
3220,An ontology for drug-drug interactions,"Drug-drug interactions form a significant risk group for adverse effects associ-ated with pharmaceutical treatment. These interactions are often reported in the literature, however, they are sparsely represented in machine-readable re-sources, such as online databases, thesauri or ontologies. These knowledge sources play a pivotal role in Natural Language Processing (NLP) systems since they provide a knowledge representation about the world or a particular do-main. While ontologies for drugs and their effects have proliferated in recent years, there is no ontology capable of describing and categorizing drug-drug in-teractions. Moreover, there is no artifact that represents all the possible mecha-nisms that can lead to a DDI. To fill this gap we propose DINTO, an ontology for drug-drug interactions and their mechanisms. In this paper we describe the classes, relationships and overall structure of DINTO. The ontology is free for use and available at https://code.google.com/p/dinto/.",2013,CEUR Workshop Proceedings,2,drug-drug interaction form a significant risk group @ adverse effect associ-ated @ pharmaceutical treatment @ @ interaction @ often reported in @ literature however @ @ sparsely represented in machine-readable re-sources @ a online database thesaurus @ ontology @ @ knowledge source play a pivotal role in natural language processing @ nlp @ system since @ provide a knowledge representation @ @ world @ a particular do-main @ @ ontology @ drug and @ effect @ proliferated in recent year @ is no ontology capable of describing and categorizing drug-drug in-teractions @ moreover @ is no artifact @ represents @ @ possible mecha-nisms @ @ lead to a ddi @ to fill @ gap @ propose dinto @ ontology @ drug-drug interaction and @ mechanism @ in @ @ @ describe @ class relationship and overall structure of dinto @ @ ontology is free @ use and available at http @ code @ google @ com p dinto @ 
3223,Semantic clustering of scientific articles using explicit semantic analysis,"This paper summarizes our recent research on semantic clustering of scientific articles. We present a case study which was focused on analysis of papers related to the Rough Sets theory. The proposed method groups the documents on the basis of their content, with an assistance of the DBpedia knowledge base. The text corpus is first processed using Natural Language Processing tools in order to produce vector representations of the content. In the second step the articles are matched against a collection of concepts retrieved from DBpedia. As a result, a new representation that better reflects the semantics of the texts, is constructed. With this new representation the documents are hierarchically clustered in order to form a partitioning of papers into semantically related groups. The steps in textual data preparation, the utilization of DBpedia and the employed clustering methods are explained and illustrated with experimental results. A quality of the resulting clustering is then discussed. It is assessed using feedback form human experts combined with typical cluster quality measures. These results are then discussed in the context of a larger framework that aims to facilitate search and information extraction from large textual repositories. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ summarizes @ recent research on semantic clustering of scientific article @ @ @ a case study @ wa focused on analysis of @ related to @ rough set theory @ @ proposed method group @ document on @ basis of @ content @ @ assistance of @ dbpedia knowledge base @ @ text corpus is first processed @ natural language processing tool in order to produce vector representation of @ content @ in @ second step @ article @ matched @ a collection of concept retrieved @ dbpedia @ a a @ a @ representation @ better reflects @ semantics of @ text is constructed @ @ @ @ representation @ document @ hierarchically clustered in order to form a partitioning of @ @ semantically related group @ @ step in textual data preparation @ utilization of dbpedia and @ employed clustering method @ explained and illustrated @ experimental @ @ a quality of @ resulting clustering is @ discussed @ @ is assessed @ feedback form human expert combined @ typical cluster quality measure @ @ @ @ @ discussed in @ context of a larger framework @ aim to facilitate search and information extraction @ @ textual repository @ springer-verlag @ @ @ 
3225,Combining text mining techniques for QA4MRE 2013,"This paper describes a lexical system developed for the main task of Question Answering for Machine Reading Evaluation 2013 (QA4MRE). The presented system executes the preprocessing of test documents, and generates hypotheses consisting of the question text combined with text from possible answers for the question. The hypotheses are compared to sentences from the text by the means of a set similarity measure. The k best similarity scores obtained by each hypothesis are averaged as ranking score for the hypothesis. Two variations of the developed system were utilized, one of them employing coreference detection and resolution techniques in order to take advantage of the discourse structure on the question answering process. The results generated by the systems in QA4MRE 2013 edition are presented and analyzed. The presented system should serve as a solid base for the development of a semantic approach on the task.",2013,CEUR Workshop Proceedings,0,@ @ describes a lexical system developed @ @ main task of question answering @ machine reading evaluation @ qa mre @ @ @ presented system executes @ preprocessing of test document and generates hypothesis consisting of @ question text combined @ text @ possible answer @ @ question @ @ hypothesis @ compared to sentence @ @ text by @ mean of a set similarity measure @ @ k best similarity score obtained by @ hypothesis @ averaged a ranking score @ @ hypothesis @ @ variation of @ developed system @ utilized @ of @ employing coreference detection and resolution technique in order to take advantage of @ discourse structure on @ question answering process @ @ @ generated by @ system in qa mre edition @ presented and analyzed @ @ presented system @ serve a a solid base @ @ development of a semantic approach on @ task @ 
3227,Predicting usefulness of online reviews using stochastic gradient boosting and randomized trees,"This paper presents our analysis of online user reviews from different business categories posted on the internet rating and review services website Yelp. We use business, reviewer, and review level data to generate predictive features for estimating the number of useful votes an online review is expected to receive. Unstructured text data are mined using natural language processing techniques and combined with structured features to train two different machine learning algorithms - Stochastic Gradient Boosted Regression Trees and Extremely Randomized Trees. The results from both of these algorithms are ensembled to generate better performing predictions. The approach described in this paper mirrors the one used by one of the authors in a Kaggle competition hosted by Yelp. Out of 352 participants, the author stood 3rd on the final leaderboard. © 2013, Australian Computer Society, Inc.",2013,Conferences in Research and Practice in Information Technology Series,1,@ @ @ @ analysis of online user review @ different @ category posted on @ internet rating and review service website yelp @ @ use @ reviewer and review level data to generate predictive feature @ estimating @ number of useful vote @ online review is expected to receive @ unstructured text data @ mined @ natural language processing technique and combined @ structured feature to train @ different machine learning algorithm stochastic gradient boosted regression tree and extremely randomized tree @ @ @ @ @ of @ algorithm @ ensembled to generate better performing prediction @ @ approach described in @ @ mirror @ @ used by @ of @ author in a kaggle competition hosted by yelp @ @ of participant @ author stood rd on @ final leaderboard @ australian computer society inc @ 
3228,IBminer: A text mining tool for constructing and populating infobox databases and knowledge bases,"Knowledge bases and structured summaries are playing a crucial role in many applications, such as text summarization, question answering, essay grading, and semantic search. Although, many systems (e.g., DBpedia and YaGo2) provide massive knowledge bases of such summaries, they all suffer from incompleteness, in-consistencies, and inaccuracies. These problems can be addressed and much improved by combining and integrating different knowl-edge bases, but their very large sizes and their reliance on different terminologies and ontologies make the task very difficult. In this demo, we will demonstrate a system that is achieving good suc-cess on this task by: i) employing available interlinks in the current knowledge bases (e.g. externalLink and redirect links in DBpe-dia) to combine information on individual entities, and ii) using widely available text corpora (e.g. Wikipedia) and our IBminer text-mining system, to generate and verify structured information, and reconcile terminologies across different knowledge bases. We will also demonstrate two tools designed to support the integration process in close collaboration with IBminer. The first is the InfoBox Knowledge-Base Browser (IBKB) which provides structured sum-maries and their provenance, and the second is the InfoBox Editor (IBE), which is designed to suggest relevant attributes for a user-specified subject, whereby the user can easily improve the knowl-edge base without requiring any knowledge about the internal ter-minology of individual systems. © 2013 VLDB Endowment.",2013,Proceedings of the VLDB Endowment,15,knowledge base and structured summary @ playing a crucial role in many application @ a text summarization question answering essay grading and semantic search @ although many system @ e @ g @ dbpedia and yago @ provide massive knowledge base of @ summary @ @ suffer @ incompleteness in-consistencies and inaccuracy @ @ problem @ @ addressed and much improved by combining and integrating different knowl-edge base @ @ @ @ size and @ reliance on different terminology and ontology make @ task @ difficult @ in @ demo @ @ demonstrate a system @ is achieving good suc-cess on @ task by @ i @ employing available interlinks in @ current knowledge base @ e @ g @ externallink and redirect link in dbpe-dia @ to combine information on individual entity and ii @ @ widely available text corpus @ e @ g @ wikipedia @ and @ ibminer text-mining system to generate and verify structured information and reconcile terminology across different knowledge base @ @ @ @ demonstrate @ tool designed to support @ integration process in close collaboration @ ibminer @ @ first is @ infobox knowledge-base browser @ ibkb @ @ provides structured sum-maries and @ provenance and @ second is @ infobox editor @ ibe @ @ is designed to suggest relevant attribute @ a user-specified subject whereby @ user @ easily improve @ knowl-edge base without requiring @ knowledge @ @ internal ter-minology of individual system @ vldb endowment @ 
3230,Clustering sentence-level text using a novel fuzzy relational clustering algorithm,"In comparison with hard clustering methods, in which a pattern belongs to a single cluster, fuzzy clustering algorithms allow patterns to belong to all clusters with differing degrees of membership. This is important in domains such as sentence clustering, since a sentence is likely to be related to more than one theme or topic present within a document or set of documents. However, because most sentence similarity measures do not represent sentences in a common metric space, conventional fuzzy clustering approaches based on prototypes or mixtures of Gaussians are generally not applicable to sentence clustering. This paper presents a novel fuzzy clustering algorithm that operates on relational input data; i.e., data in the form of a square matrix of pairwise similarities between data objects. The algorithm uses a graph representation of the data, and operates in an Expectation-Maximization framework in which the graph centrality of an object in the graph is interpreted as a likelihood. Results of applying the algorithm to sentence clustering tasks demonstrate that the algorithm is capable of identifying overlapping clusters of semantically related sentences, and that it is therefore of potential use in a variety of text mining tasks. We also include results of applying the algorithm to benchmark data sets in several other domains. © 1989-2012 IEEE.",2013,IEEE Transactions on Knowledge and Data Engineering,71,in comparison @ hard clustering method in @ a pattern belongs to a single cluster fuzzy clustering algorithm allow pattern to belong to @ cluster @ differing degree of membership @ @ is important in domain @ a sentence clustering since a sentence is likely to @ related to more @ @ theme @ topic @ within a document @ set of document @ however @ @ sentence similarity measure @ not represent sentence in a common metric space conventional fuzzy clustering approach based on prototype @ mixture of gaussians @ generally not applicable to sentence clustering @ @ @ @ a novel fuzzy clustering algorithm @ operates on relational input data @ i @ e @ data in @ form of a square matrix of pairwise similarity @ data object @ @ algorithm us a graph representation of @ data and operates in @ expectation-maximization framework in @ @ graph centrality of @ object in @ graph is interpreted a a likelihood @ @ of applying @ algorithm to sentence clustering task demonstrate @ @ algorithm is capable of identifying overlapping cluster of semantically related sentence and @ @ is therefore of potential use in a variety of text mining task @ @ @ include @ of applying @ algorithm to benchmark data set in several @ domain @ @ @ 
3231,Development of total environment for text data mining,"In this challenge, we develop and distribute an integrated environment to flexibly combine multiple text mining techniques. Text mining techniques include numerous tasks such as salient sentence extraction, keyword extraction, topic extraction, textual coherence evaluation, multi-document summarization, and text clustering. Although tools that individually perform one or more of the above-mentioned tasks exist, it is difficult to integrate and activate multiple tools for a particular task. We attempt to provide the flexibility to integrate numerous tools that exist in the community in our proposed text mining environment. Users can use a customized version of the proposed text mining environment for their specific tasks, thereby concentrating solely on their creative work.",2013,Transactions of the Japanese Society for Artificial Intelligence,1,in @ challenge @ develop and distribute @ integrated environment to flexibly combine multiple text mining technique @ text mining technique include numerous task @ a salient sentence extraction keyword extraction topic extraction textual coherence evaluation multi-document summarization and text clustering @ although tool @ individually perform @ @ more of @ above-mentioned task exist @ is difficult to integrate and activate multiple tool @ a particular task @ @ attempt to provide @ flexibility to integrate numerous tool @ exist in @ community in @ proposed text mining environment @ user @ use a customized version of @ proposed text mining environment @ @ specific task thereby concentrating solely on @ creative work @ 
3232,Improving computational trust representation based on Internet auction traces,"Computational trust representations are used by Trust Management (TM) systems to elicit information from users about the behavior of others. In most practically used TM systems, simple computational trust representations dominate, such as the three-valued discrete scale of ""negative"", ""neutral"" and ""positive"" used in reputation systems of Internet auctions. This paper asks the question: what is the appropriate system for computational representation of human trust? In order to find an answer, we study a large trace of feedbacks and textual comments from a reputation system of an Internet auction. We discover that users systematically try to add information in the textual comments. Text-mining and NLP approaches reveal a taxonomy of non-positive feedbacks and an importance order on the categories of non-positive behavior. This importance order is further supported by survey data. Based on these observations, we propose and evaluate a complete, new computational trust representation system inspired by the work of Yager. This system is complemented by operators that can be used to produce rankings of most trusted agents. The operator used to create rankings selects Pareto-optimal agents with respect to the multiple criteria revealed by our trace analysis. The proposed system takes into account all criteria utilized by auction users to evaluate behavior, and the relative importance of these criteria. The proposed system is compared to the Detailed Seller Rating system introduced by eBay. © 2012 Elsevier B.V.",2013,Decision Support Systems,7,computational trust representation @ used by trust management @ tm @ system to elicit information @ user @ @ behavior of others @ in @ practically used tm system simple computational trust representation dominate @ a @ three-valued discrete scale of @ negative @ @ neutral @ and @ positive @ used in reputation system of internet auction @ @ @ asks @ question @ @ is @ appropriate system @ computational representation of human trust @ in order to find @ answer @ study a @ trace of feedback and textual comment @ a reputation system of @ internet auction @ @ discover @ user systematically try to add information in @ textual comment @ text-mining and nlp approach reveal a taxonomy of non-positive feedback and @ importance order on @ category of non-positive behavior @ @ importance order is @ supported by survey data @ based on @ observation @ propose and evaluate a complete @ computational trust representation system inspired by @ work of yager @ @ system is complemented by operator @ @ @ used to produce ranking of @ trusted agent @ @ operator used to create ranking selects pareto-optimal agent @ respect to @ multiple criterion revealed by @ trace analysis @ @ proposed system take @ account @ criterion utilized by auction user to evaluate behavior and @ relative importance of @ criterion @ @ proposed system is compared to @ detailed seller rating system introduced by ebay @ @ b @ v @ 
3233,U-STRUCT: A framework for conversion of unstructured text documents into structured form,"The term Text Mining or Text Analytics refers to the process of extracting useful patterns or knowledge from text. The data in textual documents can be of two types, either it can be unstructured or semi-structured. Unstructured data is freely naturally occurring text, whereas web documents data (HTML or XML) is semi structured. Since the natural language text is not organized and does not represent context, it needs to be converted into structured form to perform data analysis and mine useful patterns from it. The field of text mining deals with mining useful patterns or knowledge from unstructured text. In this paper, we propose a framework for the conversion of the unstructured text documents to a structured form. We present a generalized framework called U - STRUCT which translates unstructured text into structured form. This framework analyses the text documents from different views: lexically, syntactically and semantically and produces a generalized intermediate form of documents. Further, we also discuss the opportunities and challenges in the field of text mining. © 2013 Springer-Verlag Berlin Heidelberg.",2013,Communications in Computer and Information Science,5,@ term text mining @ text analytics refers to @ process of extracting useful pattern @ knowledge @ text @ @ data in textual document @ @ of @ type either @ @ @ unstructured @ semi-structured @ unstructured data is freely naturally occurring text whereas web document data @ html @ xml @ is semi structured @ since @ natural language text is not organized and doe not represent context @ need to @ converted @ structured form to perform data analysis and mine useful pattern @ @ @ @ field of text mining deal @ mining useful pattern @ knowledge @ unstructured text @ in @ @ @ propose a framework @ @ conversion of @ unstructured text document to a structured form @ @ @ a generalized framework called u struct @ translates unstructured text @ structured form @ @ framework analysis @ text document @ different view @ lexically syntactically and semantically and produce a generalized intermediate form of document @ @ @ @ discus @ opportunity and challenge in @ field of text mining @ springer-verlag @ @ @ 
3241,Computational Modeling of Narrative,"The field of narrative (or story) understanding and generation is one of the oldest in natural language processing (NLP) and artificial intelligence (AI), which is hardly surprising, since storytelling is such a fundamental and familiar intellectual and social activity. In recent years, the demands of interactive entertainment and interest in the creation of engaging narratives with life-like characters have provided a fresh impetus to this field. This book provides an overview of the principal problems, approaches, and challenges faced today in modeling the narrative structure of stories. The book introduces classical narratological concepts from literary theory and their mapping to computational approaches. It demonstrates how research in AI and NLP has modeled character goals, causality, and time using formalisms from planning, case-based reasoning, and temporal reasoning, and discusses fundamental limitations in such approaches. It proposes new representations for embedded narratives and fictional entities, for assessing the pace of a narrative, and offers an empirical theory of audience response. These notions are incorporated into an annotation scheme called NarrativeML. The book identifies key issues that need to be addressed, including annotation methods for long literary narratives, the representation of modality and habituality, and characterizing the goals of narrators. It also suggests a future characterized by advanced text mining of narrative structure from large-scale corpora and the development of a variety of useful authoring aids. This is the first book to provide a systematic foundation that integrates together narratology, AI, and computational linguistics. It can serve as a narratology primer for computer scientists and an elucidation of computational narratology for literary theorists. It is written in a highly accessible manner and is intended for use by a broad scientific audience that includes linguists (computational and formal semanticists), AI researchers, cognitive scientists, computer scientists, game developers, and narrative theorists. © 2013 by Morgan & Claypool.",2013,Synthesis Lectures on Human Language Technologies,19,@ field of narrative @ @ story @ understanding and generation is @ of @ oldest in natural language processing @ nlp @ and artificial intelligence @ ai @ @ is hardly surprising since storytelling is @ a fundamental and familiar intellectual and social activity @ in recent year @ demand of interactive entertainment and interest in @ creation of engaging narrative @ life-like character @ provided a fresh impetus to @ field @ @ book provides @ overview of @ principal problem approach and challenge faced today in modeling @ narrative structure of story @ @ book introduces classical narratological concept @ literary theory and @ mapping to computational approach @ @ demonstrates @ research in ai and nlp ha modeled character goal causality and time @ formalism @ planning case-based reasoning and temporal reasoning and discus fundamental limitation in @ approach @ @ proposes @ representation @ embedded narrative and fictional entity @ assessing @ pace of a narrative and offer @ empirical theory of audience response @ @ notion @ incorporated @ @ annotation scheme called narrativeml @ @ book identifies key issue @ need to @ addressed including annotation method @ long literary narrative @ representation of modality and habituality and characterizing @ goal of narrator @ @ @ suggests a future characterized by advanced text mining of narrative structure @ large-scale corpus and @ development of a variety of useful authoring aid @ @ is @ first book to provide a systematic foundation @ integrates together narratology ai and computational linguistics @ @ @ serve a a narratology primer @ computer scientist and @ elucidation of computational narratology @ literary theorist @ @ is written in a highly accessible manner and is intended @ use by a broad scientific audience @ includes linguist @ computational and formal semanticist @ ai researcher cognitive scientist computer scientist game developer and narrative theorist @ by morgan claypool @ 
3242,A topic recognition system for real world human-robot conversations,"One of the main features of social robots is the ability to communicate and interact with people as partners in a natural way. However, achieving a good verbal interaction is a hard task due to the errors on speech recognition systems, and due to the understanting the natural language itself. This paper tries to overcome such kind of problems by presenting a system that enables social robots to get involved in conversation by recognizing its topic. Through the use of classical text mining approach, the presented system allows social robots to understand topics of conversation between human partners, enabling the customization of behaviours in their accordance. The system has been evaluated in different contexts, taking in account the quality and accuracy of the speech recognition syestem used by the social robot. © 2013 Springer-Verlag.",2013,Advances in Intelligent Systems and Computing,3,@ of @ main feature of social robot is @ ability to communicate and interact @ people a partner in a natural way @ however achieving a good verbal interaction is a hard task due to @ error on speech recognition system and due to @ understanting @ natural language @ @ @ @ try to overcome @ kind of problem by presenting a system @ enables social robot to get involved in conversation by recognizing @ topic @ @ @ use of classical text mining approach @ presented system allows social robot to understand topic of conversation @ human partner enabling @ customization of behaviour in @ accordance @ @ system ha @ evaluated in different context taking in account @ quality and accuracy of @ speech recognition syestem used by @ social robot @ springer-verlag @ 
3243,Chinese-mining preprocessing technology based on text trait optimizing,"How to get the target text quickly becomes a technical limitation with the using of massive data. While obtaining the Chinese target information, the segmentation of the sentence is supposed to be the key according to research. To mine the segmentation of English text is relatively simple for the space is used as a interval, meanwhile the Chinese segmentation is much more difficult. So in this paper the reciprocal crossing segmentation algorithm and the trait-optimizing vector model are designed to improve the mining efficiency of Chinese information. Based on dictionary, an improved segmentation algorithm is adopted in text pretreatment processing, which is based on vector space module, to do experiments on the segmentation algorithm and to analyze the segmentation results. And that segmentation algorithm is already proved to be very effective in the text mining of text trait vector module. © 2005 - 2013 JATIT & LLS. All rights reserved.",2013,Journal of Theoretical and Applied Information Technology,0,@ to get @ target text quickly becomes a technical limitation @ @ @ of massive data @ @ obtaining @ chinese target information @ segmentation of @ sentence is supposed to @ @ key according to research @ to mine @ segmentation of english text is relatively simple @ @ space is used a a interval meanwhile @ chinese segmentation is much more difficult @ @ in @ @ @ reciprocal crossing segmentation algorithm and @ trait-optimizing vector model @ designed to improve @ mining efficiency of chinese information @ based on dictionary @ improved segmentation algorithm is adopted in text pretreatment processing @ is based on vector space module to @ experiment on @ segmentation algorithm and to analyze @ segmentation @ @ and @ segmentation algorithm is already proved to @ @ effective in @ text mining of text trait vector module @ jatit lls @ @ right reserved @ 
3245,Regularized latent semantic indexing: A new approach to large-scale topic modeling,"Topic modeling provides a powerful way to analyze the content of a collection of documents. It has become a popular tool in many research areas, such as text mining, information retrieval, natural language processing, and other related fields. In real-world applications, however, the usefulness of topic modeling is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps, such as vastly reducing input vocabulary. In this article we introduce Regularized Latent Semantic Indexing (RLSI)-including a batch version and an online version, referred to as batch RLSI and online RLSI, respectively-to scale up topic modeling. Batch RLSI and online RLSI are as effective as existing topic modeling techniques and can scale to larger datasets without reducing input vocabulary. Moreover, online RLSI can be applied to stream data and can capture the dynamic evolution of topics. Both versions of RLSI formalize topic modeling as a problem of minimizing a quadratic loss function regularized by L1 and/or L2 norm. This formulation allows the learning process to be decomposed into multiple suboptimization problems which can be optimized in parallel, for example, via MapReduce.We particularly propose adopting L1 norm on topics and 2 norm on document representations to create a model with compact and readable topics and which is useful for retrieval. In learning, batch RLSI processes all the documents in the collection as a whole, while online RLSI processes the documents in the collection one by one. We also prove the convergence of the learning of online RLSI. Relevance ranking experiments on three TREC datasets show that batch RLSI and online RLSI perform better than LSI, PLSI, LDA, and NMF, and the improvements are sometimes statistically significant. Experiments on a Web dataset containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance. © 2013 ACM 1046-8188/2013/01- ART2 s15.00.",2013,ACM Transactions on Information Systems,30,topic modeling provides a powerful way to analyze @ content of a collection of document @ @ ha become a popular tool in many research area @ a text mining information retrieval natural language processing and @ related field @ in real-world application however @ usefulness of topic modeling is limited due to scalability issue @ scaling to larger document collection via parallelization is @ active area of research @ @ solution require drastic step @ a vastly reducing input vocabulary @ in @ article @ introduce regularized latent semantic indexing @ rlsi @ including a batch version and @ online version referred to a batch rlsi and online rlsi respectively-to scale up topic modeling @ batch rlsi and online rlsi @ a effective a existing topic modeling technique and @ scale to larger datasets without reducing input vocabulary @ moreover online rlsi @ @ applied to stream data and @ capture @ dynamic evolution of topic @ @ version of rlsi formalize topic modeling a a problem of minimizing a quadratic loss function regularized by l and @ l norm @ @ formulation allows @ learning process to @ decomposed @ multiple suboptimization problem @ @ @ optimized in parallel @ example via mapreduce @ @ particularly propose adopting l norm on topic and norm on document representation to create a model @ compact and readable topic and @ is useful @ retrieval @ in learning batch rlsi process @ @ document in @ collection a a whole @ online rlsi process @ document in @ collection @ by @ @ @ @ prove @ convergence of @ learning of online rlsi @ relevance ranking experiment on three trec datasets @ @ batch rlsi and online rlsi perform better @ lsi plsi lda and nmf and @ improvement @ sometimes statistically significant @ experiment on a web dataset containing @ @ million document and million term demonstrate a similar boost in performance @ acm - art s @ @ 
3246,An emotional polarity analysis of consumers’ airline service tweets,"Blogs and social networks have recently become a valuable resource for mining sentiments in fields as diverse as customer relationship management, public opinion tracking and text filtering. In fact, the knowledge obtained from social networks such as Twitter and Facebook has been shown to be extremely valuable to marketing research companies, public opinion organizations and other text mining entities. However, Web texts have been classified as noisy as they still pose considerable problems both at the lexical and the syntactic levels. In this research, we used a random sample of 2,105 tweets for sixteen commercial airlines to evaluate consumers’ sentiment towards airline service provided. We used an expert pre-defined lexicon to conduct the analysis. The lexicon includes around 6,800 seed adjectives with known orientation. Our results indicate a generally negative consumer sentiment towards commercial airline services, which suggests that most airline services are sub-optimal. Using both a qualitative and quantitative methodology to analyze airline service tweets, this study adds breadth and depth to the debate over airline service quality. © 2013, Springer-Verlag Wien.",2013,Social Network Analysis and Mining,15,blog and social network @ recently become a valuable resource @ mining sentiment in field a diverse a customer relationship management public opinion tracking and text filtering @ in fact @ knowledge obtained @ social network @ a twitter and facebook ha @ @ to @ extremely valuable to marketing research company public opinion organization and @ text mining entity @ however web text @ @ classified a noisy a @ still pose considerable problem @ at @ lexical and @ syntactic level @ in @ research @ used a random sample of tweet @ sixteen commercial airline to evaluate consumer sentiment towards airline service provided @ @ used @ expert pre-defined lexicon to conduct @ analysis @ @ lexicon includes around seed adjective @ known orientation @ @ @ indicate a generally negative consumer sentiment towards commercial airline service @ suggests @ @ airline service @ sub-optimal @ @ @ a qualitative and quantitative methodology to analyze airline service tweet @ study add breadth and depth to @ debate @ airline service quality @ springer-verlag wien @ 
3249,Agglomerative Hierarchical Clustering Techniques for Arabic Documents,"Arabic Documents Clustering is an important task for obtaining good results with Search Engines, Information Retrieval (IR) systems, Text Mining Applications especially with the rapid growth of the number of online documents present in Arabic language. Document clustering is the process of segmenting a particular collection of texts into subgroups including content based similar ones. Clustering algorithms are mainly divided into two categories: Hierarchical algorithms and Partition algorithms. In this paper, we propose to study the most popular approach of Hierarchical algorithms: Agglomerative Hierarchical algorithm using seven linkage techniques with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient; in order to test their effectiveness on Arabic documents clustering, and finally we recommend the best techniques tested. Furthermore, we propose also to study the effect of using the stemming for the testing dataset to cluster it with the same documents clustering technique and similarity/distance measures cited above. The obtained results show that, on the one hand, the Ward function outperformed the other linkage techniques; on the other hand, the use of the stemming will not yield good results, but makes the representation of the document smaller and the clustering faster.",2013,Advances in Intelligent Systems and Computing,3,arabic document clustering is @ important task @ obtaining good @ @ search engine information retrieval @ ir @ system text mining application especially @ @ rapid growth of @ number of online document @ in arabic language @ document clustering is @ process of segmenting a particular collection of text @ subgroup including content based similar @ @ clustering algorithm @ mainly divided @ @ category @ hierarchical algorithm and partition algorithm @ in @ @ @ propose to study @ @ popular approach of hierarchical algorithm @ agglomerative hierarchical algorithm @ seven linkage technique @ a wide variety of distance function and similarity measure @ a @ euclidean distance cosine similarity jaccard coefficient and @ pearson correlation coefficient @ in order to test @ effectiveness on arabic document clustering and finally @ recommend @ best technique tested @ furthermore @ propose @ to study @ effect of @ @ stemming @ @ testing dataset to cluster @ @ @ @ document clustering technique and similarity distance measure cited @ @ @ obtained @ @ @ on @ @ hand @ ward function outperformed @ @ linkage technique @ on @ @ hand @ use of @ stemming @ not yield good @ @ make @ representation of @ document smaller and @ clustering faster @ 
3250,Mining movie reviews - An evaluation,"Much research on textual information processing focused on mining and retrieval of factual information, like information retrieval, Web search, text classification, text clustering and related text mining and natural language processing tasks. Opinions are subjective expressions describing people's sentiments, appraisals/feelings to entities, events and their properties. A definition of opinion is very broad. In this paper, it is proposed to extract the feature set from reviews and the reviews are classified as positive or negative using Naïve Bayes, Ada Boost and Fuzzy Lattice reasoning classifier. © 2005-2013 JATIT & LLS.All rights reserved.",2013,Journal of Theoretical and Applied Information Technology,1,much research on textual information processing focused on mining and retrieval of factual information like information retrieval web search text classification text clustering and related text mining and natural language processing task @ opinion @ subjective expression describing people @ s sentiment appraisal feeling to entity event and @ property @ a definition of opinion is @ broad @ in @ @ @ is proposed to extract @ feature set @ review and @ review @ classified a positive @ negative @ naïve bayes ada boost and fuzzy lattice reasoning classifier @ jatit lls @ @ right reserved @ 
3255,Network based analysis of intertextual relations,"We present an approach of intertextuality in terms of graph theories, statistics, and bakhtinian polyphony, the latter perspective considering the way in which discourse threads are influencing each other. This paper presents theoretical models, processing techniques with their applications and results that acknowledge as important the approach based on networks of intertextuality. In the end are introduced two original applications of supervised and unsupervised analysis of antique texts of philosophical and religious nature. © 2013 Springer-Verlag.",2013,Advances in Intelligent Systems and Computing,3,@ @ @ approach of intertextuality in term of graph theory statistic and bakhtinian polyphony @ latter perspective considering @ way in @ discourse thread @ influencing @ @ @ @ @ @ theoretical model processing technique @ @ application and @ @ acknowledge a important @ approach based on network of intertextuality @ in @ end @ introduced @ original application of supervised and unsupervised analysis of antique text of philosophical and religious nature @ springer-verlag @ 
3257,Automatic assessment of information disclosure quality in Chinese annual reports,"Information disclosure in annual reports is a mandatory requirement for publicly traded companies in China. The quality of information disclosure will reduce information asymmetry and therefore support market efficiency. Currently, the evaluation of the information disclosure quality in Chinese reports is conducted manually. It remains an untapped field for NLP and text mining community. The goal of this paper is to develop automatic assessment system for information disclosure quality in Chinese annual reports. Our assessment system framework incorporates different technologies including Chinese document modeling, Chinese readability index construction, and multi-class classification. Our explorative and systematic experiment results show that: 1) our automatic assessment system can produce solid predictive accuracy for disclosure quality, especially in ""excellent"" and ""fail"" categories; 2) our system for Chinese annual reports assessment achieves better predictive accuracy in certain perspective than the counterparts of the English annual reports prediction; 3) our readability index for Chinese documents, as well as other findings from system performance, may provide enlightenment for a better understanding about the quality features of Chinese company annual reports. © Springer-Verlag Berlin Heidelberg 2013.",2013,Communications in Computer and Information Science,3,information disclosure in annual report is a mandatory requirement @ publicly traded company in china @ @ quality of information disclosure @ reduce information asymmetry and therefore support market efficiency @ currently @ evaluation of @ information disclosure quality in chinese report is conducted manually @ @ remains @ untapped field @ nlp and text mining community @ @ goal of @ @ is to develop automatic assessment system @ information disclosure quality in chinese annual report @ @ assessment system framework incorporates different technology including chinese document modeling chinese readability index construction and multi-class classification @ @ explorative and systematic experiment @ @ @ @ @ @ automatic assessment system @ produce solid predictive accuracy @ disclosure quality especially in @ excellent @ and @ fail @ category @ @ @ system @ chinese annual report assessment achieves better predictive accuracy in certain perspective @ @ counterpart of @ english annual report prediction @ @ @ readability index @ chinese document a well a @ finding @ system performance may provide enlightenment @ a better understanding @ @ quality feature of chinese company annual report @ springer-verlag @ @ @ 
3259,Plugging text processing and Mining in a cloud computing framework,"Computational methods have evolved over the years giving developers and researchers more sophisticated and faster ways to solve hard data processing tasks. However, with new data collecting and storage technologies, the amount of gathered data increases everyday making the analysis of it a more and more complex task. One of the main forms of storing data is plain unstructured text and one of the most common ways of analyzing this kind of data is through Text Mining. Text Mining is similar to other types of data mining but the problem is that differently from other forms of data that are properly structured (such as XML) in text mining data in the best case scenario is semi-structured. In order for them to derive valuable information, text mining systems have to execute a lot of complex natural language processing algorithms. In this chapter we focus on text processing tools dealing with stemming algorithms. Stemming is the step that deals with finding the stem (or root) of the word which is essential in every text processing procedure. Stemming algorithms are complex and require high computational effort. In this chapter we present an Apache Mahout plugin for a stemming algorithm making possible to execute the algorithm in a cloud computing environment. We investigate the performance of the algorithm in the cloud and show that the new approach significantly reduces the execution time of the original algorithm over a large dataset of text documents. © Springer-Verlag Berlin Heidelberg 2013.",2013,Studies in Computational Intelligence,3,computational method @ evolved @ @ year giving developer and researcher more sophisticated and faster way to solve hard data processing task @ however @ @ data collecting and storage technology @ amount of gathered data increase everyday making @ analysis of @ a more and more complex task @ @ of @ main form of storing data is plain unstructured text and @ of @ @ common way of analyzing @ kind of data is @ text mining @ text mining is similar to @ type of data mining @ @ problem is @ differently @ @ form of data @ @ properly structured @ @ a xml @ in text mining data in @ best case scenario is semi-structured @ in order @ @ to derive valuable information text mining system @ to execute a lot of complex natural language processing algorithm @ in @ chapter @ focus on text processing tool dealing @ stemming algorithm @ stemming is @ step @ deal @ finding @ stem @ @ root @ of @ word @ is essential in every text processing procedure @ stemming algorithm @ complex and require high computational effort @ in @ chapter @ @ @ apache mahout plugin @ a stemming algorithm making possible to execute @ algorithm in a cloud computing environment @ @ investigate @ performance of @ algorithm in @ cloud and @ @ @ @ approach significantly reduces @ execution time of @ original algorithm @ a @ dataset of text document @ springer-verlag @ @ @ 
3261,Definition-based Information Content Vectors for Semantic Similarity Measurement,"Ontologies, as representation of shared conceptualization for variety of specific domains, are the heart of the Semantic Web. In order to facilitate interoperability across multiple ontologies, we need an automatic mechanism to align ontologies. Therefore, many methods to measure similarity between concepts existing in two different ontologies are proposed. In this paper, we will enumerate these methods along with their shortcomings in each case. In information content (IC) based similarity measures, the process of IC computation for concepts is so challenging and in many cases with failing. We will propose our new approach that is based on concepts' definitions. These definitions would help us to compute reliable and easy to calculate information contents for concepts. Applying these methods to the biomedical domain, using MEDLINE as corpus, International Classification of Diseases, Ninth Revision, Clinical Modification (ICD9CM) as thesaurus, and available reference standard, we will find our method outperforms other similarity measures. © Springer-Verlag Berlin Heidelberg 2013.",2013,Communications in Computer and Information Science,7,ontology a representation of shared conceptualization @ variety of specific domain @ @ heart of @ semantic web @ in order to facilitate interoperability across multiple ontology @ need @ automatic mechanism to align ontology @ therefore many method to measure similarity @ concept existing in @ different ontology @ proposed @ in @ @ @ @ enumerate @ method along @ @ shortcoming in @ case @ in information content @ ic @ based similarity measure @ process of ic computation @ concept is @ challenging and in many case @ failing @ @ @ propose @ @ approach @ is based on concept @ definition @ @ definition would help u to compute reliable and easy to calculate information content @ concept @ applying @ method to @ biomedical domain @ medline a corpus international classification of disease ninth revision clinical modification @ icd cm @ a thesaurus and available reference standard @ @ find @ method outperforms @ similarity measure @ springer-verlag @ @ @ 
3267,"Using social media data for comparing brand awareness, levels of consumer engagement, public opinion and sentiment for big four Australian banks","The growing availability and popularity of opinionrich resources on the web led to an eruption of activity in the area of analysis of data coming from these resources. Opportunities exist to understand the extent of public engagement and sentiment toward a brand, a product or an event. In this paper, we present a case study for opinion extraction applied to the banking domain that illustrates how social media can be used to gain insight into the public opinion, sentiment and spread of social conversation related to this domain including changes that are triggered by a domain-relevant event. We applied advanced machine learning and data science techniques to the relevant social media and news data from the web to analyse the nature of public opinion in Australia toward the four major Australian banks in the context of the banks reaction to the Reserve Bank of Australia lowering the official interest rate. The resulting insights into public sentiment, reach, the topics discussed by the public and how these compared between the banks can be used proactively to inform organisational decision making. © 2013, Australian Computer Society, Inc.",2013,Conferences in Research and Practice in Information Technology Series,0,@ growing availability and popularity of opinionrich resource on @ web led to @ eruption of activity in @ area of analysis of data coming @ @ resource @ opportunity exist to understand @ extent of public engagement and sentiment toward a brand a product @ @ event @ in @ @ @ @ a case study @ opinion extraction applied to @ banking domain @ illustrates @ social medium @ @ used to gain insight @ @ public opinion sentiment and spread of social conversation related to @ domain including change @ @ triggered by a domain-relevant event @ @ applied advanced machine learning and data science technique to @ relevant social medium and news data @ @ web to analyse @ nature of public opinion in australia toward @ four major australian bank in @ context of @ bank reaction to @ reserve bank of australia lowering @ official interest rate @ @ resulting insight @ public sentiment reach @ topic discussed by @ public and @ @ compared @ @ bank @ @ used proactively to inform organisational decision making @ australian computer society inc @ 
3269,Large-scale semantic indexing of biomedical publications at BioASQ,"Automated annotation of scientific publications in real-world digital libraries requires dealing with challenges such as large number of concepts and training examples, multi-label training examples and hierarchical structure of concepts. BioASQ is a European project that contributes a large-scale biomedical publications corpus for working on these challenges. This paper documents the participation of our team to the large-scale biomedical semantic indexing task of BioASQ.",2013,CEUR Workshop Proceedings,8,automated annotation of scientific publication in real-world digital library requires dealing @ challenge @ a @ number of concept and training example multi-label training example and hierarchical structure of concept @ bioasq is a european project @ contributes a large-scale biomedical publication corpus @ working on @ challenge @ @ @ document @ participation of @ team to @ large-scale biomedical semantic indexing task of bioasq @ 
3272,Representations for multi-document event clustering,"We study several techniques for representing, fusing and comparing content representations of news documents. As underlying models we consider the vector space model (both in a term setting and in a latent semantic analysis setting) and probabilistic topic models based on latent Dirichlet allocation. Content terms can be classified as topical terms or named entities, yielding several models for content fusion and comparison. All used methods are completely unsupervised. We find that simple methods can still outperform the current state-of-the-art techniques. © The Author(s) 2012.",2013,Data Mining and Knowledge Discovery,8,@ study several technique @ representing fusing and comparing content representation of news document @ a underlying model @ consider @ vector space model @ @ in a term setting and in a latent semantic analysis setting @ and probabilistic topic model based on latent dirichlet allocation @ content term @ @ classified a topical term @ named entity yielding several model @ content fusion and comparison @ @ used method @ completely unsupervised @ @ find @ simple method @ still outperform @ current state-of-the-art technique @ @ author @ s @ @ 
3279,Temporal contexts: Effective text classification in evolving document collections,"The management of a huge and growing amount of information available nowadays makes Automatic Document Classification (ADC), besides crucial, a very challenging task. Furthermore, the dynamics inherent to classification problems, mainly on the Web, make this task even more challenging. Despite this fact, the actual impact of such temporal evolution on ADC is still poorly understood in the literature. In this context, this work concerns to evaluate, characterize and exploit the temporal evolution to improve ADC techniques. As first contribution we highlight the proposal of a pragmatical methodology for evaluating the temporal evolution in ADC domains. Through this methodology, we can identify measurable factors associated to ADC models degradation over time. Going a step further, based on such analyzes, we propose effective and efficient strategies to make current techniques more robust to natural shifts over time. We present a strategy, named temporal context selection, for selecting portions of the training set that minimize those factors. Our second contribution consists of proposing a general algorithm, called Chronos, for determining such contexts. By instantiating Chronos, we are able to reduce uncertainty and improve the overall classification accuracy. Empirical evaluations of heuristic instantiations of the algorithm, named WindowsChronos and FilterChronos, on two real document collections demonstrate the usefulness of our proposal. Comparing them against state-of-the-art ADC algorithms shows that selecting temporal contexts allows improvements on the classification accuracy up to 10%. Finally, we highlight the applicability and the generality of our proposal in practice, pointing out this study as a promising research direction. © 2012 Elsevier Ltd. All rights reserved.",2013,Information Systems,8,@ management of a huge and growing amount of information available nowadays make automatic document classification @ adc @ besides crucial a @ challenging task @ furthermore @ dynamic inherent to classification problem mainly on @ web make @ task even more challenging @ despite @ fact @ actual impact of @ temporal evolution on adc is still poorly understood in @ literature @ in @ context @ work concern to evaluate characterize and exploit @ temporal evolution to improve adc technique @ a first contribution @ highlight @ proposal of a pragmatical methodology @ evaluating @ temporal evolution in adc domain @ @ @ methodology @ @ identify measurable factor associated to adc model degradation @ time @ going a step @ based on @ analyzes @ propose effective and efficient strategy to make current technique more robust to natural shift @ time @ @ @ a strategy named temporal context selection @ selecting portion of @ training set @ minimize @ factor @ @ second contribution consists of proposing a general algorithm called chronos @ determining @ context @ by instantiating chronos @ @ able to reduce uncertainty and improve @ overall classification accuracy @ empirical evaluation of heuristic instantiation of @ algorithm named windowschronos and filterchronos on @ real document collection demonstrate @ usefulness of @ proposal @ comparing @ @ state-of-the-art adc algorithm @ @ selecting temporal context allows improvement on @ classification accuracy up to @ finally @ highlight @ applicability and @ generality of @ proposal in practice pointing @ @ study a a promising research direction @ @ ltd @ @ right reserved @ 
3280,Tag line generating system using information on the web,"This paper proposes a tag line generating systemusing information extracted from the web. Tag lines sometimes attract attention even when they consist of indirect word group of the target. We use web information to extract hidden data and use several tag line corpora to collect a large number of tag lines. First, knowledge related to the input is obtained from the web. Then, the proposed system selects suitable words according to the theme. Also, model tag lines are selected from the corpora using the knowledge. By inserting nouns, verbs and adjectives into model tag lines' structure, candidate sentences are generated. These tag line candidates are selected by the suitability as a sentence using a text N-gram corpus. The subjective experiment measures the quality of system-generated tag lines and some of them are quite comparable to human-made ones.",2013,Journal of Advanced Computational Intelligence and Intelligent Informatics,2,@ @ proposes a tag line generating systemusing information extracted @ @ web @ tag line sometimes attract attention even @ @ consist of indirect word group of @ target @ @ use web information to extract hidden data and use several tag line corpus to collect a @ number of tag line @ first knowledge related to @ input is obtained @ @ web @ @ @ proposed system selects suitable word according to @ theme @ @ model tag line @ selected @ @ corpus @ @ knowledge @ by inserting noun verb and adjective @ model tag line @ structure candidate sentence @ generated @ @ tag line candidate @ selected by @ suitability a a sentence @ a text n-gram corpus @ @ subjective experiment measure @ quality of system-generated tag line and some of @ @ quite comparable to human-made @ @ 
3281,MapIt: A case study for location driven knowledge discovery and mining,"In the present world scenario, everybody is on the lookout for suitable housing options, each having different needs (e.g., the elderly are looking for safe, quiet neighbourhood, while students are looking for affordable apartments close to the university/school). For e.g., Craigslist currently does not have a map version, making the process of apartment searching a very long and laborious process. This creates a need for software that is significantly superior to current web search tools. We demonstrate the development of a tool which takes the Craigslist apartment listings on Google Maps. MapIt then integrates this functionality with the information collected from location based extraction of various web sources such as the city police blotter which makes apartment searching simpler and faster, helping the user to make a better decision. The paper also discusses the challenges that are faced in the development process, the raw and unstructured nature of the documents, the existence of geo/non-geo and geo-geo disambiguities and our approach in identifying the location of the apartment from informal text (geo-parsing and geo-tagging of content) to ensure maximum coverage of the listings. © 2012 Inderscience Enterprises Ltd.",2013,"International Journal of Data Mining, Modelling and Management",0,in @ @ world scenario everybody is on @ lookout @ suitable housing option @ @ different need @ e @ g @ @ elderly @ looking @ safe quiet neighbourhood @ student @ looking @ affordable apartment close to @ university school @ @ @ e @ g @ craigslist currently doe not @ a map version making @ process of apartment searching a @ long and laborious process @ @ creates a need @ software @ is significantly superior to current web search tool @ @ demonstrate @ development of a tool @ take @ craigslist apartment listing on google map @ mapit @ integrates @ functionality @ @ information collected @ location based extraction of various web source @ a @ city police blotter @ make apartment searching simpler and faster helping @ user to make a better decision @ @ @ @ discus @ challenge @ @ faced in @ development process @ raw and unstructured nature of @ document @ existence of geo non-geo and geo-geo disambiguities and @ approach in identifying @ location of @ apartment @ informal text @ geo-parsing and geo-tagging of content @ to ensure maximum coverage of @ listing @ inderscience enterprise ltd @ 
3285,Generating extractive summaries of scientific paradigms,"Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary. © 2013 AI Access Foundation. All rights reserved.",2013,Journal of Artificial Intelligence Research,39,researcher and scientist increasingly find @ in @ position of @ to quickly understand @ amount of technical material @ @ goal is to effectively serve @ need by @ bibliometric text mining and summarization technique to generate summary of scientific literature @ @ @ @ @ @ use citation to produce automatically generated readily consumable technical extractive summary @ @ first propose c-lexrank a model @ summarizing single scientific article based on citation @ employ community detection and extract salient information-rich sentence @ next @ @ extend @ experiment to summarize a set of @ @ cover @ @ scientific topic @ @ generate extractive summary of a set of question answering @ qa @ and dependency parsing @ dp @ @ @ abstract and @ citation sentence and @ @ citation @ unique information amenable to creating a summary @ ai access foundation @ @ right reserved @ 
3288,An approach based on iterative learning algorithm for Chinese text hierarchy feature extraction without lexicon,"A great deal of information included in Chinese text is invaluable asset for further text mining, but the difference between Chinese and the western languages imposes restrictions on further utilization of Chinese text. No distinction indication between words by using spaces is one of the major differences between Chinese, also some other Asian languages, such as Japanese, Thai, etc., and Western languages. Chinese segmentation and features extraction is essential in Chinese natural language processing because it is a precondition for further Chinese text information retrieval and knowledge discovery. Maximum matching and frequency statistics (MMFS) segmentation method based on length descending and string frequency statistics is an effective segmentation and extraction method for Chinese words and phrases, but there are still some shorter words and phrases included in the longer ones extracted by MMFS can't be obtained. In order to solve this problem, this paper presents a novel Chinese hierarchy feature extraction method combined MMFS with iterative learning algorithm. This method can extract hierarchy feature according to morphology with no need for lexicon support, no need for acquiring the probability between words in advance and no need for Chinese character index. Experimental results confirm the efficiency of this statistical method in extracting Chinese hierarchy feature. This method is also beneficial to feature extraction for other Asian languages similar to Chinese. © 2005 - 2013 JATIT & LLS. All rights reserved.",2013,Journal of Theoretical and Applied Information Technology,0,a great deal of information included in chinese text is invaluable asset @ @ text mining @ @ difference @ chinese and @ western language imposes restriction on @ utilization of chinese text @ no distinction indication @ word by @ space is @ of @ major difference @ chinese @ some @ asian language @ a japanese thai etc @ and western language @ chinese segmentation and feature extraction is essential in chinese natural language processing @ @ is a precondition @ @ chinese text information retrieval and knowledge discovery @ maximum matching and frequency statistic @ mmfs @ segmentation method based on length descending and string frequency statistic is @ effective segmentation and extraction method @ chinese word and phrase @ @ @ still some shorter word and phrase included in @ longer @ extracted by mmfs @ @ t @ obtained @ in order to solve @ problem @ @ @ a novel chinese hierarchy feature extraction method combined mmfs @ iterative learning algorithm @ @ method @ extract hierarchy feature according to morphology @ no need @ lexicon support no need @ acquiring @ probability @ word in advance and no need @ chinese character index @ experimental @ confirm @ efficiency of @ statistical method in extracting chinese hierarchy feature @ @ method is @ beneficial to feature extraction @ @ asian language similar to chinese @ jatit lls @ @ right reserved @ 
3295,Discovering semantic relations using prepositional phrases,Extracting semantical relations between concepts from texts is an important research issue in text mining and ontology construction. This paper presents a machine learning-based approach to semantic relation discovery using prepositional phrases. The semantic relations are characterized by the prepositions and the semantic classes of the concepts in the prepositional phrase. WordNet and word sense disambiguation are used to extract semantic classes of concepts. Preliminary experimental results are reported here showing the promise of the proposed method. © 2012 Springer-Verlag.,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,extracting semantical relation @ concept @ text is @ important research issue in text mining and ontology construction @ @ @ @ a machine learning-based approach to semantic relation discovery @ prepositional phrase @ @ semantic relation @ characterized by @ preposition and @ semantic class of @ concept in @ prepositional phrase @ wordnet and word sense disambiguation @ used to extract semantic class of concept @ preliminary experimental @ @ reported @ showing @ promise of @ proposed method @ springer-verlag @ 
3296,Lexical ontology layer - A bridge between text and concepts,"Intelligent methods for automatic text processing require linking between lexical resources (texts) and ontologies that define semantics. However, one of the main problems is that while building ontologies, the main effort is put to the construction of the conceptual part, whereas the lexical aspects of ontologies are usually diminished. Therefore, analyzing texts, it is usually difficult to map words to concepts from the ontology. Usually one should consider various linguistic relationships, such as homonymy, synonymy, etc. However, they are not clearly reflected in the conceptual part. We propose LEXO - a special lexical layer, which is thought as a bridge between text and the conceptual core of the ontology. LEXO is dedicated to storing linguistic relationships along with textual evidence for the relationships (as discovered in the text mining process). In addition, we present an algorithm based on LEXO for determining meaning of a given term in an analyzed text. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,intelligent method @ automatic text processing require linking @ lexical resource @ text @ and ontology @ define semantics @ however @ of @ main problem is @ @ building ontology @ main effort is put to @ construction of @ conceptual part whereas @ lexical aspect of ontology @ usually diminished @ therefore analyzing text @ is usually difficult to map word to concept @ @ ontology @ usually @ @ consider various linguistic relationship @ a homonymy synonymy etc @ however @ @ not clearly reflected in @ conceptual part @ @ propose lexo a special lexical layer @ is thought a a bridge @ text and @ conceptual core of @ ontology @ lexo is dedicated to storing linguistic relationship along @ textual evidence @ @ relationship @ a discovered in @ text mining process @ @ in addition @ @ @ algorithm based on lexo @ determining meaning of a given term in @ analyzed text @ springer-verlag @ 
3297,A neuro-evolutionary corpus-based method for word sense disambiguation,The proposed approach to word sense disambiguation uses an evolutionary algorithm to automatically design the structure and learn the connection weights of neural networks. © 2011 IEEE.,2012,IEEE Intelligent Systems,2,@ proposed approach to word sense disambiguation us @ evolutionary algorithm to automatically design @ structure and learn @ connection weight of neural network @ @ @ 
3301,On text preprocessing for opinion mining outside of laboratory environments,"Opinion mining deals with scientific methods in order to find, extract and systematically analyze subjective information. When performing opinion mining to analyze content on the Web, challenges arise that usually do not occur in laboratory environments where prepared and preprocessed texts are used. This paper discusses preprocessing approaches that help coping with the emerging problems of sentiment analysis in real world situations. After outlining the identified shortcomings and presenting a general process model for opinion mining, promising solutions for language identification, content extraction and dealing with Internet slang are discussed. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),26,opinion mining deal @ scientific method in order to find extract and systematically analyze subjective information @ @ performing opinion mining to analyze content on @ web challenge arise @ usually @ not occur in laboratory environment @ prepared and preprocessed text @ used @ @ @ discus preprocessing approach @ help coping @ @ emerging problem of sentiment analysis in real world situation @ @ outlining @ identified shortcoming and presenting a general process model @ opinion mining promising solution @ language identification content extraction and dealing @ internet slang @ discussed @ springer-verlag @ 
3310,Benchmarking infrastructure for mutation text mining,"Background: Research work on the automatic extraction of information about mutations from texts is greatly hindered by the lack of consensus evaluation facilities and easy-to-use infrastructure for testing and benchmarking of mutation text mining systems. Results: We propose a community-oriented annotation and benchmarking infrastructure to support development, testing, benchmarking, and comparison of mutation text mining systems. The design is based on semantic standards, where RDF is used to represent the annotations, an OWL ontology provides an extensible schema for the data and SPARQL is used to compute various performance metrics, so that in many cases programming is not needed to analyze system results. While large benchmark corpora for biological entity and relation extraction are focused mostly on gene, proteins, diseases, and species, our benchmarking infrastructure fills the gap for mutation information. The core infrastructure comprises of: 1) an ontology for modelling annotations, 2) SPARQL queries for performance metrics computation, and 3) a sizeable collection of manually curated documents, that can minimally support mutation grounding and mutation impact extraction. Conclusion: This is the first example of benchmarking infrastructure for mutation text mining. It is designed for community uptake.",2012,CEUR Workshop Proceedings,0,background @ research work on @ automatic extraction of information @ mutation @ text is greatly hindered by @ lack of consensus evaluation facility and easy-to-use infrastructure @ testing and benchmarking of mutation text mining system @ @ @ @ propose a community-oriented annotation and benchmarking infrastructure to support development testing benchmarking and comparison of mutation text mining system @ @ design is based on semantic standard @ rdf is used to represent @ annotation @ owl ontology provides @ extensible schema @ @ data and sparql is used to compute various performance metric @ @ in many case programming is not needed to analyze system @ @ @ @ benchmark corpus @ biological entity and relation extraction @ focused mostly on gene protein disease and specie @ benchmarking infrastructure fill @ gap @ mutation information @ @ core infrastructure comprises of @ @ @ ontology @ modelling annotation @ sparql query @ performance metric computation and @ a sizeable collection of manually curated document @ @ minimally support mutation grounding and mutation impact extraction @ conclusion @ @ is @ first example of benchmarking infrastructure @ mutation text mining @ @ is designed @ community uptake @ 
3324,Extracting semantic relations to enrich domain ontologies,"Domain ontologies facilitate the organization, sharing and reuse of domain knowledge, and enable various vertical domain applications to operate successfully. Most methods for automatically constructing ontologies focus on taxonomic relations, such as is-kind-of and is-part-of relations. However, much of the domainspecific semantics is ignored. This work proposes a semi-unsupervised approach for extracting semantic relations from domain-specific text documents. The approach effectively utilizes text mining and existing taxonomic relations in domain ontologies to discover candidate keywords that can represent semantic relations. A preliminary experiment on the natural science domain (Taiwan K9 education) indicates that the proposed method yields valuable recommendations. This work enriches domain ontologies by adding distilled semantics. © Springer Science+Business Media, LLC 2012.",2012,Journal of Intelligent Information Systems,12,domain ontology facilitate @ organization sharing and reuse of domain knowledge and enable various vertical domain application to operate successfully @ @ method @ automatically constructing ontology focus on taxonomic relation @ a is-kind-of and is-part-of relation @ however much of @ domainspecific semantics is ignored @ @ work proposes a semi-unsupervised approach @ extracting semantic relation @ domain-specific text document @ @ approach effectively utilizes text mining and existing taxonomic relation in domain ontology to discover candidate keywords @ @ represent semantic relation @ a preliminary experiment on @ natural science domain @ taiwan k education @ indicates @ @ proposed method yield valuable recommendation @ @ work enriches domain ontology by adding distilled semantics @ @ science @ medium llc @ 
3325,Advances in topic models for complex document network data,"This paper reviews important advances that have been made in the past decade for topic modeling of large-scale document network data. Interest in topic modeling is worldwide and touches a number of practical text mining, computer vision and computational biology systems that are important in text summarization, information retrieval, information recommendation, topic detection and tracking, natural scene understanding, human motion categorization and microarray gene expression analysis. The main focus of this review is on the recent advances of topic modeling techniques for document network data. We introduce the four major characteristics of document network data and the current state-of-the-art topic models, with descriptions of what they are, what has been accomplished, and what remains to be done. Document network data contain dynamic, higher-order, multiplex, and distributed structures. Prior efforts on topic models focus on modeling parts of these structures for topic detection and tracking. To handle all document network structures, we discuss a three-dimensional Markov model that solves dynamic, higher-order, multiplex and distributed structures within a unified framework. In addition, we also discuss the integration of three-dimensional Markov models with type-2 fuzzy logic systems for distributed computing with words. Besides document network structure modeling, we also discuss the inference and parameter estimation method in terms of energy minimization for three-dimensional Markov models.",2012,Jisuanji Xuebao/Chinese Journal of Computers,2,@ @ review important advance @ @ @ made in @ past decade @ topic modeling of large-scale document network data @ interest in topic modeling is worldwide and touch a number of practical text mining computer vision and computational biology system @ @ important in text summarization information retrieval information recommendation topic detection and tracking natural scene understanding human motion categorization and microarray gene expression analysis @ @ main focus of @ review is on @ recent advance of topic modeling technique @ document network data @ @ introduce @ four major characteristic of document network data and @ current state-of-the-art topic model @ description of @ @ @ @ ha @ accomplished and @ remains to @ done @ document network data contain dynamic higher-order multiplex and distributed structure @ prior effort on topic model focus on modeling part of @ structure @ topic detection and tracking @ to handle @ document network structure @ discus a three-dimensional markov model @ solves dynamic higher-order multiplex and distributed structure within a unified framework @ in addition @ @ discus @ integration of three-dimensional markov model @ type fuzzy logic system @ distributed computing @ word @ besides document network structure modeling @ @ discus @ inference and parameter estimation method in term of energy minimization @ three-dimensional markov model @ 
3326,Learning to refine an automatically extracted knowledge base using Markov logic,"A number of text mining and information extraction projects such as TextRunner and NELL seek to automatically build knowledge bases from the rapidly growing amount of information on the web. In order to scale to the size of the web, these projects often employ ad hoc heuristics to reason about uncertain and contradictory information rather than reasoning jointly about all candidate facts. In this paper, we present a Markov logic-based system for cleaning an extracted knowledge base. This allows a scalable system such as NELL to take advantage of joint probabilistic inference, or, conversely, allows Markov logic to be applied to a web scale problem. Our system uses only the ontological constraints and confidence values of the original system, along with humanlabeled data if available. The labeled data can be used to calibrate the confidence scores from the original system or learn the effectiveness of individual extraction patterns. To achieve scalability, we introduce a neighborhood grounding method that only instantiates the part of the network most relevant to the given query. This allows us to partition the knowledge cleaning task into tractable pieces that can be solved individually. In experiments on NELL's knowledge base, we evaluate several variants of our approach and find that they improve both F1 and area under the precision-recall curve. © 2012 IEEE.",2012,"Proceedings - IEEE International Conference on Data Mining, ICDM",37,a number of text mining and information extraction project @ a textrunner and nell seek to automatically build knowledge base @ @ rapidly growing amount of information on @ web @ in order to scale to @ size of @ web @ project often employ ad hoc heuristic to reason @ uncertain and contradictory information rather @ reasoning jointly @ @ candidate fact @ in @ @ @ @ a markov logic-based system @ cleaning @ extracted knowledge base @ @ allows a scalable system @ a nell to take advantage of joint probabilistic inference @ conversely allows markov logic to @ applied to a web scale problem @ @ system us only @ ontological constraint and confidence value of @ original system along @ humanlabeled data if available @ @ labeled data @ @ used to calibrate @ confidence score @ @ original system @ learn @ effectiveness of individual extraction pattern @ to achieve scalability @ introduce a neighborhood grounding method @ only instantiates @ part of @ network @ relevant to @ given query @ @ allows u to partition @ knowledge cleaning task @ tractable piece @ @ @ solved individually @ in experiment on nell @ s knowledge base @ evaluate several variant of @ approach and find @ @ improve @ f and area @ @ precision-recall curve @ @ @ 
3330,Mining user-generated content for social research and other applications,"User-generated content is currently becoming a valuable means for sensing and measuring real world variables and parameters that are of interest to several actors in the society: politicians, government departments, security agencies, marketing researchers, service providers, etc. In response to this new scenario, large research efforts are being invested in the so-called ""social media"" phenomenon by a wide spectrum of institutions and organizations around the world, with many different objectives and a diverse scope of fields and disciplines. As a consequence, new technologies and applications are currently emerging on the grounds of human participation, interaction, and behavior on the Internet. The main objective of this chapter is to present a general overview of the most relevant applications of text mining and natural language processing technologies evolving and emerging around the Web 2.0 phenomenon (such as automatic categorization, document summarization, question answering, dialogue management, opinion mining, sentiment analysis, outlier identification, misbehavior detection, and social estimation and forecasting) along with the main challenges and new research opportunities that are directly and indirectly derived from them. © 2013, IGI Global.",2012,Emerging Applications of Natural Language Processing: Concepts and New Research,0,user-generated content is currently becoming a valuable mean @ sensing and measuring real world variable and parameter @ @ of interest to several actor in @ society @ politician government department security agency marketing researcher service provider etc @ in response to @ @ scenario @ research effort @ @ invested in @ so-called @ social medium @ phenomenon by a wide spectrum of institution and organization around @ world @ many different objective and a diverse scope of field and discipline @ a a consequence @ technology and application @ currently emerging on @ ground of human participation interaction and behavior on @ internet @ @ main objective of @ chapter is to @ a general overview of @ @ relevant application of text mining and natural language processing technology evolving and emerging around @ web @ phenomenon @ @ a automatic categorization document summarization question answering dialogue management opinion mining sentiment analysis outlier identification misbehavior detection and social estimation and forecasting @ along @ @ main challenge and @ research opportunity @ @ directly and indirectly derived @ @ @ igi global @ 
3336,Recognizing hierarchically related biomedical entities using MeSH-based mapping,"Identifying hierarchically related entities is a critical step towards constructing bio-networks in the field of biomedical text mining. To this end, we adopt a mapping-based approach by first mapping bio-entities to terms in an established ontology Medical Subject Headings (MeSH). We then utilize the hierarchical relationships available in MeSH to recognize hierarchically related entities. Specifically, we present two approaches to map biomedical entities identified using the Unified Medical Language System (UMLS) Metathesaurus to MeSH terms. The first approach utilizes a special feature provided by the MetaMap algorithm, whereas the other employs approximate phrase-based match to directly map entities to MeSH terms. These two approaches deliver comparable results with an accuracy of 72% and 75%, respectively, based on two evaluation datasets. A thorough error analysis demonstrates that these two approaches result in only around 10% mutual errors, indicating the complementary nature of these two approaches. © 1996-2012 Tsinghua University Press.",2012,Tsinghua Science and Technology,1,identifying hierarchically related entity is a critical step towards constructing bio-networks in @ field of biomedical text mining @ to @ end @ adopt a mapping-based approach by first mapping bio-entities to term in @ established ontology medical subject heading @ mesh @ @ @ @ utilize @ hierarchical relationship available in mesh to recognize hierarchically related entity @ specifically @ @ @ approach to map biomedical entity identified @ @ unified medical language system @ umls @ metathesaurus to mesh term @ @ first approach utilizes a special feature provided by @ metamap algorithm whereas @ @ employ approximate phrase-based match to directly map entity to mesh term @ @ @ approach deliver comparable @ @ @ accuracy of and respectively based on @ evaluation datasets @ a thorough error analysis demonstrates @ @ @ approach @ in only around mutual error indicating @ complementary nature of @ @ approach @ tsinghua university @ @ 
3347,Subjectivity and sentiment analysis: An overview of the current state of the area and envisaged developments,"In this introduction, we present an overview of the current state of research in the Natural Language Processing tasks of subjectivity and sentiment analysis, as well as their application domains and closely-related research field of emotion detection. Although many definitions exist for these tasks and the research done within their frame spans over approaches with different objectives, we consider subjectivity analysis to deal with the detection of ""private states"" (opinions, emotions, sentiments, beliefs, speculations) and sentiment analysis as the task of detecting, extracting and classifying opinions and sentiments concerning different topics, as expressed in textual input. After describing the key concepts and research directions in these tasks, we present the main achievements obtained so far and the issues that remain to be tackled. Subsequently, we introduce each of the papers in this volume and present their contribution to the research areas of subjectivity and sentiment analysis. Finally, we conclude on the present state of work in these fields and reflect on the possible future developments. © 2012 Elsevier B.V. All rights reserved.",2012,Decision Support Systems,144,in @ introduction @ @ @ overview of @ current state of research in @ natural language processing task of subjectivity and sentiment analysis a well a @ application domain and closely-related research field of emotion detection @ although many definition exist @ @ task and @ research done within @ frame span @ approach @ different objective @ consider subjectivity analysis to deal @ @ detection of @ private state @ @ opinion emotion sentiment belief speculation @ and sentiment analysis a @ task of detecting extracting and classifying opinion and sentiment concerning different topic a expressed in textual input @ @ describing @ key concept and research direction in @ task @ @ @ main achievement obtained @ far and @ issue @ remain to @ tackled @ subsequently @ introduce @ of @ @ in @ volume and @ @ contribution to @ research area of subjectivity and sentiment analysis @ finally @ conclude on @ @ state of work in @ field and reflect on @ possible future development @ @ b @ v @ @ right reserved @ 
3348,Training MEMM with PSO: A tool for part-of-speech tagging,"Maximum Entropy Markov Models (MEMM) can avoid the assumption of independence in traditional Hidden Markov Models (HMM), and thus take advantage of context information in most text mining tasks. Because the convergence rate of the classic generalized iterative scaling (GIS) algorithm is too low to be tolerated, researchers proposed a lot of improved methods such as IIS, SCGIS and LBFGS for parameters training in MEMM. However these methods sometimes do not satisfy task requirements in efficiency and robustness. This article modifies the traditional Particle Swarm Optimization (PSO) algorithm by using dynamic global mutation probability (DGMP) to solve the local optimum and infinite loops problems and use the modified PSO in MEMM for estimating the parameters. We introduce the MEMM trained by modified PSO into Chinese Part-of-Speech (POS) tagging, analysis the experimental results and find it has higher convergence rate and accuracy than traditional MEMM. © 2012 ACADEMY PUBLISHER.",2012,Journal of Software,0,maximum entropy markov model @ memm @ @ avoid @ assumption of independence in traditional hidden markov model @ hmm @ and thus take advantage of context information in @ text mining task @ @ @ convergence rate of @ classic generalized iterative scaling @ gi @ algorithm is too low to @ tolerated researcher proposed a lot of improved method @ a ii scgis and lbfgs @ parameter training in memm @ however @ method sometimes @ not satisfy task requirement in efficiency and robustness @ @ article modifies @ traditional particle swarm optimization @ pso @ algorithm by @ dynamic global mutation probability @ dgmp @ to solve @ local optimum and infinite loop problem and use @ modified pso in memm @ estimating @ parameter @ @ introduce @ memm trained by modified pso @ chinese part-of-speech @ po @ tagging analysis @ experimental @ and find @ ha higher convergence rate and accuracy @ traditional memm @ academy publisher @ 
3349,Identifying the semantic orientation of terms using S-HAL for sentiment analysis,"Sentiment analysis continues to be a most important research problem due to its abundant applications. Identifying the semantic orientation of subjective terms (words or phrases) is a fundamental task for sentiment analysis. In this paper, we propose a new method for identifying the semantic orientation of subjective terms to perform sentiment analysis. The method takes a classification approach that is based on a novel semantic orientation representation model called S-HAL (Sentiment Hyperspace Analogue to Language). S-HAL basically produces a set of weighted features based on surrounding words, and characterizes the semantic orientation information of words via a specific feature space. Because the method incorporates the idea underlying HAL and the hypothesis verified by the method of semantic orientation inference from pointwise mutual information (SO-PMI), it can quickly and accurately identify the semantic orientation of terms without the use of an Internet search engine. The results of an empirical evaluation show that our method outperforms other known methods. © 2012 Elsevier B.V. All rights reserved.",2012,Knowledge-Based Systems,51,sentiment analysis continues to @ a @ important research problem due to @ abundant application @ identifying @ semantic orientation of subjective term @ word @ phrase @ is a fundamental task @ sentiment analysis @ in @ @ @ propose a @ method @ identifying @ semantic orientation of subjective term to perform sentiment analysis @ @ method take a classification approach @ is based on a novel semantic orientation representation model called s-hal @ sentiment hyperspace analogue to language @ @ s-hal basically produce a set of weighted feature based on surrounding word and characterizes @ semantic orientation information of word via a specific feature space @ @ @ method incorporates @ idea underlying hal and @ hypothesis verified by @ method of semantic orientation inference @ pointwise mutual information @ so-pmi @ @ @ quickly and accurately identify @ semantic orientation of term without @ use of @ internet search engine @ @ @ of @ empirical evaluation @ @ @ method outperforms @ known method @ @ b @ v @ @ right reserved @ 
3350,Discovering business intelligence from online product reviews: A rule-induction framework,"Online product reviews are a major source of business intelligence (BI) that helps managers and marketers understand customers' concerns and interests. The large volume of review data makes it difficult to manually analyze customers' concerns. Automated tools have emerged to facilitate this analysis, however most lack the capability of extracting the relationships between the reviews' rich expressions and the customer ratings. Managers and marketers often resort to manually read through voluminous reviews to find the relationships. To address these challenges, we propose the development of a new class of BI systems based on rough set theory, inductive rule learning, and information retrieval methods. We developed a new framework for designing BI systems that extract the relationship between the customer ratings and their reviews. Using reviews of different products from Amazon.com, we conducted both qualitative and quantitative experiments to evaluate the performance of a BI system developed based on the framework. The results indicate that the system achieved high accuracy and coverage related to rule quality, and produced interesting and informative rules with high support and confidence values. The findings have important implications for market sentiment analysis and e-commerce reputation management. © 2012 Elsevier Ltd. All rights reserved.",2012,Expert Systems with Applications,50,online product review @ a major source of @ intelligence @ bi @ @ help manager and marketer understand customer @ concern and interest @ @ @ volume of review data make @ difficult to manually analyze customer @ concern @ automated tool @ emerged to facilitate @ analysis however @ lack @ capability of extracting @ relationship @ @ review @ rich expression and @ customer rating @ manager and marketer often resort to manually read @ voluminous review to find @ relationship @ to address @ challenge @ propose @ development of a @ class of bi system based on rough set theory inductive rule learning and information retrieval method @ @ developed a @ framework @ designing bi system @ extract @ relationship @ @ customer rating and @ review @ @ review of different product @ amazon @ com @ conducted @ qualitative and quantitative experiment to evaluate @ performance of a bi system developed based on @ framework @ @ @ indicate @ @ system achieved high accuracy and coverage related to rule quality and produced interesting and informative rule @ high support and confidence value @ @ finding @ important implication @ market sentiment analysis and e-commerce reputation management @ @ ltd @ @ right reserved @ 
3351,Document classification using POS distribution,"In this investigation, we discuss how to classify very quickly documents in Japanese putting stress on Part Of Speech (POS) distribution, not word distribution. There exist two main contributon of this investigation: linear regression approach models POS behavior in Japanese documents very well for classification, and a new excellent and efficient classification proposed based on Gaussian probability distribution, called Gaussian classifier. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ investigation @ discus @ to classify @ quickly document in japanese putting stress on part of speech @ po @ distribution not word distribution @ @ exist @ main contributon of @ investigation @ linear regression approach model po behavior in japanese document @ well @ classification and a @ excellent and efficient classification proposed based on gaussian probability distribution called gaussian classifier @ springer-verlag @ 
3352,Using NMF for analyzing war logs,"We investigate a semi-automated identification of technical problems occurred by armed forces weapon systems during mission of war. The proposed methodology is based on a semantic analysis of textual information in reports from soldiers (war logs). Latent semantic indexing (LSI) with non-negative matrix factorization (NMF) as technique from multivariate analysis and linear algebra is used to extract hidden semantic textual patterns from the reports. NMF factorizes the term-by-war log matrix - that consists of weighted term frequencies - into two non-negative matrices. This enables natural parts-based representation of the report information and it leads to an easy evaluation by human experts because human brain also uses parts-based representation. For an improved research and technology planning, the identified technical problems are a valuable source of information. A case study extracts technical problems from military logs of the Afghanistan war. Results are compared to a manual analysis written by journalists of 'Der Spiegel'. © 2012 Springer-Verlag.",2012,Communications in Computer and Information Science,9,@ investigate a semi-automated identification of technical problem occurred by armed force weapon system @ mission of war @ @ proposed methodology is based on a semantic analysis of textual information in report @ soldier @ war log @ @ latent semantic indexing @ lsi @ @ non-negative matrix factorization @ nmf @ a technique @ multivariate analysis and linear algebra is used to extract hidden semantic textual pattern @ @ report @ nmf factorizes @ term-by-war log matrix @ consists of weighted term frequency @ @ non-negative matrix @ @ enables natural parts-based representation of @ report information and @ lead to @ easy evaluation by human expert @ human brain @ us parts-based representation @ @ @ improved research and technology planning @ identified technical problem @ a valuable source of information @ a case study extract technical problem @ military log of @ afghanistan war @ @ @ compared to a manual analysis written by journalist of @ der spiegel @ @ springer-verlag @ 
3356,Sentiment classification with supervised sequence embedding,"In this paper, we introduce a novel approach for modeling n-grams in a latent space learned from supervised signals. The proposed procedure uses only unigram features to model short phrases (n-grams) in the latent space. The phrases are then combined to form document-level latent representation for a given text, where position of an n-gram in the document is used to compute corresponding combining weight. The resulting two-stage supervised embedding is then coupled with a classifier to form an end-to-end system that we apply to the large-scale sentiment classification task. The proposed model does not require feature selection to retain effective features during pre-processing, and its parameter space grows linearly with size of n-gram. We present comparative evaluations of this method using two large-scale datasets for sentiment classification in online reviews (Amazon and TripAdvisor). The proposed method outperforms standard baselines that rely on bag-of-words representation populated with n-gram features. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11,in @ @ @ introduce a novel approach @ modeling n-grams in a latent space learned @ supervised signal @ @ proposed procedure us only unigram feature to model short phrase @ n-grams @ in @ latent space @ @ phrase @ @ combined to form document-level latent representation @ a given text @ position of @ n-gram in @ document is used to compute corresponding combining weight @ @ resulting two-stage supervised embedding is @ coupled @ a classifier to form @ end-to-end system @ @ apply to @ large-scale sentiment classification task @ @ proposed model doe not require feature selection to retain effective feature @ pre-processing and @ parameter space grows linearly @ size of n-gram @ @ @ comparative evaluation of @ method @ @ large-scale datasets @ sentiment classification in online review @ amazon and tripadvisor @ @ @ proposed method outperforms standard baseline @ rely on bag-of-words representation populated @ n-gram feature @ springer-verlag @ 
3357,WikiSent: Weakly supervised sentiment analysis through extractive summarization with Wikipedia,"This paper describes a weakly supervised system for sentiment analysis in the movie review domain. The objective is to classify a movie review into a polarity class, positive or negative, based on those sentences bearing opinion on the movie alone, leaving out other irrelevant text. Wikipedia incorporates the world knowledge of movie-specific features in the system which is used to obtain an extractive summary of the review, consisting of the reviewer's opinions about the specific aspects of the movie. This filters out the concepts which are irrelevant or objective with respect to the given movie. The proposed system, WikiSent, does not require any labeled data for training. It achieves a better or comparable accuracy to the existing semi-supervised and unsupervised systems in the domain, on the same dataset. We also perform a general movie review trend analysis using WikiSent. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,@ @ describes a weakly supervised system @ sentiment analysis in @ movie review domain @ @ objective is to classify a movie review @ a polarity class positive @ negative based on @ sentence bearing opinion on @ movie alone leaving @ @ irrelevant text @ wikipedia incorporates @ world knowledge of movie-specific feature in @ system @ is used to obtain @ extractive summary of @ review consisting of @ reviewer @ s opinion @ @ specific aspect of @ movie @ @ filter @ @ concept @ @ irrelevant @ objective @ respect to @ given movie @ @ proposed system wikisent doe not require @ labeled data @ training @ @ achieves a better @ comparable accuracy to @ existing semi-supervised and unsupervised system in @ domain on @ @ dataset @ @ @ perform a general movie review trend analysis @ wikisent @ springer-verlag @ 
3358,Unsupervised similarity learning from textual data,"This paper presents a research on the construction of a new unsupervised model for learning a semantic similarity measure from text corpora. Two main components of the model are a semantic interpreter of texts and a similarity function whose properties are derived from data. The first one associates particular documents with concepts defined in a knowledge base corresponding to the topics covered by the corpus. It shifts the representation of a meaning of the texts from words that can be ambiguous to concepts with predefined semantics. With this new representation, the similarity function is derived from data using a modification of the dynamic rule-based similarity model, which is adjusted to the unsupervised case. The adjustment is based on a novel notion of an information bireduct having its origin in the theory of rough sets. This extension of classical information reducts is used in order to find diverse sets of reference documents described by diverse sets of reference concepts that determine different aspects of the similarity. The paper explains a general idea of the approach and also gives some implementation guidelines. Additionally, results of some preliminary experiments are presented in order to demonstrate usefulness of the proposed model.",2012,Fundamenta Informaticae,20,@ @ @ a research on @ construction of a @ unsupervised model @ learning a semantic similarity measure @ text corpus @ @ main component of @ model @ a semantic interpreter of text and a similarity function whose property @ derived @ data @ @ first @ associate particular document @ concept defined in a knowledge base corresponding to @ topic covered by @ corpus @ @ shift @ representation of a meaning of @ text @ word @ @ @ ambiguous to concept @ predefined semantics @ @ @ @ representation @ similarity function is derived @ data @ a modification of @ dynamic rule-based similarity model @ is adjusted to @ unsupervised case @ @ adjustment is based on a novel notion of @ information bireduct @ @ origin in @ theory of rough set @ @ extension of classical information reducts is used in order to find diverse set of reference document described by diverse set of reference concept @ determine different aspect of @ similarity @ @ @ explains a general idea of @ approach and @ give some implementation guideline @ additionally @ of some preliminary experiment @ presented in order to demonstrate usefulness of @ proposed model @ 
3368,Location recognition with fuzzy grammar,"Fuzzy grammar has been introduced as an approach to represent and learn text fragments where the set of learned patterns are represented by combining similar segments to represent regularities and marking interchangeable segments. This paper is dedicated to present a procedural scheme towards learning text fragment in text categorization task facilitated by fuzzy grammars. A few issues are involved in developing fuzzy grammars which are (i) determination of the number of text classes to develop (ii) the selection of text fragments, F relevant to each text class (iii) determining frequent and important terms or keywords, V to develop the set of terminal, T and compound grammars, N. (iv) Conversion of text fragments into grammars, (v) Combination of grammars into a compact form. Comparison between fuzzy grammar and other location entity identifier such as LbjTagger, LingPipe, Newswire and OpenCalais is observed where results have shown that this method outperforms other standard machine learning and statistical-based approach. © 2012 Springer-Verlag.",2012,Communications in Computer and Information Science,0,fuzzy grammar ha @ introduced a @ approach to represent and learn text fragment @ @ set of learned pattern @ represented by combining similar segment to represent regularity and marking interchangeable segment @ @ @ is dedicated to @ a procedural scheme towards learning text fragment in text categorization task facilitated by fuzzy grammar @ a @ issue @ involved in developing fuzzy grammar @ @ @ i @ determination of @ number of text class to develop @ ii @ @ selection of text fragment f relevant to @ text class @ iii @ determining frequent and important term @ keywords v to develop @ set of terminal t and compound grammar n @ @ @ @ conversion of text fragment @ grammar @ v @ combination of grammar @ a compact form @ comparison @ fuzzy grammar and @ location entity identifier @ a lbjtagger lingpipe newswire and opencalais is observed @ @ @ @ @ @ method outperforms @ standard machine learning and statistical-based approach @ springer-verlag @ 
3369,SBFC: An efficient feature frequency-based approach to tackle cross-lingual word sense disambiguation,"The Cross-Lingual Word Sense Disambiguation (CLWSD) problem is a challenging Natural Language Processing (NLP) task that consists of selecting the correct translation of an ambiguous word in a given context. Different approaches have been proposed to tackle this problem, but they are often complex and need tuning and parameter optimization. In this paper, we propose a new classifier, Selected Binary Feature Combination (SBFC), for the CLWSD problem. The underlying hypothesis of SBFC is that a translation is a good classification label for new instances if the features that occur frequently in the new instance also occur frequently in the training feature vectors associated with the same translation label. The advantage of SBFC over existing approaches is that it is intuitive and therefore easy to implement. The algorithm is fast, which allows processing of large text mining data sets. Moreover, no tuning is needed and experimental results show that SBFC outperforms state-of-the-art models for the CLWSD problem w.r.t. accuracy. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ cross-lingual word sense disambiguation @ clwsd @ problem is a challenging natural language processing @ nlp @ task @ consists of selecting @ correct translation of @ ambiguous word in a given context @ different approach @ @ proposed to tackle @ problem @ @ @ often complex and need tuning and parameter optimization @ in @ @ @ propose a @ classifier selected binary feature combination @ sbfc @ @ @ clwsd problem @ @ underlying hypothesis of sbfc is @ a translation is a good classification label @ @ instance if @ feature @ occur frequently in @ @ instance @ occur frequently in @ training feature vector associated @ @ @ translation label @ @ advantage of sbfc @ existing approach is @ @ is intuitive and therefore easy to implement @ @ algorithm is fast @ allows processing of @ text mining data set @ moreover no tuning is needed and experimental @ @ @ sbfc outperforms state-of-the-art model @ @ clwsd problem w @ r @ t @ accuracy @ springer-verlag @ 
3371,Bootstrapping a game with a purpose for commonsense collection,"Text mining has been very successful in extracting huge amounts of commonsense knowledge from data, but the extracted knowledge tends to be extremely noisy. Manual construction of knowledge repositories, on the other hand, tends to produce high-quality data in very small amounts. We propose an architecture to combine the best of both worlds: A game with a purpose that induces humans to clean up data automatically extracted by text mining. First, a text miner trained on a set of known commonsense facts harvests many more candidate facts from corpora. Then, a simple slot-machine-with-a-purpose game presents these candidate facts to the players for verification by playing. As a result, a new dataset of high precision commonsense knowledge is created. This combined architecture is able to produce significantly better commonsense facts than the state-of-the-art text miner alone. Furthermore, we report that bootstrapping (i.e., training the text miner on the output of the game) improves the subsequent performance of the text miner. © 2012 ACM.",2012,ACM Transactions on Intelligent Systems and Technology,3,text mining ha @ @ successful in extracting huge amount of commonsense knowledge @ data @ @ extracted knowledge tends to @ extremely noisy @ manual construction of knowledge repository on @ @ hand tends to produce high-quality data in @ small amount @ @ propose @ architecture to combine @ best of @ world @ a game @ a purpose @ induces human to clean up data automatically extracted by text mining @ first a text miner trained on a set of known commonsense fact harvest many more candidate fact @ corpus @ @ a simple slot-machine-with-a-purpose game @ @ candidate fact to @ player @ verification by playing @ a a @ a @ dataset of high precision commonsense knowledge is created @ @ combined architecture is able to produce significantly better commonsense fact @ @ state-of-the-art text miner alone @ furthermore @ report @ bootstrapping @ i @ e @ training @ text miner on @ output of @ game @ improves @ subsequent performance of @ text miner @ acm @ 
3372,Language infrastructure and information analysis technology : Information analysis technologies at NICT,"We have conducted research on information analysis technologies, which enables us to automatically analyze a huge amount of information available on the Web since 2006. Our research achievements include the information analysis service WISDOM, as well as several language resources and tools available from the ALAGIN forum. The former allows users to analyze information on the Web from several perspectives and provides users the insight necessary to help them assess the credibility of information obtained from the Web. The latter are indispensible resources for a deep analysis of textual information. In this paper we give an overview of these research activities and describe the underlying aims for which we developed them.",2012,Journal of the National Institute of Information and Communications Technology,0,@ @ conducted research on information analysis technology @ enables u to automatically analyze a huge amount of information available on @ web since @ @ research achievement include @ information analysis service wisdom a well a several language resource and tool available @ @ alagin forum @ @ former allows user to analyze information on @ web @ several perspective and provides user @ insight necessary to help @ ass @ credibility of information obtained @ @ web @ @ latter @ indispensible resource @ a deep analysis of textual information @ in @ @ @ give @ overview of @ research activity and describe @ underlying aim @ @ @ developed @ @ 
3374,"Ontology based text processing for context, similarity and key word extraction","Semantic similarity plays a vital role in Q & A systems, Text Mining, Language modeling, Information Retrieval, Natural Language Processing (NLP), text-related research and applications. Measuring Semantic similarity between sentences is closely related to Semantic similarity between words. Key word extraction is useful to understand the important information contained in a document or in a short text. This paper proposes two strategies for: (i) finding the similarity and context between two sentences. (ii) Extending this approach for a paragraph of sentences using the WordNet lexical database. © 2012 Springer-Verlag.",2012,Communications in Computer and Information Science,0,semantic similarity play a vital role in q a system text mining language modeling information retrieval natural language processing @ nlp @ text-related research and application @ measuring semantic similarity @ sentence is closely related to semantic similarity @ word @ key word extraction is useful to understand @ important information contained in a document @ in a short text @ @ @ proposes @ strategy @ @ @ i @ finding @ similarity and context @ @ sentence @ @ ii @ extending @ approach @ a paragraph of sentence @ @ wordnet lexical database @ springer-verlag @ 
3375,Comparing similarity of concepts identified by temporal patterns of terms in biomedical research documents,"In this paper, we present an analysis of a relationship between temporal trends of automatically extracted terms in medical research document and their similarities on a structured vocabulary. In order to obtain the temporal trends, we used our temporal pattern extraction method that combines an automatic term extraction, an importance index of the terms, and clustering for the values in each period. By using a set of medical research documents that were published every year, we extracted temporal patterns of the automatically extracted terms. Then, we calculated their similarities on the medical taxonomy by defining a distance on the tree structure. For analyzing the relationship between the terms included in the patterns and the similarity of the terms on the taxonomy, the differences of the averaged similarities of the terms in each pattern are compared between the two trends of the temporal patterns. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,in @ @ @ @ @ analysis of a relationship @ temporal trend of automatically extracted term in medical research document and @ similarity on a structured vocabulary @ in order to obtain @ temporal trend @ used @ temporal pattern extraction method @ combine @ automatic term extraction @ importance index of @ term and clustering @ @ value in @ period @ by @ a set of medical research document @ @ published every year @ extracted temporal pattern of @ automatically extracted term @ @ @ calculated @ similarity on @ medical taxonomy by defining a distance on @ tree structure @ @ analyzing @ relationship @ @ term included in @ pattern and @ similarity of @ term on @ taxonomy @ difference of @ averaged similarity of @ term in @ pattern @ compared @ @ @ trend of @ temporal pattern @ springer-verlag @ 
3376,A pattern discovery model for effective text mining,"The quality of extracted features is the key issue to text mining due to the large number of terms, phrases, and noise. Most existing text mining methods are based on term-based approaches which extract terms from a training set for describing relevant information. However, the quality of the extracted terms in text documents may be not high because of lot of noise in text. For many years, some researchers make use of various phrases that have more semantics than single words to improve the relevance, but many experiments do not support the effective use of phrases since they have low frequency of occurrence, and include many redundant and noise phrases. In this paper, we propose a novel pattern discovery approach for text mining. This approach first discovers closed sequential patterns in text documents for identifying the most informative contents of the documents and then utilise the identified contents to extract useful features for text mining. We develop a novel fusion method based on Dempster-Shafer's evidential reasoning which allows to combine the pieces of document to discover the knowledge (features). To evaluate the proposed approach, we adopt the feature extraction method for information filtering (IF). The experimental results conducted on Reuters Corpus Volume 1 and TREC topics confirm that the proposed approach could achieve excellent performance. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ quality of extracted feature is @ key issue to text mining due to @ @ number of term phrase and noise @ @ existing text mining method @ based on term-based approach @ extract term @ a training set @ describing relevant information @ however @ quality of @ extracted term in text document may @ not high @ of lot of noise in text @ @ many year some researcher make use of various phrase @ @ more semantics @ single word to improve @ relevance @ many experiment @ not support @ effective use of phrase since @ @ low frequency of occurrence and include many redundant and noise phrase @ in @ @ @ propose a novel pattern discovery approach @ text mining @ @ approach first discovers closed sequential pattern in text document @ identifying @ @ informative content of @ document and @ utilise @ identified content to extract useful feature @ text mining @ @ develop a novel fusion method based on dempster-shafer @ s evidential reasoning @ allows to combine @ piece of document to discover @ knowledge @ feature @ @ to evaluate @ proposed approach @ adopt @ feature extraction method @ information filtering @ if @ @ @ experimental @ conducted on reuters corpus volume and trec topic confirm @ @ proposed approach could achieve excellent performance @ springer-verlag @ 
3379,Probabilistic models for text mining,"A number of probabilistic methods such as LDA, hidden Markov models, Markov random fields have arisen in recent years for probabilistic analysis of text data. This chapter provides an overview of a variety of probabilistic models for text mining. The chapter focuses more on the fundamental probabilistic techniques, and also covers their various applications to different text mining problems. Some examples of such applications include topic modeling, language modeling, document classification, document clustering, and information extraction. © 2012 Springer Science+Business Media, LLC. All rights reserved.",2012,Mining Text Data,8,a number of probabilistic method @ a lda hidden markov model markov random field @ arisen in recent year @ probabilistic analysis of text data @ @ chapter provides @ overview of a variety of probabilistic model @ text mining @ @ chapter focus more on @ fundamental probabilistic technique and @ cover @ various application to different text mining problem @ some example of @ application include topic modeling language modeling document classification document clustering and information extraction @ @ science @ medium llc @ @ right reserved @ 
3380,Event-level textual emotion sensing based on common action distributions between event participants,"Automatic emotion sensing in textual data is crucial for the development of intelligent interfaces in interactive computer applications. This paper reports a high-precision, domain-independent approach for automatic emotion sensing for ""events"" embedded in sentences. The proposed approach is based on the common action distribution between the subject and object of an event. We have incorporated semantic labeling and web-based text mining techniques, together with a number of reference entity pairs and hand-crafted emotion generation rules to realize an event emotion detection system. Moreover, a hybrid emotion detection engine is presented by incorporating a set of predefined emotion keywords and the proposed event-level emotion detection engine. The evaluation outcome reveals a rather satisfactory result with about 73% accuracy for detecting the Happy, Sad, Fear, Angry, Surprise, Disgust, and Neutral. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,automatic emotion sensing in textual data is crucial @ @ development of intelligent interface in interactive computer application @ @ @ report a high-precision domain-independent approach @ automatic emotion sensing @ @ event @ embedded in sentence @ @ proposed approach is based on @ common action distribution @ @ subject and object of @ event @ @ @ incorporated semantic labeling and web-based text mining technique together @ a number of reference entity pair and hand-crafted emotion generation rule to realize @ event emotion detection system @ moreover a hybrid emotion detection engine is presented by incorporating a set of predefined emotion keywords and @ proposed event-level emotion detection engine @ @ evaluation outcome reveals a rather satisfactory @ @ @ accuracy @ detecting @ happy sad fear angry surprise disgust and neutral @ springer-verlag @ 
3388,Text mining for the biocuration workflow,"Molecular biology has become heavily dependent on biological knowledge encoded in expert curated biological databases. As the volume of biological literature increases, biocurators need help in keeping up with the literature; (semi-) automated aids for biocuration would seem to be an ideal application for natural language processing and text mining. However, to date, there have been few documented successes for improving biocuration throughput using text mining. Our initial investigations took place for the workshop on 'Text Mining for the BioCuration Workflow' at the third International Biocuration Conference (Berlin, 2009). We interviewed biocurators to obtain workflows from eight biological databases. This initial study revealed high-level commonalities, including (i) selection of documents for curation; (ii) indexing of documents with biologically relevant entities (e.g. genes); and (iii) detailed curation of specific relations (e.g. interactions); however, the detailed workflows also showed many variabilities. Following the workshop, we conducted a survey of biocurators. The survey identified biocurator priorities, including the handling of full text indexed with biological entities and support for the identification and prioritization of documents for curation. It also indicated that two-thirds of the biocuration teams had experimented with text mining and almost half were using text mining at that time. Analysis of our interviews and survey provide a set of requirements for the integration of text mining into the biocuration workflow. These can guide the identification of common needs across curated databases and encourage joint experimentation involving biocurators, text mining developers and the larger biomedical research community. © The Author(s) 2012.",2012,Database,97,molecular biology ha become heavily dependent on biological knowledge encoded in expert curated biological database @ a @ volume of biological literature increase biocurators need help in keeping up @ @ literature @ @ semi @ automated aid @ biocuration would seem to @ @ ideal application @ natural language processing and text mining @ however to date @ @ @ @ documented success @ improving biocuration throughput @ text mining @ @ initial investigation took place @ @ workshop on @ text mining @ @ biocuration workflow @ at @ third international biocuration conference @ @ @ @ @ interviewed biocurators to obtain workflow @ eight biological database @ @ initial study revealed high-level commonality including @ i @ selection of document @ curation @ @ ii @ indexing of document @ biologically relevant entity @ e @ g @ gene @ @ and @ iii @ detailed curation of specific relation @ e @ g @ interaction @ @ however @ detailed workflow @ showed many variability @ following @ workshop @ conducted a survey of biocurators @ @ survey identified biocurator priority including @ handling of full text indexed @ biological entity and support @ @ identification and prioritization of document @ curation @ @ @ indicated @ two-thirds of @ biocuration team @ experimented @ text mining and almost half @ @ text mining at @ time @ analysis of @ interview and survey provide a set of requirement @ @ integration of text mining @ @ biocuration workflow @ @ @ guide @ identification of common need across curated database and encourage joint experimentation involving biocurators text mining developer and @ larger biomedical research community @ @ author @ s @ @ 
3389,Developing multilingual text mining workflows in UIMA and U-compare,"We present a generic, language-independent method for the construction of multilingual text mining workflows. The proposed mechanism is implemented as an extension of U-Compare, a platform built on top of the Unstructured Information Management Architecture (UIMA) that allows the construction, comparison and evaluation of interoperable text mining workflows. UIMA was previously supporting strictly monolingual workflows. Building multilingual workflows exhibits challenging problems, such as representing multilingual document collections and executing language-dependent components in parallel. As an application of our method, we develop a multilingual workflow that extracts terms from a parallel collection using a new heuristic. For our experiments, we construct a parallel corpus consisting of approximately 188.000 PubMed article titles for French and English. Our application is evaluated against a popular monolingual term extraction method, C Value. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ a generic language-independent method @ @ construction of multilingual text mining workflow @ @ proposed mechanism is implemented a @ extension of u-compare a platform built on top of @ unstructured information management architecture @ uima @ @ allows @ construction comparison and evaluation of interoperable text mining workflow @ uima wa @ supporting strictly monolingual workflow @ building multilingual workflow exhibit challenging problem @ a representing multilingual document collection and executing language-dependent component in parallel @ a @ application of @ method @ develop a multilingual workflow @ extract term @ a parallel collection @ a @ heuristic @ @ @ experiment @ construct a parallel corpus consisting of approximately @ pubmed article title @ french and english @ @ application is evaluated @ a popular monolingual term extraction method c value @ springer-verlag @ 
3390,GPU-accelerated non-negative matrix factorization for text mining,An implementation of the non-negative matrix factorization algorithm for the purpose of text mining on graphics processing units is presented. Performance gains of more than one order of magnitude are obtained. © 2012 Springer-Verlag.,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),19,@ implementation of @ non-negative matrix factorization algorithm @ @ purpose of text mining on graphic processing unit is presented @ performance gain of more @ @ order of magnitude @ obtained @ springer-verlag @ 
3391,"Arabic rhetorical relations extraction for answering ""why"" and ""how to"" questions","In the current study we aim at exploiting discourse structure of Arabic text to automatically finding answers to non-factoid questions (""Why"" and ""How to""). Our method is based on Rhetorical Structure Theory (RST) that many studies have shown to be a very effective approach for many computational linguistics applications such as (text generation, text summarization and machine translation). For both types of questions we assign one or more rhetorical relations that help discovering the corresponding answers. This is the first Arabic Question Answering system that attempts to answer the ""Why"" and ""How to"" questions. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7,in @ current study @ aim at exploiting discourse structure of arabic text to automatically finding answer to non-factoid question @ @ @ @ and @ @ to @ @ @ @ method is based on rhetorical structure theory @ rst @ @ many study @ @ to @ a @ effective approach @ many computational linguistics application @ a @ text generation text summarization and machine translation @ @ @ @ type of question @ assign @ @ more rhetorical relation @ help discovering @ corresponding answer @ @ is @ first arabic question answering system @ attempt to answer @ @ @ @ and @ @ to @ question @ springer-verlag @ 
3399,A machine-learning approach to negation and speculation detection in clinical texts,"Detecting negative and speculative information is essential in most biomedical text-mining tasks where these language forms are used to express impressions, hypotheses, or explanations of experimental results. Our research is focused on developing a system based on machine-learning techniques that identifies negation and speculation signals and their scope in clinical texts. The proposed system works in two consecutive phases: first, a classifier decides whether each token in a sentence is a negation/speculation signal or not. Then another classifier determines, at sentence level, the tokens which are affected by the signals previously identified. The system was trained and evaluated on the clinical texts of the BioScope corpus, a freely available resource consisting of medical and biological texts: full-length articles, scientific abstracts, and clinical reports. The results obtained by our system were compared with those of two different systems, one based on regular expressions and the other based on machine learning. Our system's results outperformed the results obtained by these two systems. In the signal detection task, the F-score value was 97.3% in negation and 94.9% in speculation. In the scope-finding task, a token was correctly classified if it had been properly identified as being inside or outside the scope of all the negation signals present in the sentence. Our proposal showed an F score of 93.2% in negation and 80.9% in speculation. Additionally, the percentage of correct scopes (those with all their tokens correctly classified) was evaluated obtaining F scores of 90.9% in negation and 71.9% in speculation. © 2012 ASIS&T.",2012,Journal of the American Society for Information Science and Technology,28,detecting negative and speculative information is essential in @ biomedical text-mining task @ @ language form @ used to express impression hypothesis @ explanation of experimental @ @ @ research is focused on developing a system based on machine-learning technique @ identifies negation and speculation signal and @ scope in clinical text @ @ proposed system work in @ consecutive phase @ first a classifier decides whether @ token in a sentence is a negation speculation signal @ not @ @ another classifier determines at sentence level @ token @ @ affected by @ signal @ identified @ @ system wa trained and evaluated on @ clinical text of @ bioscope corpus a freely available resource consisting of medical and biological text @ full-length article scientific abstract and clinical report @ @ @ obtained by @ system @ compared @ @ of @ different system @ based on regular expression and @ @ based on machine learning @ @ system @ s @ outperformed @ @ obtained by @ @ system @ in @ signal detection task @ f-score value wa @ in negation and @ in speculation @ in @ scope-finding task a token wa correctly classified if @ @ @ properly identified a @ inside @ outside @ scope of @ @ negation signal @ in @ sentence @ @ proposal showed @ f score of @ in negation and @ in speculation @ additionally @ percentage of correct scope @ @ @ @ @ token correctly classified @ wa evaluated obtaining f score of @ in negation and @ in speculation @ asis t @ 
3400,Learning ontology relations from documents: The concept-feature method,"A domain ontology consists of two major components: A set of domain-specific concepts and a set of semantic-relations between these concepts. The semantic-relations, also called as ontology relations are used to support semantic retrieval and knowledge management in ontology-based applications. To reduce difficulties in manual ontology-building, this paper proposes a semantic-relation learning method for the purpose of automatically discovering relations between concepts. Given the set of domain-specific concepts and a domain corpus, the method firstly converts concepts into feature-vectors based on their local context and then calculates relevance degrees between each pair of concepts based on the similarity of their feature-vectors to discover related concepts. The method is compared with current state-of-the-art in two ways: (a) differences between learning results and the golden standard defined by domain experts, and (b) differences between learning results and the standard defined by the CNCTST (China National Committee for Terms in Sciences and Technologies). Experiments show that the proposed method is much better than currently existing ones, especially in term of recall rate, and has good potentials for applications such as ontology building, text mining and semantic retrieval.",2012,Xitong Gongcheng Lilun yu Shijian/System Engineering Theory and Practice,2,a domain ontology consists of @ major component @ a set of domain-specific concept and a set of semantic-relations @ @ concept @ @ semantic-relations @ called a ontology relation @ used to support semantic retrieval and knowledge management in ontology-based application @ to reduce difficulty in manual ontology-building @ @ proposes a semantic-relation learning method @ @ purpose of automatically discovering relation @ concept @ given @ set of domain-specific concept and a domain corpus @ method firstly convert concept @ feature-vectors based on @ local context and @ calculates relevance degree @ @ pair of concept based on @ similarity of @ feature-vectors to discover related concept @ @ method is compared @ current state-of-the-art in @ way @ @ a @ difference @ learning @ and @ golden standard defined by domain expert and @ b @ difference @ learning @ and @ standard defined by @ cnctst @ china national committee @ term in science and technology @ @ experiment @ @ @ proposed method is much better @ currently existing @ especially in term of recall rate and ha good potential @ application @ a ontology building text mining and semantic retrieval @ 
3404,Functional grouping of natural language requirements for assistance in architectural software design,"Modern software systems are becoming larger and more complex every day. One of the most challenging steps for designing a good architecture for a certain piece of software is the analysis of requirements, usually written in natural language by engineers not familiar with specific design formalisms. The main problem related to this task is the conceptual gap existing between low-level requirements and higher views of the system decomposing its functionality. In this paper, we introduce an approach for mining and grouping functionality from textual descriptions of requirements using text mining techniques aiming at helping software designers with this complex and time-consuming task. The knowledge discovered starting from informally written requirements using a combination of natural language processing (NLP) and text clustering algorithms can be then easily mapped into design concerns of a possible architecture for the system. Experimental validation in three case studies suggests a great potential of the proposed approach for providing assistance to software designers during early stages of the software development process. © 2011 Elsevier B.V. All rights reserved.",2012,Knowledge-Based Systems,22,modern software system @ becoming larger and more complex every day @ @ of @ @ challenging step @ designing a good architecture @ a certain piece of software is @ analysis of requirement usually written in natural language by engineer not familiar @ specific design formalism @ @ main problem related to @ task is @ conceptual gap existing @ low-level requirement and higher view of @ system decomposing @ functionality @ in @ @ @ introduce @ approach @ mining and grouping functionality @ textual description of requirement @ text mining technique aiming at helping software designer @ @ complex and time-consuming task @ @ knowledge discovered starting @ informally written requirement @ a combination of natural language processing @ nlp @ and text clustering algorithm @ @ @ easily mapped @ design concern of a possible architecture @ @ system @ experimental validation in three case study suggests a great potential of @ proposed approach @ providing assistance to software designer @ early stage of @ software development process @ @ b @ v @ @ right reserved @ 
3405,A survey of methods to ease the development of highly multilingual text mining applications,"Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines-most of all: extreme simplicity-can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that-to achieve this-the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools. © 2011 Springer Science+Business Media B.V.",2012,Language Resources and Evaluation,23,multilingual text processing is useful @ @ information content found in different language is complementary @ regarding fact and opinion @ @ information extraction and @ text mining software @ in principle @ developed @ many language @ text analysis tool @ only @ applied to small set of language @ @ development effort per language is @ @ self-training tool obviously alleviate @ problem @ even @ effort of providing training data and of manually tuning @ @ is usually considerable @ in @ @ @ gather insight by various multilingual system developer on @ to minimise @ effort of developing natural language processing application @ many language @ @ @ explain @ main guideline underlying @ @ effort to develop complex text mining software @ ten of language @ @ @ guidelines-most of @ @ extreme simplicity-can @ @ restrictive and limiting @ believe to @ @ @ feasibility of @ approach @ @ development of @ europe medium monitor @ emm @ family of application @ http @ emm @ newsbrief @ eu overview @ html @ @ emm is a set of complex medium monitoring tool @ process and analyse up to online news article per day in @ twenty and fifty language @ @ @ @ touch upon @ kind of language resource @ would make @ easier @ @ to develop highly multilingual text mining application @ @ @ argue that-to achieve this-the @ needed resource would @ freely available simple parallel and uniform multilingual dictionary corpus and software tool @ @ science @ medium b @ v @ 
3406,Using psycholinguistic features for profiling first language of authors,"This study empirically evaluates the effectiveness of different feature types for the classification of the first language of an author. In particular, it examines the utility of psycholinguistic features, extracted by the Linguistic Inquiry and Word Count (LIWC) tool, that have not previously been applied to the task of author profiling. As LIWC is a tool that has been developed in the psycholinguistic field rather than the computational linguistics field, it was hypothesized that it would be effective, both as a single type feature set because of its psycholinguistic basis, and in combination with other feature sets, because it should be sufficiently different to add insight rather than redundancy. It was found that LIWC features were competitive with previously used feature types in identifying the first language of an author, and that combined feature sets including LIWC features consistently showed better accuracy rates and average F measures than were achieved by the same feature sets without the LIWC features. As a secondary issue, this study also examined how effectively first language classification scaled up to a larger number of possible languages. It was found that the classification scheme scaled up effectively to the entire 16 language collection from the International Corpus of Learner English, when compared with results achieved on just 5 languages in previous research. © 2012 ASIS&T.",2012,Journal of the American Society for Information Science and Technology,8,@ study empirically evaluates @ effectiveness of different feature type @ @ classification of @ first language of @ author @ in particular @ examines @ utility of psycholinguistic feature extracted by @ linguistic inquiry and word count @ liwc @ tool @ @ not @ @ applied to @ task of author profiling @ a liwc is a tool @ ha @ developed in @ psycholinguistic field rather @ @ computational linguistics field @ wa hypothesized @ @ would @ effective @ a a single type feature set @ of @ psycholinguistic basis and in combination @ @ feature set @ @ @ @ sufficiently different to add insight rather @ redundancy @ @ wa found @ liwc feature @ competitive @ @ used feature type in identifying @ first language of @ author and @ combined feature set including liwc feature consistently showed better accuracy rate and average f measure @ @ achieved by @ @ feature set without @ liwc feature @ a a secondary issue @ study @ examined @ effectively first language classification scaled up to a larger number of possible language @ @ wa found @ @ classification scheme scaled up effectively to @ entire language collection @ @ international corpus of learner english @ compared @ @ achieved on @ language in previous research @ asis t @ 
3407,The ACODEA framework: Developing segmentation and classification schemes for fully automatic analysis of online discussions,"Research related to online discussions frequently faces the problem of analyzing huge corpora. Natural Language Processing (NLP) technologies may allow automating this analysis. However, the state-of-the-art in machine learning and text mining approaches yields models that do not transfer well between corpora related to different topics. Also, segmenting is a necessary step, but frequently, trained models are very sensitive to the particulars of the segmentation that was used when the model was trained. Therefore, in prior published research on text classification in a CSCL context, the data was segmented by hand. We discuss work towards overcoming these challenges. We present a framework for developing coding schemes optimized for automatic segmentation and context-independent coding that builds on this segmentation. The key idea is to extract the semantic and syntactic features of each single word by using the techniques of part-of-speech tagging and named-entity recognition before the raw data can be segmented and classified. Our results show that the coding on the micro-argumentation dimension can be fully automated. Finally, we discuss how fully automated analysis can enable context-sensitive support for collaborative learning. © 2012 International Society of the Learning Sciences, Inc.; Springer Science + Business Media, LLC.",2012,International Journal of Computer-Supported Collaborative Learning,57,research related to online discussion frequently face @ problem of analyzing huge corpus @ natural language processing @ nlp @ technology may allow automating @ analysis @ however @ state-of-the-art in machine learning and text mining approach yield model @ @ not transfer well @ corpus related to different topic @ @ segmenting is a necessary step @ frequently trained model @ @ sensitive to @ particular of @ segmentation @ wa used @ @ model wa trained @ therefore in prior published research on text classification in a cscl context @ data wa segmented by hand @ @ discus work towards overcoming @ challenge @ @ @ a framework @ developing coding scheme optimized @ automatic segmentation and context-independent coding @ build on @ segmentation @ @ key idea is to extract @ semantic and syntactic feature of @ single word by @ @ technique of part-of-speech tagging and named-entity recognition @ @ raw data @ @ segmented and classified @ @ @ @ @ @ coding on @ micro-argumentation dimension @ @ fully automated @ finally @ discus @ fully automated analysis @ enable context-sensitive support @ collaborative learning @ international society of @ learning science inc @ @ @ science @ medium llc @ 
3409,Automatic identification of protagonist in fairy tales using verb,"Named entity recognition (NER) has been a well-studied problem in the area of text mining for locating atomic element into predefined categories, where ""name of people"" is one of the most commonly studied categories. Numerous new NER techniques have been unfolded to accommodate the needs of the application developed. However, most research works carried out focused on non-fiction domain. Fiction domain exhibits complexity and uncertainty in locating protagonist as it represents name of person in a diverse spectrums, ranging from living things (animals, plants, person) to non-living things (vehicle, furniture). This paper proposes automated protagonist identification in fiction domain, particularly in fairy tales. Verb has been used as a determinant in substantiating the existence of protagonist with the assistance of WordNet. The experimental results show that it is viable to use verb in identifying named entity, particularly ""people"" category and it can be applied in a small text size environment. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,named entity recognition @ ner @ ha @ a well-studied problem in @ area of text mining @ locating atomic element @ predefined category @ @ name of people @ is @ of @ @ commonly studied category @ numerous @ ner technique @ @ unfolded to accommodate @ need of @ application developed @ however @ research work carried @ focused on non-fiction domain @ fiction domain exhibit complexity and uncertainty in locating protagonist a @ represents name of person in a diverse spectrum ranging @ living thing @ animal plant person @ to non-living thing @ vehicle furniture @ @ @ @ proposes automated protagonist identification in fiction domain particularly in fairy tale @ verb ha @ used a a determinant in substantiating @ existence of protagonist @ @ assistance of wordnet @ @ experimental @ @ @ @ is viable to use verb in identifying named entity particularly @ people @ category and @ @ @ applied in a small text size environment @ springer-verlag @ 
3412,Chinese patent classification based on sense disambiguation and manifold learning,"Currently, patent data have gained increasing attention in the data mining area. However, the traditional method was set by many factors in patent classification, such as professional terms and high dimensions. In this article, we propose a word sense disambiguation model, which would effectively reduce the structure of the traditional machine word. The experimental result shows that the semantic disambiguation and feature reduction strategy can effectively improve the classification accuracy. © 2011 by Binary Information Press.",2012,Journal of Computational Information Systems,0,currently patent data @ gained increasing attention in @ data mining area @ however @ traditional method wa set by many factor in patent classification @ a professional term and high dimension @ in @ article @ propose a word sense disambiguation model @ would effectively reduce @ structure of @ traditional machine word @ @ experimental @ @ @ @ semantic disambiguation and feature reduction strategy @ effectively improve @ classification accuracy @ by binary information @ @ 
3417,An ontology-based text-mining method to cluster proposals for research project selection,"Research project selection is an important task for government and private research funding agencies. When a large number of research proposals are received, it is common to group them according to their similarities in research disciplines. The grouped proposals are then assigned to the appropriate experts for peer review. Current methods for grouping proposals are based on manual matching of similar research discipline areas and/or keywords. However, the exact research discipline areas of the proposals cannot often be accurately designated by the applicants due to their subjective views and possible misinterpretations. Therefore, rich information in the proposals' full text can be used effectively. Text-mining methods have been proposed to solve the problem by automatically classifying text documents, mainly in English. However, these methods have limitations when dealing with non-English language texts, e.g., Chinese research proposals. This paper presents a novel ontology-based text-mining approach to cluster research proposals based on their similarities in research areas. The method is efficient and effective for clustering research proposals with both English and Chinese texts. The method also includes an optimization model that considers applicants' characteristics for balancing proposals by geographical regions. The proposed method is tested and validated based on the selection process at the National Natural Science Foundation of China. The results can also be used to improve the efficiency and effectiveness of research project selection processes in other government and private research funding agencies. © 2012 IEEE.",2012,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",52,research project selection is @ important task @ government and private research funding agency @ @ a @ number of research proposal @ received @ is common to group @ according to @ similarity in research discipline @ @ grouped proposal @ @ assigned to @ appropriate expert @ peer review @ current method @ grouping proposal @ based on manual matching of similar research discipline area and @ keywords @ however @ exact research discipline area of @ proposal cannot often @ accurately designated by @ applicant due to @ subjective view and possible misinterpretation @ therefore rich information in @ proposal @ full text @ @ used effectively @ text-mining method @ @ proposed to solve @ problem by automatically classifying text document mainly in english @ however @ method @ limitation @ dealing @ non-english language text e @ g @ chinese research proposal @ @ @ @ a novel ontology-based text-mining approach to cluster research proposal based on @ similarity in research area @ @ method is efficient and effective @ clustering research proposal @ @ english and chinese text @ @ method @ includes @ optimization model @ considers applicant @ characteristic @ balancing proposal by geographical region @ @ proposed method is tested and validated based on @ selection process at @ national natural science foundation of china @ @ @ @ @ @ used to improve @ efficiency and effectiveness of research project selection process in @ government and private research funding agency @ @ @ 
3418,Fuzzy ILP Classification of web reports after linguistic text mining,"In this paper we study the problem of classification of textual web reports. We are specifically focused on situations in which structured information extracted from the reports is used for classification. We present an experimental classification system based on usage of third party linguistic analyzers, our previous work on web information extraction, and fuzzy inductive logic programming (fuzzy ILP). A detailed study of the so-called 'Fuzzy ILP Classifier' is the main contribution of the paper. The study includes formal models, prototype implementation, extensive evaluation experiments and comparison of the classifier with other alternatives like decision trees, support vector machines, neural networks, etc. © 2011 Elsevier Ltd. All rights reserved.",2012,Information Processing and Management,2,in @ @ @ study @ problem of classification of textual web report @ @ @ specifically focused on situation in @ structured information extracted @ @ report is used @ classification @ @ @ @ experimental classification system based on usage of third party linguistic analyzer @ previous work on web information extraction and fuzzy inductive logic programming @ fuzzy ilp @ @ a detailed study of @ so-called @ fuzzy ilp classifier @ is @ main contribution of @ @ @ @ study includes formal model prototype implementation extensive evaluation experiment and comparison of @ classifier @ @ alternative like decision tree support vector machine neural network etc @ @ ltd @ @ right reserved @ 
3419,Movie rating and review summarization in mobile environment,"In this paper, we design and develop a movie-rating and review-summarization system in a mobile environment. The movie-rating information is based on the sentiment-classification result. The condensed descriptions of movie reviews are generated from the feature-based summarization. We propose a novel approach based on latent semantic analysis (LSA) to identify product features. Furthermore, we find a way to reduce the size of summary based on the product features obtained from LSA. We consider both sentiment-classification accuracy and system response time to design the system. The rating and review-summarization system can be extended to other product-review domains easily. © 2012 IEEE.",2012,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",71,in @ @ @ design and develop a movie-rating and review-summarization system in a mobile environment @ @ movie-rating information is based on @ sentiment-classification @ @ @ condensed description of movie review @ generated @ @ feature-based summarization @ @ propose a novel approach based on latent semantic analysis @ lsa @ to identify product feature @ furthermore @ find a way to reduce @ size of summary based on @ product feature obtained @ lsa @ @ consider @ sentiment-classification accuracy and system response time to design @ system @ @ rating and review-summarization system @ @ extended to @ product-review domain easily @ @ @ 
3420,An evaluation of classification models for question topic categorization,"We study the problem of question topic classification using a very large real-world Community Question Answering (CQA) dataset from Yahoo! Answers. The dataset comprises 3.9 million questions and these questions are organized into more than 1,000 categories in a hierarchy. To the best knowledge, this is the first systematic evaluation of the performance of different classification methods on question topic classification as well as short texts. Specifically, we empirically evaluate the following in classifying questions into CQA categories: (a) the usefulness of n-gram features and bag-of-word features; (b) the performance of three standard classification algorithms (naive Bayes, maximum entropy, and support vector machines); (c) the performance of the state-of-the-art hierarchical classification algorithms; (d) the effect of training data size on performance; and (e) the effectiveness of the different components of CQA data, including subject, content, asker, and the best answer. The experimental results show what aspects are important for question topic classification in terms of both effectiveness and efficiency. We believe that the experimental findings from this study will be useful in real-world classification problems. © 2012 ASIS&T.",2012,Journal of the American Society for Information Science and Technology,33,@ study @ problem of question topic classification @ a @ @ real-world community question answering @ cqa @ dataset @ yahoo @ answer @ @ dataset comprises @ million question and @ question @ organized @ more @ category in a hierarchy @ to @ best knowledge @ is @ first systematic evaluation of @ performance of different classification method on question topic classification a well a short text @ specifically @ empirically evaluate @ following in classifying question @ cqa category @ @ a @ @ usefulness of n-gram feature and bag-of-word feature @ @ b @ @ performance of three standard classification algorithm @ naive bayes maximum entropy and support vector machine @ @ @ c @ @ performance of @ state-of-the-art hierarchical classification algorithm @ @ @ @ @ effect of training data size on performance @ and @ e @ @ effectiveness of @ different component of cqa data including subject content asker and @ best answer @ @ experimental @ @ @ aspect @ important @ question topic classification in term of @ effectiveness and efficiency @ @ believe @ @ experimental finding @ @ study @ @ useful in real-world classification problem @ asis t @ 
3425,Ontology-driven construction of domain corpus with frame semantics annotations,"Semantic Role Labeling plays a key role in many text mining applications. The development of SRL systems for the biomedical domain is frustrated by the lack of large domain specific corpora that are labeled with semantic roles. In this paper we proposed a method for building corpus that are labeled with semantic roles for the domain of biomedicine. The method is based on the theory of frame semantics, and uses domain knowledge provided by ontologies. By using the method, we have built a corpus for transport events strictly following the domain knowledge provided by GO biological process ontology. We compared one of our frames to a BioFrameNet frame. We also examined the gaps between the semantic classification of the target words in this domain-specific corpus and in FrameNet and PropBank/VerbNet data. The successful corpus construction demonstrates that ontologies, as a formal representation of domain knowledge, can instruct us and ease all the tasks in building this kind of corpus. Furthermore, ontological domain knowledge leads to well-defined semantics exposed on the corpus, which will be very valuable in text mining applications. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,semantic role labeling play a key role in many text mining application @ @ development of srl system @ @ biomedical domain is frustrated by @ lack of @ domain specific corpus @ @ labeled @ semantic role @ in @ @ @ proposed a method @ building corpus @ @ labeled @ semantic role @ @ domain of biomedicine @ @ method is based on @ theory of frame semantics and us domain knowledge provided by ontology @ by @ @ method @ @ built a corpus @ transport event strictly following @ domain knowledge provided by go biological process ontology @ @ compared @ of @ frame to a bioframenet frame @ @ @ examined @ gap @ @ semantic classification of @ target word in @ domain-specific corpus and in framenet and propbank verbnet data @ @ successful corpus construction demonstrates @ ontology a a formal representation of domain knowledge @ instruct u and ease @ @ task in building @ kind of corpus @ furthermore ontological domain knowledge lead to well-defined semantics exposed on @ corpus @ @ @ @ valuable in text mining application @ springer-verlag @ 
3426,Using wikipedia anchor text and weighted clustering coefficient to enhance the traditional multi-document summarization,"Similar to the traditional approach, we consider the task of summarization as selection of top ranked sentences from ranked sentence-clusters. To achieve this goal, we rank the sentence clusters by using the importance of words calculated by using page rank algorithm on reverse directed word graph of sentences. Next, to rank the sentences in every cluster we introduce the use of weighted clustering coefficient. We use page rank score of words for calculation of weighted clustering coefficient. Finally the most important issue is the presence of a lot of noisy entries in the text, which downgrades the performance of most of the text mining algorithms. To solve this problem, we introduce the use of Wikipedia anchor text based phrase mapping scheme. Our experimental results on DUC-2002 and DUC-2004 dataset show that our system performs better than unsupervised systems and better than/comparable with novel supervised systems of this area. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,similar to @ traditional approach @ consider @ task of summarization a selection of top ranked sentence @ ranked sentence-clusters @ to achieve @ goal @ rank @ sentence cluster by @ @ importance of word calculated by @ page rank algorithm on reverse directed word graph of sentence @ next to rank @ sentence in every cluster @ introduce @ use of weighted clustering coefficient @ @ use page rank score of word @ calculation of weighted clustering coefficient @ finally @ @ important issue is @ presence of a lot of noisy entry in @ text @ downgrade @ performance of @ of @ text mining algorithm @ to solve @ problem @ introduce @ use of wikipedia anchor text based phrase mapping scheme @ @ experimental @ on duc and duc dataset @ @ @ system performs better @ unsupervised system and better @ comparable @ novel supervised system of @ area @ springer-verlag @ 
3427,A novel text - Mining system for generating abstract from extracted summaries using Anaphora resolution,"The amount of information available varies in length from onedocument to another. It becomes difficult and time-consuming activity to browse the information completely. It is essential to provide the information in a condensed form expressing the central idea of the document. Automatic text summarization is used for generating the summary for the document. This paper presents a novel Abstract Generation System (AGS) to generate an abstract from the extracted summary of an English language text document. The pronominal Anaphora Resolution (AR) Algorithm has been designed and used in AGS for scrutinizing and resolving the anaphors, the referring expressions present in the extract to make the summary more readable. AGS finally generates a fine-tuned summary for the given document. The experiments are conducted using a test set taken from the trained corpus, in AGS and other existing Anaphora Resolution Systems (ARS). The results are compared with the model summary written by human beings. The standard metric of Information Extraction (IE) systems namely the success rate has been used to measure and study the performance of AGS. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ amount of information available varies in length @ onedocument to another @ @ becomes difficult and time-consuming activity to browse @ information completely @ @ is essential to provide @ information in a condensed form expressing @ central idea of @ document @ automatic text summarization is used @ generating @ summary @ @ document @ @ @ @ a novel abstract generation system @ ag @ to generate @ abstract @ @ extracted summary of @ english language text document @ @ pronominal anaphora resolution @ ar @ algorithm ha @ designed and used in ag @ scrutinizing and resolving @ anaphor @ referring expression @ in @ extract to make @ summary more readable @ ag finally generates a fine-tuned summary @ @ given document @ @ experiment @ conducted @ a test set taken @ @ trained corpus in ag and @ existing anaphora resolution system @ ar @ @ @ @ @ compared @ @ model summary written by human @ @ @ standard metric of information extraction @ ie @ system namely @ success rate ha @ used to measure and study @ performance of ag @ springer-verlag @ 
3428,Semantic clustering of scientific articles with use of DBpedia knowledge base,"A case study of semantic clustering of scientific articles related to Rough Sets is presented. The proposed method groups the documents on the basis of their content and with assistance of DBpedia knowledge base. The text corpus is first treated with Natural Language Processing tools in order to produce vector representations of the content and then matched against a collection of concepts retrieved from DBpedia. As a result, a new representation is constructed that better reflects the semantics of the texts. With this new representation, the documents are hierarchically clustered in order to form partition of papers that share semantic relatedness. The steps in textual data preparation, utilization of DBpedia and clustering are explained and illustrated with experimental results. Assessment of clustering quality by human experts and by comparison to traditional approach is presented. © 2012 Springer-Verlag GmbH Berlin Heidelberg.",2012,Studies in Computational Intelligence,9,a case study of semantic clustering of scientific article related to rough set is presented @ @ proposed method group @ document on @ basis of @ content and @ assistance of dbpedia knowledge base @ @ text corpus is first treated @ natural language processing tool in order to produce vector representation of @ content and @ matched @ a collection of concept retrieved @ dbpedia @ a a @ a @ representation is constructed @ better reflects @ semantics of @ text @ @ @ @ representation @ document @ hierarchically clustered in order to form partition of @ @ share semantic relatedness @ @ step in textual data preparation utilization of dbpedia and clustering @ explained and illustrated @ experimental @ @ assessment of clustering quality by human expert and by comparison to traditional approach is presented @ springer-verlag gmbh @ @ @ 
3431,Exploring useful features and kernel combinations from dependency graph for protein-protein interactions extraction,"Automatic extraction of protein-protein interactions (PPI) has become a critical task in the field of biomedical text mining due to the dynamic progress in biomedical technology. Kernel-based machine learning methods have been widely used to extract PPI automatically from biomedical literature, and different kernels focusing on different parts of sentence structure have been proposed for the PPI extraction task. In this paper, we present a method to combine useful features explored from dependency graph for PPI extraction. Based on the shortest path between proteins in the dependency graph, a new distance feature, SentenceDistance, is proposed and used in feature-based kernel, which contributes to the improvement of the performance. Further, we combine the feature-based kernel with graph and walk-weighted subsequence kernels, obtaining 0.624 F-Score and 0.872 AUC on the AIMed corpus, which is comparable with the state-of-the-art approaches. 1553-9105/Copyright © 2012 Binary Information Press.",2012,Journal of Computational Information Systems,3,automatic extraction of protein-protein interaction @ ppi @ ha become a critical task in @ field of biomedical text mining due to @ dynamic progress in biomedical technology @ kernel-based machine learning method @ @ widely used to extract ppi automatically @ biomedical literature and different kernel focusing on different part of sentence structure @ @ proposed @ @ ppi extraction task @ in @ @ @ @ a method to combine useful feature explored @ dependency graph @ ppi extraction @ based on @ shortest path @ protein in @ dependency graph a @ distance feature sentencedistance is proposed and used in feature-based kernel @ contributes to @ improvement of @ performance @ @ @ combine @ feature-based kernel @ graph and walk-weighted subsequence kernel obtaining @ f-score and @ auc on @ aimed corpus @ is comparable @ @ state-of-the-art approach @ @ binary information @ @ 
3433,Building news sentiment indicators for stock marketing application,"It is claimed that online news holds valuable contents of information to some extent; as a matter of fact, it reflects the polarity of investigators from time to time. Especially, such information in the stock market outlines an event usually hold on to unobserved effects, e.g. law problem, which might not be immediately and completely price-revealed. Instead of traditional pure price-based indicators, this paper is to apply some sentiment technical indicators at hand into working out a news-based box through the fuzzy design. It pays to notice that the sentiment indicators automatically come out from the contents of online news. It comes out a weight on keywords in text through a combination of natural language processing techniques and membership functions together. For example, given a topic of target firm, the box collects news and mines patterns and then infers the degree of sentiment. To examine the feasibility of such concept, an experiment is built to test the rate of accuracy, using the data fully quoted on the online news took place for 3 month long. It results that the proposed fuzzy inference algorithm achieve 88% rate of accuracy in inferring the polarity of the whole news. This method is also feasible for the other languages, e.g. English and Japanese.",2012,International Journal of Advancements in Computing Technology,5,@ is claimed @ online news hold valuable content of information to some extent @ a a matter of fact @ reflects @ polarity of investigator @ time to time @ especially @ information in @ stock market outline @ event usually hold on to unobserved effect e @ g @ law problem @ might not @ immediately and completely price-revealed @ instead of traditional pure price-based indicator @ @ is to apply some sentiment technical indicator at hand @ working @ a news-based box @ @ fuzzy design @ @ pay to notice @ @ sentiment indicator automatically come @ @ @ content of online news @ @ come @ a weight on keywords in text @ a combination of natural language processing technique and membership function together @ @ example given a topic of target firm @ box collect news and mine pattern and @ infers @ degree of sentiment @ to examine @ feasibility of @ concept @ experiment is built to test @ rate of accuracy @ @ data fully quoted on @ online news took place @ month long @ @ @ @ @ proposed fuzzy inference algorithm achieve rate of accuracy in inferring @ polarity of @ whole news @ @ method is @ feasible @ @ @ language e @ g @ english and japanese @ 
3436,Athena: Text mining based discovery of scientific workflows in disperse repositories,"Scientific workflows are abstractions used to model and execute in silico scientific experiments. They represent key resources for scientists and are enacted and managed by engines called Scientific Workflow Management Systems (SWfMS). Each SWfMS has a particular workflow language. This heterogeneity of languages and formats poses as complex scenario for scientists to search or discover workflows in distributed repositories for reuse. The existing workflows in these repositories can be used to leverage the identification and construction of families of workflows (clusters) that aim at a particular goal. However it is hard to compare the structure of these workflows since they are modeled in different formats. One alternative way is to compare workflow metadata such as natural language descriptions (usually found in workflow repositories) instead of comparing workflow structure. In this scenario, we expect that the effective use of classical text mining techniques can cluster a set of workflows in families, offering to the scientists the possibility of finding and reusing existing workflows, which may decrease the complexity of modeling a new experiment. This paper presents Athena, a cloud-based approach to support workflow clustering from disperse repositories using their natural language descriptions, thus integrating these repositories and providing a facilitated form to search and reuse workflows. © 2012 Springer-Verlag.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,scientific workflow @ abstraction used to model and execute in silico scientific experiment @ @ represent key resource @ scientist and @ enacted and managed by engine called scientific workflow management system @ swfms @ @ @ swfms ha a particular workflow language @ @ heterogeneity of language and format pose a complex scenario @ scientist to search @ discover workflow in distributed repository @ reuse @ @ existing workflow in @ repository @ @ used to leverage @ identification and construction of family of workflow @ cluster @ @ aim at a particular goal @ however @ is hard to compare @ structure of @ workflow since @ @ modeled in different format @ @ alternative way is to compare workflow metadata @ a natural language description @ usually found in workflow repository @ instead of comparing workflow structure @ in @ scenario @ expect @ @ effective use of classical text mining technique @ cluster a set of workflow in family offering to @ scientist @ possibility of finding and reusing existing workflow @ may decrease @ complexity of modeling a @ experiment @ @ @ @ athena a cloud-based approach to support workflow clustering @ disperse repository @ @ natural language description thus integrating @ repository and providing a facilitated form to search and reuse workflow @ springer-verlag @ 
3440,A text mining approach as baseline for QA4MRE'12,"This paper describes the participation of the KU Leuven DTAI team in the pilot task on machine reading of biomedical texts about the Alzheimer disease, which is part of the 2012 Question Answering for Machine Reading Evaluation campaign (QA4MRE'12). The main objective of our research was to develop a text mining system as a strong baseline for the task. Based on the outcome of this system, we want to investigate which types of questions can be answered based solely on the input text and the question string, and for which ones we need more advanced techniques that also consider the previously acquired background knowledge from the reference document collection. Furthermore this should give us some insights into the system behavior for specific question types and background information for the development of a tailored inference algorithm.",2012,CEUR Workshop Proceedings,1,@ @ describes @ participation of @ ku leuven dtai team in @ pilot task on machine reading of biomedical text @ @ alzheimer disease @ is part of @ question answering @ machine reading evaluation campaign @ qa mre @ @ @ @ main objective of @ research wa to develop a text mining system a a strong baseline @ @ task @ based on @ outcome of @ system @ want to investigate @ type of question @ @ answered based solely on @ input text and @ question string and @ @ @ @ need more advanced technique @ @ consider @ @ acquired background knowledge @ @ reference document collection @ furthermore @ @ give u some insight @ @ system behavior @ specific question type and background information @ @ development of a tailored inference algorithm @ 
3441,Sentiment analysis and opinion mining,"Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Copyright © 2012 by Morgan & Claypool.",2012,Synthesis Lectures on Human Language Technologies,2034,sentiment analysis and opinion mining is @ field of study @ analyzes people @ s opinion sentiment evaluation attitude and emotion @ written language @ @ is @ of @ @ active research area in natural language processing and is @ widely studied in data mining web mining and text mining @ in fact @ research ha spread outside of computer science to @ management science and social science due to @ importance to @ and society a a whole @ @ growing importance of sentiment analysis coincides @ @ growth of social medium @ a review forum discussion blog micro-blogs twitter and social network @ @ @ first time in human history @ now @ a huge volume of opinionated data recorded in digital form @ analysis @ sentiment analysis system @ @ applied in almost every @ and social domain @ opinion @ central to almost @ human activity and @ key influencers of @ behavior @ @ belief and perception of reality and @ choice @ make @ largely conditioned on @ others see and evaluate @ world @ @ @ reason @ @ need to make a decision @ often seek @ @ opinion of others @ @ is true not only @ individual @ @ @ organization @ @ book is a comprehensive introductory and survey text @ @ cover @ important topic and @ latest development in @ field @ @ reference @ @ is suitable @ student researcher and practitioner @ @ interested in social medium analysis in general and sentiment analysis in particular @ lecturer @ readily use @ in class @ course on natural language processing social medium analysis text mining and data mining @ @ by morgan claypool @ 
3443,Using biomedical databases as knowledge sources for large-scale text mining,"In this paper we discuss how terminological knowledge extracted from biomedical databases can be used effectively in large-scale processing of the biomedical literature. We briefly present an integrated information extraction and text mining environment which is capable of reliably identifying and disambiguating several categories of relevant domain entities, which can then constitute relevant indexing entries in order to allow efficient retrieval of relevant documents and passages. Additionally the system generates ranked lists of candidate interactions among the detected entities, which can be useful for several purposes, from assisted literature curation to question answering systems.",2012,CEUR Workshop Proceedings,0,in @ @ @ discus @ terminological knowledge extracted @ biomedical database @ @ used effectively in large-scale processing of @ biomedical literature @ @ briefly @ @ integrated information extraction and text mining environment @ is capable of reliably identifying and disambiguating several category of relevant domain entity @ @ @ constitute relevant indexing entry in order to allow efficient retrieval of relevant document and passage @ additionally @ system generates ranked list of candidate interaction among @ detected entity @ @ @ useful @ several purpose @ assisted literature curation to question answering system @ 
3446,Biomedical text mining about Alzheimer's diseases for Machine Reading evaluation,"The paper presents the experiments carried out as part of the participation in the pilot task of Biomedical about Alzheimer for QA4MRE at CLEF 2012. We have submitted total five unique runs in the pilot task. One run uses Term Frequency (TF) of the query words to weight the sentence. Two runs use Term Frequency-Inverted Document Frequency (TF-IDF) of the query words to weight the sentences. The two unique runs differ in the way that when multiple answers get the same scores by our system, we choose the different answer in the different runs. The last two runs use TF or TF-IDF weighting scheme as well as the OMIM terms about Alzheimer for query expansion. Stopwords are removed from the query words and answers. Each sentence in the associated document is assigned a weighting score with respect to query words. The sentence that receives the higher weighting score corresponding to the query words is identified as the more relevant sentence to the document. The corresponding answer option to the given question is scored according to the sentence weighting score and the highest ranked answer is selected as the final answer.",2012,CEUR Workshop Proceedings,1,@ @ @ @ experiment carried @ a part of @ participation in @ pilot task of biomedical @ alzheimer @ qa mre at clef @ @ @ submitted total five unique run in @ pilot task @ @ run us term frequency @ tf @ of @ query word to weight @ sentence @ @ run use term frequency-inverted document frequency @ tf-idf @ of @ query word to weight @ sentence @ @ @ unique run differ in @ way @ @ multiple answer get @ @ score by @ system @ choose @ different answer in @ different run @ @ last @ run use tf @ tf-idf weighting scheme a well a @ omim term @ alzheimer @ query expansion @ stopwords @ removed @ @ query word and answer @ @ sentence in @ associated document is assigned a weighting score @ respect to query word @ @ sentence @ receives @ higher weighting score corresponding to @ query word is identified a @ more relevant sentence to @ document @ @ corresponding answer option to @ given question is scored according to @ sentence weighting score and @ highest ranked answer is selected a @ final answer @ 
3447,How to link ontologies and protein-protein interactions to literature: Text-mining approaches and the BioCreative experience,"There is an increasing interest in developing ontologies and controlled vocabularies to improve the efficiency and consistency of manual literature curation, to enable more formal biocuration workflow results and ultimately to improve analysis of biological data. Two ontologies that have been successfully used for this purpose are the Gene Ontology (GO) for annotating aspects of gene products and the Molecular Interaction ontology (PSI-MI) used by databases that archive protein-protein interactions. The examination of protein interactions has proven to be extremely promising for the understanding of cellular processes. Manual mapping of information from the biomedical literature to bio-ontology terms is one of the most challenging components in the curation pipeline. It requires that expert curators interpret the natural language descriptions contained in articles and infer their semantic equivalents in the ontology (controlled vocabulary). Since manual curation is a time-consuming process, there is strong motivation to implement text-mining techniques to automatically extract annotations from free text. A range of text mining strategies has been devised to assist in the automated extraction of biological data. These strategies either recognize technical terms used recurrently in the literature and propose them as candidates for inclusion in ontologies, or retrieve passages that serve as evidential support for annotating an ontology term, e.g. from the PSI-MI or GO controlled vocabularies. Here, we provide a general overview of current text-mining methods to automatically extract annotations of GO and PSI-MI ontology terms in the context of the BioCreative (Critical Assessment of Information Extraction Systems in Biology) challenge. Special emphasis is given to protein-protein interaction data and PSI-MI terms referring to interaction detection methods. © The Author(s) 2012.",2012,Database,22,@ is @ increasing interest in developing ontology and controlled vocabulary to improve @ efficiency and consistency of manual literature curation to enable more formal biocuration workflow @ and ultimately to improve analysis of biological data @ @ ontology @ @ @ successfully used @ @ purpose @ @ gene ontology @ go @ @ annotating aspect of gene product and @ molecular interaction ontology @ psi-mi @ used by database @ archive protein-protein interaction @ @ examination of protein interaction ha proven to @ extremely promising @ @ understanding of cellular process @ manual mapping of information @ @ biomedical literature to bio-ontology term is @ of @ @ challenging component in @ curation pipeline @ @ requires @ expert curator interpret @ natural language description contained in article and infer @ semantic equivalent in @ ontology @ controlled vocabulary @ @ since manual curation is a time-consuming process @ is strong motivation to implement text-mining technique to automatically extract annotation @ free text @ a range of text mining strategy ha @ devised to assist in @ automated extraction of biological data @ @ strategy either recognize technical term used recurrently in @ literature and propose @ a candidate @ inclusion in ontology @ retrieve passage @ serve a evidential support @ annotating @ ontology term e @ g @ @ @ psi-mi @ go controlled vocabulary @ @ @ provide a general overview of current text-mining method to automatically extract annotation of go and psi-mi ontology term in @ context of @ biocreative @ critical assessment of information extraction system in biology @ challenge @ special emphasis is given to protein-protein interaction data and psi-mi term referring to interaction detection method @ @ author @ s @ @ 
3455,A semi-automatic approach to construct vietnamese ontology from online text,"An ontology is an effective formal representation of knowledge used commonly in artificial intelligence, semantic web, software engineering, and information retrieval. In open and distance learning, ontologies are used as knowledge bases for e-learning supplements, educational recommenders, and question answering systems that support students with much needed resources. In such systems, ontology construction is one of the most important phases. Since there are abundant documents on the Internet, useful learning materials can be acquired openly with the use of an ontology. However, due to the lack of system support for ontology construction, it is difficult to construct self-instructional materials for Vietnamese people. In general, the cost of manual acquisition of ontologies from domain documents and expert knowledge is too high. Therefore, we present a support system for Vietnamese ontology construction using pattern-based mechanisms to discover Vietnamese concepts and conceptual relations from Vietnamese text documents. In this system, we use the combination of statistics-based, data mining, and Vietnamese natural language processing methods to develop concept and conceptual relation extraction algorithms to discover knowledge from Vietnamese text documents. From the experiments, we show that our approach provides a feasible solution to build Vietnamese ontologies used for supporting systems in education.",2012,International Review of Research in Open and Distance Learning,5,@ ontology is @ effective formal representation of knowledge used commonly in artificial intelligence semantic web software engineering and information retrieval @ in open and distance learning ontology @ used a knowledge base @ e-learning supplement educational recommenders and question answering system @ support student @ much needed resource @ in @ system ontology construction is @ of @ @ important phase @ since @ @ abundant document on @ internet useful learning material @ @ acquired openly @ @ use of @ ontology @ however due to @ lack of system support @ ontology construction @ is difficult to construct self-instructional material @ vietnamese people @ in general @ cost of manual acquisition of ontology @ domain document and expert knowledge is too high @ therefore @ @ a support system @ vietnamese ontology construction @ pattern-based mechanism to discover vietnamese concept and conceptual relation @ vietnamese text document @ in @ system @ use @ combination of statistics-based data mining and vietnamese natural language processing method to develop concept and conceptual relation extraction algorithm to discover knowledge @ vietnamese text document @ @ @ experiment @ @ @ @ approach provides a feasible solution to build vietnamese ontology used @ supporting system in education @ 
3456,TSCAN: A content anatomy approach to temporal topic summarization,"A topic is defined as a seminal event or activity along with all directly related events and activities. It is represented by a chronological sequence of documents published by different authors on the Internet. In this study, we define a task called topic anatomy, which summarizes and associates the core parts of a topic temporally so that readers can understand the content easily. The proposed topic anatomy model, called TSCAN, derives the major themes of a topic from the eigenvectors of a temporal block association matrix. Then, the significant events of the themes and their summaries are extracted by examining the constitution of the eigenvectors. Finally, the extracted events are associated through their temporal closeness and context similarity to form an evolution graph of the topic. Experiments based on the official TDT4 corpus demonstrate that the generated temporal summaries present the storylines of topics in a comprehensible form. Moreover, in terms of content coverage, coherence, and consistency, the summaries are superior to those derived by existing summarization methods based on human-composed reference summaries. © 2011 IEEE.",2012,IEEE Transactions on Knowledge and Data Engineering,24,a topic is defined a a seminal event @ activity along @ @ directly related event and activity @ @ is represented by a chronological sequence of document published by different author on @ internet @ in @ study @ define a task called topic anatomy @ summarizes and associate @ core part of a topic temporally @ @ reader @ understand @ content easily @ @ proposed topic anatomy model called tscan derives @ major theme of a topic @ @ eigenvectors of a temporal block association matrix @ @ @ significant event of @ theme and @ summary @ extracted by examining @ constitution of @ eigenvectors @ finally @ extracted event @ associated @ @ temporal closeness and context similarity to form @ evolution graph of @ topic @ experiment based on @ official tdt corpus demonstrate @ @ generated temporal summary @ @ storyline of topic in a comprehensible form @ moreover in term of content coverage coherence and consistency @ summary @ superior to @ derived by existing summarization method based on human-composed reference summary @ @ @ 
3458,Initial results in the development of SCAN: A swedish clinical abbreviation normalizer,"Abbreviations are common in clinical documentation, as this type of text is written under time-pressure and serves mostly for internal communica-tion. This study attempts to apply and extend existing rule-based algorithms that have been developed for English and Swedish abbreviation detection, in order to create an abbreviation detection algorithm for Swedish clinical texts that can identify and suggest definitions for abbreviations and acronyms. This can be used as a pre-processing step for further information extraction and text mining models, as well as for readability solutions. Through a literature review, a number of heuristics were defined for automat-ic abbreviation detection. These were used in the construction of the Swedish Clinical Abbreviation Normalizer (SCAN). The heuristics were: a) freely avail-able external resources: a dictionary of general Swedish, a dictionary of medical terms and a dictionary of known Swedish medical abbreviations, b) maximum word lengths (from three to eight characters), and c) heuristics for handling common patterns such as hyphenation. For each token in the text, the algorithm checks whether it is a known word in one of the lexicons, and whether it fulfills the criteria for word length and the created heuristics. The final algorithm was evaluated on a set of 300 Swedish clinical notes from an emergency department at the Karolinska University Hospital, Stockholm. These notes were annotated for abbreviations, a total of 2,050 tokens. This set was annotated by a physician accustomed to reading and writing medical records. The algorithm was tested in different variants, where the word lists were modified, heuristics adapted to characteristics found in the texts, and different combinations of word lengths. The best performing version of the algorithm achieved an F-Measure score of 79%, with 76% recall and 81% precision, which is a considerable improvement over the baseline where each token was only matched against the word lists (51% F-measure, 87% recall, 36% preci-sion). Not surprisingly, precision results are higher when the maximum word length is set to the lowest (three), and recall results higher when it is set to the highest (eight). Algorithms for rule-based systems, mainly developed for English, can be successfully adapted for abbreviation detection in Swedish medical records. System performance relies heavily on the quality of the external resources, as well as on the created heuristics. In order to improve results, part-of-speech in- formation and/or local context is needed for disambiguation. In the case of Swedish, compounding also needs to be handled.",2012,CEUR Workshop Proceedings,0,abbreviation @ common in clinical documentation a @ type of text is written @ time-pressure and serf mostly @ internal communica-tion @ @ study attempt to apply and extend existing rule-based algorithm @ @ @ developed @ english and swedish abbreviation detection in order to create @ abbreviation detection algorithm @ swedish clinical text @ @ identify and suggest definition @ abbreviation and acronym @ @ @ @ used a a pre-processing step @ @ information extraction and text mining model a well a @ readability solution @ @ a literature review a number of heuristic @ defined @ automat-ic abbreviation detection @ @ @ used in @ construction of @ swedish clinical abbreviation normalizer @ scan @ @ @ heuristic @ @ a @ freely avail-able external resource @ a dictionary of general swedish a dictionary of medical term and a dictionary of known swedish medical abbreviation b @ maximum word length @ @ three to eight character @ and c @ heuristic @ handling common pattern @ a hyphenation @ @ @ token in @ text @ algorithm check whether @ is a known word in @ of @ lexicon and whether @ fulfills @ criterion @ word length and @ created heuristic @ @ final algorithm wa evaluated on a set of swedish clinical note @ @ emergency department at @ karolinska university hospital stockholm @ @ note @ annotated @ abbreviation a total of token @ @ set wa annotated by a physician accustomed to reading and writing medical record @ @ algorithm wa tested in different variant @ @ word list @ modified heuristic adapted to characteristic found in @ text and different combination of word length @ @ best performing version of @ algorithm achieved @ f-measure score of @ recall and precision @ is a considerable improvement @ @ baseline @ @ token wa only matched @ @ word list @ f-measure recall preci-sion @ @ not surprisingly precision @ @ higher @ @ maximum word length is set to @ lowest @ three @ and recall @ higher @ @ is set to @ highest @ eight @ @ algorithm @ rule-based system mainly developed @ english @ @ successfully adapted @ abbreviation detection in swedish medical record @ system performance relies heavily on @ quality of @ external resource a well a on @ created heuristic @ in order to improve @ part-of-speech in formation and @ local context is needed @ disambiguation @ in @ case of swedish compounding @ need to @ handled @ 
3460,Improving information retrieval using document clusters and semantic synonym extraction,"Document clustering has been investigated for use in a number of different areas of text mining and information retrieval. Initially, document clustering was investigated for improving the precision or recall in information retrieval systems and as an efficient way of finding the nearest neighbors of a document. More recently, clustering has been proposed for use in browsing a collection of documents or in organizing the results returned by a search engine in response to a user's query. This paper presents a new semantic synonym based correlation indexing method in which documents are clustered based on nearest neighbors from the document collection and then further refined by semantically relating the query term with the retrieved documents by making use of a thesaurus or ontology model to improve the performance of Information Retrieval System (IRS) by increasing the number of relevant documents retrieved. Results show that the proposed method achieves significant improvement than the existing methods and may generate the more relevant document in the top rank. © 2005 - 2012 JATIT & LLS. All rights reserved.",2012,Journal of Theoretical and Applied Information Technology,3,document clustering ha @ investigated @ use in a number of different area of text mining and information retrieval @ initially document clustering wa investigated @ improving @ precision @ recall in information retrieval system and a @ efficient way of finding @ nearest neighbor of a document @ more recently clustering ha @ proposed @ use in browsing a collection of document @ in organizing @ @ returned by a search engine in response to a user @ s query @ @ @ @ a @ semantic synonym based correlation indexing method in @ document @ clustered based on nearest neighbor @ @ document collection and @ @ refined by semantically relating @ query term @ @ retrieved document by making use of a thesaurus @ ontology model to improve @ performance of information retrieval system @ irs @ by increasing @ number of relevant document retrieved @ @ @ @ @ proposed method achieves significant improvement @ @ existing method and may generate @ more relevant document in @ top rank @ jatit lls @ @ right reserved @ 
3462,Uncertainty reduction for knowledge discovery and information extraction on the world wide web,"In this paper, we give an overview of knowledge discovery (KD) and information extraction (IE) techniques on the World Wide Web (WWW). We intend to answer the following questions: What kind of additional uncertainty challenges are introduced by the WWW setting to basic KD and IE techniques? What are the fundamental techniques that can be used to reduce such uncertainty and achieve reasonable KD and IE performance on the WWW? What is the impact of each novel method? What types of interactions can be conducted between these techniques and information networks to make them benefit from each other? In what way can we utilize the results in more interesting applications? What are the remaining challenges and what are the possible ways to address these challenges? We hope this can provide a road map to advance KD and IE on the WWW to a higher level of performance, portability and utilization. © 2012 IEEE.",2012,Proceedings of the IEEE,12,in @ @ @ give @ overview of knowledge discovery @ kd @ and information extraction @ ie @ technique on @ world wide web @ www @ @ @ intend to answer @ following question @ @ kind of additional uncertainty challenge @ introduced by @ www setting to basic kd and ie technique @ @ @ @ fundamental technique @ @ @ used to reduce @ uncertainty and achieve reasonable kd and ie performance on @ www @ @ is @ impact of @ novel method @ @ type of interaction @ @ conducted @ @ technique and information network to make @ benefit @ @ @ @ in @ way @ @ utilize @ @ in more interesting application @ @ @ @ remaining challenge and @ @ @ possible way to address @ challenge @ @ hope @ @ provide a road map to advance kd and ie on @ www to a higher level of performance portability and utilization @ @ @ 
3463,Enriching domain-specific language models using domain independent WWW n-gram corpus,This paper describes the new techniques developed to extract and compute the domain-specific knowledge implicitly embedded in a highly structural ontology-based information system for TV Electronic Programming Guide (EPG). The domain knowledge represented by a set of mutually related n-gram data sets is then enriched by exploring the explicit structural dependencies and implicit semantic association between the data entities in the domain and the domain-independent texts from the Google 1 trillion 5-grams corpus created from general WWW documents. The knowledge-based enrichment process creates the language models required for a natural language based EPG search system that outperform the baseline model created only from the original EPG data source by a significant margin measured by an absolute improvement of 14.1% on the model coverage (recall accuracy) using large-scale test data collected from a real-world EPG search application. © 2012 Springer-Verlag Berlin Heidelberg.,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ describes @ @ technique developed to extract and compute @ domain-specific knowledge implicitly embedded in a highly structural ontology-based information system @ tv electronic programming guide @ epg @ @ @ domain knowledge represented by a set of mutually related n-gram data set is @ enriched by exploring @ explicit structural dependency and implicit semantic association @ @ data entity in @ domain and @ domain-independent text @ @ google trillion gram corpus created @ general www document @ @ knowledge-based enrichment process creates @ language model required @ a natural language based epg search system @ outperform @ baseline model created only @ @ original epg data source by a significant margin measured by @ absolute improvement of @ on @ model coverage @ recall accuracy @ @ large-scale test data collected @ a real-world epg search application @ springer-verlag @ @ @ 
3471,Named entity matching in publication databases⋆: A case study of pubmed in SONCA,"We present a case study in approximate data matching for a database system that contains information about scientific publications. The approximate matching process is meant to identify whether several records in the database are in fact repeated instances of the same real-world object. In our case study we are concerned with matching instances of objects such as XML documents, persons’ names, affiliations, journal names, and so on. The particular data we are dealing with is a representation of the PubMed Central document corpus within the data warehouse that is a part of the SONCA system. SONCA system is being developed as one of components of the general scientific information platform SYNAT. © Springer-Verlag Berlin Heidelberg 2012.",2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ a case study in approximate data matching @ a database system @ contains information @ scientific publication @ @ approximate matching process is meant to identify whether several record in @ database @ in fact repeated instance of @ @ real-world object @ in @ case study @ @ concerned @ matching instance of object @ a xml document person name affiliation journal name and @ on @ @ particular data @ @ dealing @ is a representation of @ pubmed central document corpus within @ data warehouse @ is a part of @ sonca system @ sonca system is @ developed a @ of component of @ general scientific information platform synat @ springer-verlag @ @ @ 
3473,A cognitive framework for core language understanding and its computational implementation,"The author argues that the cognitive processes underlying language understanding may not be logico-deductive or inductive, at least not for basic forms of understanding such as the ability to determine the topics of a text document. To demonstrate this point, they present a human cognition inspired framework for core language understanding and its computational implementation. The framework exploits word related knowledge stored in Long Term Memory (LTM) as well as Short Term Memory (STM) limited capacity, neuromorphic spreading activation and neural activation decay to derive the topics of text. The computational model implementing the framework shows the potential of the approach by establishing that the topics generated by the model are as good as those generated by humans. © 2011, IGI Global.",2012,International Journal of Cognitive Informatics and Natural Intelligence,1,@ author argues @ @ cognitive process underlying language understanding may not @ logico-deductive @ inductive at least not @ basic form of understanding @ a @ ability to determine @ topic of a text document @ to demonstrate @ point @ @ a human cognition inspired framework @ core language understanding and @ computational implementation @ @ framework exploit word related knowledge stored in long term memory @ ltm @ a well a short term memory @ stm @ limited capacity neuromorphic spreading activation and neural activation decay to derive @ topic of text @ @ computational model implementing @ framework @ @ potential of @ approach by establishing @ @ topic generated by @ model @ a good a @ generated by human @ igi global @ 
3475,Discourse Processing,"Discourse Processing here is framed as marking up a text with structural descriptions on several levels, which can serve to support many language-processing or text-mining tasks. We first explore some ways of assigning structure on the document level: the logical document structure as determined by the layout of the text, its genre-specific content structure, and its breakdown into topical segments. Then the focus moves to phenomena of local coherence. We introduce the problem of coreference and look at methods for building chains of coreferring entities in the text. Next, the notion of coherence relation is introduced as the second important factor of local coherence. We study the role of connectives and other means of signaling such relations in text, and then return to the level of larger textual units, where tree or graph structures can be ascribed by recursively assigning coherence relations. Taken together, these descriptions can inform text summarization, information extraction, discourse-aware sentiment analysis, question answering, and the like. © 2011 by Morgan & Claypool.",2012,Synthesis Lectures on Human Language Technologies,5,discourse processing @ is framed a marking up a text @ structural description on several level @ @ serve to support many language-processing @ text-mining task @ @ first explore some way of assigning structure on @ document level @ @ logical document structure a determined by @ layout of @ text @ genre-specific content structure and @ breakdown @ topical segment @ @ @ focus move to phenomenon of local coherence @ @ introduce @ problem of coreference and look at method @ building chain of coreferring entity in @ text @ next @ notion of coherence relation is introduced a @ second important factor of local coherence @ @ study @ role of connective and @ mean of signaling @ relation in text and @ return to @ level of larger textual unit @ tree @ graph structure @ @ ascribed by recursively assigning coherence relation @ taken together @ description @ inform text summarization information extraction discourse-aware sentiment analysis question answering and @ like @ by morgan claypool @ 
3477,A model-based em method for topic person name multi-polarization,"In this paper, we propose an unsupervised approach for multi-polarization of topic person names. We employ a model-based EM method to polarize individuals into positively correlated groups. In addition, we present off-topic block elimination and weighted correlation coefficient techniques to eliminate the off-topic blocks and reduce the text sparseness problem respectively. Our experiment results demonstrate that the proposed method can identify multi-polar person groups of topics correctly. © 2011 Springer-Verlag Berlin Heidelberg.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ propose @ unsupervised approach @ multi-polarization of topic person name @ @ employ a model-based em method to polarize individual @ positively correlated group @ in addition @ @ off-topic block elimination and weighted correlation coefficient technique to eliminate @ off-topic block and reduce @ text sparseness problem respectively @ @ experiment @ demonstrate @ @ proposed method @ identify multi-polar person group of topic correctly @ springer-verlag @ @ @ 
3479,Multi-words terminology recognition using web search,"Terminology recognition system which is a fundamental research for Technology Opportunity Discovery (TOD) has been intensively studied in limited range of domains, especially in bio-medical domain. We propose a domain independent terminology recognition system based on machine learning method using dictionary, syntactic features, and Web search results, since the previous works revealed limitation on applying their approaches to general domain because their resources were domain specific. We achieved F-score 80.4 and 6.4% improvement after comparing the proposed approach with the related approach, C-value, which has been widely used and is based on local domain frequencies. In the second experiment with various combinations of unithood features, the method combined with NGD(Normalized Google Distance) showed the best performance of 81.5 on F-score. We applied two machine learning methods such as Logistic regression and SVMs, and got the best score at SVMs method. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,11,terminology recognition system @ is a fundamental research @ technology opportunity discovery @ tod @ ha @ intensively studied in limited range of domain especially in bio-medical domain @ @ propose a domain independent terminology recognition system based on machine learning method @ dictionary syntactic feature and web search @ since @ previous work revealed limitation on applying @ approach to general domain @ @ resource @ domain specific @ @ achieved f-score @ and @ improvement @ comparing @ proposed approach @ @ related approach c-value @ ha @ widely used and is based on local domain frequency @ in @ second experiment @ various combination of unithood feature @ method combined @ ngd @ normalized google distance @ showed @ best performance of @ on f-score @ @ applied @ machine learning method @ a logistic regression and svms and got @ best score at svms method @ springer-verlag @ 
3492,Deep text mining for automatic keyphrase extraction from text documents,"Due to existence of a huge amount of textual data either on the World Wide Web or in textual databases like PubMed, the development of novel automatic keyphrase extraction methods has emerged as one of the key research problems in recent past. Consequently, a number of machine learning techniques, mostly supervised, have been proposed to extract keyphrases from text documents. But, one of the main bottlenecks that hinders the success of such systems is the requirement of annotated corpora for training purpose. In this paper, we propose the design of a deep text mining system to identify keyphrases in text documents that are either unstructured or semi-structured in nature. The novelty of our system lies in its applicability on a single document, instead of demanding a collection of annotated texts for training, to identify keyphrases embedded within it. The proposed system applies parsing techniques to identify candidate phrases. After mapping the original set of candidate phrases into a low-dimensional space using Singular Value Decomposition (SVD), the Markov Clustering (MCL) technique is applied to cluster related sentences together. Finally, considering each cluster as a document, Latent Dirichlet Allocation (LDA) is applied to identify feasible keyphrases that are presented to users in non-increasing order of their relevance score values. The efficacy of the proposed system is established through experimentation on datasets from two different domains. On comparative evaluation, we found that the proposed system outperforms KEA and KEA++ that apply the supervised machine learning approach for automatic keyphrase extraction from text documents. © de Gruyter 2011.",2011,Journal of Intelligent Systems,2,due to existence of a huge amount of textual data either on @ world wide web @ in textual database like pubmed @ development of novel automatic keyphrase extraction method ha emerged a @ of @ key research problem in recent past @ consequently a number of machine learning technique mostly supervised @ @ proposed to extract keyphrases @ text document @ @ @ of @ main bottleneck @ hinders @ success of @ system is @ requirement of annotated corpus @ training purpose @ in @ @ @ propose @ design of a deep text mining system to identify keyphrases in text document @ @ either unstructured @ semi-structured in nature @ @ novelty of @ system lie in @ applicability on a single document instead of demanding a collection of annotated text @ training to identify keyphrases embedded within @ @ @ proposed system applies parsing technique to identify candidate phrase @ @ mapping @ original set of candidate phrase @ a low-dimensional space @ singular value decomposition @ svd @ @ markov clustering @ mcl @ technique is applied to cluster related sentence together @ finally considering @ cluster a a document latent dirichlet allocation @ lda @ is applied to identify feasible keyphrases @ @ presented to user in non-increasing order of @ relevance score value @ @ efficacy of @ proposed system is established @ experimentation on datasets @ @ different domain @ on comparative evaluation @ found @ @ proposed system outperforms kea and kea @ apply @ supervised machine learning approach @ automatic keyphrase extraction @ text document @ de gruyter @ 
3496,Text mining and probabilistic language modeling for online review spam detection,"In the era of Web 2.0, huge volumes of consumer reviews are posted to the Internet every day. Manual approaches to detecting and analyzing fake reviews (i.e., spam) are not practical due to the problem of information overload. However, the design and development of automated methods of detecting fake reviews is a challenging research problem. The main reason is that fake reviews are specifically composed to mislead readers, so they may appear the same as legitimate reviews (i.e., ham). As a result, discriminatory features that would enable individual reviews to be classified as spam or ham may not be available. Guided by the design science research methodology, the main contribution of this study is the design and instantiation of novel computational models for detecting fake reviews. In particular, a novel text mining model is developed and integrated into a semantic language model for the detection of untruthful reviews. The models are then evaluated based on a real-world dataset collected from amazon.com. The results of our experiments confirm that the proposed models outperform other well-known baseline models in detecting fake reviews. To the best of our knowledge, the work discussed in this article represents the first successful attempt to apply text mining methods and semantic language models to the detection of fake consumer reviews. A managerial implication of our research is that firms can apply our design artifacts to monitor online consumer reviews to develop effective marketing or product design strategies based on genuine consumer feedback posted to the Internet. © 2011 ACM.",2011,ACM Transactions on Management Information Systems,121,in @ era of web @ huge volume of consumer review @ posted to @ internet every day @ manual approach to detecting and analyzing fake review @ i @ e @ spam @ @ not practical due to @ problem of information overload @ however @ design and development of automated method of detecting fake review is a challenging research problem @ @ main reason is @ fake review @ specifically composed to mislead reader @ @ may appear @ @ a legitimate review @ i @ e @ ham @ @ a a @ discriminatory feature @ would enable individual review to @ classified a spam @ ham may not @ available @ guided by @ design science research methodology @ main contribution of @ study is @ design and instantiation of novel computational model @ detecting fake review @ in particular a novel text mining model is developed and integrated @ a semantic language model @ @ detection of untruthful review @ @ model @ @ evaluated based on a real-world dataset collected @ amazon @ com @ @ @ of @ experiment confirm @ @ proposed model outperform @ well-known baseline model in detecting fake review @ to @ best of @ knowledge @ work discussed in @ article represents @ first successful attempt to apply text mining method and semantic language model to @ detection of fake consumer review @ a managerial implication of @ research is @ firm @ apply @ design artifact to monitor online consumer review to develop effective marketing @ product design strategy based on genuine consumer feedback posted to @ internet @ acm @ 
3497,A framework for the automatic extraction of rules from online text,"The majority of knowledge on the Web is encoded in unstructured text and is not linked to formalized knowledge, such as ontologies and rules. A potential solution to this problem is to acquire this knowledge through natural language processing and text mining methods. Prior work has focused on automatically extracting RDF-or OWL-based ontologies from text; however, the type of knowledge acquired is generally restricted to simple term hierarchies. This paper presents a general-purpose framework for acquiring more complex relationships from text and then encoding this knowledge as rules. Our approach starts with existing domain knowledge in the form of OWL ontologies and Semantic Web Rule Language (SWRL) rules and applies natural language processing and text matching techniques to deduce classes and properties. It then captures deductive knowledge in the form of new rules. We have evaluated our framework by applying it to web-based text on car rental requirements. We show that our approach can automatically and accurately generate rules for requirements of car rental companies not in the knowledge base. Our framework thus rapidly acquires complex knowledge from free text sources. We are expanding it to handle richer domains, such as medical science. © Springer-Verlag Berlin Heidelberg 2011.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),15,@ majority of knowledge on @ web is encoded in unstructured text and is not linked to formalized knowledge @ a ontology and rule @ a potential solution to @ problem is to acquire @ knowledge @ natural language processing and text mining method @ prior work ha focused on automatically extracting rdf-or owl-based ontology @ text @ however @ type of knowledge acquired is generally restricted to simple term hierarchy @ @ @ @ a general-purpose framework @ acquiring more complex relationship @ text and @ encoding @ knowledge a rule @ @ approach start @ existing domain knowledge in @ form of owl ontology and semantic web rule language @ swrl @ rule and applies natural language processing and text matching technique to deduce class and property @ @ @ capture deductive knowledge in @ form of @ rule @ @ @ evaluated @ framework by applying @ to web-based text on car rental requirement @ @ @ @ @ approach @ automatically and accurately generate rule @ requirement of car rental company not in @ knowledge base @ @ framework thus rapidly acquires complex knowledge @ free text source @ @ @ expanding @ to handle richer domain @ a medical science @ springer-verlag @ @ @ 
3499,Supervised machine learning approach for bio-molecular event extraction,"The main goal of biomedical text mining is to capture biomedical phenomena from textual data by extracting relevant entities, information and relations between biomedical entities such as proteins and genes. Most of the research in the related areas were focused on extracting only binary relations. In a recent past, the focus is shifted towards extracting more complex relations in the form of bio-molecular events that may include several entities or other relations. In this paper we propose a supervised approach that enables extraction, i.e. identification and classification of relatively complex bio-molecular events. We approach this as the supervised machine learning problems and use the well-known statistical algorithm, namely Conditional Random Field (CRF) that makes use of statistical and linguistic features that represent various morphological, syntactic and contextual information of the candidate bio-molecular trigger words. Firstly, we consider the problem of event identification and classification as a two-step process, first step of which deals with the event identification task and the second step classifies these identified events to one of the nine predefined classes. Thereafter, we perform event identification and classification together. Three-fold cross validation experiments on the Biomedical Natural Language Processing (BioNLP) 2009 shared task datasets yield the overall average recall, precision and F-measure values of 58.88%, 74.53% and 65.79%, respectively, for the event identification. We observed the overall classification accuracy of 59.34%. Evaluation results of the proposed approach when identification and classification are performed together showed the overall recall, precision and F-measure values of 59.92%, 54.25% and 56.94%, respectively. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ main goal of biomedical text mining is to capture biomedical phenomenon @ textual data by extracting relevant entity information and relation @ biomedical entity @ a protein and gene @ @ of @ research in @ related area @ focused on extracting only binary relation @ in a recent past @ focus is shifted towards extracting more complex relation in @ form of bio-molecular event @ may include several entity @ @ relation @ in @ @ @ propose a supervised approach @ enables extraction i @ e @ identification and classification of relatively complex bio-molecular event @ @ approach @ a @ supervised machine learning problem and use @ well-known statistical algorithm namely conditional random field @ crf @ @ make use of statistical and linguistic feature @ represent various morphological syntactic and contextual information of @ candidate bio-molecular trigger word @ firstly @ consider @ problem of event identification and classification a a two-step process first step of @ deal @ @ event identification task and @ second step classifies @ identified event to @ of @ nine predefined class @ thereafter @ perform event identification and classification together @ three-fold cross validation experiment on @ biomedical natural language processing @ bionlp @ shared task datasets yield @ overall average recall precision and f-measure value of @ @ and @ respectively @ @ event identification @ @ observed @ overall classification accuracy of @ @ evaluation @ of @ proposed approach @ identification and classification @ performed together showed @ overall recall precision and f-measure value of @ @ and @ respectively @ springer-verlag @ 
3501,Research on Technique of the Cyberspace Public Opinion Detection and Tracking,"The task of topic detection becomes the hotspot research direction in the field of natural language processing in recent years. The cyberspace public opinion has given the significant impact on the large numbers of internet users. This makes the effective public opinion detection and tracking become very important. We select the effective features in the story document and the vector center model is taken to represent the text document. The algorithm of distance based clustering is carried out for public opinion detection. It identifies the emergence of new events and also merges the story to the corresponding cluster. Finally, we give the performance evaluation by the F-Value and entropy value. The system achieves the performance of 76% F value on the test set. The technique of topics detection can monitor the information sources in various languages, and it will provide the efficient guidance for judging the hot spots on the web. © Springer-Verlag Berlin Heidelberg 2011.",2011,Communications in Computer and Information Science,2,@ task of topic detection becomes @ hotspot research direction in @ field of natural language processing in recent year @ @ cyberspace public opinion ha given @ significant impact on @ @ number of internet user @ @ make @ effective public opinion detection and tracking become @ important @ @ select @ effective feature in @ story document and @ vector center model is taken to represent @ text document @ @ algorithm of distance based clustering is carried @ @ public opinion detection @ @ identifies @ emergence of @ event and @ merges @ story to @ corresponding cluster @ finally @ give @ performance evaluation by @ f-value and entropy value @ @ system achieves @ performance of f value on @ test set @ @ technique of topic detection @ monitor @ information source in various language and @ @ provide @ efficient guidance @ judging @ hot spot on @ web @ springer-verlag @ @ @ 
3511,A machine learning approach to extract Drug - Drug Interactions in an unbalanced dataset,"Drug-Drug Interaction (DDI) extraction from the pharmacological literature is an emergent challenge in the text mining area. In this paper we describe a DDI extraction system based on a machine learning approach. We propose distinct solutions to deal with the high dimensionality of the problem and the unbalanced representation of classes in the dataset. On the test dataset, our best run reaches an F-measure of 0.4702.",2011,CEUR Workshop Proceedings,1,drug-drug interaction @ ddi @ extraction @ @ pharmacological literature is @ emergent challenge in @ text mining area @ in @ @ @ describe a ddi extraction system based on a machine learning approach @ @ propose distinct solution to deal @ @ high dimensionality of @ problem and @ unbalanced representation of class in @ dataset @ on @ test dataset @ best run reach @ f-measure of @ @ 
3514,Data mining,"Data Mining provides approaches for the identification and discovery of non-trivial patterns and models hidden in large collections of data. In the applied natural language processing domain, data mining usually requires preprocessed data that has been extracted from textual documents. Additionally, this data is often integrated with other data sources. This chapter provides an overview on data mining focusing on approaches for pattern mining, cluster analysis, and predictive model construction. For those, we discuss exemplary techniques that are especially useful in the applied natural language processing context. Additionally, we describe how the presented data mining approaches are connected to text mining, text classification, and clustering, and discuss interesting problems and future research directions. © 2012, IGI Global.",2011,"Applied Natural Language Processing: Identification, Investigation and Resolution",0,data mining provides approach @ @ identification and discovery of non-trivial pattern and model hidden in @ collection of data @ in @ applied natural language processing domain data mining usually requires preprocessed data @ ha @ extracted @ textual document @ additionally @ data is often integrated @ @ data source @ @ chapter provides @ overview on data mining focusing on approach @ pattern mining cluster analysis and predictive model construction @ @ @ @ discus exemplary technique @ @ especially useful in @ applied natural language processing context @ additionally @ describe @ @ presented data mining approach @ connected to text mining text classification and clustering and discus interesting problem and future research direction @ igi global @ 
3524,P-MATCH: Identifying part name in noisy text data,"Many industries keep log data on maintenance and support. The log data contains information on the problems reported as well as the actions taken to fix the problems. The log data contains a wealth of information useful for future maintenance, as well as product design and inventory management. However, it is always hard to identify parts involved automatically when the number of part types is large and the data are sloppily authored. Boeing's P-MATCH identifies part names from noisy non-professionally authored log data. In this chapter, the authors use P-MATCH to illustrate how to leverage the combined strength of natural language processing and text mining. © 2012, IGI Global.",2011,Cross-Disciplinary Advances in Applied Natural Language Processing: Issues and Approaches,0,many industry keep log data on maintenance and support @ @ log data contains information on @ problem reported a well a @ action taken to fix @ problem @ @ log data contains a wealth of information useful @ future maintenance a well a product design and inventory management @ however @ is always hard to identify part involved automatically @ @ number of part type is @ and @ data @ sloppily authored @ boeing @ s p-match identifies part name @ noisy non-professionally authored log data @ in @ chapter @ author use p-match to illustrate @ to leverage @ combined strength of natural language processing and text mining @ igi global @ 
3525,Opinion classification techniques applied to a Spanish corpus,"Sentiment analysis is a new challenging task related to Text Mining and Natural Language Processing (NLP). Although there are some current works, most of them only focus on English texts. However, web pages, blogs and opinions on the Internet are increasing every day in any language and not only in English. Other language like Spanish is increasingly present so we have carried out an experimental study with a Spanish films reviews corpus. Our main goal is to check the results obtained using several classifiers trained in order to determinate the opinion polarity. We have tested two classification algorithms (SVM, Naïve Bayes) and several weighting schemes and different linguistic preprocessing (stopper and stemmer). The accomplished experiments show that SVM works better than Naïve Bayes. In addition, the stopper and stemmer also obtain a slight improvement. © 2011 Sociedad Española Para el Procesamiento del Lenguaje Natural.",2011,Procesamiento de Lenguaje Natural,19,sentiment analysis is a @ challenging task related to text mining and natural language processing @ nlp @ @ although @ @ some current work @ of @ only focus on english text @ however web page blog and opinion on @ internet @ increasing every day in @ language and not only in english @ @ language like spanish is increasingly @ @ @ @ carried @ @ experimental study @ a spanish film review corpus @ @ main goal is to check @ @ obtained @ several classifier trained in order to determinate @ opinion polarity @ @ @ tested @ classification algorithm @ svm naïve bayes @ and several weighting scheme and different linguistic preprocessing @ stopper and stemmer @ @ @ accomplished experiment @ @ svm work better @ naïve bayes @ in addition @ stopper and stemmer @ obtain a slight improvement @ sociedad española para el procesamiento del lenguaje natural @ 
3526,The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts,"We present an evaluation task designed to provide a framework for comparing different approaches to extracting drug-drug interactions from biomedical texts.We define the task, describe the training/test data, list the participating systems and discuss their results. There were 10 teams who submitted a total of 40 runs.",2011,CEUR Workshop Proceedings,52,@ @ @ evaluation task designed to provide a framework @ comparing different approach to extracting drug-drug interaction @ biomedical text @ @ define @ task describe @ training test data list @ participating system and discus @ @ @ @ @ team @ submitted a total of run @ 
3527,Mining diverse views from related articles,"The world wide web allows for diverse articles to be available on a news event, product or any topic. It is not impossible to find a few hundred articles that discuss a specific topic thus making it difficult for a user to quickly process the information. Summarization condenses huge volume of information related to a topic but does not provide a delineation of the issues pertaining to it. We want to extract the diverse issues pertaining to a topic by mining views from a collection of articles related to it. A view is a set of sentences, related in content, that address an issue relevant to a topic. We present a framework for extraction and ranking of views and have conducted experiments to evaluate the framework.",2011,CEUR Workshop Proceedings,0,@ world wide web allows @ diverse article to @ available on a news event product @ @ topic @ @ is not impossible to find a @ hundred article @ discus a specific topic thus making @ difficult @ a user to quickly process @ information @ summarization condenses huge volume of information related to a topic @ doe not provide a delineation of @ issue pertaining to @ @ @ want to extract @ diverse issue pertaining to a topic by mining view @ a collection of article related to @ @ a view is a set of sentence related in content @ address @ issue relevant to a topic @ @ @ a framework @ extraction and ranking of view and @ conducted experiment to evaluate @ framework @ 
3528,Relation extraction for drug-drug interactions using ensemble learning,"We describe our approach for the extraction of drug-drug interactions from literature. The proposed method builds majority voting ensembles of contrasting machine learning methods, which exploit different linguistic feature spaces. We evaluated our approach in the context of the DDI Extraction 2011 challenge, where using document-wise crossvalidation, the best single classifier achieved an F1 of 57.3% and the best ensemble achieved 60.6%. On the held out test set, our best run achieved an F1 of 65.7%.",2011,CEUR Workshop Proceedings,31,@ describe @ approach @ @ extraction of drug-drug interaction @ literature @ @ proposed method build majority voting ensemble of contrasting machine learning method @ exploit different linguistic feature space @ @ evaluated @ approach in @ context of @ ddi extraction challenge @ @ document-wise crossvalidation @ best single classifier achieved @ f of @ and @ best ensemble achieved @ @ on @ held @ test set @ best run achieved @ f of @ @ 
3529,Modeling of lexical relations between topics retrieved from DBLP journals,"In this paper we present a method for getting topics from aims and scopes in DBLP journals and following construction of their hierarchical order. We focused on semantic relations between topics. Our method is fully automatic but manual cleaning of topic database would lead to much better accuracy. Another purpose of our work is to provide similar system to ACM classification system. We want to provide better, newer and fully automatic system contrary to ACM.",2011,CEUR Workshop Proceedings,0,in @ @ @ @ a method @ getting topic @ aim and scope in dblp journal and following construction of @ hierarchical order @ @ focused on semantic relation @ topic @ @ method is fully automatic @ manual cleaning of topic database would lead to much better accuracy @ another purpose of @ work is to provide similar system to acm classification system @ @ want to provide better newer and fully automatic system contrary to acm @ 
3537,A particle swarm optimizer to cluster parallel spanish-english short-text corpora,"Short-texts clustering is currently an important research area because of its applicability to web information retrieval, text summarization and text mining. These texts are often available in different languages and parallel multilingual corpora. Some previous works have demonstrated the effectiveness of a discrete Particle Swarm Optimizer algorithm, named CLUDIPSO, for clustering monolingual corpora containing very short documents. In all the considered cases, CLUDIPSO outperformed different algorithms representative of the state-of-the-art in the area. This paper presents a preliminary study showing the performance of CLUDIPSO on parallel Spanish-English corpora. The idea is to analyze how this bilingual information can be incorporated in the CLUDIPSO algorithm and to what extent this information can improve the clustering results. In order to adapt CLUDIPSO to a bilingual environment, some alternatives are proposed and evaluated. The results were compared considering CLUDIPSO in both environments, bilingual and monolingual. The experimental work shows that bilingual information allows to obtain just comparable results to those obtained with monolingual corpora. More work is required to make an effective use of this kind of information.",2011,CEUR Workshop Proceedings,2,short-texts clustering is currently @ important research area @ of @ applicability to web information retrieval text summarization and text mining @ @ text @ often available in different language and parallel multilingual corpus @ some previous work @ demonstrated @ effectiveness of a discrete particle swarm optimizer algorithm named cludipso @ clustering monolingual corpus containing @ short document @ in @ @ considered case cludipso outperformed different algorithm representative of @ state-of-the-art in @ area @ @ @ @ a preliminary study showing @ performance of cludipso on parallel spanish-english corpus @ @ idea is to analyze @ @ bilingual information @ @ incorporated in @ cludipso algorithm and to @ extent @ information @ improve @ clustering @ @ in order to adapt cludipso to a bilingual environment some alternative @ proposed and evaluated @ @ @ @ compared considering cludipso in @ environment bilingual and monolingual @ @ experimental work @ @ bilingual information allows to obtain @ comparable @ to @ obtained @ monolingual corpus @ more work is required to make @ effective use of @ kind of information @ 
3538,Literature mining for the diagnostic procedures of osteoporosis,"In this paper, we highlight the importance of osteoporosis disease in terms of medical research and healthcare and we consider a knowledge discovery approach regarding the diagnostic procedures of osteoporosis from a historical perspective. Osteoporosis is characterized by low bone mass, micro-architectural deterioration of bone tissue, and increased bone fragility and susceptibility to fracture. Osteoporosis affects an estimated 75 million people in Europe, the USA and Japan, with 10 million people suffering from osteoporosis in the United States alone. Osteoporosis may significantly affect life expectancy and quality of life and is a component of the frailty syndrome. We use a freely available biomedical search engine based on text-mining technology to extract the diagnostic procedures used in osteoporosis from MEDLINE articles. We conclude that there are some changes in diagnostic procesures in the last four decades and Dual energy x-ray absorptiometry is the most commonly used technique today. © 2011 Springer-Verlag Berlin.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ highlight @ importance of osteoporosis disease in term of medical research and healthcare and @ consider a knowledge discovery approach regarding @ diagnostic procedure of osteoporosis @ a historical perspective @ osteoporosis is characterized by low bone mass micro-architectural deterioration of bone tissue and increased bone fragility and susceptibility to fracture @ osteoporosis affect @ estimated million people in europe @ usa and japan @ million people suffering @ osteoporosis in @ united state alone @ osteoporosis may significantly affect life expectancy and quality of life and is a component of @ frailty syndrome @ @ use a freely available biomedical search engine based on text-mining technology to extract @ diagnostic procedure used in osteoporosis @ medline article @ @ conclude @ @ @ some change in diagnostic procesures in @ last four decade and dual energy x-ray absorptiometry is @ @ commonly used technique today @ springer-verlag @ @ 
3542,Intra-firm information flow: A content-structure perspective,"This paper endeavors to bring together two largely disparate areas of research. On one hand, text mining methods treat each document as an independent instance despite the fact that in many text domains, documents are linked and their topics are correlated. For example, web pages of related topics are often connected by hyperlinks and scientific papers from related fields are typically linked by citations. On the other hand, Social Network Analysis (SNA) typically treats edges between nodes according to ""flat"" attributes in binary form alone. This paper proposes a simple approach that addresses both these issues in data mining scenarios involving corpora of linked documents. According to this approach, after assigning weights to the edges between documents, based on the content of the documents associated with each edge, we apply standard SNA and network theory tools to the network. The method is tested on the Enron email corpus and successfully discovers the central people in the organization and the relevant communications between them. Furthermore, Our findings suggest that due to the non-conservative nature of information, conservative centrality measures (such as PageRank) are less adequate here than non-conservative centrality measures (such as eigenvector centrality). © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ endeavor to bring together @ largely disparate area of research @ on @ hand text mining method treat @ document a @ independent instance despite @ fact @ in many text domain document @ linked and @ topic @ correlated @ @ example web page of related topic @ often connected by hyperlink and scientific @ @ related field @ typically linked by citation @ on @ @ hand social network analysis @ sna @ typically treat edge @ node according to @ flat @ attribute in binary form alone @ @ @ proposes a simple approach @ address @ @ issue in data mining scenario involving corpus of linked document @ according to @ approach @ assigning weight to @ edge @ document based on @ content of @ document associated @ @ edge @ apply standard sna and network theory tool to @ network @ @ method is tested on @ enron email corpus and successfully discovers @ central people in @ organization and @ relevant communication @ @ @ furthermore @ finding suggest @ due to @ non-conservative nature of information conservative centrality measure @ @ a pagerank @ @ le adequate @ @ non-conservative centrality measure @ @ a eigenvector centrality @ @ springer-verlag @ 
3543,Mining biomedical text towards building a quantitative food-disease-gene network,"Advances in bio-technology and life sciences are leading to an ever-increasing volume of published research data, predominantly in unstructured text. To uncover the underlying knowledge base hidden in such data, text mining techniques have been utilized. Past and current efforts in this area have been largely focusing on recognizing gene and protein names, and identifying binary relationships among genes or proteins. In this chapter, we present an information extraction system that analyzes publications in an emerging discipline-Nutritional Genomics, a discipline that studies the interactions amongst genes, foods and diseases-aiming to build a quantitative food-disease-gene network. To this end, we adopt a host of techniques including natural language processing (NLP) techniques, domain ontology, and machine learning approaches. Specifically, the proposed system is composed of four main modules: (1) named entity recognition, which extracts five types of entities including foods, chemicals, diseases, proteins and genes; (2) relationship extraction: A verb-centric approach is implemented to extract binary relationships between two entities; (3) relationship polarity and strength analysis: We have constructed novel features to capture the syntactic, semantic and structural aspects of a relationship. A 2-phase Support Vector Machine is then used to classify the polarity, whereas a Support Vector Regression learner is applied to rate the strength level of a relationship; and (4) relationship integration and visualization, which integrates the previously extracted relationships and realizes a preliminary user interface for intuitive observation and exploration. Empirical evaluations of the first three modules demonstrate the efficacy of this system. The entity recognition module achieved a balanced precision and recall with an average F-score of 0.89. The average F-score of the extracted relationships is 0.905. Finally, an accuracy of 0.91 and 0.96 was achieved in classifying the relationship polarity and rating the relationship strength level, respectively. © 2011 Springer-Verlag Berlin Heidelberg.",2011,Studies in Computational Intelligence,6,advance in bio-technology and life science @ leading to @ ever-increasing volume of published research data predominantly in unstructured text @ to uncover @ underlying knowledge base hidden in @ data text mining technique @ @ utilized @ past and current effort in @ area @ @ largely focusing on recognizing gene and protein name and identifying binary relationship among gene @ protein @ in @ chapter @ @ @ information extraction system @ analyzes publication in @ emerging discipline-nutritional genomics a discipline @ study @ interaction amongst gene food and diseases-aiming to build a quantitative food-disease-gene network @ to @ end @ adopt a host of technique including natural language processing @ nlp @ technique domain ontology and machine learning approach @ specifically @ proposed system is composed of four main module @ @ @ named entity recognition @ extract five type of entity including food chemical disease protein and gene @ @ @ relationship extraction @ a verb-centric approach is implemented to extract binary relationship @ @ entity @ @ @ relationship polarity and strength analysis @ @ @ constructed novel feature to capture @ syntactic semantic and structural aspect of a relationship @ a phase support vector machine is @ used to classify @ polarity whereas a support vector regression learner is applied to rate @ strength level of a relationship @ and @ @ relationship integration and visualization @ integrates @ @ extracted relationship and realizes a preliminary user interface @ intuitive observation and exploration @ empirical evaluation of @ first three module demonstrate @ efficacy of @ system @ @ entity recognition module achieved a balanced precision and recall @ @ average f-score of @ @ @ average f-score of @ extracted relationship is @ @ finally @ accuracy of @ and @ wa achieved in classifying @ relationship polarity and rating @ relationship strength level respectively @ springer-verlag @ @ @ 
3545,Knowledge and reasoning for question answering: Research perspectives,This paper presents a roadmap of current promising research tracks in question answering with a focus on knowledge acquisition and reasoning. We show that many current techniques developed in the frame of text mining and natural language processing are ready to be integrated in question answering search systems. Their integration opens new avenues of research for factual answer finding and for advanced question answering. Advanced question answering refers to a situation where an understanding of the meaning of the question and the information source together with techniques for answer fusion and generation are needed. © 2010 Elsevier Ltd. All rights reserved.,2011,Information Processing and Management,7,@ @ @ a roadmap of current promising research track in question answering @ a focus on knowledge acquisition and reasoning @ @ @ @ many current technique developed in @ frame of text mining and natural language processing @ ready to @ integrated in question answering search system @ @ integration open @ avenue of research @ factual answer finding and @ advanced question answering @ advanced question answering refers to a situation @ @ understanding of @ meaning of @ question and @ information source together @ technique @ answer fusion and generation @ needed @ @ ltd @ @ right reserved @ 
3546,Extracting bio-molecular events from literature - The BIONLP'09 shared task,"This paper presents the preparation, results and analysis of the BioNLP'09 shared task on event extraction, organized to address the automatic extraction of fine-grained information from the scientific literature on molecular biology. A representation of this information was defined taking into account both the biological and computational requirements of the task, and corpus resources manually annotated by domain experts provided to task participants. To create a basis for further progress, emphasis was placed on providing fine-grained evaluation that isolates different subtasks and allows the analysis of different aspects of the results through various evaluation criteria. In introducing this new task to the community, we made an effort to reduce the cost of participation by making common natural language processing tools, data, and evaluation methods easily accessible. The task received community-wide participation, establishing the state-of-the-art performance at fine-grained event extraction as well as allowing the identification of remaining challenges and suggesting directions for future improvements. All the resources and results of the shared task are publicly available and an online evaluation on blind test data accessible at. © 2011 Wiley Periodicals, Inc.",2011,Computational Intelligence,33,@ @ @ @ preparation @ and analysis of @ bionlp @ shared task on event extraction organized to address @ automatic extraction of fine-grained information @ @ scientific literature on molecular biology @ a representation of @ information wa defined taking @ account @ @ biological and computational requirement of @ task and corpus resource manually annotated by domain expert provided to task participant @ to create a basis @ @ progress emphasis wa placed on providing fine-grained evaluation @ isolates different subtasks and allows @ analysis of different aspect of @ @ @ various evaluation criterion @ in introducing @ @ task to @ community @ made @ effort to reduce @ cost of participation by making common natural language processing tool data and evaluation method easily accessible @ @ task received community-wide participation establishing @ state-of-the-art performance at fine-grained event extraction a well a allowing @ identification of remaining challenge and suggesting direction @ future improvement @ @ @ resource and @ of @ shared task @ publicly available and @ online evaluation on blind test data accessible at @ wiley periodical inc @ 
3547,High-precision biological event extraction: Effects of system and of data,"We approached the problems of event detection, argument identification, and negation and speculation detection in the BioNLP'09 information extraction challenge through concept recognition and analysis. Our methodology involved using the OpenDMAP semantic parser with manually written rules. The original OpenDMAP system was updated for this challenge with a broad ontology defined for the events of interest, new linguistic patterns for those events, and specialized coordination handling. We achieved state-of-the-art precision for two of the three tasks, scoring the highest of 24 teams at precision of 71.81 on Task 1 and the highest of 6 teams at precision of 70.97 on Task 2. We provide a detailed analysis of the training data and show that a number of trigger words were ambiguous as to event type, even when their arguments are constrained by semantic class. The data is also shown to have a number of missing annotations. Analysis of a sampling of the comparatively small number of false positives returned by our system shows that major causes of this type of error were failing to recognize second themes in two-theme events, failing to recognize events when they were the arguments to other events, failure to recognize nontheme arguments, and sentence segmentation errors. We show that specifically handling coordination had a small but important impact on the overall performance of the system. The OpenDMAP system and the rule set are available at. © 2011 Wiley Periodicals, Inc.",2011,Computational Intelligence,18,@ approached @ problem of event detection argument identification and negation and speculation detection in @ bionlp @ information extraction challenge @ concept recognition and analysis @ @ methodology involved @ @ opendmap semantic parser @ manually written rule @ @ original opendmap system wa updated @ @ challenge @ a broad ontology defined @ @ event of interest @ linguistic pattern @ @ event and specialized coordination handling @ @ achieved state-of-the-art precision @ @ of @ three task scoring @ highest of team at precision of @ on task and @ highest of team at precision of @ on task @ @ provide a detailed analysis of @ training data and @ @ a number of trigger word @ ambiguous a to event type even @ @ argument @ constrained by semantic class @ @ data is @ @ to @ a number of missing annotation @ analysis of a sampling of @ comparatively small number of false positive returned by @ system @ @ major cause of @ type of error @ failing to recognize second theme in two-theme event failing to recognize event @ @ @ @ argument to @ event failure to recognize nontheme argument and sentence segmentation error @ @ @ @ specifically handling coordination @ a small @ important impact on @ overall performance of @ system @ @ opendmap system and @ rule set @ available at @ wiley periodical inc @ 
3548,Molecular event extraction from link grammar parse trees in the BIONLP'09 shared task,"The BioNLP'09 Shared Task deals with extracting information on molecular events, such as gene expression and protein localization, from natural language text. Information in this benchmark are given as tuples including protein names, trigger terms for each event, and possible other participants such as bindings sites. We address all three tasks of BioNLP'09: event detection, event enrichment, and recognition of negation and speculation. Our method for the first two tasks is based on a deep parser; we store the parse tree of each sentence in a relational database scheme. From the training data, we collect the dependencies connecting any two relevant terms of a known tuple, that is, the shortest paths linking these two constituents. We encode all such linkages in a query language to retrieve similar linkages from unseen text. For the third task, we rely on a hierarchy of hand-crafted regular expressions to recognize speculation and negated events. In this paper, we added extensions regarding a post-processing step that handles ambiguous event trigger terms, as well as an extension of the query language to relax linkage constraints. On the BioNLP Shared Task test data, we achieve an overall F1-measure of 32%, 29%, and 30% for the successive Tasks 1, 2, and 3, respectively. © 2011 Wiley Periodicals Inc.",2011,Computational Intelligence,2,@ bionlp @ shared task deal @ extracting information on molecular event @ a gene expression and protein localization @ natural language text @ information in @ benchmark @ given a tuples including protein name trigger term @ @ event and possible @ participant @ a binding site @ @ address @ three task of bionlp @ @ event detection event enrichment and recognition of negation and speculation @ @ method @ @ first @ task is based on a deep parser @ @ store @ parse tree of @ sentence in a relational database scheme @ @ @ training data @ collect @ dependency connecting @ @ relevant term of a known tuple @ is @ shortest path linking @ @ constituent @ @ encode @ @ linkage in a query language to retrieve similar linkage @ unseen text @ @ @ third task @ rely on a hierarchy of hand-crafted regular expression to recognize speculation and negated event @ in @ @ @ added extension regarding a post-processing step @ handle ambiguous event trigger term a well a @ extension of @ query language to relax linkage constraint @ on @ bionlp shared task test data @ achieve @ overall f measure of and @ @ successive task and respectively @ wiley periodical inc @ 
3551,Text mining for interpreting gene,"Text mining provides methods to retrieve and extract information contained in free-text automatically. The advances in molecular biology techniques have given rise to high throughput biology that produces tremendous data and related publications in the past decades. Here the efficient constituents of Co-reference resolution incorporating Natural Language Processing (NLP) to interpret Gene expression are focused. It may overcome the challenges and limitations of text mining in biological data for resolving unsolved problems and this paper describes a new phase of text mining process to uncover interesting term correlations, genomic term identification in curation process , identification of biological relations and help the biologists in their analysis of complex problems. The new phase of text mining process is organized in four tasks of namely use NLP, find correlated terms, Co-reference resolution and built a structured database. © 2011 Springer-Verlag Berlin Heidelberg.",2011,Communications in Computer and Information Science,0,text mining provides method to retrieve and extract information contained in free-text automatically @ @ advance in molecular biology technique @ given rise to high throughput biology @ produce tremendous data and related publication in @ past decade @ @ @ efficient constituent of co-reference resolution incorporating natural language processing @ nlp @ to interpret gene expression @ focused @ @ may overcome @ challenge and limitation of text mining in biological data @ resolving unsolved problem and @ @ describes a @ phase of text mining process to uncover interesting term correlation genomic term identification in curation process identification of biological relation and help @ biologist in @ analysis of complex problem @ @ @ phase of text mining process is organized in four task of namely use nlp find correlated term co-reference resolution and built a structured database @ springer-verlag @ @ @ 
3554,Text analytics for social research,"This tutorial provides software training in ""DiscoverText,"" which is text analytic software developed by Professor Shulman. His work advances text mining and natural language processing research. The training links these worlds via straightforward and easy to understand explanations of software features that can be tailored for all experience levels and industries. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ tutorial provides software training in @ discovertext @ @ is text analytic software developed by professor shulman @ @ work advance text mining and natural language processing research @ @ training link @ world via straightforward and easy to understand explanation of software feature @ @ @ tailored @ @ experience level and industry @ springer-verlag @ 
3555,Clustering of rough set related documents with use of knowledge from DBpedia,"A case study of semantic clustering of scientific articles related to Rough Sets is presented. The proposed method groups the documents on the basis of their content and with assistance of DBpedia knowledge base. The text corpus is first treated with Natural Language Processing tools in order to produce vector representations of the content and then matched against a collection of concepts retrieved from DBpedia. As a result, a new representation is constructed that better reflects the semantics of the texts. With this new representation, the documents are hierarchically clustered in order to form partition of papers that share semantic relatedness. The steps in textual data preparation, utilization of DBpedia and clustering are explained and illustrated with results of experiments performed on a corpus of scientific documents about rough sets. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),17,a case study of semantic clustering of scientific article related to rough set is presented @ @ proposed method group @ document on @ basis of @ content and @ assistance of dbpedia knowledge base @ @ text corpus is first treated @ natural language processing tool in order to produce vector representation of @ content and @ matched @ a collection of concept retrieved @ dbpedia @ a a @ a @ representation is constructed @ better reflects @ semantics of @ text @ @ @ @ representation @ document @ hierarchically clustered in order to form partition of @ @ share semantic relatedness @ @ step in textual data preparation utilization of dbpedia and clustering @ explained and illustrated @ @ of experiment performed on a corpus of scientific document @ rough set @ springer-verlag @ 
3556,Graph-based named entity linking with Wikipedia,"Named entity linking (NEL) grounds entity mentions to their corresponding Wikipedia article. State-of-the-art supervised NEL systems use features over the rich Wikipedia document and link-graph structure. Graph-based measures have been effective over WordNet for word sense disambiguation (wsd). We draw parallels between NEL and (wsd), motivating our unsupervised NEL approach that exploits the Wikipedia article and category link graphs. Our system achieves 85.5% accuracy on the TAC 2010 shared task - competitive with the best supervised and unsupervised systems. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),37,named entity linking @ nel @ ground entity mention to @ corresponding wikipedia article @ state-of-the-art supervised nel system use feature @ @ rich wikipedia document and link-graph structure @ graph-based measure @ @ effective @ wordnet @ word sense disambiguation @ wsd @ @ @ draw parallel @ nel and @ wsd @ motivating @ unsupervised nel approach @ exploit @ wikipedia article and category link graph @ @ system achieves @ accuracy on @ tac shared task competitive @ @ best supervised and unsupervised system @ springer-verlag @ 
3557,Ontology importance towards enhancing suggestions in a news recommender system for a financial investor,"In terms of the current information society, where news and information generally run at a remarkable speed, it is imperative that the events of capital importance for the world economy (and others fields) to be available and disseminated in real time. Thus, we refer to news that may influence in a large manner the stock quotes, forex, oil quotes and so on. In view of the fact that the Internet abounds in news, in our on-going research (under grant TE 316 ""Intelligent methods for decision fundamentation on stock market transactions, based on public information"") we seek to propose a solution for extracting news vital information, its interpretation in real time, followed by suggesting possible decisional alternatives to the representatives involved in the stock exchange market. We put forward in the present paper our starting ideas for a conceptual model for a semantic Web news search, which encompasses multi-agent technologies, semantic Web, text search and mining, RSS news feeds. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,0,in term of @ current information society @ news and information generally run at a remarkable speed @ is imperative @ @ event of capital importance @ @ world economy @ and others field @ to @ available and disseminated in real time @ thus @ refer to news @ may influence in a @ manner @ stock quote forex oil quote and @ on @ in view of @ fact @ @ internet abounds in news in @ on-going research @ @ grant te @ intelligent method @ decision fundamentation on stock market transaction based on public information @ @ @ seek to propose a solution @ extracting news vital information @ interpretation in real time followed by suggesting possible decisional alternative to @ representative involved in @ stock exchange market @ @ put forward in @ @ @ @ starting idea @ a conceptual model @ a semantic web news search @ encompasses multi-agent technology semantic web text search and mining r news feed @ springer-verlag @ 
3563,OCA: Opinion corpus for Arabic,"Sentiment analysis is a challenging new task related to text mining and natural language processing. Although there are, at present, several studies related to this theme, most of these focus mainly on English texts. The resources available for opinion mining (OM) in other languages are still limited. In this article, we present a new Arabic corpus for the OM task that has been made available to the scientific community for research purposes. The corpus contains 500 movie reviews collected from different web pages and blogs in Arabic, 250 of them considered as positive reviews, and the other 250 as negative opinions. Furthermore, different experiments have been carried out on this corpus, using machine learning algorithms such as support vector machines and Nave Bayes. The results obtained are very promising and we are encouraged to continue this line of research. © 2011 ASIS&T.",2011,Journal of the American Society for Information Science and Technology,171,sentiment analysis is a challenging @ task related to text mining and natural language processing @ although @ @ at @ several study related to @ theme @ of @ focus mainly on english text @ @ resource available @ opinion mining @ om @ in @ language @ still limited @ in @ article @ @ a @ arabic corpus @ @ om task @ ha @ made available to @ scientific community @ research purpose @ @ corpus contains movie review collected @ different web page and blog in arabic of @ considered a positive review and @ @ a negative opinion @ furthermore different experiment @ @ carried @ on @ corpus @ machine learning algorithm @ a support vector machine and nave bayes @ @ @ obtained @ @ promising and @ @ encouraged to continue @ line of research @ asis t @ 
3564,A vector-space dynamic feature for phrase-based statistical machine translation,"In this paper, we propose and evaluate a novel dynamic feature function for log-linear model combinations in phrase-based statistical machine translation. The feature function is inspired on the popularly known vector-space model which is typically used in information retrieval and text mining applications, and it aims at improving translation unit selection at decoding time by incorporating context information from the source language. Significant improvements on an English-Spanish experimental corpus are presented and discussed. © 2010 Springer Science+Business Media, LLC.",2011,Journal of Intelligent Information Systems,3,in @ @ @ propose and evaluate a novel dynamic feature function @ log-linear model combination in phrase-based statistical machine translation @ @ feature function is inspired on @ popularly known vector-space model @ is typically used in information retrieval and text mining application and @ aim at improving translation unit selection at decoding time by incorporating context information @ @ source language @ significant improvement on @ english-spanish experimental corpus @ presented and discussed @ @ science @ medium llc @ 
3567,Detecting human sentiment from text using a proximity-based approach,"Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text thus helping people to discover valuable information from large amounts of unstructured data. Sentiment analysis can be used for grouping search engine results, analyzing news content, reviews for books, movie, sports, blogs, web forums, etc. Several methods have been proposed for sentiment analysis at word, sentence, and document levels mostly based on common machine learning techniques. In this paper we explore a new methodology for sentiment analysis by considering a new set of features based on word proximities in a written text. We propose three proximity-based features, namely, proximity distribution, mutual information between proximity types, and proximity patterns. We applied this approach to the analysis of movie reviews, drug reviews, and music reviews. Our experimental results show that proximity-based sentiment analysis is able to extract sentiments from a specific domain, with performance comparable to the state-of-the-art. To the best of our knowledge, this is the first attempt at focusing on only proximity-based features as the primary cues in sentiment analysis.",2011,Journal of Digital Information Management,4,sentiment analysis seek to characterize opinionated @ evaluative aspect of natural language text thus helping people to discover valuable information @ @ amount of unstructured data @ sentiment analysis @ @ used @ grouping search engine @ analyzing news content review @ book movie sport blog web forum etc @ several method @ @ proposed @ sentiment analysis at word sentence and document level mostly based on common machine learning technique @ in @ @ @ explore a @ methodology @ sentiment analysis by considering a @ set of feature based on word proximity in a written text @ @ propose three proximity-based feature namely proximity distribution mutual information @ proximity type and proximity pattern @ @ applied @ approach to @ analysis of movie review drug review and music review @ @ experimental @ @ @ proximity-based sentiment analysis is able to extract sentiment @ a specific domain @ performance comparable to @ state-of-the-art @ to @ best of @ knowledge @ is @ first attempt at focusing on only proximity-based feature a @ primary cue in sentiment analysis @ 
3570,Autonomous and adaptive identification of topics in unstructured text,"Existing topic identification techniques must tackle an important problem: they depend on human intervention, thus incurring major preparation costs and lacking operational flexibility when facing novelty. To resolve this issue, we propose an adaptable and autonomous algorithm that discovers topics in unstructured text documents. The algorithm is based on principles that differ from existing natural language processing and artificial intelligence techniques. These principles involve the retrieval, activation and decay of general-purpose lexical knowledge, inspired by how the brain may process information when someone reads. The algorithm handles words sequentially in a single document, contrary to the usual corpus-based bag-of-words approach. Empirical results demonstrate the potential of the new algorithm. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,existing topic identification technique must tackle @ important problem @ @ depend on human intervention thus incurring major preparation cost and lacking operational flexibility @ facing novelty @ to resolve @ issue @ propose @ adaptable and autonomous algorithm @ discovers topic in unstructured text document @ @ algorithm is based on principle @ differ @ existing natural language processing and artificial intelligence technique @ @ principle involve @ retrieval activation and decay of general-purpose lexical knowledge inspired by @ @ brain may process information @ someone read @ @ algorithm handle word sequentially in a single document contrary to @ usual corpus-based bag-of-words approach @ empirical @ demonstrate @ potential of @ @ algorithm @ springer-verlag @ 
3572,Connecting repositories in the open access domain using text mining and semantic data,"This paper presents CORE (COnnecting REpositories), a system that aims to facilitate the access and navigation across scientific papers stored in Open Access repositories. This is being achieved by harvesting metadata and full-text content from Open Access repositories, by applying text mining techniques to discover semanticly related articles and by representing and exposing these relations as Linked Data. The information about associations between articles expressed in an interoperable format will enable the emergence of a wide range of applications. The potential of CORE can be demonstrated on two use-cases: (1) Improving the the navigation capabilities of digital libraries by the means of a CORE pluging, (2) Providing access to digital content from smart phones and tablet devices by the means of the CORE Mobile application. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ @ core @ connecting repository @ a system @ aim to facilitate @ access and navigation across scientific @ stored in open access repository @ @ is @ achieved by harvesting metadata and full-text content @ open access repository by applying text mining technique to discover semanticly related article and by representing and exposing @ relation a linked data @ @ information @ association @ article expressed in @ interoperable format @ enable @ emergence of a wide range of application @ @ potential of core @ @ demonstrated on @ use-cases @ @ @ improving @ @ navigation capability of digital library by @ mean of a core pluging @ @ providing access to digital content @ smart phone and tablet device by @ mean of @ core mobile application @ springer-verlag @ 
3577,Tagging ontologies with fuzzy WordNet Domains,"The use of WordNet Domains is confined in the present days to Text Mining field. Moreover, the tagging of WordNet synsets with WordNet Domain labels is a crisp one. This paper introduces an approach for automatically tagging both ontologies and their concepts with WordNet domains in a fuzzy fashion, for topic classification purposes. Our fuzzy WordNet Domains model is presented as well as our domain disambiguation procedure. Experiments show promising results and are introduced in this paper as well as a final discussion on envisioned scenarios for our approach. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ use of wordnet domain is confined in @ @ day to text mining field @ moreover @ tagging of wordnet synset @ wordnet domain label is a crisp @ @ @ @ introduces @ approach @ automatically tagging @ ontology and @ concept @ wordnet domain in a fuzzy fashion @ topic classification purpose @ @ fuzzy wordnet domain model is presented a well a @ domain disambiguation procedure @ experiment @ promising @ and @ introduced in @ @ a well a a final discussion on envisioned scenario @ @ approach @ springer-verlag @ 
3578,Mining significant words from customer opinions written in different natural languages,"Opinions expressed by text documents freely written in various natural languages represent a valuable source of knowledge that is hidden in large datasets. The presented research describes a text mining-method how to discover words that are significant for expressing different opinions (positive and negative). The method applies a simple but unified data pre-processing for all languages, providing the bag-of-words with words represented by their frequencies in the data. Then, the frequencies are used by the algorithm which generates decision trees. The tree decisive nodes contain the words that are significant for expressing the opinions. Positions of these words in the tree represent their significance degree, where the most significant word is in the node. As a result, a list of relevant words can be used for creating a dictionary containing only relevant information. The described method was tested using very large sets of customers' reviews concerning the on-line hotel room booking. For more than 15 languages, there were available several millions of reviews. The resulting dictionaries included only about 200 significant words. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),18,opinion expressed by text document freely written in various natural language represent a valuable source of knowledge @ is hidden in @ datasets @ @ presented research describes a text mining-method @ to discover word @ @ significant @ expressing different opinion @ positive and negative @ @ @ method applies a simple @ unified data pre-processing @ @ language providing @ bag-of-words @ word represented by @ frequency in @ data @ @ @ frequency @ used by @ algorithm @ generates decision tree @ @ tree decisive node contain @ word @ @ significant @ expressing @ opinion @ position of @ word in @ tree represent @ significance degree @ @ @ significant word is in @ node @ a a @ a list of relevant word @ @ used @ creating a dictionary containing only relevant information @ @ described method wa tested @ @ @ set of customer @ review concerning @ on-line hotel room booking @ @ more @ language @ @ available several million of review @ @ resulting dictionary included only @ significant word @ springer-verlag @ 
3579,Novel nature inspired techniques in medical information retrieval,"In this work we have studied, evaluated and proposed different swarm intelligence techniques for mining information from loosely structured medical textual records with no apriori knowledge. We describe the process of mining a large dataset of ~50,000-120,000 records × 20 attributes in DB tables, originating from the hospital information system recording over 10 years. This paper concerns only textual attributes with free text input, that means 613,000 text fields in 16 attributes. Each attribute item contains ~800-1,500 characters (diagnoses, medications, etc.). The output of this task is a set of ordered/nominal attributes suitable for rule discovery mining. Information mining from textual data becomes a very challenging task when the structure of the text record is very loose without any rules. The task becomes even harder when natural language is used and no apriori knowledge is available. The medical environment itself is also very specific: the natural language used in textual description varies with the personality creating the record, however it is restricted by terminology (i.e. medical terms, medical standards, etc.). Moreover, the typical patient record is filled with typographical errors, duplicates and many (nonstandard) abbreviations. Nature inspired methods have their origin in real nature processes and play an important role in the domain of artificial intelligence. They offer fast and robust solutions to many problems, although they belong to the branch of approximative methods. The high number of individuals and the decentralized approach to task coordination in the studied species revealed high degree of parallelism, self-organization and fault tolerance. First, classical approaches such as basic statistic approaches, word (and word sequence) frequency analysis, etc., have been used to simplify the textual data and provide an overview of the data. Finally, an ant-inspired self-organizing approach has been used to automatically provide a simplified dominant structure, presenting structure of the records in the human readable form that can be further utilized in the mining process as it describes the vast majority of the records. Note that this project is an ongoing process (and research) and new data are irregularly received from the medical facility, justifying the need for robust and fool-proof algorithms. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ work @ @ studied evaluated and proposed different swarm intelligence technique @ mining information @ loosely structured medical textual record @ no apriori knowledge @ @ describe @ process of mining a @ dataset of record attribute in db table originating @ @ hospital information system recording @ year @ @ @ concern only textual attribute @ free text input @ mean text field in attribute @ @ attribute item contains character @ diagnosis medication etc @ @ @ @ output of @ task is a set of ordered nominal attribute suitable @ rule discovery mining @ information mining @ textual data becomes a @ challenging task @ @ structure of @ text record is @ loose without @ rule @ @ task becomes even harder @ natural language is used and no apriori knowledge is available @ @ medical environment @ is @ @ specific @ @ natural language used in textual description varies @ @ personality creating @ record however @ is restricted by terminology @ i @ e @ medical term medical standard etc @ @ @ moreover @ typical patient record is filled @ typographical error duplicate and many @ nonstandard @ abbreviation @ nature inspired method @ @ origin in real nature process and play @ important role in @ domain of artificial intelligence @ @ offer fast and robust solution to many problem although @ belong to @ branch of approximative method @ @ high number of individual and @ decentralized approach to task coordination in @ studied specie revealed high degree of parallelism self-organization and fault tolerance @ first classical approach @ a basic statistic approach word @ and word sequence @ frequency analysis etc @ @ @ used to simplify @ textual data and provide @ overview of @ data @ finally @ ant-inspired self-organizing approach ha @ used to automatically provide a simplified dominant structure presenting structure of @ record in @ human readable form @ @ @ @ utilized in @ mining process a @ describes @ vast majority of @ record @ note @ @ project is @ ongoing process @ and research @ and @ data @ irregularly received @ @ medical facility justifying @ need @ robust and fool-proof algorithm @ springer-verlag @ 
3591,Automatic argumentative analysis for interaction mining,"Interaction mining is about discovering and extracting insightful information from digital conversations, namely those human-human information exchanges mediated by digital network technology. We present in this article a computational model of natural arguments and its implementation for the automatic argumentative analysis of digital conversations, which allows us to produce relevant information to build interaction business analytics applications overcoming the limitations of standard text mining and information retrieval technology. Applications include advanced visualisations and abstractive summaries. © 2011 Copyright Taylor and Francis Group, LLC.",2011,Argument and Computation,12,interaction mining is @ discovering and extracting insightful information @ digital conversation namely @ human-human information exchange mediated by digital network technology @ @ @ in @ article a computational model of natural argument and @ implementation @ @ automatic argumentative analysis of digital conversation @ allows u to produce relevant information to build interaction @ analytics application overcoming @ limitation of standard text mining and information retrieval technology @ application include advanced visualisation and abstractive summary @ @ taylor and francis group llc @ 
3592,A novel text classification approach based on cloud concept jumping up,"The uncertainty of natural language is the most important problem in text mining based on machine learning. In order to reduce the interference of this uncertainty to text classification, a novel text classifier based on cloud concept jumping up (CCJU) is proposed. Based on cloud model theory, it can efficiently accomplish conversion between qualitative concept and quantitative data. Through the conversion from text set to text information table based on VSM model, the text qualitative concept, which is extraction from the same category, is jumping up as a whole category concept. According to compare the cloud similarity between the test text and each category concept, the test text is assigned to the most similar category. By the comparison among different text classifiers in different feature selection set, it full proves CCJU not only has a strong ability to adapt to the different text features, the classification performance is also better than the traditional classifiers. © 2005 by Binary Information Press.",2011,Journal of Computational Information Systems,1,@ uncertainty of natural language is @ @ important problem in text mining based on machine learning @ in order to reduce @ interference of @ uncertainty to text classification a novel text classifier based on cloud concept jumping up @ ccju @ is proposed @ based on cloud model theory @ @ efficiently accomplish conversion @ qualitative concept and quantitative data @ @ @ conversion @ text set to text information table based on vsm model @ text qualitative concept @ is extraction @ @ @ category is jumping up a a whole category concept @ according to compare @ cloud similarity @ @ test text and @ category concept @ test text is assigned to @ @ similar category @ by @ comparison among different text classifier in different feature selection set @ full prof ccju not only ha a strong ability to adapt to @ different text feature @ classification performance is @ better @ @ traditional classifier @ by binary information @ @ 
3599,Exploiting information extraction techniques for automatic semantic video indexing with an application to Turkish news videos,"This paper targets at the problem of automatic semantic indexing of news videos by presenting a video annotation and retrieval system which is able to perform automatic semantic annotation of news video archives and provide access to the archives via these annotations. The presented system relies on the video texts as the information source and exploits several information extraction techniques on these texts to arrive at representative semantic information regarding the underlying videos. These techniques include named entity recognition, person entity extraction, coreference resolution, and semantic event extraction. Apart from the information extraction components, the proposed system also encompasses modules for news story segmentation, text extraction, and video retrieval along with a news video database to make it a full-fledged system to be employed in practical settings. The proposed system is a generic one employing a wide range of techniques to automate the semantic video indexing process and to bridge the semantic gap between what can be automatically extracted from videos and what people perceive as the video semantics. Based on the proposed system, a novel automatic semantic annotation and retrieval system is built for Turkish and evaluated on a broadcast news video collection, providing evidence for its feasibility and convenience for news videos with a satisfactory overall performance. © 2011 Elsevier B.V. All rights reserved.",2011,Knowledge-Based Systems,24,@ @ target at @ problem of automatic semantic indexing of news video by presenting a video annotation and retrieval system @ is able to perform automatic semantic annotation of news video archive and provide access to @ archive via @ annotation @ @ presented system relies on @ video text a @ information source and exploit several information extraction technique on @ text to arrive at representative semantic information regarding @ underlying video @ @ technique include named entity recognition person entity extraction coreference resolution and semantic event extraction @ apart @ @ information extraction component @ proposed system @ encompasses module @ news story segmentation text extraction and video retrieval along @ a news video database to make @ a full-fledged system to @ employed in practical setting @ @ proposed system is a generic @ employing a wide range of technique to automate @ semantic video indexing process and to bridge @ semantic gap @ @ @ @ automatically extracted @ video and @ people perceive a @ video semantics @ based on @ proposed system a novel automatic semantic annotation and retrieval system is built @ turkish and evaluated on a broadcast news video collection providing evidence @ @ feasibility and convenience @ news video @ a satisfactory overall performance @ @ b @ v @ @ right reserved @ 
3601,Text mining with application to academic libraries,"Approximately 90% of the world's data is held in unstructured formats which mean that it is getting harder for people to extract information from huge amount of data. Motivated by the practical need of specific types of real world data analysis problems, many mining algorithms and knowledge management systems are designed and studied. The management of a vast amount of unstructured customer-related knowledge in academic libraries has become area of significant recent interest. In this paper, we proposed a scheme organizing customer knowledge in academic libraries by using text mining technology. The entity extraction phase called Named Entity Recognition aims to discover proper names, their variations and classes. A database with related entities needs to be created for entity extraction and correlation processes. Benefited from LRD method for text mining, the scheme provides a formal and explicit specification to deliver a shared conceptualization of customer knowledge. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,5,approximately of @ world @ s data is held in unstructured format @ mean @ @ is getting harder @ people to extract information @ huge amount of data @ motivated by @ practical need of specific type of real world data analysis problem many mining algorithm and knowledge management system @ designed and studied @ @ management of a vast amount of unstructured customer-related knowledge in @ library ha become area of significant recent interest @ in @ @ @ proposed a scheme organizing customer knowledge in @ library by @ text mining technology @ @ entity extraction phase called named entity recognition aim to discover proper name @ variation and class @ a database @ related entity need to @ created @ entity extraction and correlation process @ benefited @ lrd method @ text mining @ scheme provides a formal and explicit specification to deliver a shared conceptualization of customer knowledge @ springer-verlag @ 
3602,"Introducing CAPER, a collaborative platform for open and closed information acquisition, processing and linking","We introduce the CAPER project (Collaborative information, Acquisition, Processing, Exploitation and Reporting), partially funded by the European Commission. The goal of CAPER is to create a common platform for the prevention of organized crime through sharing, exploitation and linking of Open and Closed information Sources. CAPER will support collaborative multilingual analysis of unstructured and audiovisual contents, based on Text Mining and Visual Analytics technologies. CAPER will allow Law Enforcement Agencies (LEAs) to share informational, investigative and experiential knowledge. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,2,@ introduce @ caper project @ collaborative information acquisition processing exploitation and reporting @ partially funded by @ european commission @ @ goal of caper is to create a common platform @ @ prevention of organized crime @ sharing exploitation and linking of open and closed information source @ caper @ support collaborative multilingual analysis of unstructured and audiovisual content based on text mining and visual analytics technology @ caper @ allow law enforcement agency @ lea @ to share informational investigative and experiential knowledge @ springer-verlag @ 
3603,Extracting named entities from prophetic narration texts (Hadith),"In this paper, we report our work on a Finite State Transducer-based entity extractor, which applies named-entity extraction techniques to identify useful entities from prophetic narrations texts. A Finite State Transducer has been implemented in order to capture different types of named entities. For development and testing purposes, we collected a set of prophetic narrations texts from ""Sahîh Al-Bukhari"" corpus. Preliminary evaluation results demonstrated that our approach is feasible. Our system achieved encouraging precision and recall rates, the overall precision and recall are 71% and 39% respectively. Our future work includes conducting larger-scale evaluation studies and enhancing the system to capture named entities from chains of transmitters (Salasil Al-Assanid) and biographical texts of narrators (Tarajims). © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,12,in @ @ @ report @ work on a finite state transducer-based entity extractor @ applies named-entity extraction technique to identify useful entity @ prophetic narration text @ a finite state transducer ha @ implemented in order to capture different type of named entity @ @ development and testing purpose @ collected a set of prophetic narration text @ @ sahîh al-bukhari @ corpus @ preliminary evaluation @ demonstrated @ @ approach is feasible @ @ system achieved encouraging precision and recall rate @ overall precision and recall @ and respectively @ @ future work includes conducting larger-scale evaluation study and enhancing @ system to capture named entity @ chain of transmitter @ salasil al-assanid @ and biographical text of narrator @ tarajims @ @ springer-verlag @ 
3604,Statistical character-based syntax similarity measurement for detecting biomedical syntax variations through named entity recognition,"In this study an approach for detecting biomedical syntax variations through the Named Entity Recognition (NER) called Statistical Character-Based Syntax Similarity (SCSS) is proposed which is used by dictionary-based NER approaches. Named Entity Recognition for biomedical literatures is extraction and recognition of biomedical names. There are different types of NER approaches, that the most common one is dictionary-based approaches. For a given unknown pattern, Dictionary-Based approaches, search through a biomedical dictionary and finds the most common similar patterns to assign their biomedical types to the given unknown pattern. Biomedical literatures include syntax variations, which means two different patterns, refer to the same biomedical named entity. Hence a similarity function should be able to support all of the possible syntax variations. There are three syntax variations namely: (i) character-level, (ii) word-level, and (iii) word order. The SCSS is able to detect all of the mentioned syntax vitiations. This study is evaluated based on two measures: recall and precision which are used to calculate a balanced F-score. Result is satisfied as recall is 92.47% and precision is 96.7%, while the f-test is 94.53%. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,2,in @ study @ approach @ detecting biomedical syntax variation @ @ named entity recognition @ ner @ called statistical character-based syntax similarity @ sc @ is proposed @ is used by dictionary-based ner approach @ named entity recognition @ biomedical literature is extraction and recognition of biomedical name @ @ @ different type of ner approach @ @ @ common @ is dictionary-based approach @ @ a given unknown pattern dictionary-based approach search @ a biomedical dictionary and find @ @ common similar pattern to assign @ biomedical type to @ given unknown pattern @ biomedical literature include syntax variation @ mean @ different pattern refer to @ @ biomedical named entity @ hence a similarity function @ @ able to support @ of @ possible syntax variation @ @ @ three syntax variation namely @ @ i @ character-level @ ii @ word-level and @ iii @ word order @ @ sc is able to detect @ of @ mentioned syntax vitiation @ @ study is evaluated based on @ measure @ recall and precision @ @ used to calculate a balanced f-score @ @ is satisfied a recall is @ and precision is @ @ @ f-test is @ @ springer-verlag @ 
3605,Using conceptual graphs for text mining in technical support services,Text mining problems of natural text classification and fact extraction are important in developing information systems for Technical Support Services. An approach which is based on joining acquisition of conceptual graphs and keywords search technique is presented to their solution. Conceptual graphs have been created from e-mail queries sent to Technical Support Service. Correct conceptual graphs acquired from e-mail texts represent facts and situations which become patterns to search in systems resources to resolve users problems. Experimental results of implementing proposed approach are presented. © 2011 Springer-Verlag Berlin Heidelberg.,2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,text mining problem of natural text classification and fact extraction @ important in developing information system @ technical support service @ @ approach @ is based on joining acquisition of conceptual graph and keywords search technique is presented to @ solution @ conceptual graph @ @ created @ e-mail query sent to technical support service @ correct conceptual graph acquired @ e-mail text represent fact and situation @ become pattern to search in system resource to resolve user problem @ experimental @ of implementing proposed approach @ presented @ springer-verlag @ @ @ 
3607,Optimization and generation of knowledge model for supporting technology innovation,"In information society, there are now worldwide economic network that are expanding prodigiously. There could be argued that social structure is in dire need of supporting technology innovation. This paper seeks to analysis and how to generate of knowledge model that supporting policy making of technology innovation. We let the user focus on some interested parts a whole decision making and to analysis using case and how to generate knowledge model and network through advanced semantic mining and formal language. Knowledge model is created to topic network that compositing driven keyword through text mining from natural language in document. And we show that the way of analyzing knowledge model and automatically generating feature keyword and relation properties into topic networks. In order that we analyze the semantic relationship and knowledge properties using CTL(Computational Tree Logic) on organized topic map and Kripke structure. © 2011 Springer-Verlag.",2011,Communications in Computer and Information Science,0,in information society @ @ now worldwide economic network @ @ expanding prodigiously @ @ could @ argued @ social structure is in dire need of supporting technology innovation @ @ @ seek to analysis and @ to generate of knowledge model @ supporting policy making of technology innovation @ @ let @ user focus on some interested part a whole decision making and to analysis @ case and @ to generate knowledge model and network @ advanced semantic mining and formal language @ knowledge model is created to topic network @ compositing driven keyword @ text mining @ natural language in document @ and @ @ @ @ way of analyzing knowledge model and automatically generating feature keyword and relation property @ topic network @ in order @ @ analyze @ semantic relationship and knowledge property @ ctl @ computational tree logic @ on organized topic map and kripke structure @ springer-verlag @ 
3609,Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics,"With the rapid growth of the Internet, the ability of users to create and publish content has created active electronic communities that provide a wealth of product information. However, the high volume of reviews that are typically published for a single product makes harder for individuals as well as manufacturers to locate the best reviews and understand the true underlying quality of a product. In this paper, we reexamine the impact of reviews on economic outcomes like product sales and see how different factors affect social outcomes such as their perceived usefulness. Our approach explores multiple aspects of review text, such as subjectivity levels, various measures of readability and extent of spelling errors to identify important text-based features. In addition, we also examine multiple reviewer-level features such as average usefulness of past reviews and the self-disclosed identity measures of reviewers that are displayed next to a review. Our econometric analysis reveals that the extent of subjectivity, informativeness, readability, and linguistic correctness in reviews matters in influencing sales and perceived usefulness. Reviews that have a mixture of objective, and highly subjective sentences are negatively associated with product sales, compared to reviews that tend to include only subjective or only objective information. However, such reviews are rated more informative (or helpful) by other users. By using Random Forest-based classifiers, we show that we can accurately predict the impact of reviews on sales and their perceived usefulness. We examine the relative importance of the three broad feature categories: reviewer-related features, review subjectivity features, and review readability features, and find that using any of the three feature sets results in a statistically equivalent performance as in the case of using all available features. This paper is the first study that integrates econometric, text mining, and predictive modeling techniques toward a more complete analysis of the information captured by user-generated online reviews in order to estimate their helpfulness and economic impact. © 2011 IEEE.",2011,IEEE Transactions on Knowledge and Data Engineering,681,@ @ rapid growth of @ internet @ ability of user to create and publish content ha created active electronic community @ provide a wealth of product information @ however @ high volume of review @ @ typically published @ a single product make harder @ individual a well a manufacturer to locate @ best review and understand @ true underlying quality of a product @ in @ @ @ reexamine @ impact of review on economic outcome like product sale and see @ different factor affect social outcome @ a @ perceived usefulness @ @ approach explores multiple aspect of review text @ a subjectivity level various measure of readability and extent of spelling error to identify important text-based feature @ in addition @ @ examine multiple reviewer-level feature @ a average usefulness of past review and @ self-disclosed identity measure of reviewer @ @ displayed next to a review @ @ econometric analysis reveals @ @ extent of subjectivity informativeness readability and linguistic correctness in review matter in influencing sale and perceived usefulness @ review @ @ a mixture of objective and highly subjective sentence @ negatively associated @ product sale compared to review @ tend to include only subjective @ only objective information @ however @ review @ rated more informative @ @ helpful @ by @ user @ by @ random forest-based classifier @ @ @ @ @ accurately predict @ impact of review on sale and @ perceived usefulness @ @ examine @ relative importance of @ three broad feature category @ reviewer-related feature review subjectivity feature and review readability feature and find @ @ @ of @ three feature set @ in a statistically equivalent performance a in @ case of @ @ available feature @ @ @ is @ first study @ integrates econometric text mining and predictive modeling technique toward a more complete analysis of @ information captured by user-generated online review in order to estimate @ helpfulness and economic impact @ @ @ 
3612,Opinion classification techniques applied to a Spanish corpus,"Sentiment analysis is a new challenging task related to Text Mining and Natural Language Processing. Although there are some current works, most of them only focus on English texts. Web pages, information and opinions on the Internet are increasing every day, and English is not the only language used to write them. Other languages like Spanish are increasingly present so we have carried out some experiments over a Spanish film reviews corpus. In this paper we present several experiments using five classification algorithms (SVM, Nave Bayes, BBR, KNN, C4.5). The results obtained are very promising and encourage us to continue investigating in this line. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),23,sentiment analysis is a @ challenging task related to text mining and natural language processing @ although @ @ some current work @ of @ only focus on english text @ web page information and opinion on @ internet @ increasing every day and english is not @ only language used to write @ @ @ language like spanish @ increasingly @ @ @ @ carried @ some experiment @ a spanish film review corpus @ in @ @ @ @ several experiment @ five classification algorithm @ svm nave bayes bbr knn c @ @ @ @ @ obtained @ @ promising and encourage u to continue investigating in @ line @ springer-verlag @ 
3613,Combining multiple disambiguation methods for gene mention normalization,"The rapid growth of biomedical literature prompts pervasive concentrations of biomedical text mining community to explore methodology for accessing and managing this ever-increasing knowledge. One important task of text mining in biomedical literature is gene mention normalization which recognizes the biomedical entities in biomedical texts and maps each gene mention discussed in the text to unique organic database identifiers. In this work, we employ an information retrieval based method which extracts gene mention's semantic profile from PubMed abstracts for gene mention disambiguation. This disambiguation method focuses on generating a more comprehensive representation of gene mention rather than the organic clues such as gene ontology which has fewer co-occurrences with the gene mention. Furthermore, we use an existing biomedical resource as another disambiguation method. Then we extract features from gene mention detection system's outcome to build a false positive filter according to Wikipedia's retrieved documents. Our system achieved F-measure of 83.1% on BioCreative II GN test data. © 2011 Elsevier Ltd. All rights reserved.",2011,Expert Systems with Applications,3,@ rapid growth of biomedical literature prompt pervasive concentration of biomedical text mining community to explore methodology @ accessing and managing @ ever-increasing knowledge @ @ important task of text mining in biomedical literature is gene mention normalization @ recognizes @ biomedical entity in biomedical text and map @ gene mention discussed in @ text to unique organic database identifier @ in @ work @ employ @ information retrieval based method @ extract gene mention @ s semantic profile @ pubmed abstract @ gene mention disambiguation @ @ disambiguation method focus on generating a more comprehensive representation of gene mention rather @ @ organic clue @ a gene ontology @ ha fewer co-occurrence @ @ gene mention @ furthermore @ use @ existing biomedical resource a another disambiguation method @ @ @ extract feature @ gene mention detection system @ s outcome to build a false positive filter according to wikipedia @ s retrieved document @ @ system achieved f-measure of @ on biocreative ii gn test data @ @ ltd @ @ right reserved @ 
3614,Context constraint disambiguation of word semantics by field association schemes,"Word sense disambiguation is important in various aspects of natural language processing, including Internet search engines, machine translation, text mining, etc. However, the traditional methods using case frames are not effective for solving context ambiguities that requires information beyond sentences. This paper presents a new scheme for solving context ambiguities using a field association scheme. Generally, the scope of case frames is restricted to one sentence; however, the scope of the field association scheme can be applied to a set of sentences. In this paper, a formal disambiguation algorithm is proposed to control the scope for a set of variable number of sentences with ambiguities as well as solve ambiguities by calculating the weight of fields. In the experiments, 52 English and 20 Chinese words are disambiguated by using 104,532 Chinese and 38,372 English field association terms. The accuracy of the proposed field association scheme for context ambiguities is 65% higher than the case frame method. The proposed scheme shows better results than other three known methods, namely UNED-LS-U, IIT-2, and Relative-based in corpus SENSEVAL-2. © 2010 Elsevier Ltd. All rights reserved.",2011,Information Processing and Management,3,word sense disambiguation is important in various aspect of natural language processing including internet search engine machine translation text mining etc @ however @ traditional method @ case frame @ not effective @ solving context ambiguity @ requires information beyond sentence @ @ @ @ a @ scheme @ solving context ambiguity @ a field association scheme @ generally @ scope of case frame is restricted to @ sentence @ however @ scope of @ field association scheme @ @ applied to a set of sentence @ in @ @ a formal disambiguation algorithm is proposed to control @ scope @ a set of variable number of sentence @ ambiguity a well a solve ambiguity by calculating @ weight of field @ in @ experiment english and chinese word @ disambiguated by @ chinese and english field association term @ @ accuracy of @ proposed field association scheme @ context ambiguity is higher @ @ case frame method @ @ proposed scheme @ better @ @ @ three known method namely uned-ls-u iit and relative-based in corpus senseval @ @ ltd @ @ right reserved @ 
3615,Building a DDC-annotated corpus from OAI metadata,"Document servers complying to the standards of the Open Archives Initiative (OAI) are rich, yet seldom exploited source of textual primary data for research fields in text mining, natural language processing or computational linguistics. We present a bilingual (English and German) text corpus consisting of bibliographic OAI records and the associated full texts. A particular added value is that we annotated each record with at least one Dewey Decimal Classification (DDC) number, inducing a subject-based categorization of the corpus. By this means, it can be used as training data for machine learning-based text categorization tasks in digital libraries, but also as primary data source for linguistic research on academic language use related to specific disciplines. We describe the construction of the corpus using data from the Bielefeld Academic Search Engine (BASE), as well as its characteristics.",2011,Journal of Digital Information,11,document server complying to @ standard of @ open archive initiative @ oai @ @ rich yet seldom exploited source of textual primary data @ research field in text mining natural language processing @ computational linguistics @ @ @ a bilingual @ english and german @ text corpus consisting of bibliographic oai record and @ associated full text @ a particular added value is @ @ annotated @ record @ at least @ dewey decimal classification @ ddc @ number inducing a subject-based categorization of @ corpus @ by @ mean @ @ @ used a training data @ machine learning-based text categorization task in digital library @ @ a primary data source @ linguistic research on @ language use related to specific discipline @ @ describe @ construction of @ corpus @ data @ @ bielefeld @ search engine @ base @ a well a @ characteristic @ 
3616,A natural language interface for data warehouse question answering,"Business Intelligence (BI) aims at providing methods and tools that lead to quick decisions from trusted data. Such advanced tools require some technical knowledge on how to formulate the queries. We propose a natural language (NL) interface for a Data Warehouse based Question Answering system. This system allows users to query with questions expressed in natural language. The proposed system is fully automated, resulting low Total Cost of Ownership. We aim at demonstrating the importance of identifying already existing semantics and using Text Mining techniques on the Web to move toward the users's need. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9,@ intelligence @ bi @ aim at providing method and tool @ lead to quick decision @ trusted data @ @ advanced tool require some technical knowledge on @ to formulate @ query @ @ propose a natural language @ nl @ interface @ a data warehouse based question answering system @ @ system allows user to query @ question expressed in natural language @ @ proposed system is fully automated resulting low total cost of ownership @ @ aim at demonstrating @ importance of identifying already existing semantics and @ text mining technique on @ web to move toward @ user @ s need @ springer-verlag @ 
3618,Style analysis of academic writing,"This paper presents an approach which performs a Style Analysis of Academic Writing in terms of formal voice, readability and scientific language. Our intention is an analysis of academic writing style as a feedback for the authors and editors. The extracted features of a document collection are used to create Self-Organizing Maps which are the interim results to generate reports in our Full Automatic Paper Analysis System (Fapas). To evaluate this method, the system has to solve different tasks to verify the informative value of the generated maps and reports. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ @ @ approach @ performs a style analysis of @ writing in term of formal voice readability and scientific language @ @ intention is @ analysis of @ writing style a a feedback @ @ author and editor @ @ extracted feature of a document collection @ used to create self-organizing map @ @ @ interim @ to generate report in @ full automatic @ analysis system @ fapas @ @ to evaluate @ method @ system ha to solve different task to verify @ informative value of @ generated map and report @ springer-verlag @ 
3620,Total environment for text data mining,"In this challenge, we develop and distribute an integrated environment to flexibly combine multiple text mining techniques. Text mining techniques include numerous tasks such as salient sentence extraction, keyword extraction, topic extraction, textual coherence evaluation, multi-document summarization, and text clustering. Although tools that individually perform one or more of the above-mentioned tasks exist, it is difficult to integrate and activate multiple tools for a particular task. We attempt to provide the flexibility to integrate numerous tools that exist in the community in our proposed text mining environment. Users can use a customized version of the proposed text mining environment for their specific tasks, thereby concentrating solely on their creative work.",2011,Transactions of the Japanese Society for Artificial Intelligence,0,in @ challenge @ develop and distribute @ integrated environment to flexibly combine multiple text mining technique @ text mining technique include numerous task @ a salient sentence extraction keyword extraction topic extraction textual coherence evaluation multi-document summarization and text clustering @ although tool @ individually perform @ @ more of @ above-mentioned task exist @ is difficult to integrate and activate multiple tool @ a particular task @ @ attempt to provide @ flexibility to integrate numerous tool @ exist in @ community in @ proposed text mining environment @ user @ use a customized version of @ proposed text mining environment @ @ specific task thereby concentrating solely on @ creative work @ 
3622,Semantic oriented clustering of documents,"Semantic web-based approaches and computational intelligence can be merged in order to get useful tools for several data mining issues. In this work a web-based tagging process followed by a validation step is carried to tag WordNet adjectives with positive, neutral or negative moods. This tagged WordNet is used to define a semantic metric for text documents clustering. Experimental results on movie reviews prove that the introduced semantically oriented metric is extremely fast and gives improved results with respect to the classical frequency based text mining metric from the accuracy point of view. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,semantic web-based approach and computational intelligence @ @ merged in order to get useful tool @ several data mining issue @ in @ work a web-based tagging process followed by a validation step is carried to tag wordnet adjective @ positive neutral @ negative mood @ @ tagged wordnet is used to define a semantic metric @ text document clustering @ experimental @ on movie review prove @ @ introduced semantically oriented metric is extremely fast and give improved @ @ respect to @ classical frequency based text mining metric @ @ accuracy point of view @ springer-verlag @ 
3623,BursT: A dynamic term weighting scheme for mining microblogging messages,"One of the basic human needs is to exchange information and socialize with each other. Online microblogging services such as Twitter allow users to post very short messages related to everything ranging from mundane daily life routines to breaking news events. A key challenging issue of mining such social messages is how to analyze the real-time distributed messages and extract significant features of them in a dynamic environment. In this work, we propose a novel term weighting method, called BursT, using sliding window techniques for weighting message streams. The experimental results show that our weighting technique has an outstanding performance to reflect the shifts of concept drift. The result of this work can be extended to perform a periodic feature extraction, and also be able to integrate other sophisticated clustering methods to enhance the efficiency for real-time event mining in social networks. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),29,@ of @ basic human need is to exchange information and socialize @ @ @ @ online microblogging service @ a twitter allow user to post @ short message related to everything ranging @ mundane daily life routine to breaking news event @ a key challenging issue of mining @ social message is @ to analyze @ real-time distributed message and extract significant feature of @ in a dynamic environment @ in @ work @ propose a novel term weighting method called burst @ sliding window technique @ weighting message stream @ @ experimental @ @ @ @ weighting technique ha @ outstanding performance to reflect @ shift of concept drift @ @ @ of @ work @ @ extended to perform a periodic feature extraction and @ @ able to integrate @ sophisticated clustering method to enhance @ efficiency @ real-time event mining in social network @ springer-verlag @ 
3625,Cross-partition clustering: Revealing corresponding themes across related datasets,"This article studies the task of discovering correspondences across related domains based on real-world data collections. We address this task through a designated extension of distributional data-clustering methods. The method is empirically demonstrated on synthetic data as well as on texts addressing different religions, where the goal is to identify commonalities shared by all religions. This article generalises and demonstrates the empirical improvement relative to our previous studies on this subject, as well as to other comparable methods. © 2011 Taylor & Francis.",2011,Journal of Experimental and Theoretical Artificial Intelligence,0,@ article study @ task of discovering correspondence across related domain based on real-world data collection @ @ address @ task @ a designated extension of distributional data-clustering method @ @ method is empirically demonstrated on synthetic data a well a on text addressing different religion @ @ goal is to identify commonality shared by @ religion @ @ article generalises and demonstrates @ empirical improvement relative to @ previous study on @ subject a well a to @ comparable method @ taylor francis @ 
3628,Fuzzy document clustering using weighted conceptual model,"Document clustering techniques mostly rely on single term analysis which can not reveal the potential semantic relationship between terms. To better capture the semantic subject of documents, this study proposes weighted conceptual model for document presentation. The new model divides the document concepts into centroid concepts and peripheral concepts due to their semantic relations to subject. The semantic similarity between two documents is calculated by centroid concepts and peripheral concepts respectively. A fuzzy semantic clustering method is put forward bases on the new semantic model. Experimental results show that the method enhances semi-structured document clustering quality significantly and outperforms K-Means and Fuzzy C-Means. © 2011 Asian Network for Scientific Information.",2011,Information Technology Journal,11,document clustering technique mostly rely on single term analysis @ @ not reveal @ potential semantic relationship @ term @ to better capture @ semantic subject of document @ study proposes weighted conceptual model @ document presentation @ @ @ model divide @ document concept @ centroid concept and peripheral concept due to @ semantic relation to subject @ @ semantic similarity @ @ document is calculated by centroid concept and peripheral concept respectively @ a fuzzy semantic clustering method is put forward base on @ @ semantic model @ experimental @ @ @ @ method enhances semi-structured document clustering quality significantly and outperforms k-means and fuzzy c-means @ asian network @ scientific information @ 
3631,Let me tell you a story - on how to build process models,"Process Modeling has been a very active research topic for the last decades. One of its main issues is the externalization of knowledge and its acquisition for further use, as this remains deeply related to the quality of the resulting process models produced by this task. This paper presents a method and a graphical supporting tool for process elicitation and modeling, combining the Group Storytelling technique with the advances of Text Mining and Natural Language Processing. The implemented tool extends its previous versions with several functionalities to facilitate group story telling by the users, as well as to improve the results of the acquired process model from the stories. © J.UCS.",2011,Journal of Universal Computer Science,16,process modeling ha @ a @ active research topic @ @ last decade @ @ of @ main issue is @ externalization of knowledge and @ acquisition @ @ use a @ remains deeply related to @ quality of @ resulting process model produced by @ task @ @ @ @ a method and a graphical supporting tool @ process elicitation and modeling combining @ group storytelling technique @ @ advance of text mining and natural language processing @ @ implemented tool extends @ previous version @ several functionality to facilitate group story telling by @ user a well a to improve @ @ of @ acquired process model @ @ story @ j @ ucs @ 
3634,Syntactic tree kernels for event-time temporal relation learning,"Temporal relation classification is one of the contemporary demanding tasks in natural language processing. This task can be used in various applications such as question answering, summarization, and language specific information retrieval. In this paper, we propose an improved algorithm for classifying temporal relations between events and times, using support vector machines (SVM). Along with gold-standard corpus features, the proposed method aims at exploiting useful syntactic features, which are automatically generated, to improve accuracy of the classification. Accordingly, a number of novel kernel functions are introduced and evaluated for temporal relation classification. The result of experiments clearly shows that adding syntactic features results in a notable performance improvement over the state of the art method, which merely employs gold-standard features. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,temporal relation classification is @ of @ contemporary demanding task in natural language processing @ @ task @ @ used in various application @ a question answering summarization and language specific information retrieval @ in @ @ @ propose @ improved algorithm @ classifying temporal relation @ event and time @ support vector machine @ svm @ @ along @ gold-standard corpus feature @ proposed method aim at exploiting useful syntactic feature @ @ automatically generated to improve accuracy of @ classification @ accordingly a number of novel kernel function @ introduced and evaluated @ temporal relation classification @ @ @ of experiment clearly @ @ adding syntactic feature @ in a notable performance improvement @ @ state of @ art method @ merely employ gold-standard feature @ springer-verlag @ 
3637,Social information retrieval based on user interesting mining,This paper proposed social information retrieval based on user interesting mining. Traditional text mining algorithms mostly represent documents based on their textual content. It is desirable to exploit information from other sources for web mining. This paper expanded the language model for social information retrieval with user interests. Experimental result shows that the proposed social information retrieval based on user interesting mining is effective. Copyright © 2011 Binary Information Press.,2011,Journal of Computational Information Systems,0,@ @ proposed social information retrieval based on user interesting mining @ traditional text mining algorithm mostly represent document based on @ textual content @ @ is desirable to exploit information @ @ source @ web mining @ @ @ expanded @ language model @ social information retrieval @ user interest @ experimental @ @ @ @ proposed social information retrieval based on user interesting mining is effective @ @ binary information @ @ 
3638,Disease named entity recognition using semisupervised learning and conditional random fields,"Information extraction is an important text-mining task that aims at extracting prespecified types of information from large text collections and making them available in structured representations such as databases. In the biomedical domain, information extraction can be applied to help biologists make the most use of their digital-literature archives. Currently, there are large amounts of biomedical literature that contain rich information about biomedical substances. Extracting such knowledge requires a good named entity recognition technique. In this article, we combine conditional random fields (CRFs), a state-of-the-art sequence-labeling algorithm, with two semisupervised learning techniques, bootstrapping and feature sampling, to recognize disease names from biomedical literature. Two data-processing strategies for each technique also were analyzed: one sequentially processing unlabeled data partitions and another one processing unlabeled data partitions in a round-robin fashion. The experimental results showed the advantage of semisupervised learning techniques given limited labeled training data. Specifically, CRFs with bootstrapping implemented in sequential fashion outperformed strictly supervised CRFs for disease name recognition. The project was supported by NIH/NLM Grant R33 LM07299-01, 2002-2005. © 2011 ASIS&T.",2011,Journal of the American Society for Information Science and Technology,12,information extraction is @ important text-mining task @ aim at extracting prespecified type of information @ @ text collection and making @ available in structured representation @ a database @ in @ biomedical domain information extraction @ @ applied to help biologist make @ @ use of @ digital-literature archive @ currently @ @ @ amount of biomedical literature @ contain rich information @ biomedical substance @ extracting @ knowledge requires a good named entity recognition technique @ in @ article @ combine conditional random field @ crfs @ a state-of-the-art sequence-labeling algorithm @ @ semisupervised learning technique bootstrapping and feature sampling to recognize disease name @ biomedical literature @ @ data-processing strategy @ @ technique @ @ analyzed @ @ sequentially processing unlabeled data partition and another @ processing unlabeled data partition in a round-robin fashion @ @ experimental @ showed @ advantage of semisupervised learning technique given limited labeled training data @ specifically crfs @ bootstrapping implemented in sequential fashion outperformed strictly supervised crfs @ disease name recognition @ @ project wa supported by nih nlm grant r lm - @ asis t @ 
3639,Using chi-square statistics to measure similarities for text categorization,"In this paper, we propose using chi-square statistics to measure similarities and chi-square tests to determine the homogeneity of two random samples of term vectors for text categorization. The properties of chi-square tests for text categorization are studied first. One of the advantages of chi-square test is that its significance level is similar to the miss rate that provides a foundation for theoretical performance (i.e. miss rate) guarantee. Generally a classifier using cosine similarities with TF IDF performs reasonably well in text categorization. However, its performance may fluctuate even near the optimal threshold value. To improve the limitation, we propose the combined usage of chi-square statistics and cosine similarities. Extensive experiment results verify properties of chi-square tests and performance of the combined usage. © 2010 Elsevier Ltd. All rights reserved.",2011,Expert Systems with Applications,56,in @ @ @ propose @ chi-square statistic to measure similarity and chi-square test to determine @ homogeneity of @ random sample of term vector @ text categorization @ @ property of chi-square test @ text categorization @ studied first @ @ of @ advantage of chi-square test is @ @ significance level is similar to @ miss rate @ provides a foundation @ theoretical performance @ i @ e @ miss rate @ guarantee @ generally a classifier @ cosine similarity @ tf idf performs reasonably well in text categorization @ however @ performance may fluctuate even near @ optimal threshold value @ to improve @ limitation @ propose @ combined usage of chi-square statistic and cosine similarity @ extensive experiment @ verify property of chi-square test and performance of @ combined usage @ @ ltd @ @ right reserved @ 
3640,A framework for efficient information retrieval using NLP techniques,"In the academic area, the Internet is used as a scientific resource. However, finding appropriate information on the Web remains difficult. To simplify this process, we designed the Information retrieval framework to retrieve information from the Web Using NLP Techniques. Many information retrieval systems are based on vector space model (VSM) that represents a document as a vector of index terms. To remedy this problem, documents and queries are optimized using NLP. In this paper, the architecture of the proposed tool is presented and it proposes a new approach over the traditional VSM by considering only the nouns and verbs of the documents extracted from NLP as the constituting terms for the VSM instead of the traditional term ""word"". Such a mechanism may raise the effectiveness of the Information Retrieval by increasing the evaluation metrics values. © 2011 Springer-Verlag Berlin Heidelberg.",2011,Communications in Computer and Information Science,9,in @ @ area @ internet is used a a scientific resource @ however finding appropriate information on @ web remains difficult @ to simplify @ process @ designed @ information retrieval framework to retrieve information @ @ web @ nlp technique @ many information retrieval system @ based on vector space model @ vsm @ @ represents a document a a vector of index term @ to remedy @ problem document and query @ optimized @ nlp @ in @ @ @ architecture of @ proposed tool is presented and @ proposes a @ approach @ @ traditional vsm by considering only @ noun and verb of @ document extracted @ nlp a @ constituting term @ @ vsm instead of @ traditional term @ word @ @ @ a mechanism may raise @ effectiveness of @ information retrieval by increasing @ evaluation metric value @ springer-verlag @ @ @ 
3646,Blogger-centric contextual advertising,"Web advertising (online advertising), a form of advertising that uses the World Wide Web to attract customers, has become one of the most commonly-used marketing channels. This paper addresses the concept of Blogger-Centric Contextual Advertising, which refers to the assignment of personal ads to any blog page, chosen in according to bloggers' interests. As blogs become a platform for expressing personal opinions, they naturally contain various kinds of statements, including facts, comments and statements about personal interests, of both a positive and negative nature. To extend the concept behind the Long Tail theory in contextual advertising, we argue that web bloggers, as the constant visitors of their own blog-sites, could be potential consumers who will respond to ads on their own blogs. Hence, in this paper, we propose using text mining techniques to discover bloggers' immediate personal interests in order to improve online contextual advertising. The proposed Blogger-Centric Contextual Advertising (BCCA) framework aims to combine contextual advertising matching with text mining in order to select ads that are related to personal interests as revealed in a blog and rank them according to their relevance. We validate our approach experimentally using a set of data that includes both real ads and actual blog pages. The results indicate that our proposed method could effectively identify those ads that are positively-correlated with a blogger's personal interests. © 2010 Elsevier Ltd. All rights reserved.",2011,Expert Systems with Applications,30,web advertising @ online advertising @ a form of advertising @ us @ world wide web to attract customer ha become @ of @ @ commonly-used marketing channel @ @ @ address @ concept of blogger-centric contextual advertising @ refers to @ assignment of personal ad to @ blog page chosen in according to blogger @ interest @ a blog become a platform @ expressing personal opinion @ naturally contain various kind of statement including fact comment and statement @ personal interest of @ a positive and negative nature @ to extend @ concept behind @ long tail theory in contextual advertising @ argue @ web blogger a @ constant visitor of @ @ blog-sites could @ potential consumer @ @ respond to ad on @ @ blog @ hence in @ @ @ propose @ text mining technique to discover blogger @ immediate personal interest in order to improve online contextual advertising @ @ proposed blogger-centric contextual advertising @ bcca @ framework aim to combine contextual advertising matching @ text mining in order to select ad @ @ related to personal interest a revealed in a blog and rank @ according to @ relevance @ @ validate @ approach experimentally @ a set of data @ includes @ real ad and actual blog page @ @ @ indicate @ @ proposed method could effectively identify @ ad @ @ positively-correlated @ a blogger @ s personal interest @ @ ltd @ @ right reserved @ 
3647,Automatic keyword extraction from documents based on multiple content-based measures,"Automatic keyword extraction has attracted much attention due to the many applications in information retrieval, text mining, information processing, etc. To extract desired keywords from documents automatically, other than lexical relations among words or sentences, the content-based measures are critical at the same time. From the perspective of information retrieval, we are to facilitate effective and intuitively-interpretable methods for automatic keyword extraction. In this paper, we propose multiple content-based measures for the weights, contributions to document classifications and coverage of words. We give the measure based on the Laplace's law to determine the weight of a word in documents, and give the measure based on the concept of average mutual information to determine the influence of a word on document classifications. Furthermore, we propose the concept of semantic coacervation degree as the measure for the coverage of words on documents, and give a branch and bound algorithm to find the minimal set of keywords that maximize the coverage. By addressing these content-based measures one by one, the desired keywords can be extracted automatically from documents. Experimental results show that our methods are not only efficient but also feasible. © 2011 CRL Publishing Ltd.",2011,Computer Systems Science and Engineering,5,automatic keyword extraction ha attracted much attention due to @ many application in information retrieval text mining information processing etc @ to extract desired keywords @ document automatically @ @ lexical relation among word @ sentence @ content-based measure @ critical at @ @ time @ @ @ perspective of information retrieval @ @ to facilitate effective and intuitively-interpretable method @ automatic keyword extraction @ in @ @ @ propose multiple content-based measure @ @ weight contribution to document classification and coverage of word @ @ give @ measure based on @ laplace @ s law to determine @ weight of a word in document and give @ measure based on @ concept of average mutual information to determine @ influence of a word on document classification @ furthermore @ propose @ concept of semantic coacervation degree a @ measure @ @ coverage of word on document and give a branch and bound algorithm to find @ minimal set of keywords @ maximize @ coverage @ by addressing @ content-based measure @ by @ @ desired keywords @ @ extracted automatically @ document @ experimental @ @ @ @ method @ not only efficient @ @ feasible @ crl publishing ltd @ 
3648,A probabilistic rating inference framework for mining user preferences from reviews,"We propose a novel Probabilistic Rating infErence Framework, known as PREF, for mining user preferences from reviews and then mapping such preferences onto numerical rating scales. PREF applies existing linguistic processing techniques to extract opinion words and product features from reviews. It then estimates the sentimental orientations (SO) and strength of the opinion words using our proposed relative-frequency-based method. This method allows semantically similar words to have different SO, thereby addresses a major limitation of existing methods. PREF takes the intuitive relationships between class labels, which are scalar ratings, into consideration when assigning ratings to reviews. Empirical results validated the effectiveness of PREF against several related algorithms, and suggest that PREF can produce reasonably good results using a small training corpus. We also describe a useful application of PREF as a rating inference framework. Rating inference transforms user preferences described as natural language texts into numerical rating scales. This allows Collaborative Filtering (CF) algorithms, which operate mostly on databases of scalar ratings, to utilize textual reviews as an additional source of user preferences. We integrated PREF with a classical CF algorithm, and empirically demonstrated the advantages of using rating inference to augment ratings for CF. © 2011 Springer Science+Business Media, LLC.",2011,World Wide Web,50,@ propose a novel probabilistic rating inference framework known a pref @ mining user preference @ review and @ mapping @ preference onto numerical rating scale @ pref applies existing linguistic processing technique to extract opinion word and product feature @ review @ @ @ estimate @ sentimental orientation @ @ @ and strength of @ opinion word @ @ proposed relative-frequency-based method @ @ method allows semantically similar word to @ different @ thereby address a major limitation of existing method @ pref take @ intuitive relationship @ class label @ @ scalar rating @ consideration @ assigning rating to review @ empirical @ validated @ effectiveness of pref @ several related algorithm and suggest @ pref @ produce reasonably good @ @ a small training corpus @ @ @ describe a useful application of pref a a rating inference framework @ rating inference transforms user preference described a natural language text @ numerical rating scale @ @ allows collaborative filtering @ cf @ algorithm @ operate mostly on database of scalar rating to utilize textual review a @ additional source of user preference @ @ integrated pref @ a classical cf algorithm and empirically demonstrated @ advantage of @ rating inference to augment rating @ cf @ @ science @ medium llc @ 
3649,"A comparative study of TF*IDF, LSI and multi-words for text classification","One of the main themes in text mining is text representation, which is fundamental and indispensable for text-based intellegent information processing. Generally, text representation inludes two tasks: indexing and weighting. This paper has comparatively studied TFIDF, LSI and multi-word for text representation. We used a Chinese and an English document collection to respectively evaluate the three methods in information retreival and text categorization. Experimental results have demonstrated that in text categorization, LSI has better performance than other methods in both document collections. Also, LSI has produced the best performance in retrieving English documents. This outcome has shown that LSI has both favorable semantic and statistical quality and is different with the claim that LSI can not produce discriminative power for indexing. © 2010 Elsevier Ltd. All rights reserved.",2011,Expert Systems with Applications,286,@ of @ main theme in text mining is text representation @ is fundamental and indispensable @ text-based intellegent information processing @ generally text representation inludes @ task @ indexing and weighting @ @ @ ha comparatively studied tfidf lsi and multi-word @ text representation @ @ used a chinese and @ english document collection to respectively evaluate @ three method in information retreival and text categorization @ experimental @ @ demonstrated @ in text categorization lsi ha better performance @ @ method in @ document collection @ @ lsi ha produced @ best performance in retrieving english document @ @ outcome ha @ @ lsi ha @ favorable semantic and statistical quality and is different @ @ claim @ lsi @ not produce discriminative power @ indexing @ @ ltd @ @ right reserved @ 
3652,An accuracy-enhanced light stemmer for arabic text,"Stemming is a key step in most text mining and information retrieval applications. Information extraction, semantic annotation, as well as ontology learning are but a few examples where using a stemmer is a must. While the use of light stemmers in Arabic texts has proven highly effective for the task of information retrieval, this class of stemmers falls short of providing the accuracy required by many text mining applications. This can be attributed to the fact that light stemmers employ a set of rules that they apply indiscriminately and that they do not address stemming of broken plurals at all, even though this class of plurals is very commonly used in Arabic texts. The goal of this work is to overcome these limitations. The evaluation of the work shows that it significantly improves stemming accuracy. It also shows that by improving stemming accuracy, tasks such as automatic annotation and keyphrase extraction can also be significantly improved. © 2011 ACM.",2011,ACM Transactions on Speech and Language Processing,21,stemming is a key step in @ text mining and information retrieval application @ information extraction semantic annotation a well a ontology learning @ @ a @ example @ @ a stemmer is a must @ @ @ use of light stemmer in arabic text ha proven highly effective @ @ task of information retrieval @ class of stemmer fall short of providing @ accuracy required by many text mining application @ @ @ @ attributed to @ fact @ light stemmer employ a set of rule @ @ apply indiscriminately and @ @ @ not address stemming of broken plural at @ even though @ class of plural is @ commonly used in arabic text @ @ goal of @ work is to overcome @ limitation @ @ evaluation of @ work @ @ @ significantly improves stemming accuracy @ @ @ @ @ by improving stemming accuracy task @ a automatic annotation and keyphrase extraction @ @ @ significantly improved @ acm @ 
3653,Classifying biomedical text abstracts based on hierarchical 'concept' structure,"Classifying biomedical literature is a difficult and challenging task, especially when a large number of biomedical articles should be organized into a hierarchical structure. In this paper, we present an approach for classifying a collection of biomedical text abstracts downloaded from Medline database with the help of ontology alignment. To accomplish our goal, we construct two types of hierarchies, the OHSUMED disease hierarchy and the Medline abstract disease hierarchies from the OHSUMED dataset and the Medline abstracts, respectively. Then, we enrich the OHSUMED disease hierarchy before adapting it to ontology alignment process for finding probable concepts or categories. Subsequently, we compute the cosine similarity between the vector in probable concepts (in the ""enriched"" OHSUMED disease hierarchy) and the vector in Medline abstract disease hierarchies. Finally, we assign category to the new Medline abstracts based on the similarity score. The results obtained from the experiments show the performance of our proposed approach for hierarchical classification is slightly better than the performance of the multi-class flat classification.",2011,"World Academy of Science, Engineering and Technology",0,classifying biomedical literature is a difficult and challenging task especially @ a @ number of biomedical article @ @ organized @ a hierarchical structure @ in @ @ @ @ @ approach @ classifying a collection of biomedical text abstract downloaded @ medline database @ @ help of ontology alignment @ to accomplish @ goal @ construct @ type of hierarchy @ ohsumed disease hierarchy and @ medline abstract disease hierarchy @ @ ohsumed dataset and @ medline abstract respectively @ @ @ enrich @ ohsumed disease hierarchy @ adapting @ to ontology alignment process @ finding probable concept @ category @ subsequently @ compute @ cosine similarity @ @ vector in probable concept @ in @ @ enriched @ ohsumed disease hierarchy @ and @ vector in medline abstract disease hierarchy @ finally @ assign category to @ @ medline abstract based on @ similarity score @ @ @ obtained @ @ experiment @ @ performance of @ proposed approach @ hierarchical classification is slightly better @ @ performance of @ multi-class flat classification @ 
3654,Classifying biomedical text abstracts based on hierarchical 'concept' structure,"Classifying biomedical literature is a difficult and challenging task, especially when a large number of biomedical articles should be organized into a hierarchical structure. In this paper, we present an approach for classifying a collection of biomedical text abstracts downloaded from Medline database with the help of ontology alignment. To accomplish our goal, we construct two types of hierarchies, the OHSUMED disease hierarchy and the Medline abstract disease hierarchies from the OHSUMED dataset and the Medline abstracts, respectively. Then, we enrich the OHSUMED disease hierarchy before adapting it to ontology alignment process for finding probable concepts or categories. Subsequently, we compute the cosine similarity between the vector in probable concepts (in the ""enriched"" OHSUMED disease hierarchy) and the vector in Medline abstract disease hierarchies. Finally, we assign category to the new Medline abstracts based on the similarity score. The results obtained from the experiments show the performance of our proposed approach for hierarchical classification is slightly better than the performance of the multi-class flat classification.",2011,"World Academy of Science, Engineering and Technology",0,classifying biomedical literature is a difficult and challenging task especially @ a @ number of biomedical article @ @ organized @ a hierarchical structure @ in @ @ @ @ @ approach @ classifying a collection of biomedical text abstract downloaded @ medline database @ @ help of ontology alignment @ to accomplish @ goal @ construct @ type of hierarchy @ ohsumed disease hierarchy and @ medline abstract disease hierarchy @ @ ohsumed dataset and @ medline abstract respectively @ @ @ enrich @ ohsumed disease hierarchy @ adapting @ to ontology alignment process @ finding probable concept @ category @ subsequently @ compute @ cosine similarity @ @ vector in probable concept @ in @ @ enriched @ ohsumed disease hierarchy @ and @ vector in medline abstract disease hierarchy @ finally @ assign category to @ @ medline abstract based on @ similarity score @ @ @ obtained @ @ experiment @ @ performance of @ proposed approach @ hierarchical classification is slightly better @ @ performance of @ multi-class flat classification @ 
3655,Selecting attributes for sentiment classification using feature relation networks,"A major concern when incorporating large sets of diverse n-gram features for sentiment classification is the presence of noisy, irrelevant, and redundant attributes. These concerns can often make it difficult to harness the augmented discriminatory potential of extended feature sets. We propose a rule-based multivariate text feature selection method called Feature Relation Network (FRN) that considers semantic information and also leverages the syntactic relationships between n-gram features. FRN is intended to efficiently enable the inclusion of extended sets of heterogeneous n-gram features for enhanced sentiment classification. Experiments were conducted on three online review testbeds in comparison with methods used in prior sentiment classification research. FRN outperformed the comparison univariate, multivariate, and hybrid feature selection methods; it was able to select attributes resulting in significantly better classification accuracy irrespective of the feature subset sizes. Furthermore, by incorporating syntactic information about n-gram relations, FRN is able to select features in a more computationally efficient manner than many multivariate and hybrid techniques. © 2006 IEEE.",2011,IEEE Transactions on Knowledge and Data Engineering,129,a major concern @ incorporating @ set of diverse n-gram feature @ sentiment classification is @ presence of noisy irrelevant and redundant attribute @ @ concern @ often make @ difficult to harness @ augmented discriminatory potential of extended feature set @ @ propose a rule-based multivariate text feature selection method called feature relation network @ frn @ @ considers semantic information and @ leverage @ syntactic relationship @ n-gram feature @ frn is intended to efficiently enable @ inclusion of extended set of heterogeneous n-gram feature @ enhanced sentiment classification @ experiment @ conducted on three online review testbeds in comparison @ method used in prior sentiment classification research @ frn outperformed @ comparison univariate multivariate and hybrid feature selection method @ @ wa able to select attribute resulting in significantly better classification accuracy irrespective of @ feature subset size @ furthermore by incorporating syntactic information @ n-gram relation frn is able to select feature in a more computationally efficient manner @ many multivariate and hybrid technique @ @ @ 
3656,Graph-based natural language processing and information retrieval,"Graph theory and the fields of natural language processing and information retrieval are well-studied disciplines. Traditionally, these areas have been perceived as distinct, with different algorithms, different applications and different potential end-users. However, recent research has shown that these disciplines are intimately connected, with a large variety of natural language processing and information retrieval applications finding efficient solutions within graph-theoretical frameworks. This book extensively covers the use of graph-based algorithms for natural language processing and information retrieval. It brings together topics as diverse as lexical semantics, text summarization, text mining, ontology construction, text classification and information retrieval, which are connected by the common underlying theme of the use of graph-theoretical methods for text and information processing tasks. Readers will come away with a firm understanding of the major methods and applications in natural language processing and information retrieval that rely on graph-based representations and algorithms. © Rada Mihalcea and Dragomir Radev 2011.",2011,Graph-Based Natural Language Processing and Information Retrieval,115,graph theory and @ field of natural language processing and information retrieval @ well-studied discipline @ traditionally @ area @ @ perceived a distinct @ different algorithm different application and different potential end-users @ however recent research ha @ @ @ discipline @ intimately connected @ a @ variety of natural language processing and information retrieval application finding efficient solution within graph-theoretical framework @ @ book extensively cover @ use of graph-based algorithm @ natural language processing and information retrieval @ @ brings together topic a diverse a lexical semantics text summarization text mining ontology construction text classification and information retrieval @ @ connected by @ common underlying theme of @ use of graph-theoretical method @ text and information processing task @ reader @ come away @ a firm understanding of @ major method and application in natural language processing and information retrieval @ rely on graph-based representation and algorithm @ rada mihalcea and dragomir radev @ 
3661,Using syntactic-based kernels for classifying temporal relations,"Temporal relation classification is one of contemporary demanding tasks of natural language processing. This task can be used in various applications such as question answering, summarization, and language specific information retrieval. In this paper, we propose an improved algorithm for classifying temporal relations, between events or between events and time, using support vector machines (SVM). Along with gold-standard corpus features, the proposed method aims at exploiting some useful automatically generated syntactic features to improve the accuracy of classification. Accordingly, a number of novel kernel functions are introduced and evaluated. Our evaluations clearly demonstrate that adding syntactic features results in a considerable improvement over the state-of-the-art method of classifying temporal relations. © 2011 Springer Science+Business Media, LLC & Science Press, China.",2011,Journal of Computer Science and Technology,11,temporal relation classification is @ of contemporary demanding task of natural language processing @ @ task @ @ used in various application @ a question answering summarization and language specific information retrieval @ in @ @ @ propose @ improved algorithm @ classifying temporal relation @ event @ @ event and time @ support vector machine @ svm @ @ along @ gold-standard corpus feature @ proposed method aim at exploiting some useful automatically generated syntactic feature to improve @ accuracy of classification @ accordingly a number of novel kernel function @ introduced and evaluated @ @ evaluation clearly demonstrate @ adding syntactic feature @ in a considerable improvement @ @ state-of-the-art method of classifying temporal relation @ @ science @ medium llc science @ china @ 
3662,Using concept formation for mining linguistic and biological texts,"We present, discuss and exemplify a fully implemented specialization of the Concept Formation Cognitive model into a model of text mining that can be applied to spoken languages as well as to molecular biology languages. © 2011 The authors and IOS Press. All rights reserved.",2011,Frontiers in Artificial Intelligence and Applications,0,@ @ discus and exemplify a fully implemented specialization of @ concept formation cognitive model @ a model of text mining @ @ @ applied to spoken language a well a to molecular biology language @ @ author and io @ @ @ right reserved @ 
3663,High-order co-clustering text data on semantics-based representation model,"The language modeling approach is widely used to improve the performance of text mining in recent years because of its solid theoretical foundation and empirical effectiveness. In essence, this approach centers on the issue of estimating an accurate model by choosing appropriate language models as well as smooth techniques. Semantic smoothing, which incorporates semantic and contextual information into the language models, is effective and potentially significant to improve the performance of text mining. In this paper, we proposed a high-order structure to represent text data by incorporating background knowledge, Wikipedia. The proposed structure consists of three types of objects, term, document and concept. Moreover, we firstly combined the high-order co-clustering algorithm with the proposed model to simultaneously cluster documents, terms and concepts. Experimental results on benchmark data sets (20Newsgroups and Reuters-21578) have shown that our proposed high-order co-clustering on high-order structure outperforms the general co-clustering algorithm on bipartite text data, such as document-term, document-concept and document-(term+concept). © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),11,@ language modeling approach is widely used to improve @ performance of text mining in recent year @ of @ solid theoretical foundation and empirical effectiveness @ in essence @ approach center on @ issue of estimating @ accurate model by choosing appropriate language model a well a smooth technique @ semantic smoothing @ incorporates semantic and contextual information @ @ language model is effective and potentially significant to improve @ performance of text mining @ in @ @ @ proposed a high-order structure to represent text data by incorporating background knowledge wikipedia @ @ proposed structure consists of three type of object term document and concept @ moreover @ firstly combined @ high-order co-clustering algorithm @ @ proposed model to simultaneously cluster document term and concept @ experimental @ on benchmark data set @ newsgroups and reuters @ @ @ @ @ proposed high-order co-clustering on high-order structure outperforms @ general co-clustering algorithm on bipartite text data @ a document-term document-concept and document @ term concept @ @ springer-verlag @ 
3665,"Information mining - Reflections on recent advancements and the road ahead in data, text, and media mining","In this introduction, we briefly summarize the state of data and text mining today. Taking a very broad view, we use the term information mining to refer to the organization and analysis of structured or unstructured data that can be quantitative, textual, and/or pictorial in nature. The key question, in our view, is, ""How can we transform data (in the very broad sense of this term) into 'actionable knowledge', knowledge that we can use in pursuit of a specified objective(s)."" After detailing a set of key components of information mining, we introduce each of the papers in this volume and detail the focus of their contributions. © 2011 Elsevier B.V. All rights reserved.",2011,Decision Support Systems,40,in @ introduction @ briefly summarize @ state of data and text mining today @ taking a @ broad view @ use @ term information mining to refer to @ organization and analysis of structured @ unstructured data @ @ @ quantitative textual and @ pictorial in nature @ @ key question in @ view is @ @ @ @ transform data @ in @ @ broad sense of @ term @ @ @ actionable knowledge @ knowledge @ @ @ use in pursuit of a specified objective @ s @ @ @ @ detailing a set of key component of information mining @ introduce @ of @ @ in @ volume and detail @ focus of @ contribution @ @ b @ v @ @ right reserved @ 
3668,Using a heterogeneous dataset for emotion analysis in text,"In this paper, we adopt a supervised machine learning approach to recognize six basic emotions (anger, disgust, fear, happiness, sadness and surprise) using a heterogeneous emotion-annotated dataset which combines news headlines, fairy tales and blogs. For this purpose, different features sets, such as bags of words, and N-grams, were used. The Support Vector Machines classifier (SVM) performed significantly better than other classifiers, and it generalized well on unseen examples. © 2011 Springer-Verlag.",2011,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),52,in @ @ @ adopt a supervised machine learning approach to recognize six basic emotion @ anger disgust fear happiness sadness and surprise @ @ a heterogeneous emotion-annotated dataset @ combine news headline fairy tale and blog @ @ @ purpose different feature set @ a bag of word and n-grams @ used @ @ support vector machine classifier @ svm @ performed significantly better @ @ classifier and @ generalized well on unseen example @ springer-verlag @ 
3669,Bitext Alignment,"This book provides an overview of various techniques for the alignment of bitexts. It describes general concepts and strategies that can be applied to map corresponding parts in parallel documents on various levels of granularity. Bitexts are valuable linguistic resources for many different research fields and practical applications. The most predominant application is machine translation, in particular, statistical machine translation. However, there are various other threads that can be followed which may be supported by the rich linguistic knowledge implicitly stored in parallel resources. Bitexts have been explored in lexicography, word sense disambiguation, terminology extraction, computer-aided language learning and translation studies to name just a few. The book covers the essential tasks that have to be carried out when building parallel corpora starting from the collection of translated documents up to sub-sentential alignments. In particular, it describes various approaches to document alignment, sentence alignment, word alignment and tree structure alignment. It also includes a list of resources and a comprehensive review of the literature on alignment techniques. Copyright © 2011 by Morgan & Claypool.",2011,Synthesis Lectures on Human Language Technologies,15,@ book provides @ overview of various technique @ @ alignment of bitexts @ @ describes general concept and strategy @ @ @ applied to map corresponding part in parallel document on various level of granularity @ bitexts @ valuable linguistic resource @ many different research field and practical application @ @ @ predominant application is machine translation in particular statistical machine translation @ however @ @ various @ thread @ @ @ followed @ may @ supported by @ rich linguistic knowledge implicitly stored in parallel resource @ bitexts @ @ explored in lexicography word sense disambiguation terminology extraction computer-aided language learning and translation study to name @ a @ @ @ book cover @ essential task @ @ to @ carried @ @ building parallel corpus starting @ @ collection of translated document up to sub-sentential alignment @ in particular @ describes various approach to document alignment sentence alignment word alignment and tree structure alignment @ @ @ includes a list of resource and a comprehensive review of @ literature on alignment technique @ @ by morgan claypool @ 
3672,Two-step sentence extraction for summarization of meeting minutes,"These days a number of meeting minutes of various organizations are publicly available and the interest in these documents by people is increasing. However it is time-consuming and tedious to read and understand whole documents even if the documents can be accessed easily. In addition, what most people want from meeting minutes is to catch the main issues of the meeting and to understand its contexts rather than to know whole discussions of the meetings. Existing text summarization techniques applied to this problem often fail because they are developed without considering the characteristics of the meeting minutes. In order to improve the performance of summarization of meeting minutes, this paper proposes a novel method for summarizing documents based-on two-step sentence extraction. It first extracts the sentences which are addressing the main issues. For each issue expressed in the extracted sentences, the sentences related with the issue are then extracted in the second step. Then, by transforming the extracted sentences into a tree-structure form, the results of the proposed method can be understood better than existing methods. In the experiments, the proposed method shows remarkable improvement in performance and this result implies that the proposed method is plausible for summarizing meeting minutes. © 2011 IEEE.",2011,"Proceedings - 2011 8th International Conference on Information Technology: New Generations, ITNG 2011",5,@ day a number of meeting minute of various organization @ publicly available and @ interest in @ document by people is increasing @ however @ is time-consuming and tedious to read and understand whole document even if @ document @ @ accessed easily @ in addition @ @ people want @ meeting minute is to catch @ main issue of @ meeting and to understand @ context rather @ to know whole discussion of @ meeting @ existing text summarization technique applied to @ problem often fail @ @ @ developed without considering @ characteristic of @ meeting minute @ in order to improve @ performance of summarization of meeting minute @ @ proposes a novel method @ summarizing document based-on two-step sentence extraction @ @ first extract @ sentence @ @ addressing @ main issue @ @ @ issue expressed in @ extracted sentence @ sentence related @ @ issue @ @ extracted in @ second step @ @ by transforming @ extracted sentence @ a tree-structure form @ @ of @ proposed method @ @ understood better @ existing method @ in @ experiment @ proposed method @ remarkable improvement in performance and @ @ implies @ @ proposed method is plausible @ summarizing meeting minute @ @ @ 
3673,Entity set expansion in opinion documents,"Opinion mining has been an active research area in recent years. The task is to extract opinions expressed on entities and their attributes. For example, the sentence, ""I love the picture quality of Sony cameras,"" expresses a positive opinion on the picture quality attribute of Sony cameras. Sony is the entity. This paper focuses on mining entities (e.g., Sony). This is an important problem because without knowing the entity, the extracted opinion is of little use. The problem is similar to the classic named entity recognition problem. However, there is a major difference. In a typical opinion mining application, the user wants to find opinions on some competing entities, e.g., competing or relevant products. However, he/she often can only provide a few names as there are too many of them. The system has to find the rest from a corpus. This implies that the discovered entities must be of the same type/class. This is the set expansion problem. Classic methods for solving the problem are based on distributional similarity. However, we found this method is inaccurate. We then employ a learning-based method called Bayesian Sets. However, directly applying Bayesian Sets produces poor results. We then propose a more sophisticated way to use Bayesian Sets. This method, however, causes two major problems: entity ranking and feature sparseness. For entity ranking, we propose a re-ranking method to solve the problem. For feature sparseness, we propose two methods to re-weight features and to determine the quality of features. These methods help improve the mining results substantially. Additionally, like any learning algorithm, Bayesian Sets requires the user to engineer a set of features. We design some generic features based on part-of-speech tags of words for learning, which thus does not need to engineer features for each specific domain. Experimental results using 10 real-life datasets from diverse domains demonstrated the effectiveness of the proposed technique. © 2011 ACM.",2011,HT 2011 - Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia,6,opinion mining ha @ @ active research area in recent year @ @ task is to extract opinion expressed on entity and @ attribute @ @ example @ sentence @ i love @ picture quality of sony camera @ express a positive opinion on @ picture quality attribute of sony camera @ sony is @ entity @ @ @ focus on mining entity @ e @ g @ sony @ @ @ is @ important problem @ without knowing @ entity @ extracted opinion is of little use @ @ problem is similar to @ classic named entity recognition problem @ however @ is a major difference @ in a typical opinion mining application @ user want to find opinion on some competing entity e @ g @ competing @ relevant product @ however he @ often @ only provide a @ name a @ @ too many of @ @ @ system ha to find @ rest @ a corpus @ @ implies @ @ discovered entity must @ of @ @ type class @ @ is @ set expansion problem @ classic method @ solving @ problem @ based on distributional similarity @ however @ found @ method is inaccurate @ @ @ employ a learning-based method called bayesian set @ however directly applying bayesian set produce poor @ @ @ @ propose a more sophisticated way to use bayesian set @ @ method however cause @ major problem @ entity ranking and feature sparseness @ @ entity ranking @ propose a re-ranking method to solve @ problem @ @ feature sparseness @ propose @ method to re-weight feature and to determine @ quality of feature @ @ method help improve @ mining @ substantially @ additionally like @ learning algorithm bayesian set requires @ user to engineer a set of feature @ @ design some generic feature based on part-of-speech tag of word @ learning @ thus doe not need to engineer feature @ @ specific domain @ experimental @ @ real-life datasets @ diverse domain demonstrated @ effectiveness of @ proposed technique @ acm @ 
3674,Evaluating the effectiveness of VSM model and topic segmentation in retrieving arabic documents,"Information retrieval needs to match relevant texts with a given query. Selecting appropriate parts is useful when documents are long, and only portions are interesting to the user. In this paper, a set of IR experiments was carried out to study the impact of topic segmentation and its effect on Arabic information retrieval (IR). The system evaluation was conducted in two cases based on precision/recall criteria. Evaluate the system without using Arabic text segmentation and evaluate the system with Arabic text segmentation. Some famous information retrieval models, i.e., Vector Space Model, Relevance feedback Model were also adopted in our study for ranking relevant documents. Traditional data recall, precision and F1 measures were used to gauge IR effectiveness. A number of queries were selected and subjected to further detailed analysis to further explore the influence of topic segmentation on IR. The findings reveal that the system with topic segmentation gives better performance than the system without topic segmentation. © 2011 CRL Publishing Ltd.",2011,Computer Systems Science and Engineering,3,information retrieval need to match relevant text @ a given query @ selecting appropriate part is useful @ document @ long and only portion @ interesting to @ user @ in @ @ a set of ir experiment wa carried @ to study @ impact of topic segmentation and @ effect on arabic information retrieval @ ir @ @ @ system evaluation wa conducted in @ case based on precision recall criterion @ evaluate @ system without @ arabic text segmentation and evaluate @ system @ arabic text segmentation @ some famous information retrieval model i @ e @ vector space model relevance feedback model @ @ adopted in @ study @ ranking relevant document @ traditional data recall precision and f measure @ used to gauge ir effectiveness @ a number of query @ selected and subjected to @ detailed analysis to @ explore @ influence of topic segmentation on ir @ @ finding reveal @ @ system @ topic segmentation give better performance @ @ system without topic segmentation @ crl publishing ltd @ 
3675,Fuzzy ontology mining and semantic information granulation for effective information retrieval decision making,"The notion of semantic information granulation is explored to estimate the information specificity or generality of documents. Basically, a document is considered more specific than another document if it contains more cohesive domain-specific terminologies than that of the other one. We believe that the dimension of semantic granularity is an important supplement to the existing similarity-based and popularity-based measures for building effective document ranking functions. The main contributions of this paper is the illustration of the design and development of a fuzzy ontology based granular information retrieval (IR) system to improve the effectiveness of IR decision making for various domains. Based on the notion of semantic information granulation, a novel computational model is developed to estimate the semantic granularity of documents; these documents can then be ranked according to the information seekers' specific semantic granularity requirements. One main component of the proposed computational model is the fuzzy ontology mining mechanism which can automatically build domain-specific ontology for the estimation of semantic granularity of documents. Our TREC-based experiment reveals that the proposed fuzzy ontology based granular IR system outperforms a classical vector space based IR system in domain specific IR. Our research work opens the door to the applications of granular computing and fuzzy ontology mining methods to enhance domain specific IR decision making. © 2011 Taylor & Francis Group, LLC.",2011,International Journal of Computational Intelligence Systems,6,@ notion of semantic information granulation is explored to estimate @ information specificity @ generality of document @ basically a document is considered more specific @ another document if @ contains more cohesive domain-specific terminology @ @ of @ @ @ @ @ believe @ @ dimension of semantic granularity is @ important supplement to @ existing similarity-based and popularity-based measure @ building effective document ranking function @ @ main contribution of @ @ is @ illustration of @ design and development of a fuzzy ontology based granular information retrieval @ ir @ system to improve @ effectiveness of ir decision making @ various domain @ based on @ notion of semantic information granulation a novel computational model is developed to estimate @ semantic granularity of document @ @ document @ @ @ ranked according to @ information seeker @ specific semantic granularity requirement @ @ main component of @ proposed computational model is @ fuzzy ontology mining mechanism @ @ automatically build domain-specific ontology @ @ estimation of semantic granularity of document @ @ trec-based experiment reveals @ @ proposed fuzzy ontology based granular ir system outperforms a classical vector space based ir system in domain specific ir @ @ research work open @ door to @ application of granular computing and fuzzy ontology mining method to enhance domain specific ir decision making @ taylor francis group llc @ 
3681,Adaptive term weighting through stochastic optimization,"Term weighting strongly influences the performance of text mining and information retrieval approaches. Usually term weights are determined through statistical estimates based on static weighting schemes. Such static approaches lack the capability to generalize to different domains and different data sets. In this paper, we introduce an on-line learning method for adapting term weights in a supervised manner.Via stochastic optimizationwe determine a linear transformation of the termspace to approximate expected similarity values among documents.We evaluate our approach on 18 standard text data sets and show that the performance improvement of a k-NN classifier ranges between 1% and 12% by using adaptive term weighting as preprocessing step. Further, we provide empirical evidence that our approach is efficient to cope with larger problems. © Springer-Verlag 2010.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,term weighting strongly influence @ performance of text mining and information retrieval approach @ usually term weight @ determined @ statistical estimate based on static weighting scheme @ @ static approach lack @ capability to generalize to different domain and different data set @ in @ @ @ introduce @ on-line learning method @ adapting term weight in a supervised manner @ via stochastic optimizationwe determine a linear transformation of @ termspace to approximate expected similarity value among document @ @ evaluate @ approach on standard text data set and @ @ @ performance improvement of a k-nn classifier range @ and by @ adaptive term weighting a preprocessing step @ @ @ provide empirical evidence @ @ approach is efficient to cope @ larger problem @ springer-verlag @ 
3682,An empirical study on the feature's type effect on the automatic classification of Arabic documents,"The Arabic language is a highly flexional and morphologically very rich language. It presents serious challenges to the automatic classification of documents, one of which is determining what type of attribute to use in order to get the optimal classification results. Some people use roots or lemmas which, they say, are able to handle problems with the inflections that do not appear in other languages in that fashion. Others prefer to use character-level n-grams since n-grams are simpler to implement, language independent, and produce satisfactory results. So which of these two approaches is better, if any? This paper tries to answer this question by offering a comparative study between four feature types: words in their original form, lemmas, roots, and character level n-grams and shows how each affects the performance of the classifier. We used and compared the performance of Support Vector Machines and Naïve Bayesian Networks algorithms respectively. © Springer-Verlag 2010.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9,@ arabic language is a highly flexional and morphologically @ rich language @ @ @ serious challenge to @ automatic classification of document @ of @ is determining @ type of attribute to use in order to get @ optimal classification @ @ some people use root @ lemma @ @ say @ able to handle problem @ @ inflection @ @ not appear in @ language in @ fashion @ others prefer to use character-level n-grams since n-grams @ simpler to implement language independent and produce satisfactory @ @ @ @ of @ @ approach is better if @ @ @ @ try to answer @ question by offering a comparative study @ four feature type @ word in @ original form lemma root and character level n-grams and @ @ @ affect @ performance of @ classifier @ @ used and compared @ performance of support vector machine and naïve bayesian network algorithm respectively @ springer-verlag @ 
3685,A platform for mining and visualizing regional collective culture,"This paper proposes computational methods for mining and visualizing collective culture among the community members of a region. This paper first outlines a procedure to extract significant narratives with text mining technique and spatiotemporal analysis on the textual data transcribed from oral-history interviews with the regional community members. It also introduces the KACHINA-CUBE system that imports the narratives as contextualized fragments of sentences based on spatiotemporal information, visualizes them onto a virtual 3D space, and assist researchers to discover commonalities and diversities among them based on the trajectory equifinality model (TEM), which is a theoretical framework to clarify both the similarities and differences among the trajectories of individual life courses. At the end of this paper, we illustrate a test case on collective culture regarding the once-flourishing film industry in Kyoto. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ @ proposes computational method @ mining and visualizing collective culture among @ community member of a region @ @ @ first outline a procedure to extract significant narrative @ text mining technique and spatiotemporal analysis on @ textual data transcribed @ oral-history interview @ @ regional community member @ @ @ introduces @ kachina-cube system @ import @ narrative a contextualized fragment of sentence based on spatiotemporal information visualizes @ onto a virtual @ space and assist researcher to discover commonality and diversity among @ based on @ trajectory equifinality model @ tem @ @ is a theoretical framework to clarify @ @ similarity and difference among @ trajectory of individual life course @ at @ end of @ @ @ illustrate a test case on collective culture regarding @ once-flourishing film industry in kyoto @ springer-verlag @ @ @ 
3686,Ontology extension towards analysis of business news,"This paper addresses the process of the ontology extension for a selected domain of interest which is defined by keywords and a glossary of relevant terms with descriptions. A new methodology for semiautomatic ontology extension, aggregating the elements of text mining and user-dialog approaches for ontology extension, is proposed and evaluated. We conduct a set of ranking, tagging and illustrative question answering experiments using Cyc ontology and business news collection. We evaluate the importance of using the textual content and structure of the ontology concept in the process of ontology extension. The experiments show that the best results are obtained with giving more to weight to ontology concept content and less weight to ontology concept structure.",2010,Informatica (Ljubljana),4,@ @ address @ process of @ ontology extension @ a selected domain of interest @ is defined by keywords and a glossary of relevant term @ description @ a @ methodology @ semiautomatic ontology extension aggregating @ element of text mining and user-dialog approach @ ontology extension is proposed and evaluated @ @ conduct a set of ranking tagging and illustrative question answering experiment @ cyc ontology and @ news collection @ @ evaluate @ importance of @ @ textual content and structure of @ ontology concept in @ process of ontology extension @ @ experiment @ @ @ best @ @ obtained @ giving more to weight to ontology concept content and le weight to ontology concept structure @ 
3697,An information-extraction system for Urdu - A resource-poor language,"There has been an increase in the amount of multilingual text on the Internet due to the proliferation of news sources and blogs. The Urdu language, in particular, has experienced explosive growth on theWeb. Text mining for information discovery, which includes tasks such as identifying topics, relationships and events, and sentiment analysis, requires sophisticated natural language processing (NLP). NLP systems begin with modules such as word segmentation, part-of-speech tagging, and morphological analysis and progress to modules such as shallow parsing and named entity tagging. While there have been considerable advances in developing such comprehensive NLP systems for English, the work for Urdu is still in its infancy. The tasks of interest in Urdu NLP includes analyzing data sources such as blogs and comments to news articles to provide insight into social and human behavior. All of this requires a robust NLP system. The objective of this work is to develop an NLP infrastructure for Urdu that is customizable and capable of providing basic analysis on which more advanced information extraction tools can be built. This system assimilates resources from various online sources to facilitate improved named entity tagging and Urdu-to-English transliteration. The annotated data required to train the learning models used here is acquired by standardizing the currently limited resources available for Urdu. Techniques such as bootstrap learning and resource sharing from a syntactically similar language, Hindi, areexplored to augment the available annotated Urdu data. Each of the new Urdu text processing modules has been integrated into a general text-mining platform. The evaluations performed demonstrate that the accuracies have either met or exceeded the state of the art. © 2010 ACM.",2010,ACM Transactions on Asian Language Information Processing,25,@ ha @ @ increase in @ amount of multilingual text on @ internet due to @ proliferation of news source and blog @ @ urdu language in particular ha experienced explosive growth on theweb @ text mining @ information discovery @ includes task @ a identifying topic relationship and event and sentiment analysis requires sophisticated natural language processing @ nlp @ @ nlp system begin @ module @ a word segmentation part-of-speech tagging and morphological analysis and progress to module @ a shallow parsing and named entity tagging @ @ @ @ @ considerable advance in developing @ comprehensive nlp system @ english @ work @ urdu is still in @ infancy @ @ task of interest in urdu nlp includes analyzing data source @ a blog and comment to news article to provide insight @ social and human behavior @ @ of @ requires a robust nlp system @ @ objective of @ work is to develop @ nlp infrastructure @ urdu @ is customizable and capable of providing basic analysis on @ more advanced information extraction tool @ @ built @ @ system assimilates resource @ various online source to facilitate improved named entity tagging and urdu-to-english transliteration @ @ annotated data required to train @ learning model used @ is acquired by standardizing @ currently limited resource available @ urdu @ technique @ a bootstrap learning and resource sharing @ a syntactically similar language hindi areexplored to augment @ available annotated urdu data @ @ of @ @ urdu text processing module ha @ integrated @ a general text-mining platform @ @ evaluation performed demonstrate @ @ accuracy @ either met @ exceeded @ state of @ art @ acm @ 
3705,Multi-document summarization using minimum distortion,"Document summarization plays an important role in the area of natural language processing and text mining. This paper proposes several novel information-theoretic models for multi-document summarization. They consider document summarization as a transmission system and assume that the best summary should have the minimum distortion. By defining a proper distortion measure and a new representation method, the combination of the last two models (the linear representation model and the facility location model) gains good experimental results on the DUC2002 and DUC2004 datasets. Moreover, we also indicate that the model has high interpretability and extensibility. © 2010 IEEE.",2010,"Proceedings - IEEE International Conference on Data Mining, ICDM",13,document summarization play @ important role in @ area of natural language processing and text mining @ @ @ proposes several novel information-theoretic model @ multi-document summarization @ @ consider document summarization a a transmission system and assume @ @ best summary @ @ @ minimum distortion @ by defining a proper distortion measure and a @ representation method @ combination of @ last @ model @ @ linear representation model and @ facility location model @ gain good experimental @ on @ duc and duc datasets @ moreover @ @ indicate @ @ model ha high interpretability and extensibility @ @ @ 
3708,Location and scatter matching for dataset shift in text mining,"Dataset shift from the training data in a source domain to the data in a target domain poses a great challenge for many statistical learning methods. Most algorithms can be viewed as exploiting only the first-order statistics, namely, the empirical mean discrepancy to evaluate the distribution gap. Intuitively, considering only the empirical mean may not be statistically efficient. In this paper, we propose a non-parametric distance metric with a good property which jointly considers the empirical mean (Location) and sample covariance (Scatter) difference. More specifically, we propose an improved symmetric Stein's loss function which combines the mean and covariance discrepancy into a unified Bregman matrix divergence of which Jensen-Shannon divergence between normal distributions is a particular case. Our target is to find a good feature representation which can reduce the distribution gap between different domains, at the same time, ensure that the new derived representation can encode most discriminative components with respect to the label information. We have conducted extensive experiments on several document classification datasets to demonstrate the effectiveness of our proposed method. © 2010 IEEE.",2010,"Proceedings - IEEE International Conference on Data Mining, ICDM",8,dataset shift @ @ training data in a source domain to @ data in a target domain pose a great challenge @ many statistical learning method @ @ algorithm @ @ viewed a exploiting only @ first-order statistic namely @ empirical mean discrepancy to evaluate @ distribution gap @ intuitively considering only @ empirical mean may not @ statistically efficient @ in @ @ @ propose a non-parametric distance metric @ a good property @ jointly considers @ empirical mean @ location @ and sample covariance @ scatter @ difference @ more specifically @ propose @ improved symmetric stein @ s loss function @ combine @ mean and covariance discrepancy @ a unified bregman matrix divergence of @ jensen-shannon divergence @ normal distribution is a particular case @ @ target is to find a good feature representation @ @ reduce @ distribution gap @ different domain at @ @ time ensure @ @ @ derived representation @ encode @ discriminative component @ respect to @ label information @ @ @ conducted extensive experiment on several document classification datasets to demonstrate @ effectiveness of @ proposed method @ @ @ 
3718,Analysis and algorithms for stemming inversion,"Stemming is a fundamental technique for processing large amounts of data in information retrieval and text mining. However, after processing the reversal of this process is often desirable, e.g., for human interpretation, or methods which operate on sequences of characters. We present a formal analysis of the stemming inversion problem, and show that the underlying optimization problem capturing conceptual groups as known from under- and overstemming, is of high computational complexity. We present efficient heuristic algorithms for practical application in information retrieval and test our approach on real data. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,stemming is a fundamental technique @ processing @ amount of data in information retrieval and text mining @ however @ processing @ reversal of @ process is often desirable e @ g @ @ human interpretation @ method @ operate on sequence of character @ @ @ a formal analysis of @ stemming inversion problem and @ @ @ underlying optimization problem capturing conceptual group a known @ @ and overstemming is of high computational complexity @ @ @ efficient heuristic algorithm @ practical application in information retrieval and test @ approach on real data @ springer-verlag @ 
3720,Semantic representation of gene ontology terms by using gene regulation ontology,"Gene Ontology (GO) has been developed to provide concepts for the functional annotation of biological entities. This development has already contributed to significant biomedical research results. Nonetheless, GO could have provided even stronger support to biomedical text mining, if it delivered domain-independent logical definitions of its concepts. We present a method that extracts the semantic structures of GO terms by using the Gene Regulation Ontology (GRO). The method annotates substrings of GO terms with, if any, corresponding concepts of OBO ontologies and then converts the syntactic structures of GO terms into GRO-based semantic structures. We show that the semantic structures can be used to deduce implied relations from GO terms.",2010,CEUR Workshop Proceedings,0,gene ontology @ go @ ha @ developed to provide concept @ @ functional annotation of biological entity @ @ development ha already contributed to significant biomedical research @ @ nonetheless go could @ provided even stronger support to biomedical text mining if @ delivered domain-independent logical definition of @ concept @ @ @ a method @ extract @ semantic structure of go term by @ @ gene regulation ontology @ gro @ @ @ method annotates substring of go term @ if @ corresponding concept of obo ontology and @ convert @ syntactic structure of go term @ gro-based semantic structure @ @ @ @ @ semantic structure @ @ used to deduce implied relation @ go term @ 
3727,The CALBC RDF triple store: Retrieval over large literature content,"Integration of the scientific literature into a biomedical research infrastructure requires the processing of the literature, identification of the contained named entities (NEs) and concepts, and to represent the content in a standardised way. The CALBC project partners (PPs) have produced a large-scale annotated biomedical corpus with four different semantic groups through the hamionisation of annotations from automatic text mining solutions (Silver Standard Corpus, SSC). The four semantic groups were chemical entities and dings (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and species (SPE). The content of the SSC has been frilly integrated into RDF Triple Store (4,568,678 triples) and has been aligned with content from the Gene Atlas (182,840 triples), UmProtKb (12,552,239 triples for human) and the lexical resource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific literature and bioinformatics resources at the same time for evidence of genetic causes, such as drug targets and disease involvement.",2010,CEUR Workshop Proceedings,5,integration of @ scientific literature @ a biomedical research infrastructure requires @ processing of @ literature identification of @ contained named entity @ ne @ and concept and to represent @ content in a standardised way @ @ calbc project partner @ pps @ @ produced a large-scale annotated biomedical corpus @ four different semantic group @ @ hamionisation of annotation @ automatic text mining solution @ silver standard corpus ssc @ @ @ four semantic group @ chemical entity and ding @ ched @ gene and protein @ prge @ disease and disorder @ diso @ and specie @ spe @ @ @ content of @ ssc ha @ frilly integrated @ rdf triple store @ triple @ and ha @ aligned @ content @ @ gene atlas @ triple @ umprotkb @ triple @ human @ and @ lexical resource lexebi @ biolexicon @ @ rdf triple store enables querying @ scientific literature and bioinformatics resource at @ @ time @ evidence of genetic cause @ a drug target and disease involvement @ 
3734,A study on the relation between linguistics-oriented and domain-specific semantics,"In this paper we dealt with the comparison and linking between lexical resources with domain knowledge provided by ontologies. It is one of the issues for the combination of the Semantic Web Ontologies and Text Mining. We investigated the relations between the linguisticsoriented and domain-specific semantics, by associating the GO biological process concepts to the FrameNet semantic frames. The result shows the gaps between the linguistics-oriented and domain-specific semantics on the classification of events and the grouping of target words. The result provides valuable information for the improvement of domain ontologies supporting for text mining systems. And also, it will result in benefits to language understanding technology.",2010,CEUR Workshop Proceedings,0,in @ @ @ dealt @ @ comparison and linking @ lexical resource @ domain knowledge provided by ontology @ @ is @ of @ issue @ @ combination of @ semantic web ontology and text mining @ @ investigated @ relation @ @ linguisticsoriented and domain-specific semantics by associating @ go biological process concept to @ framenet semantic frame @ @ @ @ @ gap @ @ linguistics-oriented and domain-specific semantics on @ classification of event and @ grouping of target word @ @ @ provides valuable information @ @ improvement of domain ontology supporting @ text mining system @ and @ @ @ @ in benefit to language understanding technology @ 
3736,Random indexing spaces for bridging the human and data webs,"There exists a wide gap between the information that people and computers respectively can operate with online. Because most of the web is in plain text and the Semantic Web requires structured information (RDF), bridging the two worlds is an important current research topic. Here we propose a web service that uses a Random Indexing (RI) semantic space trained on the plain text of the one million most central Wikipedia concepts. The space provides us with vectors for each of the equivalent DBpedia concepts and vectors for any text or webpage. It can also provide a hashed version of the RI vector that works as unique handler like URIs do, but with the additional advantage that it represents text meaning. As a result, any page (previously readable only for humans) is now integrated with the Semantic Web graph using links to one of its most central parts, DBpedia.",2010,CEUR Workshop Proceedings,1,@ exists a wide gap @ @ information @ people and computer respectively @ operate @ online @ @ @ of @ web is in plain text and @ semantic web requires structured information @ rdf @ bridging @ @ world is @ important current research topic @ @ @ propose a web service @ us a random indexing @ ri @ semantic space trained on @ plain text of @ @ million @ central wikipedia concept @ @ space provides u @ vector @ @ of @ equivalent dbpedia concept and vector @ @ text @ webpage @ @ @ @ provide a hashed version of @ ri vector @ work a unique handler like uris @ @ @ @ additional advantage @ @ represents text meaning @ a a @ @ page @ @ readable only @ human @ is now integrated @ @ semantic web graph @ link to @ of @ @ central part dbpedia @ 
3737,Animal disease event recognition and classification,"Monitoring epidemic crises, caused by rapid spread of infectious animal diseases, can be facilitated by the plethora of information about disease-related events that is available online. Therefore, the ability to use this information to perform domain-specific entity recognition and event-related sentence classification, which in turn can support time and space visualization of automatically extracted events, is highly desirable. Towards this goal, we present a rule-based approach to the problem of extracting animal disease-related events from web documents. Our approach relies on the recognition of structured entity tuples, consisting of attributes, which describe events related to animal diseases. The event attributes that we consider include animal diseases, dates, species and geo-referenced locations. We perform disease names and species recognition using an automatically- constructed ontology, dates are extracted using regular expressions, while location are extracted using a conditional random fields tool. The extracted events are further classified as confirmed or suspected based on semantic features, obtained from the e.g., GoogleSets1 and WordNet2. Our preliminary results demonstrate the feasibility of the proposed approach.",2010,CEUR Workshop Proceedings,2,monitoring epidemic crisis caused by rapid spread of infectious animal disease @ @ facilitated by @ plethora of information @ disease-related event @ is available online @ therefore @ ability to use @ information to perform domain-specific entity recognition and event-related sentence classification @ in turn @ support time and space visualization of automatically extracted event is highly desirable @ towards @ goal @ @ a rule-based approach to @ problem of extracting animal disease-related event @ web document @ @ approach relies on @ recognition of structured entity tuples consisting of attribute @ describe event related to animal disease @ @ event attribute @ @ consider include animal disease date specie and geo-referenced location @ @ perform disease name and specie recognition @ @ automatically constructed ontology date @ extracted @ regular expression @ location @ extracted @ a conditional random field tool @ @ extracted event @ @ classified a confirmed @ suspected based on semantic feature obtained @ @ e @ g @ googlesets and wordnet @ @ preliminary @ demonstrate @ feasibility of @ proposed approach @ 
3749,Multi-viewpoint based similarity measure and optimality criteria for document clustering,"The aim of this work is to produce fast, easy-to-apply but effective algorithms for clustering large text collections. In this paper, we propose a novel concept of similarity measure among objects and its related clustering algorithms. The similarity between two objects within a cluster is measured from the view of all other objects outside that cluster. As a result, two optimality criteria are formulated as the objective functions for the clustering problem. We analyze and compare the proposed clustering approaches with the popular algorithms for document clustering in the literature. Extensive empirical experiments are carried out on various benchmark datasets and evaluated by different metrics. The results show that our proposed criterion functions consistently outperform the other well-known clustering criteria, and give the best overall performance with the same computational efficiency. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ aim of @ work is to produce fast easy-to-apply @ effective algorithm @ clustering @ text collection @ in @ @ @ propose a novel concept of similarity measure among object and @ related clustering algorithm @ @ similarity @ @ object within a cluster is measured @ @ view of @ @ object outside @ cluster @ a a @ @ optimality criterion @ formulated a @ objective function @ @ clustering problem @ @ analyze and compare @ proposed clustering approach @ @ popular algorithm @ document clustering in @ literature @ extensive empirical experiment @ carried @ on various benchmark datasets and evaluated by different metric @ @ @ @ @ @ proposed criterion function consistently outperform @ @ well-known clustering criterion and give @ best overall performance @ @ @ computational efficiency @ springer-verlag @ 
3750,Evaluating term concept association measures for short text expansion: Two case studies of classification and clustering,"The proliferation of Web applications based on short texts represents both an opportunity and a challenge to text mining algorithms, because of sparse representations and lack of shared context. To address this problem, we investigate a term expansion approach based on analyzing the relationships between the term concepts present in the concept lattice associated with the document corpus. We define five term concept association measures: proximity, concept similarity, connection strength, damping-weighted proximity, proximity&strength. By means of two case studies, we evaluate the effectiveness of these measures for expansion-enhanced K-NN classification and K-Means clustering of short texts. The results suggest that the five measures are highly competitive, with the best measure showing a clear improvement over the corresponding unenhanced K-NN and K-Means algorithms, as well as over two alternative term expansion enhancements (i.e., based on Wordnet and on pseudo-relevance feedback).",2010,CEUR Workshop Proceedings,3,@ proliferation of web application based on short text represents @ @ opportunity and a challenge to text mining algorithm @ of sparse representation and lack of shared context @ to address @ problem @ investigate a term expansion approach based on analyzing @ relationship @ @ term concept @ in @ concept lattice associated @ @ document corpus @ @ define five term concept association measure @ proximity concept similarity connection strength damping-weighted proximity proximity strength @ by mean of @ case study @ evaluate @ effectiveness of @ measure @ expansion-enhanced k-nn classification and k-means clustering of short text @ @ @ suggest @ @ five measure @ highly competitive @ @ best measure showing a clear improvement @ @ corresponding unenhanced k-nn and k-means algorithm a well a @ @ alternative term expansion enhancement @ i @ e @ based on wordnet and on pseudo-relevance feedback @ @ 
3751,A semantic assistant for mutation mentions in PubMed ab-stracts,"Biomedical researchers consume and analyze PubMed abstracts on a daily basis seeking to update their existing knowledge with insights from newly published literature. Plain text descriptions fail to deliver contextual knowledge to users who require a comprehensive understanding of the content of a paper before deciding to access it. To achieve this biological named entities described in the abstracts must be linked to their related entries in biological databases and established controlled vocabularies such as SwissProt and Gene Ontology. Semantic Assistants support users in content retrieval, analysis, and development, by offering context-sensitive NLP services directly integrated in standard desktop clients, like a word processor. They are implemented through an open service-oriented architecture, using Semantic Web ontologies and W3C Web Services. Here we present a deployment of the Semantic Assistants framework to provide links from mutation, protein, protein property, gene and organism mentions in abstracts to their related entry in standardized biological databases and controlled vocabularies. The underlying text mining pipeline used to identify named entities has previously shown high levels of precision and we make this functionality easily accessible through a Semantic Assistant, to end users when reviewing PubMed abstracts in through a Firefox client.",2010,CEUR Workshop Proceedings,0,biomedical researcher consume and analyze pubmed abstract on a daily basis seeking to update @ existing knowledge @ insight @ newly published literature @ plain text description fail to deliver contextual knowledge to user @ require a comprehensive understanding of @ content of a @ @ deciding to access @ @ to achieve @ biological named entity described in @ abstract must @ linked to @ related entry in biological database and established controlled vocabulary @ a swissprot and gene ontology @ semantic assistant support user in content retrieval analysis and development by offering context-sensitive nlp service directly integrated in standard desktop client like a word processor @ @ @ implemented @ @ open service-oriented architecture @ semantic web ontology and w c web service @ @ @ @ a deployment of @ semantic assistant framework to provide link @ mutation protein protein property gene and organism mention in abstract to @ related entry in standardized biological database and controlled vocabulary @ @ underlying text mining pipeline used to identify named entity ha @ @ high level of precision and @ make @ functionality easily accessible @ a semantic assistant to end user @ reviewing pubmed abstract in @ a firefox client @ 
3753,Cross-lingual document representation and semantic similarity measure: A fuzzy set and rough set based approach,"As cross-lingual information retrieval is attracting increasing attention, tools that measure cross-lingual semantic similarity between documents are becoming desirable. In this paper, two aspects of cross-lingual semantic document similarity measures are investigated: One is document representation, and the other is the formulation of similarity measures. Fuzzy set and rough set theories are applied to capture the inherently fuzzy relationships among concepts expressed by natural languages. Our approach first develops a language-independent sense-level document representation based on the fuzzy set model to reduce the barrier between different languages and further explores the fuzzyrough hybrid approach to obtain a more robust macrosense-level document representation through the partitioning of the integrated sense association network of the document collection into macrosenses. Then, Tverskys notion of similarity and the F1 measure on information retrieval are adopted to formulate, respectively, two document similarity measures with fuzzy set operations on the two proposed document representations. The effectiveness of our approach is demonstrated by its success rate in identifying the English translations to their corresponding Chinese documents in a collection of ChineseEnglish parallel documents. Moreover, the proposed approach can be easily extended to process documents in other languages. It is believed that the proposed representations, along with the similarity measures, will enable more effective text mining processes. © 2006 IEEE.",2010,IEEE Transactions on Fuzzy Systems,33,a cross-lingual information retrieval is attracting increasing attention tool @ measure cross-lingual semantic similarity @ document @ becoming desirable @ in @ @ @ aspect of cross-lingual semantic document similarity measure @ investigated @ @ is document representation and @ @ is @ formulation of similarity measure @ fuzzy set and rough set theory @ applied to capture @ inherently fuzzy relationship among concept expressed by natural language @ @ approach first develops a language-independent sense-level document representation based on @ fuzzy set model to reduce @ barrier @ different language and @ explores @ fuzzyrough hybrid approach to obtain a more robust macrosense-level document representation @ @ partitioning of @ integrated sense association network of @ document collection @ macrosenses @ @ tverskys notion of similarity and @ f measure on information retrieval @ adopted to formulate respectively @ document similarity measure @ fuzzy set operation on @ @ proposed document representation @ @ effectiveness of @ approach is demonstrated by @ success rate in identifying @ english translation to @ corresponding chinese document in a collection of chineseenglish parallel document @ moreover @ proposed approach @ @ easily extended to process document in @ language @ @ is believed @ @ proposed representation along @ @ similarity measure @ enable more effective text mining process @ @ @ 
3762,The Cinderella of biological data integration: Addressing some of the challenges of entity and relationship mining from patent sources,"Most of the global corpus of medicinal chemistry data is only published in patents. However, extracting this from patent documents and subsequent integration with literature and database sources poses unique challenges. This work presents the investigation of an extensive full-text patent resource, including automated name-to-chemical structure conversion, licensed by AstraZeneca via a consortium arrangement with IBM. Our initial focus was identifying protein targets in patent titles linked to extracted bioactive compounds. We benchmarked target recognition strategies against target-assay-compound relationships manually curated from patents by GVKBIO. By analysis of word frequencies and protein names we assessed the false-negative problem of targets not specified in titles and false-positives from non-target proteins in titles. We also examined the time-signals for selected target and non-target names by year of patent publication. Our results exemplify problems and some solutions for extracting data from this source. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,@ of @ global corpus of medicinal chemistry data is only published in patent @ however extracting @ @ patent document and subsequent integration @ literature and database source pose unique challenge @ @ work @ @ investigation of @ extensive full-text patent resource including automated name-to-chemical structure conversion licensed by astrazeneca via a consortium arrangement @ ibm @ @ initial focus wa identifying protein target in patent title linked to extracted bioactive compound @ @ benchmarked target recognition strategy @ target-assay-compound relationship manually curated @ patent by gvkbio @ by analysis of word frequency and protein name @ assessed @ false-negative problem of target not specified in title and false-positives @ non-target protein in title @ @ @ examined @ time-signals @ selected target and non-target name by year of patent publication @ @ @ exemplify problem and some solution @ extracting data @ @ source @ springer-verlag @ 
3763,Semi-supervised abstraction-augmented string kernel for multi-level bio-relation extraction,"Bio-relation extraction (bRE), an important goal in bio-text mining, involves subtasks identifying relationships between bio-entities in text at multiple levels, e.g., at the article, sentence or relation level. A key limitation of current bRE systems is that they are restricted by the availability of annotated corpora. In this work we introduce a semi-supervised approach that can tackle multi-level bRE via string comparisons with mismatches in the string kernel framework. Our string kernel implements an abstraction step, which groups similar words to generate more abstract entities, which can be learnt with unlabeled data. Specifically, two unsupervised models are proposed to capture contextual (local or global) semantic similarities between words from a large unannotated corpus. This Abstraction-augmented String Kernel (ASK) allows for better generalization of patterns learned from annotated data and provides a unified framework for solving bRE with multiple degrees of detail. ASK shows effective improvements over classic string kernels on four datasets and achieves state-of-the-art bRE performance without the need for complex linguistic features. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,bio-relation extraction @ bre @ @ important goal in bio-text mining involves subtasks identifying relationship @ bio-entities in text at multiple level e @ g @ at @ article sentence @ relation level @ a key limitation of current bre system is @ @ @ restricted by @ availability of annotated corpus @ in @ work @ introduce a semi-supervised approach @ @ tackle multi-level bre via string comparison @ mismatch in @ string kernel framework @ @ string kernel implement @ abstraction step @ group similar word to generate more abstract entity @ @ @ learnt @ unlabeled data @ specifically @ unsupervised model @ proposed to capture contextual @ local @ global @ semantic similarity @ word @ a @ unannotated corpus @ @ abstraction-augmented string kernel @ ask @ allows @ better generalization of pattern learned @ annotated data and provides a unified framework @ solving bre @ multiple degree of detail @ ask @ effective improvement @ classic string kernel on four datasets and achieves state-of-the-art bre performance without @ need @ complex linguistic feature @ springer-verlag @ @ @ 
3765,Knowledge for everyman (extended abstract),"Increasing globalization creates situations with wide-ranging effects on large communities, often requiring global responses and innovative solutions. Timely examples are climate and environmental changes related to rapid growth and economic development. Natural and man-made unforeseen catastrophes like oil spills, landslides and floods require immediate action that might crucially rely on information and expertise available only from sources far removed from the crisis site. Knowledge sharing and transfer are also essential for sustainable long-term growth and development. In both kinds of cases, it is important that information and experience be made available and widely shared, communicated and encoded for future re-use. The global scope of many problems and their solutions requires furthermore that information and communication be accessible to communities crossing languages and cultures. Finally, an appropriate system for recording, maintaining and sharing information must be accessible to both experts and laymen. The goal of the European Union-funded KYOTO project (Knowledge-Yielding Ontologies for Transition-Based Organization, http://www.kyoto-project.eu ) is to develop an information and knowledge sharing system that relates documents in several languages to lexical resources and a common central ontology and allows for deep semantic analysis. KYOTO facilitates the crosslinguistic and crosscultural construction and maintenance of a sophisticated knowledge system among the members of domain-specific communities. Representation, storage and retrieval of a shared terminology takes place via a Wiki platform. Relevant terms are anchored in a language-independent, customizable formal ontology that connects the lexicons of seven languages (Basque, Chinese, Dutch, English, Italian, Japanese, and Spanish) and that guarantees a uniform interpretation of terms across languages. The semantic representations in the ontology are accessible to a computer and allow deep textual analysis and reasoning operations. KYOTO's target domains are the environment and biodiversity, with appropriate experts acting as ""users"". Once developed, the system will be available for extension to any domain. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,increasing globalization creates situation @ wide-ranging effect on @ community often requiring global response and innovative solution @ timely example @ climate and environmental change related to rapid growth and economic development @ natural and man-made unforeseen catastrophe like oil spill landslide and flood require immediate action @ might crucially rely on information and expertise available only @ source far removed @ @ crisis site @ knowledge sharing and transfer @ @ essential @ sustainable long-term growth and development @ in @ kind of case @ is important @ information and experience @ made available and widely shared communicated and encoded @ future re-use @ @ global scope of many problem and @ solution requires furthermore @ information and communication @ accessible to community crossing language and culture @ finally @ appropriate system @ recording maintaining and sharing information must @ accessible to @ expert and layman @ @ goal of @ european union-funded kyoto project @ knowledge-yielding ontology @ transition-based organization http @ www @ kyoto-project @ eu @ is to develop @ information and knowledge sharing system @ relates document in several language to lexical resource and a common central ontology and allows @ deep semantic analysis @ kyoto facilitates @ crosslinguistic and crosscultural construction and maintenance of a sophisticated knowledge system among @ member of domain-specific community @ representation storage and retrieval of a shared terminology take place via a wiki platform @ relevant term @ anchored in a language-independent customizable formal ontology @ connects @ lexicon of seven language @ basque chinese dutch english italian japanese and spanish @ and @ guarantee a uniform interpretation of term across language @ @ semantic representation in @ ontology @ accessible to a computer and allow deep textual analysis and reasoning operation @ kyoto @ s target domain @ @ environment and biodiversity @ appropriate expert acting a @ user @ @ @ developed @ system @ @ available @ extension to @ domain @ springer-verlag @ @ @ 
3767,A community mining algorithm for web texts based on Multi-agent system,"Most of the existing methods for community discovery only deal with social network with a fixed structure, so they can not effectively deal with dynamic social network. This paper proposes a Multi-agent system method which is applied to real-time dynamic web texts for community discovery. This method combines the multi-agent system control mechanisms with community discovery algorithm. The approach not only divides community for the published web text, but also changes the community division according to the newly updated contents of web texts. Meanwhile, the similarity of two web texts is evaluated through similarity algorithm which compares content similarity and semantic similarity. The effectiveness of this algorithm has been tested on real web texts networks and experiment 3 illustrated the scalability of our algorithm for dynamic community discovery of web texts. © 2010 Binary Information Press.",2010,Journal of Computational Information Systems,3,@ of @ existing method @ community discovery only deal @ social network @ a fixed structure @ @ @ not effectively deal @ dynamic social network @ @ @ proposes a multi-agent system method @ is applied to real-time dynamic web text @ community discovery @ @ method combine @ multi-agent system control mechanism @ community discovery algorithm @ @ approach not only divide community @ @ published web text @ @ change @ community division according to @ newly updated content of web text @ meanwhile @ similarity of @ web text is evaluated @ similarity algorithm @ compare content similarity and semantic similarity @ @ effectiveness of @ algorithm ha @ tested on real web text network and experiment illustrated @ scalability of @ algorithm @ dynamic community discovery of web text @ binary information @ @ 
3768,Clustering techniques and discrete particle swarm optimization algorithm for multi-document summarization,"Multi-document summarization is a process of automatic creation of a compressed version of a given collection of documents that provides useful information to users. In this article we propose a generic multi-document summarization method based on sentence clustering. We introduce five clustering methods, which optimize various aspects of intra-cluster similarity, inter-cluster dissimilarity and their combinations. To solve the clustering problem a modification of discrete particle swarm optimization algorithm has been proposed. The experimental results on open benchmark data sets from DUC2005 and DUC2007 show that our method significantly outperforms the baseline methods for multi-document summarization. © 2010 Wiley Periodicals, Inc.",2010,Computational Intelligence,31,multi-document summarization is a process of automatic creation of a compressed version of a given collection of document @ provides useful information to user @ in @ article @ propose a generic multi-document summarization method based on sentence clustering @ @ introduce five clustering method @ optimize various aspect of intra-cluster similarity inter-cluster dissimilarity and @ combination @ to solve @ clustering problem a modification of discrete particle swarm optimization algorithm ha @ proposed @ @ experimental @ on open benchmark data set @ duc and duc @ @ @ method significantly outperforms @ baseline method @ multi-document summarization @ wiley periodical inc @ 
3769,Unsupervised subjectivity-lexicon generation based on vector space model for multi-dimensional opinion analysis in blogosphere,"This paper presents an unsupervised framework to generate a vector-space-modeled subjectivity-lexicon for multi-dimensional opinion mining and sentiment analysis, such as criticism analysis, for which the traditional polarity analysis alone is not adequate. The framework consists of four major steps: first, creating a dataset by crawling blog posts of fiction reviews; secondly, creating a ""subjectivity-term to object"" matrix, with each subjectivity-term being modeled as a dimension of a vector space; thirdly, feature-transforming each subjectivity-term into the new feature-space to create the final multi-dimensional subjectivity-lexicon (MDSL); and fourthly, using the generated MDSL for opinion analysis. In the experiments, it shows that the improvement by the feature transform can be up to 31% in terms of the entropy of features. In addition, the subjectivity-terms and objects are also successfully and reasonably clustered in the demonstration of fiction review (literary criticism) analysis. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ @ @ @ unsupervised framework to generate a vector-space-modeled subjectivity-lexicon @ multi-dimensional opinion mining and sentiment analysis @ a criticism analysis @ @ @ traditional polarity analysis alone is not adequate @ @ framework consists of four major step @ first creating a dataset by crawling blog post of fiction review @ secondly creating a @ subjectivity-term to object @ matrix @ @ subjectivity-term @ modeled a a dimension of a vector space @ thirdly feature-transforming @ subjectivity-term @ @ @ feature-space to create @ final multi-dimensional subjectivity-lexicon @ mdsl @ @ and fourthly @ @ generated mdsl @ opinion analysis @ in @ experiment @ @ @ @ improvement by @ feature transform @ @ up to in term of @ entropy of feature @ in addition @ subjectivity-terms and object @ @ successfully and reasonably clustered in @ demonstration of fiction review @ literary criticism @ analysis @ springer-verlag @ @ @ 
3774,A short text modeling method combining semantic and statistical information,"A novel modeling method for a collection of short text snippets is presented in this paper to measure the similarity between pairs of snippets. The method takes account of both the semantic and statistical information within the short text snippets, and consists of three steps. Given a set of raw short text snippets, it first establishes the initial similarity between words by using a lexical database. The method then iteratively calculates both word similarity and short text similarity. Finally, a proximity matrix is constructed based on word similarity and used to convert the raw text snippets into vectors. Word similarity and text clustering experiments show that the proposed short text modeling method improves the performance of existing text-related information retrieval (IR) techniques. © 2010 Elsevier Inc. All rights reserved.",2010,Information Sciences,49,a novel modeling method @ a collection of short text snippet is presented in @ @ to measure @ similarity @ pair of snippet @ @ method take account of @ @ semantic and statistical information within @ short text snippet and consists of three step @ given a set of raw short text snippet @ first establishes @ initial similarity @ word by @ a lexical database @ @ method @ iteratively calculates @ word similarity and short text similarity @ finally a proximity matrix is constructed based on word similarity and used to convert @ raw text snippet @ vector @ word similarity and text clustering experiment @ @ @ proposed short text modeling method improves @ performance of existing text-related information retrieval @ ir @ technique @ @ inc @ @ right reserved @ 
3786,Clustering analysis for vasculitic diseases,"We introduce knowledge discovery for vasculitic diseases in this paper. Vasculitic diseases affect some organs and tissues and diagnosing can be quite difficult. Biomedical literature can contain hidden and useful knowledge for biomedical research and we develop a study based on co-occurrence analysis by using the articles in MEDLINE which is a widely used database.The mostly seen vasculitic diseases are selected to explore hidden patterns. We select PolySearch system as a web based biomedical text mining tool to find organs and tissues in the articles and create two separate datasets with their frequencies for each disease. After forming these datasets, we apply hierarchical clustering analysis to find similarities between the diseases. Clustering analysis reveals some similarities between diseases. We think that the results of clustered diseases positively affect on the medical research of vasculitic diseases especially during the diagnosis and certain similarities can provide different views to medical specialists. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Communications in Computer and Information Science,2,@ introduce knowledge discovery @ vasculitic disease in @ @ @ vasculitic disease affect some organ and tissue and diagnosing @ @ quite difficult @ biomedical literature @ contain hidden and useful knowledge @ biomedical research and @ develop a study based on co-occurrence analysis by @ @ article in medline @ is a widely used database @ @ mostly seen vasculitic disease @ selected to explore hidden pattern @ @ select polysearch system a a web based biomedical text mining tool to find organ and tissue in @ article and create @ separate datasets @ @ frequency @ @ disease @ @ forming @ datasets @ apply hierarchical clustering analysis to find similarity @ @ disease @ clustering analysis reveals some similarity @ disease @ @ think @ @ @ of clustered disease positively affect on @ medical research of vasculitic disease especially @ @ diagnosis and certain similarity @ provide different view to medical specialist @ springer-verlag @ @ @ 
3791,"Natural Language Processing and Information Systems - 15th International Conference on Applications of Natural Language to Information Systems, NLDB 2010, Proceedings",The proceedings contain 32 papers. The topics discussed include: an approach for adding noise-tolerance to restricted-domain information retrieval; measuring tree similarity for natural language processing based information retrieval; semantic content access using domain-independent NLP ontologies; automatic word sense disambiguation using cooccurrence and hierarchical information; automatic quality assessment of source code comments: the JavadocMiner; towards approximating COSMIC functional size from user requirements in agile development processes using text mining; semantic enriching of natural language texts with automatic thematic role annotation; adaptive topic modeling with probabilistic pseudo feedback in online topic detection; an approach to indexing and clustering news stories using continuous language models; and topology estimation of hierarchical hidden Markov models for language models.,2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proceeding contain @ @ @ topic discussed include @ @ approach @ adding noise-tolerance to restricted-domain information retrieval @ measuring tree similarity @ natural language processing based information retrieval @ semantic content access @ domain-independent nlp ontology @ automatic word sense disambiguation @ cooccurrence and hierarchical information @ automatic quality assessment of source code comment @ @ javadocminer @ towards approximating cosmic functional size @ user requirement in agile development process @ text mining @ semantic enriching of natural language text @ automatic thematic role annotation @ adaptive topic modeling @ probabilistic pseudo feedback in online topic detection @ @ approach to indexing and clustering news story @ continuous language model @ and topology estimation of hierarchical hidden markov model @ language model @ 
3792,Towards approximating COSMIC functional size from user requirements in agile development processes using text mining,"Measurement of software size from user requirements is crucial for the estimation of the developmental time and effort. COSMIC, an ISO/IEC international standard for functional size measurement, provides an objective method of measuring the functional size of the software from user requirements. COSMIC requires the user requirements to be written at a level of granularity, where interactions between the internal and the external environments to the system are visible to the human measurer, in a form similar to use case descriptions. On the other hand, requirements during an agile software development iteration are written in a less formal way than use case descriptions - often in the form of user stories, for example, keeping with the goal of delivering a planned release as quickly as possible. Therefore, size measurement in agile processes uses methods (e.g. story-points, smart estimation) that strictly depend on the subjective judgment of the experts, and avoid using objective measurement methods like COSMIC. In this paper, we presented an innovative concept showing that using a supervised text mining approach, COSMIC functional size can be automatically approximated from informally written textual requirements, demonstrating its applicability in popular agile software development processes, such as Scrum. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,measurement of software size @ user requirement is crucial @ @ estimation of @ developmental time and effort @ cosmic @ iso iec international standard @ functional size measurement provides @ objective method of measuring @ functional size of @ software @ user requirement @ cosmic requires @ user requirement to @ written at a level of granularity @ interaction @ @ internal and @ external environment to @ system @ visible to @ human measurer in a form similar to use case description @ on @ @ hand requirement @ @ agile software development iteration @ written in a le formal way @ use case description often in @ form of user story @ example keeping @ @ goal of delivering a planned release a quickly a possible @ therefore size measurement in agile process us method @ e @ g @ story-points smart estimation @ @ strictly depend on @ subjective judgment of @ expert and avoid @ objective measurement method like cosmic @ in @ @ @ presented @ innovative concept showing @ @ a supervised text mining approach cosmic functional size @ @ automatically approximated @ informally written textual requirement demonstrating @ applicability in popular agile software development process @ a scrum @ springer-verlag @ 
3795,Semantic content access using domain-independent NLP ontologies,"We present a lightweight, user-centred approach for document navigation and analysis that is based on an ontology of text mining results. This allows us to bring the result of existing text mining pipelines directly to end users. Our approach is domain-independent and relies on existing NLP analysis tasks such as automatic multi-document summarization, clustering, question-answering, and opinion mining. Users can interactively trigger semantic processing services for tasks such as analyzing product reviews, daily news, or other document sets. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ a lightweight user-centred approach @ document navigation and analysis @ is based on @ ontology of text mining @ @ @ allows u to bring @ @ of existing text mining pipeline directly to end user @ @ approach is domain-independent and relies on existing nlp analysis task @ a automatic multi-document summarization clustering question-answering and opinion mining @ user @ interactively trigger semantic processing service @ task @ a analyzing product review daily news @ @ document set @ springer-verlag @ 
3801,Exploring the sentiment strength of user reviews,"Existing research efforts in sentiment analysis of online user reviews mainly focus on extracting features (such as quality and price) of products/services and classifying users' sentiments into semantic orientations (such as positive, negative or neutral). However, few of them take the strength of user sentiments into consideration, which is particularly important in measuring the overall quality of products/services. Intuitively, different reviews for the same feature should have quite different sentiment strength, even though they may express the same polarity of sentiment. This paper presents an approach to estimating the sentiment strength of user reviews according to the strength of adverbs and adjectives expressed by users in their opinion phrases. Experimental result on a hotel review dataset in Chinese shows that the proposed approach is effective in the task of sentiment classification and achieves a good performance on a multi-scale evaluation. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),28,existing research effort in sentiment analysis of online user review mainly focus on extracting feature @ @ a quality and price @ of product service and classifying user @ sentiment @ semantic orientation @ @ a positive negative @ neutral @ @ however @ of @ take @ strength of user sentiment @ consideration @ is particularly important in measuring @ overall quality of product service @ intuitively different review @ @ @ feature @ @ quite different sentiment strength even though @ may express @ @ polarity of sentiment @ @ @ @ @ approach to estimating @ sentiment strength of user review according to @ strength of adverb and adjective expressed by user in @ opinion phrase @ experimental @ on a hotel review dataset in chinese @ @ @ proposed approach is effective in @ task of sentiment classification and achieves a good performance on a multi-scale evaluation @ springer-verlag @ 
3802,Text adaptation using formal concept analysis,"This paper addresses the issue of adapting cases represented by plain text with the help of formal concept analysis and natural language processing technologies. The actual cases represent recipes in which we classify ingredients according to culinary techniques applied to them. The complex nature of linguistic anaphoras in recipe texts make usual text mining techniques inefficient so a stronger approach, using syntactic and dynamic semantic analysis to build a formal representation of a recipe, had to be used. This representation is useful for various applications but, in this paper, we show how one can extract ingredient-action relations from it in order to use formal concept analysis and select an appropriate replacement sequence of culinary actions to use in adapting the recipe text. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),18,@ @ address @ issue of adapting case represented by plain text @ @ help of formal concept analysis and natural language processing technology @ @ actual case represent recipe in @ @ classify ingredient according to culinary technique applied to @ @ @ complex nature of linguistic anaphora in recipe text make usual text mining technique inefficient @ a stronger approach @ syntactic and dynamic semantic analysis to build a formal representation of a recipe @ to @ used @ @ representation is useful @ various application @ in @ @ @ @ @ @ @ extract ingredient-action relation @ @ in order to use formal concept analysis and select @ appropriate replacement sequence of culinary action to use in adapting @ recipe text @ springer-verlag @ 
3812,The semantic gap of formalized meaning,"Recent work in Ontology learning and Text mining has mainly focused on engineering methods to solve practical problem. In this thesis, we investigate methods that can substantially improve a wide range of existing approaches by minimizing the underlying problem: The Semantic Gap between formalized meaning and human cognition. We deploy OWL as a Meaning Representation Language and create a unified model, which combines existing NLP methods with Linguistic knowledge and aggregates disambiguated background knowledge from the Web of Data. The presented methodology here allows to study and evaluate the capabilities of such aggregated knowledge to improve the efficiency of methods in NLP and Ontology learning. © 2010 Springer-Verlag.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,recent work in ontology learning and text mining ha mainly focused on engineering method to solve practical problem @ in @ thesis @ investigate method @ @ substantially improve a wide range of existing approach by minimizing @ underlying problem @ @ semantic gap @ formalized meaning and human cognition @ @ deploy owl a a meaning representation language and create a unified model @ combine existing nlp method @ linguistic knowledge and aggregate disambiguated background knowledge @ @ web of data @ @ presented methodology @ allows to study and evaluate @ capability of @ aggregated knowledge to improve @ efficiency of method in nlp and ontology learning @ springer-verlag @ 
3815,Recherche d'information centrée sur l'utilisateur,"In this paper, we present a novel approach to text mining that helps to build intelligent user interfaces for recommender and information retrieval systems. The main problem for the user in information retrieval is that he must have almost perfect knowledge of the domain and the domain terminology. Our approach eases this burden by showing a way how to encode domain knowledge so that an information retrieval system can transform the user's way to talk about the domain in the expert's way to do that. After that transformation the system can search its data bases for appropriate information. We demonstrate the practicability of our approach in a case study on a TV recommender system. © 2010 Lavoisier, Paris.",2010,Revue d'Intelligence Artificielle,1,in @ @ @ @ a novel approach to text mining @ help to build intelligent user interface @ recommender and information retrieval system @ @ main problem @ @ user in information retrieval is @ he must @ almost perfect knowledge of @ domain and @ domain terminology @ @ approach eas @ burden by showing a way @ to encode domain knowledge @ @ @ information retrieval system @ transform @ user @ s way to talk @ @ domain in @ expert @ s way to @ @ @ @ @ transformation @ system @ search @ data base @ appropriate information @ @ demonstrate @ practicability of @ approach in a case study on a tv recommender system @ lavoisier paris @ 
3818,An information-theoretic foundation for the measurement of discrimination information,"Hitherto, it has not been easy to interpret the meaning of the amount of discrimination information conveyed in a term rationally and explicitly within practical application contexts; it has not been simple to introduce the concept of the extent of semantic relatedness between two terms meaningfully and successfully into scientific discussions. This study is part of an attempt to do this. We attempt to answer two important questions: 1) What is the discrimination information conveyed by a term and how to measure it? 2) What is the relatedness between two terms and how to estimate it? We focus on the first question and present an in-depth investigation into the discrimination measures based on several information measures, which are widely used in a variety of applications. The relatedness measures are then naturally defined according to the individual discrimination measures. Some key points are made for clarifying potential problems arising from using the relatedness measures, and solutions are suggested. Two example applications in the contexts of text mining and information retrieval are provided. The aim of this study, of which this paper forms part, is to establish a unified theoretical framework, with measurement of discrimination information (MDI) at the core, for achieving effective measurement of semantic relatedness (MSR). Due to its generality, our method can be expected to be a useful tool with a wide range of application areas. © 2006 IEEE.",2010,IEEE Transactions on Knowledge and Data Engineering,7,hitherto @ ha not @ easy to interpret @ meaning of @ amount of discrimination information conveyed in a term rationally and explicitly within practical application context @ @ ha not @ simple to introduce @ concept of @ extent of semantic relatedness @ @ term meaningfully and successfully @ scientific discussion @ @ study is part of @ attempt to @ @ @ @ attempt to answer @ important question @ @ @ is @ discrimination information conveyed by a term and @ to measure @ @ @ @ is @ relatedness @ @ term and @ to estimate @ @ @ focus on @ first question and @ @ in-depth investigation @ @ discrimination measure based on several information measure @ @ widely used in a variety of application @ @ relatedness measure @ @ naturally defined according to @ individual discrimination measure @ some key point @ made @ clarifying potential problem arising @ @ @ relatedness measure and solution @ suggested @ @ example application in @ context of text mining and information retrieval @ provided @ @ aim of @ study of @ @ @ form part is to establish a unified theoretical framework @ measurement of discrimination information @ mdi @ at @ core @ achieving effective measurement of semantic relatedness @ msr @ @ due to @ generality @ method @ @ expected to @ a useful tool @ a wide range of application area @ @ @ 
3820,Dynamic visualization of statistical learning in the context of high-dimensional textual data,"Our ability to record increasingly larger and more complex sets of data is accompanied by a decline in our capacity to interpret and understand these data in the fullest sense. Multivariate analysis partially assists us in our quest by reducing the dimensionality in optimal ways, but our view is stuck in two dimensions because of the planar nature of the graphical medium, be it the printed page or the computer screen. We are developing protocols and tools to add motion to scientific graphics so that high-dimensional data can be visualized dynamically. Using the freely available R language and modern methods of statistical learning and data mining, we construct animation sequences that take the viewer on a dynamic journey through the data. The idea is illustrated using a large data set of all the abstracts of the journal Vaccine in the years 2003-2006, according to their word frequencies and citation counts. © 2010 Elsevier B.V.",2010,Journal of Web Semantics,9,@ ability to record increasingly larger and more complex set of data is accompanied by a decline in @ capacity to interpret and understand @ data in @ fullest sense @ multivariate analysis partially assist u in @ quest by reducing @ dimensionality in optimal way @ @ view is stuck in @ dimension @ of @ planar nature of @ graphical medium @ @ @ printed page @ @ computer screen @ @ @ developing protocol and tool to add motion to scientific graphic @ @ high-dimensional data @ @ visualized dynamically @ @ @ freely available r language and modern method of statistical learning and data mining @ construct animation sequence @ take @ viewer on a dynamic journey @ @ data @ @ idea is illustrated @ a @ data set of @ @ abstract of @ journal vaccine in @ year according to @ word frequency and citation count @ @ b @ v @ 
3822,Lemmagen: Multilingual lemmatisation with induced ripple-down rules,"Lemmatisation is the process of finding the normalised forms of words appearing in text. It is a useful preprocessing step for a number of language engineering and text mining tasks, and especially important for languages with rich inflectional morphology. This paper presents a new lemmatisation system, LemmaGen, which was trained to generate accurate and efficient lemmatisers for twelve different languages. Its evaluation on the corresponding lexicons shows that LemmaGen outperforms the lemmatisers generated by two alternative approaches, RDR and CST, both in terms of accuracy and efficiency. To our knowledge, LemmaGen is the most efficient publicly available lemmatiser trained on large lexicons of multiple languages, whose learning engine can be retrained to effectively generate lemmatisers of other languages. © J.UCS.",2010,Journal of Universal Computer Science,28,lemmatisation is @ process of finding @ normalised form of word appearing in text @ @ is a useful preprocessing step @ a number of language engineering and text mining task and especially important @ language @ rich inflectional morphology @ @ @ @ a @ lemmatisation system lemmagen @ wa trained to generate accurate and efficient lemmatisers @ twelve different language @ @ evaluation on @ corresponding lexicon @ @ lemmagen outperforms @ lemmatisers generated by @ alternative approach rdr and cst @ in term of accuracy and efficiency @ to @ knowledge lemmagen is @ @ efficient publicly available lemmatiser trained on @ lexicon of multiple language whose learning engine @ @ retrained to effectively generate lemmatisers of @ language @ j @ ucs @ 
3823,Approaches to text mining arguments from legal cases,"This paper describes recent approaches using text-mining to automatically profile and extract arguments from legal cases. We outline some of the background context and motivations. We then turn to consider issues related to the construction and composition of corpora of legal cases. We show how a Context-Free Grammar can be used to extract arguments, and how ontologies and Natural Language Processing can identify complex information such as case factors and participant roles. Together the results bring us closer to automatic identification of legal arguments. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),53,@ @ describes recent approach @ text-mining to automatically profile and extract argument @ legal case @ @ outline some of @ background context and motivation @ @ @ turn to consider issue related to @ construction and composition of corpus of legal case @ @ @ @ a context-free grammar @ @ used to extract argument and @ ontology and natural language processing @ identify complex information @ a case factor and participant role @ together @ @ bring u closer to automatic identification of legal argument @ springer-verlag @ @ @ 
3824,"Overview of the ninth annual meeting of the BioLINK SIG at ISMB: Linking literature, information and knowledge for biology","With the increasing availability of textual information related to biological research, such information has become an important component of many bioinformatics applications. Much recent work aims to develop practical tools to facilitate the use of the literature for annotating the vast amounts of molecular data, including gene sequences, transcription profiles and biological pathways. The broad area of biomedical text mining is concerned with using methods from natural language processing, information extraction, information retrieval and summarization to automate knowledge discovery from biomedical text. In the biomedical domain, research has focused on several complex text-based applications, including the identification of relevant literature (information retrieval) for specific information needs, the extraction of experimental findings for assistance in building biological knowledge bases, and summarization - aiming to present key biological facts in a succinct form. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ increasing availability of textual information related to biological research @ information ha become @ important component of many bioinformatics application @ much recent work aim to develop practical tool to facilitate @ use of @ literature @ annotating @ vast amount of molecular data including gene sequence transcription profile and biological pathway @ @ broad area of biomedical text mining is concerned @ @ method @ natural language processing information extraction information retrieval and summarization to automate knowledge discovery @ biomedical text @ in @ biomedical domain research ha focused on several complex text-based application including @ identification of relevant literature @ information retrieval @ @ specific information need @ extraction of experimental finding @ assistance in building biological knowledge base and summarization aiming to @ key biological fact in a succinct form @ springer-verlag @ @ @ 
3825,Extracting and normalizing gene/protein mentions with the flexible and trainable Moara Java library,"Gene/protein recognition and normalization are important prerequisite steps for many biological text mining tasks. Even if great efforts have been dedicated to these problems and effective solutions have been reported, the availability of easily integrated tools to perform these tasks is still deficient. We therefore propose Moara, a Java library that implements gene/protein recognition and normalization steps based on machine learning approaches. The system may be trained with extra documents for the recognition procedure and new organism may be added in the normalization step. The novelty of the methodology used in Moara lies in the design of a system that is not tailored to a specific organism and therefore does not need any organism-dependent tuning in the algorithms and in the dictionaries it uses. Moara can be used either as a standalone application or incorporated in a text mining system and it is available at: http://moara.dacya.ucm.es © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,gene protein recognition and normalization @ important prerequisite step @ many biological text mining task @ even if great effort @ @ dedicated to @ problem and effective solution @ @ reported @ availability of easily integrated tool to perform @ task is still deficient @ @ therefore propose moara a java library @ implement gene protein recognition and normalization step based on machine learning approach @ @ system may @ trained @ extra document @ @ recognition procedure and @ organism may @ added in @ normalization step @ @ novelty of @ methodology used in moara lie in @ design of a system @ is not tailored to a specific organism and therefore doe not need @ organism-dependent tuning in @ algorithm and in @ dictionary @ us @ moara @ @ used either a a standalone application @ incorporated in a text mining system and @ is available at @ http @ moara @ dacya @ ucm @ e springer-verlag @ @ @ 
3826,Toward computer-assisted text curation: Classification is easy (choosing training data can be hard...),"We aim to design a system for classifying scientific articles based on the presence of protein characterization experiments, intending to aid the curators populating JCVI's Characterized Protein (CHAR) Database of experimentally characterized proteins. We trained two classifiers using small datasets labeled by CHAR curators, and another classifier based on a much larger dataset using annotations from public databases. Performance varied greatly, in ways we did not anticipate. We describe the datasets, the classification method, and discuss the unexpected results. © 2010 Springer-Verlag Berlin Heidelberg.",2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,@ aim to design a system @ classifying scientific article based on @ presence of protein characterization experiment intending to aid @ curator populating jcvi @ s characterized protein @ char @ database of experimentally characterized protein @ @ trained @ classifier @ small datasets labeled by char curator and another classifier based on a much larger dataset @ annotation @ public database @ performance varied greatly in way @ @ not anticipate @ @ describe @ datasets @ classification method and discus @ unexpected @ @ springer-verlag @ @ @ 
3827,Pairwise-adaptive dissimilarity measure for document clustering,"This paper introduces a novel pairwise-adaptive dissimilarity measure for large high dimensional document datasets that improves the unsupervised clustering quality and speed compared to the original cosine dissimilarity measure. This measure dynamically selects a number of important features of the compared pair of document vectors. Two approaches for selecting the number of features in the application of the measure are discussed. The proposed feature selection process makes this dissimilarity measure especially applicable in large, high dimensional document collections. Its performance is validated on several test sets originating from standardized datasets. The dissimilarity measure is compared to the well-known cosine dissimilarity measure using the average F-measures of the hierarchical agglomerative clustering result. This new dissimilarity measure results in an improved clustering result obtained with a lower required computational time. © 2010 Elsevier Inc. All rights reserved.",2010,Information Sciences,27,@ @ introduces a novel pairwise-adaptive dissimilarity measure @ @ high dimensional document datasets @ improves @ unsupervised clustering quality and speed compared to @ original cosine dissimilarity measure @ @ measure dynamically selects a number of important feature of @ compared pair of document vector @ @ approach @ selecting @ number of feature in @ application of @ measure @ discussed @ @ proposed feature selection process make @ dissimilarity measure especially applicable in @ high dimensional document collection @ @ performance is validated on several test set originating @ standardized datasets @ @ dissimilarity measure is compared to @ well-known cosine dissimilarity measure @ @ average f-measures of @ hierarchical agglomerative clustering @ @ @ @ dissimilarity measure @ in @ improved clustering @ obtained @ a lower required computational time @ @ inc @ @ right reserved @ 
3829,A web-mining approach to disambiguate biomedical acronym expansions,"Named Entities Recognition (NER) has become one of the major issues in Information Retrieval (IR), knowledge extraction, and document classification. This paper addresses a particular case of NER, acronym expansion (or definition) when this expansion does not exist in the document using the acronym. Since acronyms may obviously expand into several distinct sets of words, this paper provides nine quality measures of the relevant definition prediction based on mutual information (MI), cubic MI (MI3), and Dice's coefficient. A combinaison of these statistical measures with the cosine approach is proposed. Experiments have been run on biomedical domain where acronyms are numerous. The results on our biomedical corpus showed that the proposed measures were accurate devices to predict relevant definitions. Povzetek: Predstavljene so metode spletnega preiskovanja dvoumnih akronimov v domeni biomedicinskih baz.",2010,Informatica (Ljubljana),7,named entity recognition @ ner @ ha become @ of @ major issue in information retrieval @ ir @ knowledge extraction and document classification @ @ @ address a particular case of ner acronym expansion @ @ definition @ @ @ expansion doe not exist in @ document @ @ acronym @ since acronym may obviously expand @ several distinct set of word @ @ provides nine quality measure of @ relevant definition prediction based on mutual information @ mi @ cubic mi @ mi @ and dice @ s coefficient @ a combinaison of @ statistical measure @ @ cosine approach is proposed @ experiment @ @ run on biomedical domain @ acronym @ numerous @ @ @ on @ biomedical corpus showed @ @ proposed measure @ accurate device to predict relevant definition @ povzetek @ predstavljene @ metode spletnega preiskovanja dvoumnih akronimov v domeni biomedicinskih baz @ 
3839,Master defect record retrieval using network-based feature association,"As electronic records (e.g., medical records and technical defect records) accumulate, the retrieval of a record from a past instance with the same or similar circumstances, has become extremely valuable. This is because a past record may contain the correct diagnosis or correct solution to the current circumstance. We refer to the two records of the same or similar circumstances as master and duplicate records. Current record retrieval techniques are lacking when applied to this special master defect record retrieval problem. In this study, we propose a new paradigm for master defect record retrieval using network-based feature association (NBFA). We train the master record retrieval process by constructing feature associations to limit the search space. The retrieval paradigm was employed and tested on a real-world large-scale defect record database from a telecommunications company. The empirical results suggest that the NBFA was able to significantly improve the performance of master record retrieval, and should be implemented in practice. This paper presents an overview of technical aspects of the master defect record retrieval problem, describes general methodologies for retrieval of master defect records, proposes a new feature association paradigm, provides performance assessments on real data from a telecommunications company, and highlights difficulties and challenges in this line of research that should be addressed in the future. © 2006 IEEE.",2010,"IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews",8,a electronic record @ e @ g @ medical record and technical defect record @ accumulate @ retrieval of a record @ a past instance @ @ @ @ similar circumstance ha become extremely valuable @ @ is @ a past record may contain @ correct diagnosis @ correct solution to @ current circumstance @ @ refer to @ @ record of @ @ @ similar circumstance a master and duplicate record @ current record retrieval technique @ lacking @ applied to @ special master defect record retrieval problem @ in @ study @ propose a @ paradigm @ master defect record retrieval @ network-based feature association @ nbfa @ @ @ train @ master record retrieval process by constructing feature association to limit @ search space @ @ retrieval paradigm wa employed and tested on a real-world large-scale defect record database @ a telecommunication company @ @ empirical @ suggest @ @ nbfa wa able to significantly improve @ performance of master record retrieval and @ @ implemented in practice @ @ @ @ @ overview of technical aspect of @ master defect record retrieval problem describes general methodology @ retrieval of master defect record proposes a @ feature association paradigm provides performance assessment on real data @ a telecommunication company and highlight difficulty and challenge in @ line of research @ @ @ addressed in @ future @ @ @ 
3844,Extending lexical association measures for collocation extraction,"Collocations are linguistic phenomena that occur when two or more words appear together more often than by chance and whose meaning often cannot be inferred from the meanings of its parts. As collocations have found many applications in the fields of natural language processing, information retrieval, and text mining, extracting them from large corpora has been the focus of many studies over the past few years. In this paper, we introduce the notion of an extension pattern, a formalization of the idea of extending lexical association measures (AMs) defined for bigrams. An extension pattern provides a measure-independent way of extending AMs for extracting collocations of arbitrary length. We define different extension patterns and compare them on a task of extracting collocations from a newspaper corpus. We show that the stopword-sensitive extension patterns we propose outperform other extensions, which indicates that AMs could benefit by taking into account linguistic information about an n-gram's part-of-speech pattern. © 2009 Elsevier Ltd. All rights reserved.",2010,Computer Speech and Language,16,collocation @ linguistic phenomenon @ occur @ @ @ more word appear together more often @ by chance and whose meaning often cannot @ inferred @ @ meaning of @ part @ a collocation @ found many application in @ field of natural language processing information retrieval and text mining extracting @ @ @ corpus ha @ @ focus of many study @ @ past @ year @ in @ @ @ introduce @ notion of @ extension pattern a formalization of @ idea of extending lexical association measure @ @ @ defined @ bigram @ @ extension pattern provides a measure-independent way of extending @ @ extracting collocation of arbitrary length @ @ define different extension pattern and compare @ on a task of extracting collocation @ a newspaper corpus @ @ @ @ @ stopword-sensitive extension pattern @ propose outperform @ extension @ indicates @ @ could benefit by taking @ account linguistic information @ @ n-gram @ s part-of-speech pattern @ @ ltd @ @ right reserved @ 
3848,Text Mining: Applications and Theory,"Text Mining: Applications and Theory presents the state-of-the-art algorithms for text mining from both the academic and industrial perspectives. The contributors span several countries and scientific domains: universities, industrial corporations, and government laboratories, and demonstrate the use of techniques from machine learning, knowledge discovery, natural language processing and information retrieval to design computational models for automated text analysis and mining. This volume demonstrates how advancements in the fields of applied mathematics, computer science, machine learning, and natural language processing can collectively capture, classify, and interpret words and their contexts. As suggested in the preface, text mining is needed when ""words are not enough."" This book: Provides state-of-the-art algorithms and techniques for critical tasks in text mining applications, such as clustering, classification, anomaly and trend detection, and stream analysis. •Presents a survey of text visualization techniques and looks at the multilingual text classification problem. •Discusses the issue of cybercrime associated with chatrooms. •Features advances in visual analytics and machine learning along with illustrative examples. Is accompanied by a supporting website featuring datasets. CApplied mathematicians, statisticians, practitioners and students in computer science, bioinformatics and engineering will find this book extremely useful. © 2010 John Wiley & Sons, Ltd.",2010,Text Mining: Applications and Theory,154,text mining @ application and theory @ @ state-of-the-art algorithm @ text mining @ @ @ @ and industrial perspective @ @ contributor span several country and scientific domain @ university industrial corporation and government laboratory and demonstrate @ use of technique @ machine learning knowledge discovery natural language processing and information retrieval to design computational model @ automated text analysis and mining @ @ volume demonstrates @ advancement in @ field of applied mathematics computer science machine learning and natural language processing @ collectively capture classify and interpret word and @ context @ a suggested in @ preface text mining is needed @ @ word @ not enough @ @ @ book @ provides state-of-the-art algorithm and technique @ critical task in text mining application @ a clustering classification anomaly and trend detection and stream analysis @ @ a survey of text visualization technique and look at @ multilingual text classification problem @ discus @ issue of cybercrime associated @ chatroom @ feature advance in visual analytics and machine learning along @ illustrative example @ is accompanied by a supporting website featuring datasets @ capplied mathematician statistician practitioner and student in computer science bioinformatics and engineering @ find @ book extremely useful @ john wiley son ltd @ 
3850,Using contextual information to clarify cross-species gene normalization ambiguity,"The goal of Gene Normalization (GN) is to identify the unique database IDs of genes and proteins mentioned in biomedical literature. A major difficulty in GN comes from the ambiguity of gene names. That is, the same gene name can refer to different database IDs depending on the species in question. In this paper, we introduce a method to exploit contextual information in an abstract, like tissue type, chromosome location, etc., to tackle this problem. Using this technique, we have been able to improve system performance (F-score) by 14.3% on the BioCreAtIvE-II GN task test set. We also examined our method on a full-text dataset with cross-species genes. The experimental results show a promising performance (AUC) of 42.94%. Our experimental results also show that with full text, versus abstract only, the system performance was 12.24% higher. © 2010 World Scientific Publishing Company.",2010,International Journal of Software Engineering and Knowledge Engineering,0,@ goal of gene normalization @ gn @ is to identify @ unique database id of gene and protein mentioned in biomedical literature @ a major difficulty in gn come @ @ ambiguity of gene name @ @ is @ @ gene name @ refer to different database id depending on @ specie in question @ in @ @ @ introduce a method to exploit contextual information in @ abstract like tissue type chromosome location etc @ to tackle @ problem @ @ @ technique @ @ @ able to improve system performance @ f-score @ by @ on @ biocreative-ii gn task test set @ @ @ examined @ method on a full-text dataset @ cross-species gene @ @ experimental @ @ a promising performance @ auc @ of @ @ @ experimental @ @ @ @ @ full text versus abstract only @ system performance wa @ higher @ world scientific publishing company @ 
3851,Mining synonymous transliterations from the world wide web,"The World Wide Web has been considered one of the important sources for information. Using search engines to retrieve Web pages can gather lots of information, including foreign information. However, to be better understood by local readers, proper names in a foreign language, such as English, are often transliterated to a local language such as Chinese. Due to different translators and the lack of translation standard, translating foreign proper nouns may result in different transliterations and pose a notorious headache. In particular, it may cause incomplete search results. Using one transliteration as a query keyword will fail to retrieve the Web pages which use a different word as the transliteration. Consequently, important information may be missed. We present a framework for mining synonymous transliterations as many as possible from the Web for a given transliteration. The results can be used to construct a database of synonymous transliterations which can be utilized for query expansion so as to alleviate the incomplete search problem. Experimental results show that the proposed framework can effectively retrieve the set of snippets which may contain synonymous transliterations and then extract the target terms. Most of the extracted synonymous transliterations have higher rank of similarity to the input transliteration compared to other noise terms. © 2010 ACM.",2010,ACM Transactions on Asian Language Information Processing,3,@ world wide web ha @ considered @ of @ important source @ information @ @ search engine to retrieve web page @ gather lot of information including foreign information @ however to @ better understood by local reader proper name in a foreign language @ a english @ often transliterated to a local language @ a chinese @ due to different translator and @ lack of translation standard translating foreign proper noun may @ in different transliteration and pose a notorious headache @ in particular @ may cause incomplete search @ @ @ @ transliteration a a query keyword @ fail to retrieve @ web page @ use a different word a @ transliteration @ consequently important information may @ missed @ @ @ a framework @ mining synonymous transliteration a many a possible @ @ web @ a given transliteration @ @ @ @ @ used to construct a database of synonymous transliteration @ @ @ utilized @ query expansion @ a to alleviate @ incomplete search problem @ experimental @ @ @ @ proposed framework @ effectively retrieve @ set of snippet @ may contain synonymous transliteration and @ extract @ target term @ @ of @ extracted synonymous transliteration @ higher rank of similarity to @ input transliteration compared to @ noise term @ acm @ 
3853,A pilot study of the characterization of English-Chinese web bilingual data,"With the rapid development of WWW, bilingual data appears on blogs, forums, etc. To the best of our knowledge, little work has been done to intensively study the characterization of bilingual data in English-Chinese mixed language pages. However, its distribution features do matters a lot for many text mining research topics, e. g. estimating training data quantity and quality for statistical machine translation. In this paper, we state several key issues to understand the characterizations of bilingual corpora on the web, and then we build an experimental platform to study the features of web bilingual data. Finally, we conduct the experiments and present the preliminary results. Copyright © 2010 Binary Information Press.",2010,Journal of Information and Computational Science,0,@ @ rapid development of www bilingual data appears on blog forum etc @ to @ best of @ knowledge little work ha @ done to intensively study @ characterization of bilingual data in english-chinese mixed language page @ however @ distribution feature @ matter a lot @ many text mining research topic e @ g @ estimating training data quantity and quality @ statistical machine translation @ in @ @ @ state several key issue to understand @ characterization of bilingual corpus on @ web and @ @ build @ experimental platform to study @ feature of web bilingual data @ finally @ conduct @ experiment and @ @ preliminary @ @ @ binary information @ @ 
3855,"Handbook of natural language processing, second edition","The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis. New to the Second Edition • Greater prominence of statistical approaches • New applications section • Broader multilingual scope to include Asian and European languages, along with English • An actively maintained wiki (http://handbookofnlp.cse.unsw.edu.au) that provides online resources, supplementary information, and up-to-date developments Divided into three sections, the book first surveys classical techniques, including both symbolic and empirical approaches. The second section focuses on statistical approaches in natural language processing. In the final section of the book, each chapter describes a particular class of application, from Chinese machine translation to information visualization to ontology construction to biomedical text mining. Fully updated with the latest developments in the field, this comprehensive, modern handbook emphasizes how to implement practical language processing tools in computational systems. © 2010 by Taylor & Francis Group, LLC.",2010,"Handbook of Natural Language Processing, Second Edition",33,@ handbook of natural language processing second edition @ practical tool and technique @ implementing natural language processing in computer system @ along @ removing outdated material @ edition update every chapter and expands @ content to include emerging area @ a sentiment analysis @ @ to @ second edition greater prominence of statistical approach @ application section broader multilingual scope to include asian and european language along @ english @ actively maintained wiki @ http @ handbookofnlp @ cse @ unsw @ edu @ au @ @ provides online resource supplementary information and up-to-date development divided @ three section @ book first survey classical technique including @ symbolic and empirical approach @ @ second section focus on statistical approach in natural language processing @ in @ final section of @ book @ chapter describes a particular class of application @ chinese machine translation to information visualization to ontology construction to biomedical text mining @ fully updated @ @ latest development in @ field @ comprehensive modern handbook emphasizes @ to implement practical language processing tool in computational system @ by taylor francis group llc @ 
3856,BioNLP: Biomedical text mining,"BioNLP, also known as biomedical language processing or biomedical text mining, is the application of natural language processing techniques to biomedical data. The biomedical domain presents a number of unique data types and tasks, but simultaneously has many aspects that are of interest to the “mainstream” natural language processing community. Additionally, there are ethical issues in BioNLP that necessitate an attention to software quality assurance beyond the normal attention (or lack thereof) that is paid to it in the mainstream academic NLP community. © 2010 by Taylor & Francis Group, LLC.",2010,"Handbook of Natural Language Processing, Second Edition",6,bionlp @ known a biomedical language processing @ biomedical text mining is @ application of natural language processing technique to biomedical data @ @ biomedical domain @ a number of unique data type and task @ simultaneously ha many aspect @ @ of interest to @ mainstream natural language processing community @ additionally @ @ ethical issue in bionlp @ necessitate @ attention to software quality assurance beyond @ normal attention @ @ lack thereof @ @ is paid to @ in @ mainstream @ nlp community @ by taylor francis group llc @ 
3860,From bursty patterns to bursty facts: The effectiveness of temporal text mining for news,"Many document collections are by nature dynamic, evolving as the topics or events they describe change. The goal of temporal text mining is to discover bursty patterns and to identify and highlight these changes to better enable readers to track stories. Here, we focus on the news domain, where the changes revolve around novel, previously unpublished, ""facts"" that have an effect on the story developments. However, despite intense research activities on bursty patterns, a lack of common procedures today makes it impossible to compare methods in a principled way. To close this gap, we (a) investigate how different temporal text mining methods discover novel facts and (b) present an evaluation framework for methods assessment, consisting of a set of procedures and metrics for cross-evaluating models. Bursty patterns are transformed into queries for sentence retrieval, either with or without taking into account internal pattern structure, and these sentences are compared with a set of editor-selected ground-truth reference sentences. Our experiments on different classes of temporal text mining show that different methods perform at similar levels overall, but provide distinctive advantages in some settings. The experiments also demonstrate the benefits of using patterns' internal structure for query generation. © 2010 The authors and IOS Press. All rights reserved.",2010,Frontiers in Artificial Intelligence and Applications,9,many document collection @ by nature dynamic evolving a @ topic @ event @ describe change @ @ goal of temporal text mining is to discover bursty pattern and to identify and highlight @ change to better enable reader to track story @ @ @ focus on @ news domain @ @ change revolve around novel @ unpublished @ fact @ @ @ @ effect on @ story development @ however despite intense research activity on bursty pattern a lack of common procedure today make @ impossible to compare method in a principled way @ to close @ gap @ @ a @ investigate @ different temporal text mining method discover novel fact and @ b @ @ @ evaluation framework @ method assessment consisting of a set of procedure and metric @ cross-evaluating model @ bursty pattern @ transformed @ query @ sentence retrieval either @ @ without taking @ account internal pattern structure and @ sentence @ compared @ a set of editor-selected ground-truth reference sentence @ @ experiment on different class of temporal text mining @ @ different method perform at similar level overall @ provide distinctive advantage in some setting @ @ experiment @ demonstrate @ benefit of @ pattern @ internal structure @ query generation @ @ author and io @ @ @ right reserved @ 
3861,AIS: An approach to Web information processing based on Web text mining,"Web text mining (WTM) is a technology for information support as one component of the machine system of HWMSE. Concerning the deficiencies of current search engine for retrieval of WWW, improvements are expected. In this paper, a brief review on recent WTM developments was presented at first. Then a technology on augmented information support, AIS, was proposed to cope with "" information explosion "" based on WTM technologies. Finally, AIS is applied to the development of the AIS4XSSC (AIS for Xiangshan Science Conference) system, which is customized for information retrieval and knowledge discovery from XSSWebsite. The practical application demonstrates that AIS is useful to extract information from Web documents and improve the performance of information retrieval.",2010,Xitong Gongcheng Lilun yu Shijian/System Engineering Theory and Practice,4,web text mining @ wtm @ is a technology @ information support a @ component of @ machine system of hwmse @ concerning @ deficiency of current search engine @ retrieval of www improvement @ expected @ in @ @ a brief review on recent wtm development wa presented at first @ @ a technology on augmented information support ai wa proposed to cope @ @ information explosion @ based on wtm technology @ finally ai is applied to @ development of @ ai xssc @ ai @ xiangshan science conference @ system @ is customized @ information retrieval and knowledge discovery @ xsswebsite @ @ practical application demonstrates @ ai is useful to extract information @ web document and improve @ performance of information retrieval @ 
3862,Geographic information retrieval and text mining on Chinese tourism web pages,"The World Wide Web (WWW) offers an enormous wealth of information and data, and assembles a tremendous amount of knowledge. Much of this knowledge, however, comprises either non-structured data or semistructured data. To make use of these unexploited or underexploited resources more efficiently, the management of information and data gathering has become an essential task for research and development. In this paper, the author examines the task of researching a hostel or homestay using the Google search web service as a base search engine. From the search results, mining, retrieving and sorting out location and semantic data were carried out by combining the Chinese Word Segmentation System with text mining technology to find geographic information gleaned from web pages. The results obtained from this particular searching method allowed users to get closer to the answers they sought and achieve greater accuracy, as the results included graphics and textual geographic information. In the future, this method may be suitable for and applicable to various types of queries, analyses, geographic data collection, and in managing spatial knowledge related to different keywords within a document Mining, Web Mining Copyright © 2010, IGI Global.",2010,International Journal of Information Technology and Web Engineering,2,@ world wide web @ www @ offer @ enormous wealth of information and data and assembles a tremendous amount of knowledge @ much of @ knowledge however comprises either non-structured data @ semistructured data @ to make use of @ unexploited @ underexploited resource more efficiently @ management of information and data gathering ha become @ essential task @ research and development @ in @ @ @ author examines @ task of researching a hostel @ homestay @ @ google search web service a a base search engine @ @ @ search @ mining retrieving and sorting @ location and semantic data @ carried @ by combining @ chinese word segmentation system @ text mining technology to find geographic information gleaned @ web page @ @ @ obtained @ @ particular searching method allowed user to get closer to @ answer @ sought and achieve greater accuracy a @ @ included graphic and textual geographic information @ in @ future @ method may @ suitable @ and applicable to various type of query analysis geographic data collection and in managing spatial knowledge related to different keywords within a document mining web mining @ igi global @ 
3863,Sentiment analysis and subjectivity,"Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events, and their properties. Opinions are usually subjective expressions that describe people’s sentiments, appraisals, or feelings toward entities, events, and their properties. The concept of opinion is very broad. In this chapter, we only focus on opinion expressions that convey people’s positive or negative sentiments. Much of the existing research on textual information processing has been focused on themining and retrieval of factual information, e.g., information retrieval (IR), Web search, text classification, text clustering, and many other text mining and natural language processing tasks. Littleworkhadbeendone on the processing of opinions until only recently. Yet, opinions are so important that whenever we need to make a decision we want to hear others’ opinions. This is not only true for individuals but also true for organizations. © 2010 by Taylor & Francis Group, LLC.",2010,"Handbook of Natural Language Processing, Second Edition",870,textual information in @ world @ @ broadly categorized @ @ main type @ fact and opinion @ fact @ objective expression @ entity event and @ property @ opinion @ usually subjective expression @ describe people s sentiment appraisal @ feeling toward entity event and @ property @ @ concept of opinion is @ broad @ in @ chapter @ only focus on opinion expression @ convey people s positive @ negative sentiment @ much of @ existing research on textual information processing ha @ focused on themining and retrieval of factual information e @ g @ information retrieval @ ir @ web search text classification text clustering and many @ text mining and natural language processing task @ littleworkhadbeendone on @ processing of opinion @ only recently @ yet opinion @ @ important @ whenever @ need to make a decision @ want to hear others opinion @ @ is not only true @ individual @ @ true @ organization @ by taylor francis group llc @ 
3864,CRCTOL: A semantic-based domain ontology learning system,"Domain ontologies play an important role in supporting knowledge-based applications in the Semantic Web. To facilitate the building of ontologies, text mining techniques have been used to perform ontology learning from texts. However, traditional systems employ shallow natural language processing techniques and focus only on concept and taxonomic relation extraction. In this paper we present a system, known as Concept-Relation-Concept Tuple-based Ontology Learning (CRCTOL), for mining ontologies automatically from domain-specific documents. Specifically, CRCTOL adopts a full text parsing technique and employs a combination of statistical and lexico-syntactic methods, including a statistical algorithm that extracts key concepts from a document collection, a word sense disambiguation algorithm that disambiguates words in the key concepts, a rule-based algorithm that extracts relations between the key concepts, and a modified generalized association rule mining algorithm that prunes unimportant relations for ontology learning. As a result, the ontologies learned by CRCTOL are more concise and contain a richer semantics in terms of the range and number of semantic relations compared with alternative systems. We present two case studies where CRCTOL is used to build a terrorism domain ontology and a sport event domain ontology. At the component level, quantitative evaluation by comparing with Text-To-Onto and its successor Text2Onto has shown that CRCTOL is able to extract concepts and semantic relations with a significantly higher level of accuracy. At the ontology level, the quality of the learned ontologies is evaluated by either employing a set of quantitative and qualitative methods including analyzing the graph structural property, comparison to WordNet, and expert rating, or directly comparing with a human-edited bench-mark ontology, demonstrating the high quality of the ontologies learned.",2010,Journal of the American Society for Information Science and Technology,83,domain ontology play @ important role in supporting knowledge-based application in @ semantic web @ to facilitate @ building of ontology text mining technique @ @ used to perform ontology learning @ text @ however traditional system employ shallow natural language processing technique and focus only on concept and taxonomic relation extraction @ in @ @ @ @ a system known a concept-relation-concept tuple-based ontology learning @ crctol @ @ mining ontology automatically @ domain-specific document @ specifically crctol adopts a full text parsing technique and employ a combination of statistical and lexico-syntactic method including a statistical algorithm @ extract key concept @ a document collection a word sense disambiguation algorithm @ disambiguates word in @ key concept a rule-based algorithm @ extract relation @ @ key concept and a modified generalized association rule mining algorithm @ prune unimportant relation @ ontology learning @ a a @ @ ontology learned by crctol @ more concise and contain a richer semantics in term of @ range and number of semantic relation compared @ alternative system @ @ @ @ case study @ crctol is used to build a terrorism domain ontology and a sport event domain ontology @ at @ component level quantitative evaluation by comparing @ text-to-onto and @ successor text onto ha @ @ crctol is able to extract concept and semantic relation @ a significantly higher level of accuracy @ at @ ontology level @ quality of @ learned ontology is evaluated by either employing a set of quantitative and qualitative method including analyzing @ graph structural property comparison to wordnet and expert rating @ directly comparing @ a human-edited bench-mark ontology demonstrating @ high quality of @ ontology learned @ 
3869,High-order concept associations mining and inferential language modeling for online review spam detection,"Despite many incidents about fake online consumer reviews have been reported, very few studies have been conducted to date to examine the trustworthiness of online consumer reviews. One of the reasons is the lack of an effective computational method to separate the untruthful reviews (i.e., spam) from the legitimate ones (i.e., ham) given the fact that prominent spam features are often missing in online reviews. The main contribution of our research work is the development of a novel review spam detection method which is underpinned by an unsupervised inferential language modeling framework. Another contribution of this work is the development of a high-order concept association mining method which provides the essential term association knowledge to bootstrap the performance for untruthful review detection. Our experimental results confirm that the proposed inferential language model equipped with high-order concept association knowledge is effective in untruthful review detection when compared with other baseline methods. © 2010 IEEE.",2010,"Proceedings - IEEE International Conference on Data Mining, ICDM",11,despite many incident @ fake online consumer review @ @ reported @ @ study @ @ conducted to date to examine @ trustworthiness of online consumer review @ @ of @ reason is @ lack of @ effective computational method to separate @ untruthful review @ i @ e @ spam @ @ @ legitimate @ @ i @ e @ ham @ given @ fact @ prominent spam feature @ often missing in online review @ @ main contribution of @ research work is @ development of a novel review spam detection method @ is underpinned by @ unsupervised inferential language modeling framework @ another contribution of @ work is @ development of a high-order concept association mining method @ provides @ essential term association knowledge to bootstrap @ performance @ untruthful review detection @ @ experimental @ confirm @ @ proposed inferential language model equipped @ high-order concept association knowledge is effective in untruthful review detection @ compared @ @ baseline method @ @ @ 
3870,Inferential language modeling for selective Web search personalization and contextualization,"Personalized Web search systems have been explored to alleviate the problem of information overload by keeping track of a user's specific information retrieval (IR) preferences, and then pushing information to the user according to their preferences maintained in a user profile. Nevertheless, personalization and contextualization is always associated with a computational cost. Therefore, it is more advantageous for a personalized Web search system to evaluate the necessity of personalization for a query before invoking the personalization mechanism. Unfortunately, most of the existing personalized Web search approaches only blindly personalize users' queries without considering the characteristic of the queries or the searchers who issue those queries. The main contributions of our research work presented in this paper are two fold. First, a novel selective Web search personalization and contextualization method is developed to enhance the effectiveness of personalized Web search. Second, an inferential language model which can take into account the semantic and contextual information associated with a Web search scenario is developed to enhance the selective personalization and contextualization process. The results of our initial experiment show that the proposed selective personalization and contextualization method underpinned by inferential language modeling significantly outperforms a baseline method developed based on syntactic click entropy. To the best of our knowledge, this is the first inferential language modeling approach that has been successfully applied to Web search personalization and contextualization. © 2010 IEEE.",2010,"ICACTE 2010 - 2010 3rd International Conference on Advanced Computer Theory and Engineering, Proceedings",0,personalized web search system @ @ explored to alleviate @ problem of information overload by keeping track of a user @ s specific information retrieval @ ir @ preference and @ pushing information to @ user according to @ preference maintained in a user profile @ nevertheless personalization and contextualization is always associated @ a computational cost @ therefore @ is more advantageous @ a personalized web search system to evaluate @ necessity of personalization @ a query @ invoking @ personalization mechanism @ unfortunately @ of @ existing personalized web search approach only blindly personalize user @ query without considering @ characteristic of @ query @ @ searcher @ issue @ query @ @ main contribution of @ research work presented in @ @ @ @ fold @ first a novel selective web search personalization and contextualization method is developed to enhance @ effectiveness of personalized web search @ second @ inferential language model @ @ take @ account @ semantic and contextual information associated @ a web search scenario is developed to enhance @ selective personalization and contextualization process @ @ @ of @ initial experiment @ @ @ proposed selective personalization and contextualization method underpinned by inferential language modeling significantly outperforms a baseline method developed based on syntactic click entropy @ to @ best of @ knowledge @ is @ first inferential language modeling approach @ ha @ successfully applied to web search personalization and contextualization @ @ @ 
3871,Approximate retrieval of postal addresses,"In this article it is introduced FuMaS (Fuzzy Matching System), a system that allows efficient recovery of addresses through noisy queries. The fuzzy retrieval of this information has countless applications, from finding / clean duplicates in databases (voter registration, find nests of mail fraud, etc.) to correct the input from users on systems such as street directories or any type of form where an address has to be filled. The results of these experiments show that FuMaS is a very useful tool for addresses retrieval in noisy queries, being able to resolve about 85% of the addresses with errors which were introduced into the system; a 15% higher efficiency than any other similar system tested.",2010,CEUR Workshop Proceedings,0,in @ article @ is introduced fumas @ fuzzy matching system @ a system @ allows efficient recovery of address @ noisy query @ @ fuzzy retrieval of @ information ha countless application @ finding clean duplicate in database @ voter registration find nest of mail fraud etc @ @ to correct @ input @ user on system @ a street directory @ @ type of form @ @ address ha to @ filled @ @ @ of @ experiment @ @ fumas is a @ useful tool @ address retrieval in noisy query @ able to resolve @ of @ address @ error @ @ introduced @ @ system @ a higher efficiency @ @ @ similar system tested @ 
3872,A new approach for multi-document update summarization,"Fast changing knowledge on the Internet can be acquired more efficiently with the help of automatic document summarization and updating techniques. This paper describes a novel approach for multi-document update summarization. The best summary is defined to be the one which has the minimum information distance to the entire document set. The best update summary has the minimum conditional information distance to a document cluster given that a prior document cluster has already been read. Experiments on the DUC/TAC 2007 to 2009 datasets (http://duc.nist.gov/, http://www.nist.gov/tac/) have proved that our method closely correlates with the human summaries and outperforms other programs such as LexRank in many categories under the ROUGE evaluation criterion. © 2010 Springer Science+Business Media, LLC &Science Press, China.",2010,Journal of Computer Science and Technology,7,fast changing knowledge on @ internet @ @ acquired more efficiently @ @ help of automatic document summarization and updating technique @ @ @ describes a novel approach @ multi-document update summarization @ @ best summary is defined to @ @ @ @ ha @ minimum information distance to @ entire document set @ @ best update summary ha @ minimum conditional information distance to a document cluster given @ a prior document cluster ha already @ read @ experiment on @ duc tac to datasets @ http @ duc @ nist @ gov http @ www @ nist @ gov tac @ @ proved @ @ method closely correlate @ @ human summary and outperforms @ program @ a lexrank in many category @ @ rouge evaluation criterion @ @ science @ medium llc science @ china @ 
3892,Framework for evolutionary modelling in text mining,Special framework for Evolutionary Modelling is presented. It is oriented on experimental investigations of schemes and properties of genetic algorithms. With the help of this framework Evolutionary Approach to Conceptual Graphs Clustering is investigated. Some experimental results of clustering scientific papers abstracts are presented.,2009,CEUR Workshop Proceedings,0,special framework @ evolutionary modelling is presented @ @ is oriented on experimental investigation of scheme and property of genetic algorithm @ @ @ help of @ framework evolutionary approach to conceptual graph clustering is investigated @ some experimental @ of clustering scientific @ abstract @ presented @ 
3893,GoWeb: A semantic search engine for the life science web,"Background: Current search engines are keyword-based. Semantic technologies promise a next generation of semantic search engines, which will be able to answer questions. Current approaches either apply natural language processing to unstructured text or they assume the existence of structured statements over which they can reason. Results: Here, we introduce a third approach, GoWeb, which combines classical keyword-based Web search with text-mining and ontologies to navigate large results sets and facilitate question answering. We evaluate GoWeb on three benchmarks of questions on genes and functions, on symptoms and diseases, and on proteins and diseases. The first benchmark is based on the BioCreAtivE 1 Task 2 and links 457 gene names with 1352 functions. GoWeb finds 58% of the functional GeneOntology annotations. The second benchmark is based on 26 case reports and links symptoms with diseases. GoWeb achieves 77% success rate improving an existing approach by nearly 20%. The third benchmark is based on 28 questions in the TREC genomics challenge and links proteins to diseases. GoWeb achieves a success rate of 79%. Conclusion: GoWeb's combination of classical Web search with textmining and ontologies is a first step towards answering questions in the biomedical domain. GoWeb is online at: www.gopubmed.org/goweb.",2009,CEUR Workshop Proceedings,0,background @ current search engine @ keyword-based @ semantic technology promise a next generation of semantic search engine @ @ @ able to answer question @ current approach either apply natural language processing to unstructured text @ @ assume @ existence of structured statement @ @ @ @ reason @ @ @ @ @ introduce a third approach goweb @ combine classical keyword-based web search @ text-mining and ontology to navigate @ @ set and facilitate question answering @ @ evaluate goweb on three benchmark of question on gene and function on symptom and disease and on protein and disease @ @ first benchmark is based on @ biocreative task and link gene name @ function @ goweb find of @ functional geneontology annotation @ @ second benchmark is based on case report and link symptom @ disease @ goweb achieves success rate improving @ existing approach by nearly @ @ third benchmark is based on question in @ trec genomics challenge and link protein to disease @ goweb achieves a success rate of @ conclusion @ goweb @ s combination of classical web search @ textmining and ontology is a first step towards answering question in @ biomedical domain @ goweb is online at @ www @ gopubmed @ org goweb @ 
3895,Research on Chinese proper nouns recognition based on pattern matching,"Chinese proper nouns recognition (CPNR) plays an important role in the fields of Information Extraction, Question Answering and Text Mining. In this paper we proposed a novel pattern-matching-based method to recognize proper nouns (PNs), which includes person names, location names and organization names, and mainly conducted the following studies: (1) constructing the PNs inner-pattern set; (2) acquiring the PNs outer-patterns by clustering and evaluating automatically; (3) resolving the conflicts of the PNs recognition by computing PNs reliability; (4) conducting the experiment on 1.2M word corpus that are chosen from People Daily corpora. The experimental results, whose recall and precision are 83.4% and 80.1% respectively, indicate that the proposed method is feasible and effective. 1553-9105/ Copyright © 2009 Binary Information Press.",2009,Journal of Computational Information Systems,3,chinese proper noun recognition @ cpnr @ play @ important role in @ field of information extraction question answering and text mining @ in @ @ @ proposed a novel pattern-matching-based method to recognize proper noun @ pns @ @ includes person name location name and organization name and mainly conducted @ following study @ @ @ constructing @ pns inner-pattern set @ @ @ acquiring @ pns outer-patterns by clustering and evaluating automatically @ @ @ resolving @ conflict of @ pns recognition by computing pns reliability @ @ @ conducting @ experiment on @ @ word corpus @ @ chosen @ people daily corpus @ @ experimental @ whose recall and precision @ @ and @ respectively indicate @ @ proposed method is feasible and effective @ @ binary information @ @ 
3899,A ConceptLink graph for text structure mining,"Most text mining methods are based on representing documents using a vector space model, commonly known as a bag of word model, where each document is modeled as a linear vector representing the occurrence of independent words in the text corpus. It is well known that using this vector-based representation, important information, such as semantic relationship among concepts, is lost. This paper proposes a novel text representation model called ConceptLink graph. The ConceptLink graph does not only represent the content of the document, but also captures some of its underlying semantic structure in terms of the relationships among concepts. The ConceptLink graph is constructed in two main stages. First, we find a set of concepts by clustering conceptually related terms using the self-organizing map method. Secondly, by mapping each document's content to concept, we generate a graph of concepts based on the occurrences of concepts using a singular value decomposition technique. The ConceptLink graph will overcome the keyword independence limitation in the vector space model to take advantage of the implicit concept relationships exhibit in all natural language texts. As an information-rich text representation model, the ConceptLink graph will advance text mining technology beyond feature-based to structure-based knowledge discovery. We will illustrate the ConceptLink graph method using samples generated from benchmark text mining dataset. Copyright © 2009, Australian Computer Society, Inc.",2009,Conferences in Research and Practice in Information Technology Series,16,@ text mining method @ based on representing document @ a vector space model commonly known a a bag of word model @ @ document is modeled a a linear vector representing @ occurrence of independent word in @ text corpus @ @ is well known @ @ @ vector-based representation important information @ a semantic relationship among concept is lost @ @ @ proposes a novel text representation model called conceptlink graph @ @ conceptlink graph doe not only represent @ content of @ document @ @ capture some of @ underlying semantic structure in term of @ relationship among concept @ @ conceptlink graph is constructed in @ main stage @ first @ find a set of concept by clustering conceptually related term @ @ self-organizing map method @ secondly by mapping @ document @ s content to concept @ generate a graph of concept based on @ occurrence of concept @ a singular value decomposition technique @ @ conceptlink graph @ overcome @ keyword independence limitation in @ vector space model to take advantage of @ implicit concept relationship exhibit in @ natural language text @ a @ information-rich text representation model @ conceptlink graph @ advance text mining technology beyond feature-based to structure-based knowledge discovery @ @ @ illustrate @ conceptlink graph method @ sample generated @ benchmark text mining dataset @ @ australian computer society inc @ 
3905,User interface for a geo-temporal search service using DIGMAP components,"This demo presents a user interface for a Geo-Temporal search service built in the sequence of DIGMAP project. DIGMAP was a co-funded European Union project on old digitized maps and deals with resources rich in geographic and temporal information. This search interface followed a mashup approach using existing DIGMAP components: a metadata repository, a text mining tool, a Gazetteer, and a service to generate geographic contextual thumbnails. Google Maps API is used to provide a friendly and interactive user interface. This demo will present the resulting geo-temporal search engine functionalities, whose interface uses WEB 2.0 capabilities to provide contextualization in time and space and text clustering. © 2009 Springer.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ demo @ a user interface @ a geo-temporal search service built in @ sequence of digmap project @ digmap wa a co-funded european union project on old digitized map and deal @ resource rich in geographic and temporal information @ @ search interface followed a mashup approach @ existing digmap component @ a metadata repository a text mining tool a gazetteer and a service to generate geographic contextual thumbnail @ google map api is used to provide a friendly and interactive user interface @ @ demo @ @ @ resulting geo-temporal search engine functionality whose interface us web @ capability to provide contextualization in time and space and text clustering @ @ @ 
3911,Improving full text search with text mining tools,,2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,
3912,Biomedical text mining applied to document retrieval and semantic indexing,"In Biomedical research, the ability to retrieve the adequate information from the ever growing literature is an extremely important asset. This work provides an enhanced and general purpose approach to the process of document retrieval that enables the filtering of PubMed query results. The system is based on semantic indexing providing, for each set of retrieved documents, a network that links documents and relevant terms obtained by the annotation of biological entities (e.g. genes or proteins). This network provides distinct user perspectives and allows navigation over documents with similar terms and is also used to assess document relevance. A network learning procedure, based on previous work from e-mail spam filtering, is proposed, receiving as input a training set of manually classified documents. © 2009 Springer Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,in biomedical research @ ability to retrieve @ adequate information @ @ ever growing literature is @ extremely important asset @ @ work provides @ enhanced and general purpose approach to @ process of document retrieval @ enables @ filtering of pubmed query @ @ @ system is based on semantic indexing providing @ @ set of retrieved document a network @ link document and relevant term obtained by @ annotation of biological entity @ e @ g @ gene @ protein @ @ @ network provides distinct user perspective and allows navigation @ document @ similar term and is @ used to ass document relevance @ a network learning procedure based on previous work @ e-mail spam filtering is proposed receiving a input a training set of manually classified document @ @ @ @ @ 
3915,Syntactic extraction approach to processing local document collections,"Techniques of processing databases like free text searching, or proximity search are one of the key factors that influence efficiency of query answering. Since most users prefer querying systems in natural language, a correct answer formulation based on the electronic document content seems a real challenge. Processing queries in multilingual environment usually impedes the system responsiveness even more. This paper proposes an approach of overcoming these obstacles by implementation of syntactic information extraction. Some evaluation methodologies commonly used by TREC, NTCIR, SIGIR etc are studied in order to suggest that it is not only a system architecture itself, a translation model or the document format, but also other factors that determine the system performance. The shallow technique of the syntactic information extraction used appears to be a robust of the system described. In this light, it is possible to achieve comparable results when processing monolingual and cross-lingual collections. © 2009 Springer-Verlag Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,technique of processing database like free text searching @ proximity search @ @ of @ key factor @ influence efficiency of query answering @ since @ user prefer querying system in natural language a correct answer formulation based on @ electronic document content seems a real challenge @ processing query in multilingual environment usually impedes @ system responsiveness even more @ @ @ proposes @ approach of overcoming @ obstacle by implementation of syntactic information extraction @ some evaluation methodology commonly used by trec ntcir sigir etc @ studied in order to suggest @ @ is not only a system architecture @ a translation model @ @ document format @ @ @ factor @ determine @ system performance @ @ shallow technique of @ syntactic information extraction used appears to @ a robust of @ system described @ in @ light @ is possible to achieve comparable @ @ processing monolingual and cross-lingual collection @ springer-verlag @ @ @ 
3917,Extracting conceptual graphs from Japanese documents for software requirements modeling,"A requirements analysis step plays a significant role on the development of information systems, and in this step we produce various kinds of abstract models of the systems (called requirements models) according to the adopted development processes, e.g. class diagrams in the case of adopting object-oriented development. However, constructing these models of sufficient quality requires highest intellectual tasks and skills of human requirements analysts. In this paper, we develop a computerized tool to extract from a set of Japanese text documents conceptual information, called conceptual graph, which can be used as intermediate representation to generate software requirements models. More concretely, by applying the variation of text-mining techniques that we have developed, we extract significant words from text documents referring to the same problem domain and identify relevant relationships among them. The extracted words can be considered as concepts and they are constituents of a conceptual graph in the domain. This constructed graph can be used for generating requirements models, e.g. object oriented models, feature model, and even as a domain ontology that can be utilized during requirements analysis activities. We have made experimental analyses of our tool. This paper also includes the discussion on how the extracted conceptual graph can act as an object-oriented model, a feature model and a domain ontology, in order to show its wide applicability. Copyright © 2009, Australian Computer Society, Inc.",2009,Conferences in Research and Practice in Information Technology Series,12,a requirement analysis step play a significant role on @ development of information system and in @ step @ produce various kind of abstract model of @ system @ called requirement model @ according to @ adopted development process e @ g @ class diagram in @ case of adopting object-oriented development @ however constructing @ model of sufficient quality requires highest intellectual task and skill of human requirement analyst @ in @ @ @ develop a computerized tool to extract @ a set of japanese text document conceptual information called conceptual graph @ @ @ used a intermediate representation to generate software requirement model @ more concretely by applying @ variation of text-mining technique @ @ @ developed @ extract significant word @ text document referring to @ @ problem domain and identify relevant relationship among @ @ @ extracted word @ @ considered a concept and @ @ constituent of a conceptual graph in @ domain @ @ constructed graph @ @ used @ generating requirement model e @ g @ object oriented model feature model and even a a domain ontology @ @ @ utilized @ requirement analysis activity @ @ @ made experimental analysis of @ tool @ @ @ @ includes @ discussion on @ @ extracted conceptual graph @ act a @ object-oriented model a feature model and a domain ontology in order to @ @ wide applicability @ @ australian computer society inc @ 
3919,Question answering based on semantic graphs,"In this paper we present a question answering system supported by semantic graphs. Aside from providing answers to natural language questions, the system offers explanations for these answers via a visual representation of documents, their associated list of facts described by subject - verb - object triplets, and their summaries. The triplets, automatically extracted from the Penn Treebank parse tree obtained for each sentence in the document collection, can be searched, and we have implemented a question answering system to serve as a natural language interface to this search. The vocabulary of questions is general because it is not limited to a specific domain, however the questions's grammatical structure is restricted to a predetermined template because our system can understand only a limited number of question types. The answers are retrieved from the set of facts, and they are supported by sentences and their corresponding document. The document overview, comprising the semantic representation of the document generated in the form of a semantic graph, the list of facts it contains and its automatically derived summary, offers an explanation to each answer. The extracted triplets are further refined by assigning the corresponding co referenced named entity, by resolving pronominal anaphors, as well as attaching the associated WordNet synset. The semantic graph belonging to the document is developed based on the enhanced triplets while the document summary is automatically generated from the semantic description of the document and the extracted facts.",2009,CEUR Workshop Proceedings,4,in @ @ @ @ a question answering system supported by semantic graph @ aside @ providing answer to natural language question @ system offer explanation @ @ answer via a visual representation of document @ associated list of fact described by subject verb object triplet and @ summary @ @ triplet automatically extracted @ @ penn treebank parse tree obtained @ @ sentence in @ document collection @ @ searched and @ @ implemented a question answering system to serve a a natural language interface to @ search @ @ vocabulary of question is general @ @ is not limited to a specific domain however @ question @ s grammatical structure is restricted to a predetermined template @ @ system @ understand only a limited number of question type @ @ answer @ retrieved @ @ set of fact and @ @ supported by sentence and @ corresponding document @ @ document overview comprising @ semantic representation of @ document generated in @ form of a semantic graph @ list of fact @ contains and @ automatically derived summary offer @ explanation to @ answer @ @ extracted triplet @ @ refined by assigning @ corresponding co referenced named entity by resolving pronominal anaphor a well a attaching @ associated wordnet synset @ @ semantic graph belonging to @ document is developed based on @ enhanced triplet @ @ document summary is automatically generated @ @ semantic description of @ document and @ extracted fact @ 
3920,A relation mining and visualization framework for automated text summarization,"In this paper, we present a relation mining and visualization framework to identify important semi-structured information components using semantic and linguistic analysis of text documents. The novelty of the paper lies in identifying key snippet from text to validate the interaction between a pair of entities. The extracted information components are exploited to generate semantic network which provides distinct user perspectives and allows navigation over documents with similar information components. The efficacy of the proposed framework is established through experiments carried out on biomedical text documents extracted through PubMed search engine. © 2009 Springer-Verlag Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,in @ @ @ @ a relation mining and visualization framework to identify important semi-structured information component @ semantic and linguistic analysis of text document @ @ novelty of @ @ lie in identifying key snippet @ text to validate @ interaction @ a pair of entity @ @ extracted information component @ exploited to generate semantic network @ provides distinct user perspective and allows navigation @ document @ similar information component @ @ efficacy of @ proposed framework is established @ experiment carried @ on biomedical text document extracted @ pubmed search engine @ springer-verlag @ @ @ 
3921,Feature and opinion mining for customer review summarization,"In this paper, we present an opinion mining system to identify product features and opinions from review documents. The features and opinions are extracted using semantic and linguistic analysis of text documents. The polarity of opinion sentences is established using polarity scores of the opinion words through Senti-WordNet to generate a feature-based summary of review documents. The system is also integrated with a visualization module to present feature-based summary of review documents in a comprehendible way. © 2009 Springer-Verlag Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),45,in @ @ @ @ @ opinion mining system to identify product feature and opinion @ review document @ @ feature and opinion @ extracted @ semantic and linguistic analysis of text document @ @ polarity of opinion sentence is established @ polarity score of @ opinion word @ senti-wordnet to generate a feature-based summary of review document @ @ system is @ integrated @ a visualization module to @ feature-based summary of review document in a comprehendible way @ springer-verlag @ @ @ 
3922,Sales intelligence using web mining,"This paper presents a knowledge extraction system for providing sales intelligence based on information downloaded from the WWW. The information is first located and downloaded from relevant companies' websites and then machine learning is used to find these web pages that contain useful information where useful is defined as containing news about orders for specific products. Several machine learning algorithms were tested from which k-nearest neighbour, support vector machines, multi-layer perceptron and C4.5 decision tree produced best results in one or both experiments however k-nearest neighbour and support vector machines proved to be most robust which is a highly desired characteristic in the particular application. K-nearest neighbour slightly outperformed the support vector machines in both experiments which contradicts the results reported previously in the literature. © 2009 Springer Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ @ a knowledge extraction system @ providing sale intelligence based on information downloaded @ @ www @ @ information is first located and downloaded @ relevant company @ website and @ machine learning is used to find @ web page @ contain useful information @ useful is defined a containing news @ order @ specific product @ several machine learning algorithm @ tested @ @ k-nearest neighbour support vector machine multi-layer perceptron and c @ decision tree produced best @ in @ @ @ experiment however k-nearest neighbour and support vector machine proved to @ @ robust @ is a highly desired characteristic in @ particular application @ k-nearest neighbour slightly outperformed @ support vector machine in @ experiment @ contradicts @ @ reported @ in @ literature @ @ @ @ @ 
3934,Identification of chemical entities in patent documents,"Biomedical literature is an important source of information for chemical compounds. However, different representations and nomenclatures for chemical entities exist, which makes the reference of chemical entities ambiguous. Many systems already exist for gene and protein entity recognition, however very few exist for chemical entities. The main reason for this is the lack of corpus to train named entity recognition systems and perform evaluation. In this paper we present a chemical entity recognizer that uses a machine learning approach based on conditional random fields (CRF) and compare the performance with dictionary-based approaches using several terminological resources. For the training and evaluation, a gold standard of manually curated patent documents was used. While the dictionary-based systems perform well in partial identification of chemical entities, the machine learning approach performs better (10% increase in F-score in comparison to the best dictionary-based system) when identifying complete entities. © 2009 Springer Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),16,biomedical literature is @ important source of information @ chemical compound @ however different representation and nomenclature @ chemical entity exist @ make @ reference of chemical entity ambiguous @ many system already exist @ gene and protein entity recognition however @ @ exist @ chemical entity @ @ main reason @ @ is @ lack of corpus to train named entity recognition system and perform evaluation @ in @ @ @ @ a chemical entity recognizer @ us a machine learning approach based on conditional random field @ crf @ and compare @ performance @ dictionary-based approach @ several terminological resource @ @ @ training and evaluation a gold standard of manually curated patent document wa used @ @ @ dictionary-based system perform well in partial identification of chemical entity @ machine learning approach performs better @ increase in f-score in comparison to @ best dictionary-based system @ @ identifying complete entity @ @ @ @ @ 
3935,Multi-document summarization by information distance,Fast changing knowledge on the Internet can be acquired more efficiently with the help of automatic document summarization and updating techniques. This paper described a novel approach for multi-document update summarization. The best summary is defined to be the one which has the minimum information distance to the entire document set. The best update summary has the minimum conditional information distance to a document cluster given that a prior document cluster has already been read. Experiments on the DUC 2007 dataset1 and the TAC 2008 dataset2 have proved that our method closely correlates with the human summaries and outperforms other programs such as LexRank in many categories under the ROUGE evaluation criterion. © 2009 IEEE.,2009,"Proceedings - IEEE International Conference on Data Mining, ICDM",10,fast changing knowledge on @ internet @ @ acquired more efficiently @ @ help of automatic document summarization and updating technique @ @ @ described a novel approach @ multi-document update summarization @ @ best summary is defined to @ @ @ @ ha @ minimum information distance to @ entire document set @ @ best update summary ha @ minimum conditional information distance to a document cluster given @ a prior document cluster ha already @ read @ experiment on @ duc dataset and @ tac dataset @ proved @ @ method closely correlate @ @ human summary and outperforms @ program @ a lexrank in many category @ @ rouge evaluation criterion @ @ @ 
3948,New semantic similarity based model for text clustering using extended gloss overlaps,"Most text clustering techniques are based on words and/or phrases weights in the text. Such representation is often unsatisfactory because it ignores the relationships between terms, and considers them as independent features. In this paper, a new semantic similarity based model (SSBM) is proposed. The semantic similarity based model computes semantic similarities by utilizing WordNet as an ontology. The proposed model captures the semantic similarities between documents that contain semantically similar terms but unnecessarily syntactically identical. The semantic similarity based model assigns a new weight to document terms reflecting the semantic relationships between terms that co-occur literally in the document. Our model in conjunction with the extended gloss overlaps measure and the adapted Lesk algorithm solves ambiguity, synonymy problems that are not detected using traditional term frequency based text mining techniques. The proposed model is evaluated on the Reuters-21578 and the 20-Newsgroups text collections datasets. The performance is assessed in terms of the Fmeasure, Purity and Entropy quality measures. The obtained results show promising performance improvements compared to the traditional term based vector space model (VSM) as well as other existing methods that include semantic similarity measures in text clustering. © 2009 Springer Berlin Heidelberg.",2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13,@ text clustering technique @ based on word and @ phrase weight in @ text @ @ representation is often unsatisfactory @ @ ignores @ relationship @ term and considers @ a independent feature @ in @ @ a @ semantic similarity based model @ ssbm @ is proposed @ @ semantic similarity based model computes semantic similarity by utilizing wordnet a @ ontology @ @ proposed model capture @ semantic similarity @ document @ contain semantically similar term @ unnecessarily syntactically identical @ @ semantic similarity based model assigns a @ weight to document term reflecting @ semantic relationship @ term @ co-occur literally in @ document @ @ model in conjunction @ @ extended gloss overlap measure and @ adapted lesk algorithm solves ambiguity synonymy problem @ @ not detected @ traditional term frequency based text mining technique @ @ proposed model is evaluated on @ reuters and @ newsgroups text collection datasets @ @ performance is assessed in term of @ fmeasure purity and entropy quality measure @ @ obtained @ @ promising performance improvement compared to @ traditional term based vector space model @ vsm @ a well a @ existing method @ include semantic similarity measure in text clustering @ @ @ @ @ 
3949,Semantic graphs derived from triplets with application in document summarization,"Information nowadays has become more and more accessible, so much as to give birth to an information overload issue. Yet important decisions have to be made, depending on the available information. As it is impossible to read all the relevant content that helps one stay informed, a possible solution would be condensing data and obtaining the kernel of a text by automatically summarizing it. We present an approach to analyzing text and retrieving valuable information in the form of a semantic graph based on subject-verb-object triplets extracted from sentences. Once triplets have been generated, we apply several techniques in order to obtain the semantic graph of the document: co-reference and anaphora resolution of named entities and semantic normalization of triplets. Finally, we describe the automatic document summarization process starting from the semantic representation of the text. The experimental evaluation carried out step by step on several Reuters newswire articles shows a comparable performance of the proposed approach with other existing methodologies. For the assessment of the document summaries we utilize an automatic summarization evaluation package, so as to show a ranking of various summarizers.",2009,Informatica (Ljubljana),21,information nowadays ha become more and more accessible @ much a to give birth to @ information overload issue @ yet important decision @ to @ made depending on @ available information @ a @ is impossible to read @ @ relevant content @ help @ stay informed a possible solution would @ condensing data and obtaining @ kernel of a text by automatically summarizing @ @ @ @ @ approach to analyzing text and retrieving valuable information in @ form of a semantic graph based on subject-verb-object triplet extracted @ sentence @ @ triplet @ @ generated @ apply several technique in order to obtain @ semantic graph of @ document @ co-reference and anaphora resolution of named entity and semantic normalization of triplet @ finally @ describe @ automatic document summarization process starting @ @ semantic representation of @ text @ @ experimental evaluation carried @ step by step on several reuters newswire article @ a comparable performance of @ proposed approach @ @ existing methodology @ @ @ assessment of @ document summary @ utilize @ automatic summarization evaluation package @ a to @ a ranking of various summarizers @ 
3961,Discovering implicit intention-level knowledge from natural-language texts,"In this paper, we propose a new approach to automatic discovery of implicit rhetorical information from texts based on evolutionary computation methods. In order to guide the search for rhetorical connections from natural-language texts, the model uses previously obtained training information which involves semantic and structural criteria. The main features of the model and new designed operators and evaluation functions are discussed, and the different experiments assessing the robustness and accuracy of the approach are described. Experimental results show the promise of evolutionary methods for rhetorical role discovery. © 2009 Elsevier B.V. All rights reserved.",2009,Knowledge-Based Systems,22,in @ @ @ propose a @ approach to automatic discovery of implicit rhetorical information @ text based on evolutionary computation method @ in order to guide @ search @ rhetorical connection @ natural-language text @ model us @ obtained training information @ involves semantic and structural criterion @ @ main feature of @ model and @ designed operator and evaluation function @ discussed and @ different experiment assessing @ robustness and accuracy of @ approach @ described @ experimental @ @ @ promise of evolutionary method @ rhetorical role discovery @ @ b @ v @ @ right reserved @ 
3968,Mining meaning from Wikipedia,"Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced. © 2009 Elsevier Ltd. All rights reserved.",2009,International Journal of Human Computer Studies,250,wikipedia is a goldmine of information @ not @ @ @ many reader @ @ @ @ growing community of researcher @ recognize @ a a resource of exceptional scale and utility @ @ represents a vast investment of manual effort and judgment @ a huge constantly evolving tapestry of concept and relation @ is @ applied to a host of task @ @ article provides a comprehensive description of @ work @ @ focus on research @ extract and make use of @ concept relation fact and description found in wikipedia and organizes @ work @ four broad category @ applying wikipedia to natural language processing @ @ @ to facilitate information retrieval and information extraction @ and a a resource @ ontology building @ @ article address @ wikipedia is @ used a is @ @ is @ improved and adapted and @ @ is @ combined @ @ structure to create entirely @ resource @ @ identify @ research group and individual involved and @ @ work ha developed in @ last @ year @ @ provide a comprehensive list of @ open-source software @ @ produced @ @ ltd @ @ right reserved @ 
3978,Searching for definitional answers on the web using surface patterns,A novel question-answering system employs query rewriting techniques to increase the probability of extracting nuggets from various Web snippets by matching surface patterns. Experimental results show the approach's promise versus existing techniques. © 2009 IEEE.,2009,Computer,1,a novel question-answering system employ query rewriting technique to increase @ probability of extracting nugget @ various web snippet by matching surface pattern @ experimental @ @ @ approach @ s promise versus existing technique @ @ @ 
3979,Exploiting noun phrases and semantic relationships for text document clustering,"Text document clustering plays an important role in providing better document retrieval, document browsing, and text mining. Traditionally, clustering techniques do not consider the semantic relationships between words, such as synonymy and hypernymy. To exploit semantic relationships, ontologies such as WordNet have been used to improve clustering results. However, WordNet-based clustering methods mostly rely on single-term analysis of text; they do not perform any phrase-based analysis. In addition, these methods utilize synonymy to identify concepts and only explore hypernymy to calculate concept frequencies, without considering other semantic relationships such as hyponymy. To address these issues, we combine detection of noun phrases with the use of WordNet as background knowledge to explore better ways of representing documents semantically for clustering. First, based on noun phrases as well as single-term analysis, we exploit different document representation methods to analyze the effectiveness of hypernymy, hyponymy, holonymy, and meronymy. Second, we choose the most effective method and compare it with the WordNet-based clustering method proposed by others. The experimental results show the effectiveness of semantic relationships for clustering are (from highest to lowest): hypernymy, hyponymy, meronymy, and holonymy. Moreover, we found that noun phrase analysis improves the WordNet-based clustering method. © 2009 Elsevier Inc. All rights reserved.",2009,Information Sciences,71,text document clustering play @ important role in providing better document retrieval document browsing and text mining @ traditionally clustering technique @ not consider @ semantic relationship @ word @ a synonymy and hypernymy @ to exploit semantic relationship ontology @ a wordnet @ @ used to improve clustering @ @ however wordnet-based clustering method mostly rely on single-term analysis of text @ @ @ not perform @ phrase-based analysis @ in addition @ method utilize synonymy to identify concept and only explore hypernymy to calculate concept frequency without considering @ semantic relationship @ a hyponymy @ to address @ issue @ combine detection of noun phrase @ @ use of wordnet a background knowledge to explore better way of representing document semantically @ clustering @ first based on noun phrase a well a single-term analysis @ exploit different document representation method to analyze @ effectiveness of hypernymy hyponymy holonymy and meronymy @ second @ choose @ @ effective method and compare @ @ @ wordnet-based clustering method proposed by others @ @ experimental @ @ @ effectiveness of semantic relationship @ clustering @ @ @ highest to lowest @ @ hypernymy hyponymy meronymy and holonymy @ moreover @ found @ noun phrase analysis improves @ wordnet-based clustering method @ @ inc @ @ right reserved @ 
3981,Enhancing learning objects with an ontology-based memory,"The reusability in learning objects has always been a hot issue. However, we believe that current approaches to e-Learning failed to find a satisfying answer to this concern. This paper presents an approach that enables capitalization of existing learning resources by first creating ""content metadata through text mining and natural language processing and second by creating dynamically learning knowledge objects, i.e., active, adaptable, reusable, and independent learning objects. The proposed model also suggests integrating explicitly instructional theories in an on-the-fly composition process of learning objects. Semantic Web technologies are used to satisfy such an objective by creating an ontology-based organizational memory able to act as a knowledge base for multiple training environments. © 2006 IEEE.",2009,IEEE Transactions on Knowledge and Data Engineering,16,@ reusability in learning object ha always @ a hot issue @ however @ believe @ current approach to e-learning failed to find a satisfying answer to @ concern @ @ @ @ @ approach @ enables capitalization of existing learning resource by first creating @ content metadata @ text mining and natural language processing and second by creating dynamically learning knowledge object i @ e @ active adaptable reusable and independent learning object @ @ proposed model @ suggests integrating explicitly instructional theory in @ on-the-fly composition process of learning object @ semantic web technology @ used to satisfy @ @ objective by creating @ ontology-based organizational memory able to act a a knowledge base @ multiple training environment @ @ @ 
3982,Distribution of multi-words in Chinese and English documents,"As a hybrid of N-gram in natural language processing and collocation in statistical linguistics, multi-word is becoming a hot topic in area of text mining and information retrieval. In this paper, a study concerning distribution of multi-words is carried out to explore a theoretical basis for probabilistic term-weighting scheme. Specifically, the Poisson distribution, zero-inflated binomial distribution, and G-distribution are comparatively studied on a task of predicting probabilities of multi-words' occurrences using these distributions, for both technical multi-words and nontechnical multi-words. In addition, a rule-based multi-word extraction algorithm is proposed to extract multi-words from texts based on words' occurring patterns and syntactical structures. Our experimental results demonstrate that G-distribution has the best capability to predict probabilities of frequency of multi-words' occurrence and the Poisson distribution is comparable to zero-inflated binomial distribution in estimation of multi-word distribution. The outcome of this study validates that burstiness is a universal phenomenon in linguistic count data, which is applicable not only for individual content words but also for multi-words. © 2009 World Scientific Publishing Company.",2009,International Journal of Information Technology and Decision Making,6,a a hybrid of n-gram in natural language processing and collocation in statistical linguistics multi-word is becoming a hot topic in area of text mining and information retrieval @ in @ @ a study concerning distribution of multi-words is carried @ to explore a theoretical basis @ probabilistic term-weighting scheme @ specifically @ poisson distribution zero-inflated binomial distribution and g-distribution @ comparatively studied on a task of predicting probability of multi-words @ occurrence @ @ distribution @ @ technical multi-words and nontechnical multi-words @ in addition a rule-based multi-word extraction algorithm is proposed to extract multi-words @ text based on word @ occurring pattern and syntactical structure @ @ experimental @ demonstrate @ g-distribution ha @ best capability to predict probability of frequency of multi-words @ occurrence and @ poisson distribution is comparable to zero-inflated binomial distribution in estimation of multi-word distribution @ @ outcome of @ study validates @ burstiness is a universal phenomenon in linguistic count data @ is applicable not only @ individual content word @ @ @ multi-words @ world scientific publishing company @ 
3987,"Computational linguistics for metadata building (CLiMB): Using text mining for the automatic identification, categorization, and disambiguation of subject terms for image metadata","In this paper, we present a system using computational linguistic techniques to extract metadata for image access. We discuss the implementation, functionality and evaluation of an image catalogers' toolkit, developed in the Computational Linguistics for Metadata Building (CLiMB) research project. We have tested components of the system, including phrase finding for the art and architecture domain, functional semantic labeling using machine learning, and disambiguation of terms in domain-specific text vis a vis a rich thesaurus of subject terms, geographic and artist names. We present specific results on disambiguation techniques and on the nature of the ambiguity problem given the thesaurus, resources, and domain-specific text resource, with a comparison of domain-general resources and text. Our primary user group for evaluation has been the cataloger expert with specific expertise in the fields of painting, sculpture, and vernacular and landscape architecture. © 2008 Springer Science+Business Media, LLC.",2009,Multimedia Tools and Applications,11,in @ @ @ @ a system @ computational linguistic technique to extract metadata @ image access @ @ discus @ implementation functionality and evaluation of @ image cataloger @ toolkit developed in @ computational linguistics @ metadata building @ climb @ research project @ @ @ tested component of @ system including phrase finding @ @ art and architecture domain functional semantic labeling @ machine learning and disambiguation of term in domain-specific text vi a vi a rich thesaurus of subject term geographic and artist name @ @ @ specific @ on disambiguation technique and on @ nature of @ ambiguity problem given @ thesaurus resource and domain-specific text resource @ a comparison of domain-general resource and text @ @ primary user group @ evaluation ha @ @ cataloger expert @ specific expertise in @ field of painting sculpture and vernacular and landscape architecture @ @ science @ medium llc @ 
3990,"Stalker, a multilingual text mining search engine for open source intelligence","Open Source Intelligence (OSINT) is an intelligence gathering discipline that involves collecting information from open sources and analyzing it to produce usable intelligence. The international Intelligence Communities have seen open sources grow increasingly easier and cheaper to acquire in recent years. But up to 80% of electronic data is textual and most valuable information is often hidden and encoded in pages which are neither structured, nor classified. The process of accessing all these raw data, heterogeneous in terms of source and language, and transforming them into information is therefore strongly linked to automatic textual analysis and synthesis, which are greatly related to the ability to master the problems of multilinguality. This paper describes a content enabling system that provides deep semantic search and information access to large quantities of distributed multimedia data for both experts and general public. STALKER provides with a language independent search and dynamic classification features for a broad range of data collected from several sources in a number of culturally diverse languages. © 2009 Springer-Verlag Berlin Heidelberg.",2009,Advances in Soft Computing,2,open source intelligence @ osint @ is @ intelligence gathering discipline @ involves collecting information @ open source and analyzing @ to produce usable intelligence @ @ international intelligence community @ seen open source grow increasingly easier and cheaper to acquire in recent year @ @ up to of electronic data is textual and @ valuable information is often hidden and encoded in page @ @ neither structured @ classified @ @ process of accessing @ @ raw data heterogeneous in term of source and language and transforming @ @ information is therefore strongly linked to automatic textual analysis and synthesis @ @ greatly related to @ ability to master @ problem of multilinguality @ @ @ describes a content enabling system @ provides deep semantic search and information access to @ quantity of distributed multimedia data @ @ expert and general public @ stalker provides @ a language independent search and dynamic classification feature @ a broad range of data collected @ several source in a number of culturally diverse language @ springer-verlag @ @ @ 
3992,A bio-inspired application of natural language processing: A case study in extracting multiword expression,"For the multiword expression (MWE) extraction, the multiple sequence alignment (MSA) is proposed on the motivation of gene recognition. Because textual sequence is similar to gene sequence in pattern analysis. This MSA technique is combined with error-driven rules, with the improved efficiency beyond the traditional methods. It provides a guarantee for the MWE recall. It uses the dynamic programming method to prevent candidates from combinational explosion, and provides a global solution for pattern extraction instead of sub-pattern redundancy. Consequently, it has accurate measures for flexible patterns. In experiment, some advanced statistical measures are performed for ranking candidates. In the comparison experiment, the MSA approach achieved better results. © 2008 Elsevier Ltd. All rights reserved.",2009,Expert Systems with Applications,3,@ @ multiword expression @ mwe @ extraction @ multiple sequence alignment @ msa @ is proposed on @ motivation of gene recognition @ @ textual sequence is similar to gene sequence in pattern analysis @ @ msa technique is combined @ error-driven rule @ @ improved efficiency beyond @ traditional method @ @ provides a guarantee @ @ mwe recall @ @ us @ dynamic programming method to prevent candidate @ combinational explosion and provides a global solution @ pattern extraction instead of sub-pattern redundancy @ consequently @ ha accurate measure @ flexible pattern @ in experiment some advanced statistical measure @ performed @ ranking candidate @ in @ comparison experiment @ msa approach achieved better @ @ @ ltd @ @ right reserved @ 
4004,Stemming malay text and its application in automatic text categorization,"In Malay language, there are no conjugations and declensions and affixes have important grammatical functions. In Malay, the same word may function as a noun, an adjective, an adverb, or, a verb, depending on its position in the sentence. Although extensively simple root words are used in informal conversations, it is essential to use the precise words in formal speech or written texts. In Malay, to make sentences clear, derivative words are used. Derivation is achieved mainly by the use of affixes. There are approximately a hundred possible derivative forms of a root word in written language of the educated Malay. Therefore, the composition of Malay words may be complicated. Although there are several types of stemming algorithms available for text processing in English and some other languages, they cannot be used to overcome the difficulties in Malay word stemming. Stemming is the process of reducing various words to their root forms in order to improve the effectiveness of text processing in information systems. It is essential to avoid both overstemming and under-stemming errors. We have developed a new Malay stemmer (stemming algorithm) for removing inflectional and derivational affixes. Our stemmer uses a set of affix rules and two types of dictionaries: a root-word dictionary and a derivative-word dictionary. The use of set of rules is aimed at reducing the occurrence of under-stemming errors, while that of the dictionaries is believed to reduce the occurrence of overstemming errors. We performed an experiment to evaluate the application of our stemmer in text mining software. For the experiment, text data used were actual web pages collected from the World Wide Web to demonstrate the effectiveness of our Malay stemming algorithm. The experimental results showed that our stemmer can effectively increase the precision of the extracted Boolean expressions for text categorization. Copyright © 2009 The Institute of Electronics.",2009,IEICE Transactions on Information and Systems,13,in malay language @ @ no conjugation and declension and affix @ important grammatical function @ in malay @ @ word may function a a noun @ adjective @ adverb @ a verb depending on @ position in @ sentence @ although extensively simple root word @ used in informal conversation @ is essential to use @ precise word in formal speech @ written text @ in malay to make sentence clear derivative word @ used @ derivation is achieved mainly by @ use of affix @ @ @ approximately a hundred possible derivative form of a root word in written language of @ educated malay @ therefore @ composition of malay word may @ complicated @ although @ @ several type of stemming algorithm available @ text processing in english and some @ language @ cannot @ used to overcome @ difficulty in malay word stemming @ stemming is @ process of reducing various word to @ root form in order to improve @ effectiveness of text processing in information system @ @ is essential to avoid @ overstemming and under-stemming error @ @ @ developed a @ malay stemmer @ stemming algorithm @ @ removing inflectional and derivational affix @ @ stemmer us a set of affix rule and @ type of dictionary @ a root-word dictionary and a derivative-word dictionary @ @ use of set of rule is aimed at reducing @ occurrence of under-stemming error @ @ of @ dictionary is believed to reduce @ occurrence of overstemming error @ @ performed @ experiment to evaluate @ application of @ stemmer in text mining software @ @ @ experiment text data used @ actual web page collected @ @ world wide web to demonstrate @ effectiveness of @ malay stemming algorithm @ @ experimental @ showed @ @ stemmer @ effectively increase @ precision of @ extracted boolean expression @ text categorization @ @ @ institute of electronics @ 
4005,Automatic extraction of new words based on Google News corpora for supporting lexicon-based Chinese word segmentation systems,"Chinese word segmentation is an essential step in a processing of Chinese natural language because it is beneficial to the Chinese text mining and information retrieval. Currently, the lexicon-based Chinese word segmentation scheme is widely adopted, which can correctly identify Chinese sentences as distinct words from Chinese language texts in real-word applications. However, the word identification ability of the lexicon-based scheme is highly dependent with a well prepared lexicon with sufficient amount of lexical entries which covers all of the Chinese words. In particular, this scheme cannot perform Chinese word segmentation process well for highly changeable texts with time, such as newspaper articles and web documents. This is because highly changeable documents often contain many new words that cannot be identified by a lexicon-based Chinese word segmentation system with a constant lexicon. Moreover, to maintain a lexicon by manpower is an inefficient and time-consuming job. Therefore, this study proposes a novel statistics-based scheme for extraction of new words based on the categorized corpora of Google News retrieved automatically from the Google News site to promote the word identification ability for lexicon-based Chinese word segmentation systems. Since corpora of news almost contain all words used in daily life, to extract news words from corpora of news and to incrementally add them into lexicon for lexicon-based Chinese word segmentation systems provide benefits in terms of automatically constructing a professional lexicon and enhancing word identification capability. Compared to another proposed scheme of new word extraction, the experimental results indicated that the proposed extraction scheme of new words not only more correctly retrieves new words from the categorized corpora of Google News, but also obtains larger amount of new words. Moreover, the proposed scheme of new word extraction has been applied to automatically expand the lexicon of the Chinese word segmentation system ECScanner (A Chinese Lexicon Scanner with Lexicon Extension). Currently, the ECScanner has been published on the Web to provide Chinese word segmentation service based on Web service. Experimental results also confirmed that ECScanner is superior to CKIP (Chinese knowledge information processing) in identifying meaningful Chinese words. © 2008 Elsevier Ltd. All rights reserved.",2009,Expert Systems with Applications,19,chinese word segmentation is @ essential step in a processing of chinese natural language @ @ is beneficial to @ chinese text mining and information retrieval @ currently @ lexicon-based chinese word segmentation scheme is widely adopted @ @ correctly identify chinese sentence a distinct word @ chinese language text in real-word application @ however @ word identification ability of @ lexicon-based scheme is highly dependent @ a well prepared lexicon @ sufficient amount of lexical entry @ cover @ of @ chinese word @ in particular @ scheme cannot perform chinese word segmentation process well @ highly changeable text @ time @ a newspaper article and web document @ @ is @ highly changeable document often contain many @ word @ cannot @ identified by a lexicon-based chinese word segmentation system @ a constant lexicon @ moreover to maintain a lexicon by manpower is @ inefficient and time-consuming job @ therefore @ study proposes a novel statistics-based scheme @ extraction of @ word based on @ categorized corpus of google news retrieved automatically @ @ google news site to promote @ word identification ability @ lexicon-based chinese word segmentation system @ since corpus of news almost contain @ word used in daily life to extract news word @ corpus of news and to incrementally add @ @ lexicon @ lexicon-based chinese word segmentation system provide benefit in term of automatically constructing a professional lexicon and enhancing word identification capability @ compared to another proposed scheme of @ word extraction @ experimental @ indicated @ @ proposed extraction scheme of @ word not only more correctly retrieves @ word @ @ categorized corpus of google news @ @ obtains larger amount of @ word @ moreover @ proposed scheme of @ word extraction ha @ applied to automatically expand @ lexicon of @ chinese word segmentation system ecscanner @ a chinese lexicon scanner @ lexicon extension @ @ currently @ ecscanner ha @ published on @ web to provide chinese word segmentation service based on web service @ experimental @ @ confirmed @ ecscanner is superior to ckip @ chinese knowledge information processing @ in identifying meaningful chinese word @ @ ltd @ @ right reserved @ 
4008,Exploring models for semantic category verification,"Many artificial intelligence tasks, such as automated question answering, reasoning, or heterogeneous database integration, involve verification of a semantic category (e.g. ""coffee"" is a drink, ""red"" is a color, while ""steak"" is not a drink and ""big"" is not a color). In this research, we explore completely automated on-the-fly verification of a membership in any arbitrary category which has not been expected a priori. Our approach does not rely on any manually codified knowledge (such as WordNet or Wikipedia) but instead capitalizes on the diversity of topics and word usage on the World Wide Web, thus can be considered ""knowledge-light"" and complementary to the ""knowledge-intensive"" approaches. We have created a quantitative verification model and established (1) what specific variables are important and (2) what ranges and upper limits of accuracy are attainable. While our semantic verification algorithm is entirely self-contained (not involving any previously reported components that are beyond the scope of this paper), we have tested it empirically within our fact seeking engine on the well known TREC conference test questions. Due to our implementation of semantic verification, the answer accuracy has improved by up to 16% depending on the specific models and metrics used. © 2009 Elsevier B.V. All rights reserved.",2009,Information Systems,2,many artificial intelligence task @ a automated question answering reasoning @ heterogeneous database integration involve verification of a semantic category @ e @ g @ @ coffee @ is a drink @ red @ is a color @ @ steak @ is not a drink and @ big @ is not a color @ @ in @ research @ explore completely automated on-the-fly verification of a membership in @ arbitrary category @ ha not @ expected a priori @ @ approach doe not rely on @ manually codified knowledge @ @ a wordnet @ wikipedia @ @ instead capitalizes on @ diversity of topic and word usage on @ world wide web thus @ @ considered @ knowledge-light @ and complementary to @ @ knowledge-intensive @ approach @ @ @ created a quantitative verification model and established @ @ @ specific variable @ important and @ @ @ range and upper limit of accuracy @ attainable @ @ @ semantic verification algorithm is entirely self-contained @ not involving @ @ reported component @ @ beyond @ scope of @ @ @ @ @ tested @ empirically within @ fact seeking engine on @ well known trec conference test question @ due to @ implementation of semantic verification @ answer accuracy ha improved by up to depending on @ specific model and metric used @ @ b @ v @ @ right reserved @ 
4025,Methods for analyzing information contained in an enterprise email database,"Email is one of the most successful asynchronous communications yet devised. Many Researchers and Scientists often spend large proportions of their time using email for information and knowledge sharing. Research has not yet addressed how we can use emails as a source of information and knowledge. This study therefore presents a quantitative analysis of the emails to address these new questions. We discus the challenges that arise in email analyzing and classification We provide background, procedures for using natural language processing and text mining techniques for dealing with automatic knowledge extraction from email database. © 2008 Springer-Verlag.",2008,Communications in Computer and Information Science,0,email is @ of @ @ successful asynchronous communication yet devised @ many researcher and scientist often spend @ proportion of @ time @ email @ information and knowledge sharing @ research ha not yet addressed @ @ @ use email a a source of information and knowledge @ @ study therefore @ a quantitative analysis of @ email to address @ @ question @ @ discus @ challenge @ arise in email analyzing and classification @ provide background procedure @ @ natural language processing and text mining technique @ dealing @ automatic knowledge extraction @ email database @ springer-verlag @ 
4031,"Survey of text mining II: Clustering, classification, and retrieval","The proliferation of digital computing devices and their use in communication has resulted in an increased demand for systems and algorithms capable of mining textual data. Thus, the development of techniques for mining unstructured, semi-structured, and fully-structured textual data has become increasingly important in both academia and industry. This second volume continues to survey the evolving field of text mining - the application of techniques of machine learning, in conjunction with natural language processing, information extraction and algebraic/mathematical approaches, to computational information retrieval. Numerous diverse issues are addressed, ranging from the development of new learning approaches to novel document clustering algorithms, collectively spanning several major topic areas in text mining. Features: Acts as an important benchmark in the development of current and future approaches to mining textual information Serves as an excellent companion text for courses in text and data mining, information retrieval and computational statistics Experts from academia and industry share their experiences in solving large-scale retrieval and classification problems Presents an overview of current methods and software for text mining Highlights open research questions in document categorization and clustering, and trend detection Describes new application problems in areas such as email surveillance and anomaly detection Survey of Text Mining II offers a broad selection in state-of-the art algorithms and software for text mining from both academic and industrial perspectives, to generate interest and insight into the state of the field. This book will be an indispensable resource for researchers, practitioners, and professionals involved in information retrieval, computational statistics, and data mining. Michael W. Berry is a professor in the Department of Electrical Engineering and Computer Science at the University of Tennessee, Knoxville. Malu Castellanos is a senior researcher at Hewlett-Packard Laboratories in Palo Alto, California. © Springer-Verlag London Limited 2008.",2008,"Survey of Text Mining II: Clustering, Classification, and Retrieval",58,@ proliferation of digital computing device and @ use in communication ha resulted in @ increased demand @ system and algorithm capable of mining textual data @ thus @ development of technique @ mining unstructured semi-structured and fully-structured textual data ha become increasingly important in @ academia and industry @ @ second volume continues to survey @ evolving field of text mining @ application of technique of machine learning in conjunction @ natural language processing information extraction and algebraic mathematical approach to computational information retrieval @ numerous diverse issue @ addressed ranging @ @ development of @ learning approach to novel document clustering algorithm collectively spanning several major topic area in text mining @ feature @ act a @ important benchmark in @ development of current and future approach to mining textual information serf a @ excellent companion text @ course in text and data mining information retrieval and computational statistic expert @ academia and industry share @ experience in solving large-scale retrieval and classification problem @ @ overview of current method and software @ text mining highlight open research question in document categorization and clustering and trend detection describes @ application problem in area @ a email surveillance and anomaly detection survey of text mining ii offer a broad selection in state-of-the art algorithm and software @ text mining @ @ @ and industrial perspective to generate interest and insight @ @ state of @ field @ @ book @ @ @ indispensable resource @ researcher practitioner and professional involved in information retrieval computational statistic and data mining @ michael w @ berry is a professor in @ department of electrical engineering and computer science at @ university of tennessee knoxville @ malu castellanos is a senior researcher at hewlett-packard laboratory in palo alto california @ springer-verlag london limited @ 
4040,Wikipedia link structure and text mining for semantic relation extraction towards a huge scale global web ontology,"Wikipedia, a collaborative Wiki-based encyclopedia, has become a huge phenomenon among Internet users. It covers huge number of concepts of various fields such as Arts, Geography, History, Science, Sports and Games. Since it is becoming a database storing all human knowledge, Wikipedia mining is a promising approach that bridges the Semantic Web and the Social Web (a. k. a. Web 2.0). In fact, in the previous researches on Wikipedia mining, it is strongly proved that Wikipedia has a remarkable capability as a corpus for knowledge extraction, especially for relatedness measurement among concepts. However, semantic relatedness is just a numerical strength of a relation but does not have an explicit relation type. To extract inferable semantic relations with explicit relation types, we need to analyze not only the link structure but also texts in Wikipedia. In this paper, we propose a consistent approach of semantic relation extraction from Wikipedia. The method consists of three sub-processes highly optimized for Wikipedia mining; 1) fast preprocessing, 2) POS (Part Of Speech) tag tree analysis, and 3) mainstay extraction. Furthermore, our detailed evaluation proved that link structure mining improves both the accuracy and the scalability of semantic relations extraction.",2008,CEUR Workshop Proceedings,18,wikipedia a collaborative wiki-based encyclopedia ha become a huge phenomenon among internet user @ @ cover huge number of concept of various field @ a art geography history science sport and game @ since @ is becoming a database storing @ human knowledge wikipedia mining is a promising approach @ bridge @ semantic web and @ social web @ a @ k @ a @ web @ @ @ in fact in @ previous research on wikipedia mining @ is strongly proved @ wikipedia ha a remarkable capability a a corpus @ knowledge extraction especially @ relatedness measurement among concept @ however semantic relatedness is @ a numerical strength of a relation @ doe not @ @ explicit relation type @ to extract inferable semantic relation @ explicit relation type @ need to analyze not only @ link structure @ @ text in wikipedia @ in @ @ @ propose a consistent approach of semantic relation extraction @ wikipedia @ @ method consists of three sub-processes highly optimized @ wikipedia mining @ @ fast preprocessing @ po @ part of speech @ tag tree analysis and @ mainstay extraction @ furthermore @ detailed evaluation proved @ link structure mining improves @ @ accuracy and @ scalability of semantic relation extraction @ 
4041,Intended boundaries detection in topic change tracking for text segmentation,"This paper presents a topical text segmentation method based on intended boundaries detection and compares it to a well known default boundaries detection method, c99. We compared the two methods by running them on two different corpora of French texts and results are evaluated by two different methods: one using a modified classic measure, the FScore, the other based on a manual evaluation one the Internet. Our results showed that algorithms that are close when automatically evaluated can be quite far when manually evaluated. © 2009 Springer Science+Business Media, LLC.",2008,International Journal of Speech Technology,1,@ @ @ a topical text segmentation method based on intended boundary detection and compare @ to a well known default boundary detection method c @ @ compared @ @ method by running @ on @ different corpus of french text and @ @ evaluated by @ different method @ @ @ a modified classic measure @ fscore @ @ based on a manual evaluation @ @ internet @ @ @ showed @ algorithm @ @ close @ automatically evaluated @ @ quite far @ manually evaluated @ @ science @ medium llc @ 
4043,"SPYWatch, overcoming linguistic barriers in information management","With Internet, the bulk of predictive intelligence can be obtained from public and unclassified sources, which are more accessible, ubiquitous, and valuable. Up to 80% of electronic data is textual and most valuable information is often encoded in pages which are neither structured, nor classified. The process of accessing all these raw data, heterogeneous for language used, and transforming them into information is therefore inextricably linked to the concepts of textual analysis and synthesis, hinging greatly on the ability to master the problems of multilinguality. Through Multilingual Text Mining, users can get an overview of great volumes of textual data having available a highly readable grid, which helps them discover meaningful similarities among documents and find all related information. This paper describes the approach used by SYNTHEMA, showing a content enabling system for OSINT that provides deep semantic search and information access to large quantities of distributed multimedia. SPYWatch provides with a language independent search and dynamic classification features for a broad range of data collected from several sources in a number of culturally diverse languages. © 2008 Springer Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ internet @ bulk of predictive intelligence @ @ obtained @ public and unclassified source @ @ more accessible ubiquitous and valuable @ up to of electronic data is textual and @ valuable information is often encoded in page @ @ neither structured @ classified @ @ process of accessing @ @ raw data heterogeneous @ language used and transforming @ @ information is therefore inextricably linked to @ concept of textual analysis and synthesis hinging greatly on @ ability to master @ problem of multilinguality @ @ multilingual text mining user @ get @ overview of great volume of textual data @ available a highly readable grid @ help @ discover meaningful similarity among document and find @ related information @ @ @ describes @ approach used by synthema showing a content enabling system @ osint @ provides deep semantic search and information access to @ quantity of distributed multimedia @ spywatch provides @ a language independent search and dynamic classification feature @ a broad range of data collected @ several source in a number of culturally diverse language @ @ @ @ @ 
4045,Minors as miners modelling and evaluating ontological and linguistic learning,"Growing up is in large measure learning about the world and our social and linguistic environment. We might call this data mining, although it is far more multimodal and immersive than most applications. This paper describes computational research into how children learn, with a particular focus on evaluation in both supervised and unsupervised paradigms. Conversely, we gain additional insight into association mining by considering psycholinguistic experiments that quantify the way human association by both adults and children relate to a variety of association measures. Learning and evaluation are not dealt with in isolation, but a program of formal and application-based evaluation is expounded and exemplified to show how to evaluate discovered patterns with and without a gold standard. In this context, some serious issues with current evaluation techniques and accuracy measures are identified and the unbiased techniques identified. © 2008, Australian Computer Society; Inc.",2008,Conferences in Research and Practice in Information Technology Series,0,growing up is in @ measure learning @ @ world and @ social and linguistic environment @ @ might call @ data mining although @ is far more multimodal and immersive @ @ application @ @ @ describes computational research @ @ child learn @ a particular focus on evaluation in @ supervised and unsupervised paradigm @ conversely @ gain additional insight @ association mining by considering psycholinguistic experiment @ quantify @ way human association by @ adult and child relate to a variety of association measure @ learning and evaluation @ not dealt @ in isolation @ a program of formal and application-based evaluation is expounded and exemplified to @ @ to evaluate discovered pattern @ and without a gold standard @ in @ context some serious issue @ current evaluation technique and accuracy measure @ identified and @ unbiased technique identified @ australian computer society @ inc @ 
4047,Methodology of transformation of a thesaurus into an ontology of domain,"Information Retrieval techniques make use of terms that are automatically extracted from documents; these terms are used to give information access. In this paper we propose an approach to enrich semantically this extraction by adding knowledge from thesauri. More specifically, the methodology we promote in this paper aims at transforming a thesaurus into a domain ontology which will then be used to semantically index documents (indexes are concepts rather than terms). We also propose techniques that implement this transformation as well as an evaluation in the field of astronomy. © 2008 Lavoisier, Paris.",2008,Revue d'Intelligence Artificielle,9,information retrieval technique make use of term @ @ automatically extracted @ document @ @ term @ used to give information access @ in @ @ @ propose @ approach to enrich semantically @ extraction by adding knowledge @ thesaurus @ more specifically @ methodology @ promote in @ @ aim at transforming a thesaurus @ a domain ontology @ @ @ @ used to semantically index document @ index @ concept rather @ term @ @ @ @ propose technique @ implement @ transformation a well a @ evaluation in @ field of astronomy @ lavoisier paris @ 
4056,Extraction of socio-semantic data from chat conversations in collaborative learning communities,"Online collaboration among communities of practice using text-based tools, such as instant messaging, forums and web logs (blogs), has become very popular in the last years, but it is difficult to automatically analyze all their content due to the problems of natural language understanding software. However, useful socio-semantic data can be retrieved from a chat conversation using ontology-based text mining techniques. In this paper, a novel approach for detecting several kinds of semantic data from a chat conversation is presented. This method uses a combination of a dialogistic, socio-cultural perspective and of classical knowledge-based text processing methods. Lexical and domain ontologies are used. A tool has been developed for the discovery of the most important topics and of the contribution of each participant in the conversation. The system also discovers new, implicit references among the utterances of the chat in order to offer a multi-voiced representation of the conversation. The application offers a panel for visualizing the threading of the subjects in the chat and the contributions function. The system was experimented on chat sessions of small groups of students participating in courses on Human-Computer Interaction and Natural Language Processing in ""Politehnica"" University of Bucharest, Romania. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7,online collaboration among community of practice @ text-based tool @ a instant messaging forum and web log @ blog @ ha become @ popular in @ last year @ @ is difficult to automatically analyze @ @ content due to @ problem of natural language understanding software @ however useful socio-semantic data @ @ retrieved @ a chat conversation @ ontology-based text mining technique @ in @ @ a novel approach @ detecting several kind of semantic data @ a chat conversation is presented @ @ method us a combination of a dialogistic socio-cultural perspective and of classical knowledge-based text processing method @ lexical and domain ontology @ used @ a tool ha @ developed @ @ discovery of @ @ important topic and of @ contribution of @ participant in @ conversation @ @ system @ discovers @ implicit reference among @ utterance of @ chat in order to offer a multi-voiced representation of @ conversation @ @ application offer a panel @ visualizing @ threading of @ subject in @ chat and @ contribution function @ @ system wa experimented on chat session of small group of student participating in course on human-computer interaction and natural language processing in @ politehnica @ university of bucharest romania @ springer-verlag @ @ @ 
4060,An intelligent system for semantic information retrieval information from textual web documents,"Text data, which are represented as free text in World Wide Web (WWW), are inherently unstructured and hence it becomes difficult to directly process the text data by computer programs. There has been great interest in text mining techniques recently for helping users to quickly gain knowledge from the Web. Text mining technologies usually involve tasks such as text refining which transforms free text into an intermediate representation form which is machine-processable and knowledge distillation which deduces patterns or knowledge from the intermediate form. These text representation methodologies consider documents as bags of words and ignore the meanings and ideas their authors want to convey. As terms are treated as individual items in such simplistic representations, terms lose their semantic relations and texts lose their original meanings. In this paper, we propose a system that overcomes the limitations of the existing technologies to retrieve the information from the knowledge discovered through data mining based on the detailed meanings of the text. For this, we propose a Knowledge representation technique, which uses Resources Description Framework (RDF) metadata to represent the semantic relations, which are extracted from textual web document using natural language processing techniques. The main objective of the creation of RDF metadata in this system is to have flexibility for easy retrieval of the semantic information effectively. We also propose an effective SEMantic INformation RETrieval algorithm called SEMINRET algorithm. The experimental results obtained from this system show that the computations of Precision and Recall in RDF databases are highly accurate when compared to XML databases. Moreover, it is observed from our experiments that the document retrieval from the RDF database is more efficient than the document retrieval using XML databases. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,text data @ @ represented a free text in world wide web @ www @ @ inherently unstructured and hence @ becomes difficult to directly process @ text data by computer program @ @ ha @ great interest in text mining technique recently @ helping user to quickly gain knowledge @ @ web @ text mining technology usually involve task @ a text refining @ transforms free text @ @ intermediate representation form @ is machine-processable and knowledge distillation @ deduces pattern @ knowledge @ @ intermediate form @ @ text representation methodology consider document a bag of word and ignore @ meaning and idea @ author want to convey @ a term @ treated a individual item in @ simplistic representation term lose @ semantic relation and text lose @ original meaning @ in @ @ @ propose a system @ overcomes @ limitation of @ existing technology to retrieve @ information @ @ knowledge discovered @ data mining based on @ detailed meaning of @ text @ @ @ @ propose a knowledge representation technique @ us resource description framework @ rdf @ metadata to represent @ semantic relation @ @ extracted @ textual web document @ natural language processing technique @ @ main objective of @ creation of rdf metadata in @ system is to @ flexibility @ easy retrieval of @ semantic information effectively @ @ @ propose @ effective semantic information retrieval algorithm called seminret algorithm @ @ experimental @ obtained @ @ system @ @ @ computation of precision and recall in rdf database @ highly accurate @ compared to xml database @ moreover @ is observed @ @ experiment @ @ document retrieval @ @ rdf database is more efficient @ @ document retrieval @ xml database @ springer-verlag @ @ @ 
4062,The evaluation of sentence similarity measures,"The ability to accurately judge the similarity between natural language sentences is critical to the performance of several applications such as text mining, question answering, and text summarization. Given two sentences, an effective similarity measure should be able to determine whether the sentences are semantically equivalent or not, taking into account the variability of natural language expression. That is, the correct similarity judgment should be made even if the sentences do not share similar surface form. In this work, we evaluate fourteen existing text similarity measures which have been used to calculate similarity score between sentences in many text applications. The evaluation is conducted on three different data sets, TREC9 question variants, Microsoft Research paraphrase corpus, and the third recognizing textual entailment data set. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),134,@ ability to accurately judge @ similarity @ natural language sentence is critical to @ performance of several application @ a text mining question answering and text summarization @ given @ sentence @ effective similarity measure @ @ able to determine whether @ sentence @ semantically equivalent @ not taking @ account @ variability of natural language expression @ @ is @ correct similarity judgment @ @ made even if @ sentence @ not share similar surface form @ in @ work @ evaluate fourteen existing text similarity measure @ @ @ used to calculate similarity score @ sentence in many text application @ @ evaluation is conducted on three different data set trec question variant microsoft research paraphrase corpus and @ third recognizing textual entailment data set @ springer-verlag @ @ @ 
4063,Interoperability for Global Observation Data by Ontological Information,"The Ontology registry system is developed to collect, manage, and compare ontological information for integrating global observation data. Data sharing and data service such as support of metadata deign, structuring of data contents, support of text mining are applied for better use of data as data interoperability. Semantic network dictionary and gazetteers are constructed as a trans-disciplinary dictionary. Ontological information is added to the system by digitalizing text based dictionaries, developing ""knowledge writing tool"" for experts, and extracting semantic relations from authoritative documents with natural language processing technique. The system is developed to collect lexicographic ontology and geographic ontology. © 2008 Tsinghua University Press.",2008,Tsinghua Science and Technology,0,@ ontology registry system is developed to collect manage and compare ontological information @ integrating global observation data @ data sharing and data service @ a support of metadata deign structuring of data content support of text mining @ applied @ better use of data a data interoperability @ semantic network dictionary and gazetteer @ constructed a a trans-disciplinary dictionary @ ontological information is added to @ system by digitalizing text based dictionary developing @ knowledge writing tool @ @ expert and extracting semantic relation @ authoritative document @ natural language processing technique @ @ system is developed to collect lexicographic ontology and geographic ontology @ tsinghua university @ @ 
4065,Exploration of a collection of documents in neuroscience and extraction of topics by clustering,"This paper presents a preliminary analysis of the neuroscience knowledge domain, and an application of cluster analysis to identify topics in neuroscience. A collection of posters presented at the Society for Neuroscience (SfN) Annual Meeting in 2006 is first explored by viewing existing topics and poster sessions using multidimensional scaling. Based on the Vector Space Model, several Term Spaces were built on the basis of a set of terms extracted from the posters' abstracts and titles, and a set of free keywords assigned to the posters by their authors. The ensuing Term Spaces were compared from the point of view of retrieving the genuine category titles. Topics were extracted from the abstracts of posters by clustering the documents using a bisecting k-means algorithm and selecting the most salient terms for each cluster by ranking. The terms extracted as topic descriptors were evaluated by comparing them to existing titles assigned to thematic categories defined by human experts in neuroscience. A comparison of two approaches for terms ranking (Document Frequency and Log-Entropy) resulted in better performance of the Log-Entropy scores, allowing to retrieve 31.0% of original title terms in clustered documents (and 37.1% in original thematic categories). © 2008 Elsevier Ltd. All rights reserved.",2008,Neural Networks,4,@ @ @ a preliminary analysis of @ neuroscience knowledge domain and @ application of cluster analysis to identify topic in neuroscience @ a collection of poster presented at @ society @ neuroscience @ sfn @ annual meeting in is first explored by viewing existing topic and poster session @ multidimensional scaling @ based on @ vector space model several term space @ built on @ basis of a set of term extracted @ @ poster @ abstract and title and a set of free keywords assigned to @ poster by @ author @ @ ensuing term space @ compared @ @ point of view of retrieving @ genuine category title @ topic @ extracted @ @ abstract of poster by clustering @ document @ a bisecting k-means algorithm and selecting @ @ salient term @ @ cluster by ranking @ @ term extracted a topic descriptor @ evaluated by comparing @ to existing title assigned to thematic category defined by human expert in neuroscience @ a comparison of @ approach @ term ranking @ document frequency and log-entropy @ resulted in better performance of @ log-entropy score allowing to retrieve @ of original title term in clustered document @ and @ in original thematic category @ @ @ ltd @ @ right reserved @ 
4068,A web-based self-training approach for authorship attribution,"As any other text categorization task, authorship attribution requires a large number of training examples. These examples, which are easily obtained for most of the tasks, are particularly difficult to obtain for this case. Based on this fact, in this paper we investigate the possibility of using Web-based text mining methods for the identification of the author of a given poem. In particular, we propose a semi-supervised method that is specially suited to work with justfew training examples in order to tackle the problem of the lack of data with the same writing style. The method considers the automatic extraction of the unlabeled examples from the Web and its iterative integration into the training data set. To the knowledge of the authors, a semi-supervised method which makes use of the Web as support lexical resource has not been previously employed in this task. The results obtained on poem categorization show that this method may improve the classification accuracy and it is appropriate to handle the attribution of short documents. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,a @ @ text categorization task authorship attribution requires a @ number of training example @ @ example @ @ easily obtained @ @ of @ task @ particularly difficult to obtain @ @ case @ based on @ fact in @ @ @ investigate @ possibility of @ web-based text mining method @ @ identification of @ author of a given poem @ in particular @ propose a semi-supervised method @ is specially suited to work @ justfew training example in order to tackle @ problem of @ lack of data @ @ @ writing style @ @ method considers @ automatic extraction of @ unlabeled example @ @ web and @ iterative integration @ @ training data set @ to @ knowledge of @ author a semi-supervised method @ make use of @ web a support lexical resource ha not @ @ employed in @ task @ @ @ obtained on poem categorization @ @ @ method may improve @ classification accuracy and @ is appropriate to handle @ attribution of short document @ springer-verlag @ @ @ 
4069,ASAP- An advanced system for assessing chat participants,"The paper presents a method and an implemented system for the assessment of the participants' competences in a collaborative environment based on an instant messenger conversation (chat). For each utterance in the chat, a score is computed that takes into account several features, specific to text mining (like the presence and the density of keywords, via synonymy), natural language pragmatics and to social networks. The total rating of the competence of a participant is computed considering the scores of utterances and inter-utterance factors. Within the frame of the developed system, special attention was given to multiple ways of visualizing the analysis' results. An annotation editor was also implemented and used in order to construct a ""golden standard"", which was further employed for the evaluation of the developed assessment tools. © Springer-Verlag Berlin Heidelberg 2008.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),16,@ @ @ a method and @ implemented system @ @ assessment of @ participant @ competence in a collaborative environment based on @ instant messenger conversation @ chat @ @ @ @ utterance in @ chat a score is computed @ take @ account several feature specific to text mining @ like @ presence and @ density of keywords via synonymy @ natural language pragmatic and to social network @ @ total rating of @ competence of a participant is computed considering @ score of utterance and inter-utterance factor @ within @ frame of @ developed system special attention wa given to multiple way of visualizing @ analysis @ @ @ @ annotation editor wa @ implemented and used in order to construct a @ golden standard @ @ wa @ employed @ @ evaluation of @ developed assessment tool @ springer-verlag @ @ @ 
4072,Mining knowledge from natural language texts using fuzzy associated concept mapping,"Natural Language Processing (NLP) techniques have been successfully used to automatically extract information from unstructured text through a detailed analysis of their content, often to satisfy particular information needs. In this paper, an automatic concept map construction technique, Fuzzy Association Concept Mapping (FACM), is proposed for the conversion of abstracted short texts into concept maps. The approach consists of a linguistic module and a recommendation module. The linguistic module is a text mining method that does not require the use to have any prior knowledge about using NLP techniques. It incorporates rule-based reasoning (RBR) and case based reasoning (CBR) for anaphoric resolution. It aims at extracting the propositions in text so as to construct a concept map automatically. The recommendation module is arrived at by adopting fuzzy set theories. It is an interactive process which provides suggestions of propositions for further human refinement of the automatically generated concept maps. The suggested propositions are relationships among the concepts which are not explicitly found in the paragraphs. This technique helps to stimulate individual reflection and generate new knowledge. Evaluation was carried out by using the Science Citation Index (SCI) abstract database and CNET News as test data, which are well known databases and the quality of the text is assured. Experimental results show that the automatically generated concept maps conform to the outputs generated manually by domain experts, since the degree of difference between them is proportionally small. The method provides users with the ability to convert scientific and short texts into a structured format which can be easily processed by computer. Moreover, it provides knowledge workers with extra time to re-think their written text and to view their knowledge from another angle. © 2008 Elsevier Ltd. All rights reserved.",2008,Information Processing and Management,30,natural language processing @ nlp @ technique @ @ successfully used to automatically extract information @ unstructured text @ a detailed analysis of @ content often to satisfy particular information need @ in @ @ @ automatic concept map construction technique fuzzy association concept mapping @ facm @ is proposed @ @ conversion of abstracted short text @ concept map @ @ approach consists of a linguistic module and a recommendation module @ @ linguistic module is a text mining method @ doe not require @ use to @ @ prior knowledge @ @ nlp technique @ @ incorporates rule-based reasoning @ rbr @ and case based reasoning @ cbr @ @ anaphoric resolution @ @ aim at extracting @ proposition in text @ a to construct a concept map automatically @ @ recommendation module is arrived at by adopting fuzzy set theory @ @ is @ interactive process @ provides suggestion of proposition @ @ human refinement of @ automatically generated concept map @ @ suggested proposition @ relationship among @ concept @ @ not explicitly found in @ paragraph @ @ technique help to stimulate individual reflection and generate @ knowledge @ evaluation wa carried @ by @ @ science citation index @ sci @ abstract database and cnet news a test data @ @ well known database and @ quality of @ text is assured @ experimental @ @ @ @ automatically generated concept map conform to @ output generated manually by domain expert since @ degree of difference @ @ is proportionally small @ @ method provides user @ @ ability to convert scientific and short text @ a structured format @ @ @ easily processed by computer @ moreover @ provides knowledge worker @ extra time to re-think @ written text and to view @ knowledge @ another angle @ @ ltd @ @ right reserved @ 
4081,A general architecture for connecting NLP frameworks and desktop clients using Web services,"Despite impressive advances in the development of generic NLP frameworks, content-specific text mining algorithms, and NLP services, little progress has been made in enhancing existing end-user clients with text analysis capabilities. To overcome this software engineering gap between desktop environments and text analysis frameworks, we developed an open service-oriented architecture, based on Semantic Web ontologies and W3C Web services, which makes it possible to easily integrate any NLP service into client applications. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,despite impressive advance in @ development of generic nlp framework content-specific text mining algorithm and nlp service little progress ha @ made in enhancing existing end-user client @ text analysis capability @ to overcome @ software engineering gap @ desktop environment and text analysis framework @ developed @ open service-oriented architecture based on semantic web ontology and w c web service @ make @ possible to easily integrate @ nlp service @ client application @ springer-verlag @ @ @ 
4083,Semi-joint labeling for Chinese named entity recognition,"Named entity recognition (NER) is an essential component of text mining applications. In Chinese sentences, words do not have delimiters; thus, incorporating word segmentation information into an NER model can improve its performance. Based on the framework of dynamic conditional random fields, we propose a novel labeling format, called semi-joint labeling which partially integrates word segmentation information and named entity tags for NER. The model enhances the interaction of segmentation tags and NER achieved by traditional approaches. Moreover, it allows us to consider interactions between multiple chains in a linear-chain model. We use data from the SIGHAN 2006 NER bakeoff to evaluate the proposed model. The experimental results demonstrate that our approach outperforms state-of-the-art systems. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,named entity recognition @ ner @ is @ essential component of text mining application @ in chinese sentence word @ not @ delimiters @ thus incorporating word segmentation information @ @ ner model @ improve @ performance @ based on @ framework of dynamic conditional random field @ propose a novel labeling format called semi-joint labeling @ partially integrates word segmentation information and named entity tag @ ner @ @ model enhances @ interaction of segmentation tag and ner achieved by traditional approach @ moreover @ allows u to consider interaction @ multiple chain in a linear-chain model @ @ use data @ @ sighan ner bakeoff to evaluate @ proposed model @ @ experimental @ demonstrate @ @ approach outperforms state-of-the-art system @ springer-verlag @ @ @ 
4084,Text onto miner - A semi automated ontology building system,"This paper presents an overview of the results of the project undertaken by the Warsaw University of Technology Institute of Computer Science as a part of research agreement with France Telecom. The project goal was to create a set of tools - both software and methods, that could be used to speed up and improve a process of creating ontologies. In the course of the project a new ontology building methodology has been devised, new text mining algorithms optimized for extracting information useful for building an ontology from text corpora have been proposed and an universal text mining toolkit - TOM Platform - have been implemented. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,@ @ @ @ overview of @ @ of @ project undertaken by @ warsaw university of technology institute of computer science a a part of research agreement @ france telecom @ @ project goal wa to create a set of tool @ software and method @ could @ used to speed up and improve a process of creating ontology @ in @ course of @ project a @ ontology building methodology ha @ devised @ text mining algorithm optimized @ extracting information useful @ building @ ontology @ text corpus @ @ proposed and @ universal text mining toolkit tom platform @ @ implemented @ springer-verlag @ @ @ 
4086,Classifiers ensemble approaches for automatic recognition of biomedical named entities,"As a new branch of data mining and knowledge discovery, the research of biomedical text mining has a rapid progress currently. Biomedical named entity recognition is a basic technique in the biomedical knowledge discovery and its performance has direct effects on further discovery and processing in biomedical texts. In this paper, we present classifiers ensemble approaches for biomedical named entity recognition. Four individual classifiers, Generalized Winnow, Conditional Random Fields, Support Vector Machine, and Maximum Entropy are combined through three different strategies. We demonstrate the effectiveness of the strategies and compare their performances with standalone classifier system. The experiments are carried on JNLPBA2004 corpus with an F-sore of 77.57%. Experimental results show that the proposed method, stacking ensemble strategy, can yield promising performances.",2008,Journal of Computational Information Systems,0,a a @ branch of data mining and knowledge discovery @ research of biomedical text mining ha a rapid progress currently @ biomedical named entity recognition is a basic technique in @ biomedical knowledge discovery and @ performance ha direct effect on @ discovery and processing in biomedical text @ in @ @ @ @ classifier ensemble approach @ biomedical named entity recognition @ four individual classifier generalized winnow conditional random field support vector machine and maximum entropy @ combined @ three different strategy @ @ demonstrate @ effectiveness of @ strategy and compare @ performance @ standalone classifier system @ @ experiment @ carried on jnlpba corpus @ @ f-sore of @ @ experimental @ @ @ @ proposed method stacking ensemble strategy @ yield promising performance @ 
4088,Chinese word segmentation as morpheme-based lexical chunking,"Chinese word segmentation plays an important role in many Chinese language processing tasks such as information retrieval and text mining. Recent research in Chinese word segmentation focuses on tagging approaches with either characters or words as tagging units. In this paper we present a morpheme-based chunking approach and implement it in a two-stage system. It consists of two main components, namely a morpheme segmentation component to segment an input sentence to a sequence of morphemes based on morpheme-formation models and bigram language models, and a lexical chunking component to label each segmented morpheme's position in a word of a special type with the aid of lexicalized hidden Markov models. To facilitate these tasks, a statistically-based technique is also developed for automatically compiling a morpheme dictionary from a segmented or tagged corpus. To evaluate this approach, we conduct a closed test and an open test using the 2005 SIGHAN Bakeoff data. Our system demonstrates state-of-the-art performance on different test sets, showing the benefits of choosing morphemes as tagging units. Furthermore, the open test results indicate significant performance enhancement using lexicalization and part-of-speech features. © 2008 Elsevier Inc. All rights reserved.",2008,Information Sciences,39,chinese word segmentation play @ important role in many chinese language processing task @ a information retrieval and text mining @ recent research in chinese word segmentation focus on tagging approach @ either character @ word a tagging unit @ in @ @ @ @ a morpheme-based chunking approach and implement @ in a two-stage system @ @ consists of @ main component namely a morpheme segmentation component to segment @ input sentence to a sequence of morpheme based on morpheme-formation model and bigram language model and a lexical chunking component to label @ segmented morpheme @ s position in a word of a special type @ @ aid of lexicalized hidden markov model @ to facilitate @ task a statistically-based technique is @ developed @ automatically compiling a morpheme dictionary @ a segmented @ tagged corpus @ to evaluate @ approach @ conduct a closed test and @ open test @ @ sighan bakeoff data @ @ system demonstrates state-of-the-art performance on different test set showing @ benefit of choosing morpheme a tagging unit @ furthermore @ open test @ indicate significant performance enhancement @ lexicalization and part-of-speech feature @ @ inc @ @ right reserved @ 
4095,HAL-based evolutionary inference for pattern induction from psychiatry web resources,"Negative and stressful life events play a significant role in triggering depressive episodes. Psychiatric services that can identify such events efficiently are vital for mental health care and prevention. Meaningful patterns, e.g., <lost, parents>, must be extracted from psychiatric texts before these services can be provided. This study presents an evolutionary text-mining framework capable of inducing variable-length patterns from unannotated psychiatry web resources. The proposed framework can be divided into two parts: 1) a cognitive motivated model such as Hyperspace Analog to Language (HAL) and 2) an Evolutionary Inference Algorithm (EIA). The HAL model constructs a high-dimensional context space to represent words as well as combinations of words. Based on the HAL model, the EIA bootstraps with a small set of seed patterns, and then iteratively induces additional relevant patterns. To avoid moving in the wrong direction, the EIA further incorporates relevance feedback to guide the induction process. Experimental results indicate that combining the HAL model and relevance feedback enables the EIA to not only induce patterns from the unannotated web corpora, but also achieve useful results in a reasonable amount of time. The proposed framework thus significantly reduces reliance on annotated corpora. © 2007 IEEE.",2008,IEEE Transactions on Evolutionary Computation,12,negative and stressful life event play a significant role in triggering depressive episode @ psychiatric service @ @ identify @ event efficiently @ vital @ mental health care and prevention @ meaningful pattern e @ g @ lost parent must @ extracted @ psychiatric text @ @ service @ @ provided @ @ study @ @ evolutionary text-mining framework capable of inducing variable-length pattern @ unannotated psychiatry web resource @ @ proposed framework @ @ divided @ @ part @ @ a cognitive motivated model @ a hyperspace analog to language @ hal @ and @ @ evolutionary inference algorithm @ eia @ @ @ hal model construct a high-dimensional context space to represent word a well a combination of word @ based on @ hal model @ eia bootstrap @ a small set of seed pattern and @ iteratively induces additional relevant pattern @ to avoid moving in @ wrong direction @ eia @ incorporates relevance feedback to guide @ induction process @ experimental @ indicate @ combining @ hal model and relevance feedback enables @ eia to not only induce pattern @ @ unannotated web corpus @ @ achieve useful @ in a reasonable amount of time @ @ proposed framework thus significantly reduces reliance on annotated corpus @ @ @ 
4098,Text mining and software engineering: An integrated source code and document analysis approach,"Documents written in natural languages constitute a major part of the artefacts produced during the software engineering life cycle. Especially during software maintenance or reverse engineering, semantic information conveyed in these documents can provide important knowledge for the software engineer. A text mining system capable of populating a software ontology with information detected in documents is presented. A particular novelty is the integration of results from automated source code analysis into a natural language processing pipeline, allowing to cross-link software artefacts represented in code and natural language on a semantic level. © The Institution of Engineering and Technology 2008.",2008,IET Software,26,document written in natural language constitute a major part of @ artefact produced @ @ software engineering life cycle @ especially @ software maintenance @ reverse engineering semantic information conveyed in @ document @ provide important knowledge @ @ software engineer @ a text mining system capable of populating a software ontology @ information detected in document is presented @ a particular novelty is @ integration of @ @ automated source code analysis @ a natural language processing pipeline allowing to cross-link software artefact represented in code and natural language on a semantic level @ @ institution of engineering and technology @ 
4106,Unsupervised learning of semantic relations for molecular biology ontologies,"Manual ontology building in the biomedical domain is a work-intensive task requiring the participation of both domain and knowledge representation experts. The representation of biomedical knowledge has been found of great use for biomedical text mining and integration of biomedical data. In this chapter we present an unsupervised method for learning arbitrary semantic relations between ontological concepts in the molecular biology domain. The method uses the GENIA corpus and ontology to learn relations between annotated named-entities by means of several standard natural language processing techniques. An in-depth analysis of the output evaluates the accuracy of the model and its potentials for text mining and ontology building applications. The proposed learning method does not require domain-specific optimization or tuning and can be straightforwardly applied to arbitrary domains, provided the basic processing components exist. © 2008 The authors and IOS Press. All rights reserved.",2008,Frontiers in Artificial Intelligence and Applications,10,manual ontology building in @ biomedical domain is a work-intensive task requiring @ participation of @ domain and knowledge representation expert @ @ representation of biomedical knowledge ha @ found of great use @ biomedical text mining and integration of biomedical data @ in @ chapter @ @ @ unsupervised method @ learning arbitrary semantic relation @ ontological concept in @ molecular biology domain @ @ method us @ genia corpus and ontology to learn relation @ annotated named-entities by mean of several standard natural language processing technique @ @ in-depth analysis of @ output evaluates @ accuracy of @ model and @ potential @ text mining and ontology building application @ @ proposed learning method doe not require domain-specific optimization @ tuning and @ @ straightforwardly applied to arbitrary domain provided @ basic processing component exist @ @ author and io @ @ @ right reserved @ 
4107,Knowledge discovery in online repositories: A text mining approach,"Before the advent of the Internet, the newspapers were the prominent instrument of mobilization for independence and political struggles. Since independence in Nigeria, the political class has adopted newspapers as a medium of Political Competition and Communication. Consequently, most political information exists in unstructured form and hence the need to tap into it using text mining algorithm. This paper implements a text mining algorithm on some unstructured data format in some newspapers. The algorithm involves the following natural language processing techniques: tokenization, text filtering and refinement. As a follow-up to the natural language techniques, association rule mining technique of data mining is used to extract knowledge using the Modified Generating Association Rules based on Weighting scheme (GARW). The main contributions of the technique are that it integrates information retrieval scheme (Term Frequency Inverse Document Frequency) (for keyword/feature selection that automatically selects the most discriminative keywords for use in association rules generation) with Data Mining technique for association rules discovery. The program is applied to Pre-Election information gotten from the website of the Nigerian Guardian newspaper. The extracted association rules contained important features and described the informative news included in the documents collection when related to the concluded 2007 presidential election. The system presented useful information that could help sanitize the polity as well as protect the nascent democracy. © EuroJournals Publishing, Inc. 2008.",2008,European Journal of Scientific Research,13,@ @ advent of @ internet @ newspaper @ @ prominent instrument of mobilization @ independence and political struggle @ since independence in nigeria @ political class ha adopted newspaper a a medium of political competition and communication @ consequently @ political information exists in unstructured form and hence @ need to tap @ @ @ text mining algorithm @ @ @ implement a text mining algorithm on some unstructured data format in some newspaper @ @ algorithm involves @ following natural language processing technique @ tokenization text filtering and refinement @ a a follow-up to @ natural language technique association rule mining technique of data mining is used to extract knowledge @ @ modified generating association rule based on weighting scheme @ garw @ @ @ main contribution of @ technique @ @ @ integrates information retrieval scheme @ term frequency inverse document frequency @ @ @ keyword feature selection @ automatically selects @ @ discriminative keywords @ use in association rule generation @ @ data mining technique @ association rule discovery @ @ program is applied to pre-election information gotten @ @ website of @ nigerian guardian newspaper @ @ extracted association rule contained important feature and described @ informative news included in @ document collection @ related to @ concluded presidential election @ @ system presented useful information @ could help sanitize @ polity a well a protect @ nascent democracy @ eurojournals publishing inc @ @ 
4108,A modular information extraction system,"In today's information age, the amount of text documents available electronically (on the Web, on corporate intranets, on news wires and elsewhere) is overwhelming. Search engines and information retrieval, while useful to find documents that satisfy a certain query, offer little help with analyzing the unstructured documents themselves. Text Mining is the automated process of analyzing unstructured, natural language text in order to discover information and knowledge that are difficult to retrieve. Information Extraction (IE) centers on finding entities and relations in free text and provides a solid foundation for text mining. In this paper we present a modular IE system, based on the DIAL language. DIAL allows users to implement IE solutions for various domains rapidly, based on a common Natural Language Processing (NLP) infrastructure. We demonstrate in detail an implementation of a system for extracting relations in the intelligence news domain. We present an evaluation of our system and discuss enhancements for other domains, such as emails. © 2008 IOS Press. All rights reserved.",2008,Intelligent Data Analysis,10,in today @ s information age @ amount of text document available electronically @ on @ web on corporate intranet on news wire and elsewhere @ is overwhelming @ search engine and information retrieval @ useful to find document @ satisfy a certain query offer little help @ analyzing @ unstructured document @ @ text mining is @ automated process of analyzing unstructured natural language text in order to discover information and knowledge @ @ difficult to retrieve @ information extraction @ ie @ center on finding entity and relation in free text and provides a solid foundation @ text mining @ in @ @ @ @ a modular ie system based on @ dial language @ dial allows user to implement ie solution @ various domain rapidly based on a common natural language processing @ nlp @ infrastructure @ @ demonstrate in detail @ implementation of a system @ extracting relation in @ intelligence news domain @ @ @ @ evaluation of @ system and discus enhancement @ @ domain @ a email @ io @ @ @ right reserved @ 
4109,Text mining,"Most data mining methods assume that the data to be mined is represented in a structured relational database. However, in many applications, available electronic information is in the form of unstructured natural-language documents rather than structured databases. This tutorial will review machine learning methods for text mining. First, we will review standard classification and clustering methods for text which assume a vector-space or ""bag of words"" representation of documents that ignores the order of words in text.We will discuss naive Bayes, Rocchio, nearest neighbor, and SVMs for classifying texts and hierarchical agglomerative, spherical k-means and Expectation Maximization (EM) methods for clustering texts. Next we will review information extraction (IE) methods that use sequence information to identify entities and relations in documents. We will discuss hidden Markov models (HMMs) and conditional random fields (CRFs) for sequence labeling and IE. We will motivate the methods discussed with applications in spam filtering, information retrieval, recommendation systems, and bioinformatics. © 2008 Springer Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ data mining method assume @ @ data to @ mined is represented in a structured relational database @ however in many application available electronic information is in @ form of unstructured natural-language document rather @ structured database @ @ tutorial @ review machine learning method @ text mining @ first @ @ review standard classification and clustering method @ text @ assume a vector-space @ @ bag of word @ representation of document @ ignores @ order of word in text @ @ @ discus naive bayes rocchio nearest neighbor and svms @ classifying text and hierarchical agglomerative spherical k-means and expectation maximization @ em @ method @ clustering text @ next @ @ review information extraction @ ie @ method @ use sequence information to identify entity and relation in document @ @ @ discus hidden markov model @ hmms @ and conditional random field @ crfs @ @ sequence labeling and ie @ @ @ motivate @ method discussed @ application in spam filtering information retrieval recommendation system and bioinformatics @ @ @ @ @ 
4113,A compact arabic lexical semantics language resource based on the theory of semantic fields,"Applications of statistical Arabic NLP in general, and text mining in specific, along with the tools underneath perform much better as the statistical processing operates on deeper language factorizations than on raw text. Lexical semantic factorization is very important in this regard due to its feasibility, high level of abstraction, and the language independence of its output. In the core of such a factorization lies an Arabic lexical semantic DB. While building this LR, we had to go beyond the conventional exclusive collection of words from dictionaries and thesauri that cannot alone produce a satisfactory coverage of this highly inflective and derivative language. This paper is hence devoted to the design and implementation of an Arabic lexical semantics LR that enables the retrieval of the possible senses of any given Arabic word at a high coverage. Instead of tying full Arabic words to their possible senses, our LR flexibly relates morphologically and PoS-tags constrained Arabic lexical compounds to a predefined limited set of semantic fields across which the standard semantic relations are defined. With the aid of the same large-scale Arabic morphological analyzer and PoS tagger in the runtime, the possible senses of virtually any given Arabic word are retrievable. © 2008 Springer-Verlag Berlin Heidelberg.",2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9,application of statistical arabic nlp in general and text mining in specific along @ @ tool underneath perform much better a @ statistical processing operates on deeper language factorization @ on raw text @ lexical semantic factorization is @ important in @ regard due to @ feasibility high level of abstraction and @ language independence of @ output @ in @ core of @ a factorization lie @ arabic lexical semantic db @ @ building @ lr @ @ to go beyond @ conventional exclusive collection of word @ dictionary and thesaurus @ cannot alone produce a satisfactory coverage of @ highly inflective and derivative language @ @ @ is hence devoted to @ design and implementation of @ arabic lexical semantics lr @ enables @ retrieval of @ possible sens of @ given arabic word at a high coverage @ instead of tying full arabic word to @ possible sens @ lr flexibly relates morphologically and pos-tags constrained arabic lexical compound to a predefined limited set of semantic field across @ @ standard semantic relation @ defined @ @ @ aid of @ @ large-scale arabic morphological analyzer and po tagger in @ runtime @ possible sens of virtually @ given arabic word @ retrievable @ springer-verlag @ @ @ 
4124,Rule-based protein term identification with help from automatic species tagging,"In biomedical articles, terms often refer to different protein entities. For example, an arbitrary occurrence of term p53 might denote thousands of proteins across a number of species. A human annotator is able to resolve this ambiguity relatively easily, by looking at its context and if necessary, by searching an appropriate protein database. However, this phenomenon may cause much trouble to a text mining system, which does not understand human languages and hence can not identify the correct protein that the term refers to. In this paper, we present a Term Identification system which automatically assigns unique identifiers, as found in a protein database, to ambiguous protein mentions in texts. Unlike other solutions described in literature, which only work on gene/protein mentions on a specific model organism, our system is able to tackle protein mentions across many species, by integrating a machine-learning based species tagger. We have compared the performance of our automatic system to that of human annotators, with very promising results. © Springer-Verlag Berlin Heidelberg 2007.",2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,in biomedical article term often refer to different protein entity @ @ example @ arbitrary occurrence of term p might denote thousand of protein across a number of specie @ a human annotator is able to resolve @ ambiguity relatively easily by looking at @ context and if necessary by searching @ appropriate protein database @ however @ phenomenon may cause much trouble to a text mining system @ doe not understand human language and hence @ not identify @ correct protein @ @ term refers to @ in @ @ @ @ a term identification system @ automatically assigns unique identifier a found in a protein database to ambiguous protein mention in text @ unlike @ solution described in literature @ only work on gene protein mention on a specific model organism @ system is able to tackle protein mention across many specie by integrating a machine-learning based specie tagger @ @ @ compared @ performance of @ automatic system to @ of human annotator @ @ promising @ @ springer-verlag @ @ @ 
4125,Natural language processing and text mining,"With the increasing importance of the Web and other text-heavy application areas, the demands for and interest in both text mining and natural language processing (NLP) have been rising. Researchers in text mining have hoped that NLP-the attempt to extract a fuller meaning representation from free text-can provide useful improvements to text mining applications of all kinds. Bringing together a variety of perspectives from internationally renowned researchers, Natural Language Processing and Text Mining not only discusses applications of certain NLP techniques to certain Text Mining tasks, but also the converse, i.e., use of Text Mining to facilitate NLP. It explores a variety of real-world applications of NLP and text-mining algorithms in comprehensive detail, placing emphasis on the description of end-to-end solutions to real problems, and detailing the associated difficulties that must be resolved before the algorithm can be applied and its full benefits realized. In addition, it explores a number of cutting-edge techniques and approaches, as well as novel ways of integrating various technologies. Nevertheless, even readers with only a basic knowledge of data mining or text mining will benefit from the many illustrative examples and solutions. Topics and features: • Describes novel and high-impact text mining and/or natural language applications • Points out typical traps in trying to apply NLP to text mining • Illustrates preparation and preprocessing of text data - offering practical issues and examples • Surveys related supporting techniques, problem types, and potential technique enhancements • Examines the interaction of text mining and NLP This state-of-the-art, practical volume will be an essential resource for professionals and researchers who wish to learn how to apply text mining and language processing techniques to real world problems. In addition, it can be used as a supplementary text for advanced students studying text mining and NLP. © Springer-Verlag London Limited 2007.",2007,Natural Language Processing and Text Mining,135,@ @ increasing importance of @ web and @ text-heavy application area @ demand @ and interest in @ text mining and natural language processing @ nlp @ @ @ rising @ researcher in text mining @ hoped @ nlp-the attempt to extract a fuller meaning representation @ free text-can provide useful improvement to text mining application of @ kind @ bringing together a variety of perspective @ internationally renowned researcher natural language processing and text mining not only discus application of certain nlp technique to certain text mining task @ @ @ converse i @ e @ use of text mining to facilitate nlp @ @ explores a variety of real-world application of nlp and text-mining algorithm in comprehensive detail placing emphasis on @ description of end-to-end solution to real problem and detailing @ associated difficulty @ must @ resolved @ @ algorithm @ @ applied and @ full benefit realized @ in addition @ explores a number of cutting-edge technique and approach a well a novel way of integrating various technology @ nevertheless even reader @ only a basic knowledge of data mining @ text mining @ benefit @ @ many illustrative example and solution @ topic and feature @ describes novel and high-impact text mining and @ natural language application point @ typical trap in trying to apply nlp to text mining illustrates preparation and preprocessing of text data offering practical issue and example survey related supporting technique problem type and potential technique enhancement examines @ interaction of text mining and nlp @ state-of-the-art practical volume @ @ @ essential resource @ professional and researcher @ wish to learn @ to apply text mining and language processing technique to real world problem @ in addition @ @ @ used a a supplementary text @ advanced student studying text mining and nlp @ springer-verlag london limited @ 
4126,"Natural Language Processing and Information Systems - 12th International Conference on Applications of Natural Language to Information Systems, NLDB 2007, Proceedings",The proceedings contain 42 papers. The topics discussed include: an alternative approach to tagging; an efficient denotational semantics for natural language database queries; developing methods and heuristics with low time complexities for filtering spam messages; exploit semantic information for category annotation recommendation in wikipedia; a lightweight approach to semantic annotation of research papers; a new text clustering method using hidden markov model; identifying event sequences using hidden markov model; selecting labels for news document clusters; generating ontologies via language components and ontology reuse; experiences using the researchcyc upper level ontology; ontological text mining of software documents; treatment of passive voice and conjunctions in use case documents; and natural language processing and the conceptual model self-organizing map; and automatic issue extraction from a focused dialogue.,2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proceeding contain @ @ @ topic discussed include @ @ alternative approach to tagging @ @ efficient denotational semantics @ natural language database query @ developing method and heuristic @ low time complexity @ filtering spam message @ exploit semantic information @ category annotation recommendation in wikipedia @ a lightweight approach to semantic annotation of research @ @ a @ text clustering method @ hidden markov model @ identifying event sequence @ hidden markov model @ selecting label @ news document cluster @ generating ontology via language component and ontology reuse @ experience @ @ researchcyc upper level ontology @ ontological text mining of software document @ treatment of passive voice and conjunction in use case document @ and natural language processing and @ conceptual model self-organizing map @ and automatic issue extraction @ a focused dialogue @ 
4135,Graph decomposition approaches for terminology graphs,"We propose a graph-based decomposition methodology of a network of document features represented by a terminology graph. The graph is automatically extracted from raw data based on Natural Language Processing techniques implemented in the TermWatch system. These graphs are Small Worlds. Based on clique minimal separators and the associated graph of atoms: a subgraph without clique separator, we show that the terminology graph can be divided into a central kernel which is a single atom and a periphery made of small atoms. Moreover, the central kernel can be separated based on small optimal minimal separators. © Springer-Verlag Berlin Heidelberg 2007.",2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5,@ propose a graph-based decomposition methodology of a network of document feature represented by a terminology graph @ @ graph is automatically extracted @ raw data based on natural language processing technique implemented in @ termwatch system @ @ graph @ small world @ based on clique minimal separator and @ associated graph of atom @ a subgraph without clique separator @ @ @ @ terminology graph @ @ divided @ a central kernel @ is a single atom and a periphery made of small atom @ moreover @ central kernel @ @ separated based on small optimal minimal separator @ springer-verlag @ @ @ 
4136,Sentence boundary detection in conversational speech transcripts using noisily labeled examples,"This paper presents a technique for adding sentence boundaries to text obtained by Automatic Speech Recognition (ASR) of conversational speech audio. We show that starting with imprecise boundary information, added using only silence information from an ASR system, we can improve boundary detection using Head and Tail phrases. We develop our technique and show its effectiveness on two manually transcribed and one automatically transcribed corpus. The main purpose of adding sentence boundaries to ASR transcripts is to improve linguistic analysis, namely information extraction, for text mining systems that handle huge volumes of textual data and analyze trends and features of the concepts. Hence, we also show how the addition of boundaries improves two basic natural language processing tasks - PoS label assignment and adjective-noun extraction. © Springer-Verlag 2007.",2007,International Journal on Document Analysis and Recognition,2,@ @ @ a technique @ adding sentence boundary to text obtained by automatic speech recognition @ asr @ of conversational speech audio @ @ @ @ starting @ imprecise boundary information added @ only silence information @ @ asr system @ @ improve boundary detection @ head and tail phrase @ @ develop @ technique and @ @ effectiveness on @ manually transcribed and @ automatically transcribed corpus @ @ main purpose of adding sentence boundary to asr transcript is to improve linguistic analysis namely information extraction @ text mining system @ handle huge volume of textual data and analyze trend and feature of @ concept @ hence @ @ @ @ @ addition of boundary improves @ basic natural language processing task po label assignment and adjective-noun extraction @ springer-verlag @ 
4137,New functions for unsupervised asymmetrical paraphrase detection,"Monolingual text-to-text generation is an emerging research area in Natural Language Processing. One reason for the interest in such generation systems is the possibility to automatically learn text-to-text generation strategies from aligned monolingual corpora. In this context, paraphrase detection can be seen as the task of aligning sentences that convey the same information but yet are written in different forms, thereby building a training set of rewriting examples. In this paper, we propose a new type of mathematical functions for unsupervised detection of paraphrases, and test it over a set of standard paraphrase corpora. The results are promising as they outperform stateof- the-art functions developed for similar tasks. We consider two types of paraphrases - symmetrical and asymmetrical entailed - and show that although our proposed functions were conceived and oriented toward the asymmetrical detection, they perform rather well for symmetrical sentence pairs identification. © 2006 ACADEMY PUBLISHER.",2007,Journal of Software,20,monolingual text-to-text generation is @ emerging research area in natural language processing @ @ reason @ @ interest in @ generation system is @ possibility to automatically learn text-to-text generation strategy @ aligned monolingual corpus @ in @ context paraphrase detection @ @ seen a @ task of aligning sentence @ convey @ @ information @ yet @ written in different form thereby building a training set of rewriting example @ in @ @ @ propose a @ type of mathematical function @ unsupervised detection of paraphrase and test @ @ a set of standard paraphrase corpus @ @ @ @ promising a @ outperform stateof the-art function developed @ similar task @ @ consider @ type of paraphrase symmetrical and asymmetrical entailed and @ @ although @ proposed function @ conceived and oriented toward @ asymmetrical detection @ perform rather well @ symmetrical sentence pair identification @ academy publisher @ 
4140,"Evaluation of Multilingual and Multi-modal Information Retrieval - 7th Workshop of the Cross-Language Evaluation Forum, CLEF 2006, Revised Selected Papers",The proceedings contain 127 papers. The topics discussed include: what happened in CLEF 2006; query and document translation for English-Indonesian cross language IR; benefits of resource-based stemming in Hungarian information retrieval; comparing the robustness of expansion techniques and retrieval measures; local query expansion using terms windows for robust retrieval; reranking documents with antagonistic terms; domain specific retrieval: back to basics; providing multilingual access to FLICKR for Arabic users; overview of the CLEF 2006 multilingual question answering track; overview of the answer validation exercise 2006; re-ranking passages with LSA in a question answering system; question types specification for the use of specialized patterns in prodicos system; using syntactic knowledge for QA; question answering beyond CLEF document collections; and using machine learning and text mining in question answering.,2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,@ proceeding contain @ @ @ topic discussed include @ @ happened in clef @ query and document translation @ english-indonesian cross language ir @ benefit of resource-based stemming in hungarian information retrieval @ comparing @ robustness of expansion technique and retrieval measure @ local query expansion @ term window @ robust retrieval @ reranking document @ antagonistic term @ domain specific retrieval @ back to basic @ providing multilingual access to flickr @ arabic user @ overview of @ clef multilingual question answering track @ overview of @ answer validation exercise @ re-ranking passage @ lsa in a question answering system @ question type specification @ @ use of specialized pattern in prodicos system @ @ syntactic knowledge @ qa @ question answering beyond clef document collection @ and @ machine learning and text mining in question answering @ 
4143,Automatic NLP for competitive intelligence,"This chapter integrates elements from natural language processing, information retrieval, data mining and text mining to support competitive intelligence. It shows how text mining algorithms can attend to three important functionalities of CI: filtering, event alerts and search. Each of them can be mapped as a different pipeline of NLP tasks. The chapter goes in-depth in NLP techniques like spelling correction, stemming, augmenting, normalization, entity recognition, entity classification, acronyms and co-reference process. Each of them must be used in a specific moment to do a specific job. All these jobs will be integrated in a whole system. These will be 'assembled' in a manner specific to each application. The reader's better understanding of the theories of NLP provided herein will result in a better 'assembly'. © 2008 by IGI Global.",2007,Emerging Technologies of Text Mining: Techniques and Applications,2,@ chapter integrates element @ natural language processing information retrieval data mining and text mining to support competitive intelligence @ @ @ @ text mining algorithm @ attend to three important functionality of ci @ filtering event alert and search @ @ of @ @ @ mapped a a different pipeline of nlp task @ @ chapter go in-depth in nlp technique like spelling correction stemming augmenting normalization entity recognition entity classification acronym and co-reference process @ @ of @ must @ used in a specific moment to @ a specific job @ @ @ job @ @ integrated in a whole system @ @ @ @ @ assembled @ in a manner specific to @ application @ @ reader @ s better understanding of @ theory of nlp provided herein @ @ in a better @ assembly @ @ by igi global @ 
4144,Mining diagnostic text reports by learning to annotate knowledge roles,"Several tasks approached by using text mining techniques, like text categorization, document clustering, or information retrieval, operate on the document level, making use of the so-called bag-of-words model. Other tasks, like document summarization, information extraction, or question answering, have to operate on the sentence level, in order to fulfill their specific requirements. While both groups of text mining tasks are typically affected by the problem of data sparsity, this is more accentuated for the latter group of tasks. Thus, while the tasks of the first group can be tackled by statistical and machine learning methods based on a bag-of-words approach alone, the tasks of the second group need natural language processing (NLP) at the sentence or paragraph level in order to produce more informative features. © 2007 Springer-Verlag London Limited.",2007,Natural Language Processing and Text Mining,3,several task approached by @ text mining technique like text categorization document clustering @ information retrieval operate on @ document level making use of @ so-called bag-of-words model @ @ task like document summarization information extraction @ question answering @ to operate on @ sentence level in order to fulfill @ specific requirement @ @ @ group of text mining task @ typically affected by @ problem of data sparsity @ is more accentuated @ @ latter group of task @ thus @ @ task of @ first group @ @ tackled by statistical and machine learning method based on a bag-of-words approach alone @ task of @ second group need natural language processing @ nlp @ at @ sentence @ paragraph level in order to produce more informative feature @ springer-verlag london limited @ 
4147,Extracting relations from text: From word sequences to dependency paths,"Extracting semantic relationships between entities mentioned in text documents is an important task in natural language processing. The various types of relationships that are discovered between mentions of entities can provide useful structured information to a text mining system [1]. Traditionally, the task specifies a predefined set of entity types and relation types that are deemed to be relevant to a potential user and that are likely to occur in a particular text collection. For example, information extraction from newspaper articles is usually concerned with identifying mentions of people, organizations, locations, and extracting useful relations between them. Relevant relation types range from social relationships, to roles that people hold inside an organization, to relations between organizations, to physical locations of people and organizations. Scientific publications in the biomedical domain offer a type of narrative that is very different from the newspaper discourse. A significant effort is currently spent on automatically extracting relevant pieces of information from Medline, an online collection of biomedical abstracts. Proteins, genes, and cells are examples of relevant entities in this task, whereas subcellular localizations and protein-protein interactions are two of the relation types that have received significant attention recently. The inherent difficulty of the relation extraction task is further compounded in the biomedical domain by the relative scarcity of tools able to analyze the corresponding type of narrative. Most existing natural language processing tools, such as tokenizers, sentence segmenters, part-of-speech (POS) taggers, shallow or full parsers are trained on newspaper corpora, and consequently they inccur a loss in accuracy when applied to biomedical literature. Therefore, information extraction systems developed for biological corpora need to be robust to POS or parsing errors, or to give reasonable performance using shallower but more reliable information, such as chunking instead of full parsing. © 2007 Springer-Verlag London Limited.",2007,Natural Language Processing and Text Mining,18,extracting semantic relationship @ entity mentioned in text document is @ important task in natural language processing @ @ various type of relationship @ @ discovered @ mention of entity @ provide useful structured information to a text mining system @ traditionally @ task specifies a predefined set of entity type and relation type @ @ deemed to @ relevant to a potential user and @ @ likely to occur in a particular text collection @ @ example information extraction @ newspaper article is usually concerned @ identifying mention of people organization location and extracting useful relation @ @ @ relevant relation type range @ social relationship to role @ people hold inside @ organization to relation @ organization to physical location of people and organization @ scientific publication in @ biomedical domain offer a type of narrative @ is @ different @ @ newspaper discourse @ a significant effort is currently spent on automatically extracting relevant piece of information @ medline @ online collection of biomedical abstract @ protein gene and cell @ example of relevant entity in @ task whereas subcellular localization and protein-protein interaction @ @ of @ relation type @ @ received significant attention recently @ @ inherent difficulty of @ relation extraction task is @ compounded in @ biomedical domain by @ relative scarcity of tool able to analyze @ corresponding type of narrative @ @ existing natural language processing tool @ a tokenizers sentence segmenters part-of-speech @ po @ tagger shallow @ full parser @ trained on newspaper corpus and consequently @ inccur a loss in accuracy @ applied to biomedical literature @ therefore information extraction system developed @ biological corpus need to @ robust to po @ parsing error @ to give reasonable performance @ shallower @ more reliable information @ a chunking instead of full parsing @ springer-verlag london limited @ 
4148,Overview,"Text mining is the discovery and extraction of interesting, non-trivial knowledge from free or unstructured text. This encompasses everything from information retrieval (i.e., document or web site retrieval) to text classification and clustering, to (somewhat more recently) entity, relation, and event extraction. Natural language processing (NLP), is the attempt to extract a fuller meaning representation from free text. This can be put roughly as figuring out who did what to whom, when, where, how and why. NLP typically makes use of linguistic concepts such as part-of-speech (noun, verb, adjective, etc.) and grammatical structure (either represented as phrases like noun phrase or prepositional phrase, or dependency relations like subject-of or object-of). It has to deal with anaphora (what previous noun does a pronoun or other back-referring phrase correspond to) and ambiguities (both of words and of grammatical structure, such as what is being modified by a given word or prepositional phrase). To do this, it makes use of various knowledge representations, such as a lexicon of words and their meanings and grammatical properties and a set of grammar rules and often other resources such as an ontology of entities and actions, or a thesaurus of synonyms or abbreviations. © 2007 Springer-Verlag London Limited.",2007,Natural Language Processing and Text Mining,16,text mining is @ discovery and extraction of interesting non-trivial knowledge @ free @ unstructured text @ @ encompasses everything @ information retrieval @ i @ e @ document @ web site retrieval @ to text classification and clustering to @ somewhat more recently @ entity relation and event extraction @ natural language processing @ nlp @ is @ attempt to extract a fuller meaning representation @ free text @ @ @ @ put roughly a figuring @ @ @ @ to @ @ @ @ and @ @ nlp typically make use of linguistic concept @ a part-of-speech @ noun verb adjective etc @ @ and grammatical structure @ either represented a phrase like noun phrase @ prepositional phrase @ dependency relation like subject-of @ object-of @ @ @ ha to deal @ anaphora @ @ previous noun doe a pronoun @ @ back-referring phrase correspond to @ and ambiguity @ @ of word and of grammatical structure @ a @ is @ modified by a given word @ prepositional phrase @ @ to @ @ @ make use of various knowledge representation @ a a lexicon of word and @ meaning and grammatical property and a set of grammar rule and often @ resource @ a @ ontology of entity and action @ a thesaurus of synonym @ abbreviation @ springer-verlag london limited @ 
4158,A robust multilingual portable phrase chunking system,"Automatic text chunking aims to recognize grammatical phrase structures in natural language text. Text chunking provides downstream syntactic information for further analysis, which is also an important technology in the area of text mining (TM) and natural language processing (NLP). Existing chunking systems make use of external knowledge, e.g. grammar parsers, or integrate multiple learners to achieve higher performance. However, the external knowledge is almost unavailable in many domains and languages. Besides, employing multiple learners does not only complicate the system architecture, but also increase training and testing time costs. In this paper, we present a novel phrase chunking model based on the proposed mask method without employing external knowledge and multiple learners. The mask method could automatically derive more training examples from the original training data, which significantly improves system performance. We had evaluated our method in different chunking tasks and languages in comparison to previous studies. The experimental results show that our method achieves state of the art performance in chunking tasks. In two English chunking tasks, i.e., shallow parsing and base-chunking, our method achieves 94.22 and 93.23 in F(β=1) rates. When porting to Chinese, the F(β=1) rate is 92.30. Also, our chunker is quite efficient. The complete chunking time of a 50K-words is less than 10 s. © 2006 Elsevier Ltd. All rights reserved.",2007,Expert Systems with Applications,11,automatic text chunking aim to recognize grammatical phrase structure in natural language text @ text chunking provides downstream syntactic information @ @ analysis @ is @ @ important technology in @ area of text mining @ tm @ and natural language processing @ nlp @ @ existing chunking system make use of external knowledge e @ g @ grammar parser @ integrate multiple learner to achieve higher performance @ however @ external knowledge is almost unavailable in many domain and language @ besides employing multiple learner doe not only complicate @ system architecture @ @ increase training and testing time cost @ in @ @ @ @ a novel phrase chunking model based on @ proposed mask method without employing external knowledge and multiple learner @ @ mask method could automatically derive more training example @ @ original training data @ significantly improves system performance @ @ @ evaluated @ method in different chunking task and language in comparison to previous study @ @ experimental @ @ @ @ method achieves state of @ art performance in chunking task @ in @ english chunking task i @ e @ shallow parsing and base-chunking @ method achieves @ and @ in f @ β @ rate @ @ porting to chinese @ f @ β @ rate is @ @ @ @ chunker is quite efficient @ @ complete chunking time of a k-words is le @ s @ @ ltd @ @ right reserved @ 
4163,Overview and semantic issues of text mining,"Text mining refers to the discovery of previously unknown knowledge that can be found in text collections. In recent years, the text mining field has received great attention due to the abundance of textual data. A researcher in this area is requested to cope with issues originating from the natural language particularities. This survey discusses such semantic issues along with the approaches and methodologies proposed in the existing literature. It covers syntactic matters, tokenization concerns and it focuses on the different text representation techniques, categorisation tasks and similarity measures suggested.",2007,SIGMOD Record,80,text mining refers to @ discovery of @ unknown knowledge @ @ @ found in text collection @ in recent year @ text mining field ha received great attention due to @ abundance of textual data @ a researcher in @ area is requested to cope @ issue originating @ @ natural language particularity @ @ survey discus @ semantic issue along @ @ approach and methodology proposed in @ existing literature @ @ cover syntactic matter tokenization concern and @ focus on @ different text representation technique categorisation task and similarity measure suggested @ 
4164,Information access technologies for processing a very large number of natural language documents,"We have developed various information access technologies for information retrieval, information extraction (text mining), question answering, and document classification used in processing natural language documents. The effectiveness of these technologies was confirmed when they produced the highest level of precision in the NTCIR evaluation workshop. As the number of electronic documents continues to increase, these information access technologies will become increasingly useful.",2007,Journal of the National Institute of Information and Communications Technology,0,@ @ developed various information access technology @ information retrieval information extraction @ text mining @ question answering and document classification used in processing natural language document @ @ effectiveness of @ technology wa confirmed @ @ produced @ highest level of precision in @ ntcir evaluation workshop @ a @ number of electronic document continues to increase @ information access technology @ become increasingly useful @ 
4171,Information professionals in the text mine,"Text mining is growing rapidly for analyzing large volumes of unstructured textual documents. It was developed from computational linguistics, also known as natural language processing. The ultimate goal of computational linguistics is to develop mechanical language analysis tools that use statistical methods to interpret and assign meaning to parts of the text. Data mining can be used in clustering, question answering, and concept linkages. Clustering refers to seeing a large amount of information from a high-level perspective. Questioning answering is investigation of very specific questions at a level of detail not possible with standard text-searching techniques. Concept linkage connects related documents by identifying shared concepts between two unrelated data sets.",2007,"Online (Wilton, Connecticut)",4,text mining is growing rapidly @ analyzing @ volume of unstructured textual document @ @ wa developed @ computational linguistics @ known a natural language processing @ @ ultimate goal of computational linguistics is to develop mechanical language analysis tool @ use statistical method to interpret and assign meaning to part of @ text @ data mining @ @ used in clustering question answering and concept linkage @ clustering refers to seeing a @ amount of information @ a high-level perspective @ questioning answering is investigation of @ specific question at a level of detail not possible @ standard text-searching technique @ concept linkage connects related document by identifying shared concept @ @ unrelated data set @ 
4172,Constraint-based ontology induction from online customer reviews,"We present an unsupervised, domain-independent technique for inducing a product-specific ontology of product features based upon online customer reviews. We frame ontology induction as a logical assignment problem and solve it with a bounds consistency constrained logic program. Using shallow natural language processing techniques, reviews are parsed into phrase sequences where each phrase refers to a single concept. Traditional document clustering techniques are adapted to collect phrases into initial concepts. We generate a token graph for each initial concept cluster and find a maximal clique to define the corresponding logical set of concept sub-elements. The logic program assigns tokens to clique sub-elements. We apply the technique to several thousand digital camera customer reviews and evaluate the results by comparing them to the ontologies represented by several prominent online buying guides. Because our results are drawn directly from customer comments, differences between our automatically induced product features and those in extant guides may reflect opportunities for better managing customer-producer relationships rather than errors in the process. © 2006 Springer Science + Business Media B.V.",2007,Group Decision and Negotiation,11,@ @ @ unsupervised domain-independent technique @ inducing a product-specific ontology of product feature based upon online customer review @ @ frame ontology induction a a logical assignment problem and solve @ @ a bound consistency constrained logic program @ @ shallow natural language processing technique review @ parsed @ phrase sequence @ @ phrase refers to a single concept @ traditional document clustering technique @ adapted to collect phrase @ initial concept @ @ generate a token graph @ @ initial concept cluster and find a maximal clique to define @ corresponding logical set of concept sub-elements @ @ logic program assigns token to clique sub-elements @ @ apply @ technique to several thousand digital camera customer review and evaluate @ @ by comparing @ to @ ontology represented by several prominent online buying guide @ @ @ @ @ drawn directly @ customer comment difference @ @ automatically induced product feature and @ in extant guide may reflect opportunity @ better managing customer-producer relationship rather @ error in @ process @ @ science @ medium b @ v @ 
4177,Named entity recognition in Vietnamese documents,"Named Entity Recognition (NER) aims to classify words in a document into pre-defined target entity classes and is now considered to be fundamental for many natural language processing tasks such as information retrieval, machine translation, information extraction and question answering. This paper presents the results of an experiment in which a Support Vector Machine (SVM) based NER model is applied to the Vietnamese language. Though this state of the art machine learning method has been widely applied to NER in several well-studied languages, this is the first time this method has been applied to Vietnamese. In a comparison against Conditional Random Fields (CRFs) the SVM model was shown to outperform CRF by optimizing its feature window size, obtaining an overall F-score of 87.75. The paper also presents a detailed discussion about the characteristics of the Vietnamese language and provides an analysis of the factors which influence performance in this task. © 2007 National Instiute of Informatics.",2007,Progress in Informatics,13,named entity recognition @ ner @ aim to classify word in a document @ pre-defined target entity class and is now considered to @ fundamental @ many natural language processing task @ a information retrieval machine translation information extraction and question answering @ @ @ @ @ @ of @ experiment in @ a support vector machine @ svm @ based ner model is applied to @ vietnamese language @ though @ state of @ art machine learning method ha @ widely applied to ner in several well-studied language @ is @ first time @ method ha @ applied to vietnamese @ in a comparison @ conditional random field @ crfs @ @ svm model wa @ to outperform crf by optimizing @ feature window size obtaining @ overall f-score of @ @ @ @ @ @ a detailed discussion @ @ characteristic of @ vietnamese language and provides @ analysis of @ factor @ influence performance in @ task @ national instiute of informatics @ 
4178,Mining generalized associations of semantic relations from textual web content,"Traditional text mining techniques transform free text into flat bags of words representation, which does not preserve sufficient semantics for the purpose of knowledge discovery. In this paper, we present a two-step procedure to mine generalized associations of semantic relations conveyed by the textual content of Web documents. First, RDF (Resource Description Framework) metadata representing semantic relations are extracted from raw text using a myriad of natural language processing techniques. The relation extraction process also creates a term taxonomy in the form of a sense hierarchy inferred from WordNet. Then, a novel generalized association pattern mining algorithm (GP-Close) is applied to discover the underlying relation association patterns on RDF metadata. For pruning the large number of redundant overgeneralized patterns in relation pattern search space, the GP-Close algorithm adopts the notion of generalization closure for systematic overgeneralization reduction. The efficacy of our approach is demonstrated through empirical experiments conducted on an online database of terrorist activities. © 2007 IEEE.",2007,IEEE Transactions on Knowledge and Data Engineering,39,traditional text mining technique transform free text @ flat bag of word representation @ doe not preserve sufficient semantics @ @ purpose of knowledge discovery @ in @ @ @ @ a two-step procedure to mine generalized association of semantic relation conveyed by @ textual content of web document @ first rdf @ resource description framework @ metadata representing semantic relation @ extracted @ raw text @ a myriad of natural language processing technique @ @ relation extraction process @ creates a term taxonomy in @ form of a sense hierarchy inferred @ wordnet @ @ a novel generalized association pattern mining algorithm @ gp-close @ is applied to discover @ underlying relation association pattern on rdf metadata @ @ pruning @ @ number of redundant overgeneralized pattern in relation pattern search space @ gp-close algorithm adopts @ notion of generalization closure @ systematic overgeneralization reduction @ @ efficacy of @ approach is demonstrated @ empirical experiment conducted on @ online database of terrorist activity @ @ @ 
4179,Ontology design for biomedical text mining,"Text Mining in biology and biomedicine requires a large amount of domain-specific knowledge. Publicly accessible resources hold much of the information needed, yet their practical integration into natural language processing (NLP) systems is fraught with manifold hurdles, especially the problem of semantic disconnectedness throughout the various resources and components. Ontologies can provide the necessary framework for a consistent semantic integration, while additionally delivering formal reasoning capabilities to NLP. In this chapter, we address four important aspects relating to the integration of ontology and NLP: (i) An analysis of the different integration alternatives and their respective vantages; (ii) The design requirements for an ontology supporting NLP tasks; (iii) Creation and initialization of an ontology using publicly available tools and databases; and (iv) The connection of common NLP tasks with an ontology, including technical aspects of ontology deployment in a text mining framework. A concrete application example-text mining of enzyme mutations-is provided to motivate and illustrate these points. © 2007 Springer Science+Business Media, LLC. All rights reserved.",2007,Semantic Web: Revolutionizing Knowledge Discovery in the Life Sciences,18,text mining in biology and biomedicine requires a @ amount of domain-specific knowledge @ publicly accessible resource hold much of @ information needed yet @ practical integration @ natural language processing @ nlp @ system is fraught @ manifold hurdle especially @ problem of semantic disconnectedness throughout @ various resource and component @ ontology @ provide @ necessary framework @ a consistent semantic integration @ additionally delivering formal reasoning capability to nlp @ in @ chapter @ address four important aspect relating to @ integration of ontology and nlp @ @ i @ @ analysis of @ different integration alternative and @ respective vantage @ @ ii @ @ design requirement @ @ ontology supporting nlp task @ @ iii @ creation and initialization of @ ontology @ publicly available tool and database @ and @ @ @ @ connection of common nlp task @ @ ontology including technical aspect of ontology deployment in a text mining framework @ a concrete application example-text mining of enzyme mutations-is provided to motivate and illustrate @ point @ @ science @ medium llc @ @ right reserved @ 
4181,NLP-based curation of bacterial regulatory networks,"Manual curation of biological databases is an expensive and laborintensive process in Genomics and Systems Biology. We report the implementation of a state-of-the-art, rule-based Natural Language Processing system that creates computer-readable networks of regulatory interactions directly from abstracts and full-text papers. We evaluate its output against a manually-curated standard database, and test the possibilities and limitations of automatic and semi-automatic curation of the so-called biobibliome. We also propose a novel Regulatory Interaction Mining Markup Language suited for representing this data, useful both for biologists and for text-mining specialists. © Springer-Verlag Berlin Heidelberg 2007.",2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,manual curation of biological database is @ expensive and laborintensive process in genomics and system biology @ @ report @ implementation of a state-of-the-art rule-based natural language processing system @ creates computer-readable network of regulatory interaction directly @ abstract and full-text @ @ @ evaluate @ output @ a manually-curated standard database and test @ possibility and limitation of automatic and semi-automatic curation of @ so-called biobibliome @ @ @ propose a novel regulatory interaction mining markup language suited @ representing @ data useful @ @ biologist and @ text-mining specialist @ springer-verlag @ @ @ 
4184,Ontological text mining of software documents,"Documents written in natural languages constitute a major part of the software engineering lifecycle artifacts. Especially during software maintenance or reverse engineering, semantic information conveyed in these documents can provide important knowledge for the software engineer. In this paper, we present a text mining system capable of populating a software ontology with information detected in documents. © Springer-Verlag Berlin Heidelberg 2007.",2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,document written in natural language constitute a major part of @ software engineering lifecycle artifact @ especially @ software maintenance @ reverse engineering semantic information conveyed in @ document @ provide important knowledge @ @ software engineer @ in @ @ @ @ a text mining system capable of populating a software ontology @ information detected in document @ springer-verlag @ @ @ 
4186,Knowledge acquisition from the biomedical literature,"This article focuses on knowledge acquisition from the biomedical literature, and on the infrastructure, specifically text mining, needed to access, extract and integrate the information. The biomedical literature is the major repository of biomedical knowledge. It serves as the source for structured information that populates biological databases, via the process of expert distillation (or curation) of the literature. Today, the literature has grown to the point where an individual scientist cannot read all the relevant literature, and curators of the major biological databases have trouble keeping up to date with newly published articles. Furthermore, important biomedical applications, such as drug discovery and analysis of high-throughput data sets, are dependent on integration of all available information from both biological databases and the literature. The article reviews these applications, focusing on the role of text mining in providing semantic indices into the literature, as well as the importance of interactive tools to augment the power of the human expert to extract information from the literature. These tools are critical in supporting expert curation, finding relationships among biological entities, and creating content for a Semantic Web. © 2007 Springer Science+Business Media, LLC. All rights reserved.",2007,Semantic Web: Revolutionizing Knowledge Discovery in the Life Sciences,4,@ article focus on knowledge acquisition @ @ biomedical literature and on @ infrastructure specifically text mining needed to access extract and integrate @ information @ @ biomedical literature is @ major repository of biomedical knowledge @ @ serf a @ source @ structured information @ populates biological database via @ process of expert distillation @ @ curation @ of @ literature @ today @ literature ha grown to @ point @ @ individual scientist cannot read @ @ relevant literature and curator of @ major biological database @ trouble keeping up to date @ newly published article @ furthermore important biomedical application @ a drug discovery and analysis of high-throughput data set @ dependent on integration of @ available information @ @ biological database and @ literature @ @ article review @ application focusing on @ role of text mining in providing semantic index @ @ literature a well a @ importance of interactive tool to augment @ power of @ human expert to extract information @ @ literature @ @ tool @ critical in supporting expert curation finding relationship among biological entity and creating content @ a semantic web @ @ science @ medium llc @ @ right reserved @ 
4187,"Ontos solutions for semantic web: Text mining, navigation and analytics","This paper deals with the problem of development and implementation of semantic navigation through Web-content. Multi-agent architecture of a solution for Semantic Web and innovative services are presented. In the context of the proposed solution Web mining is carried out by special OntosMiner agents, which provide the ontology-driven processing of multilingual text collections on the basis of the special kind of content extraction technologies. First evaluation results of the presented solution are discussed as well. © Spnnger-Verlag Berlin Heidelberg 2007.",2007,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ @ deal @ @ problem of development and implementation of semantic navigation @ web-content @ multi-agent architecture of a solution @ semantic web and innovative service @ presented @ in @ context of @ proposed solution web mining is carried @ by special ontosminer agent @ provide @ ontology-driven processing of multilingual text collection on @ basis of @ special kind of content extraction technology @ first evaluation @ of @ presented solution @ discussed a well @ spnnger-verlag @ @ @ 
4190,Enhancing in automatic recognition and extraction of term variants with linguistic features,"The recognition and extraction of terms and their variants in texts are crucial processes in text mining. We use the ILC platform, an automatic controlled indexing platform, to perform these linguistic processes. We present a methodology for enhancing the recognition of syntactic term variation in English, using syntactic and morpho-syntactic features. Principal spurious variants of terms are ascribed to incorrect word dependencies. To overcome these problems, we consider each term variant as a window on the sentence and introduce two criteria: an internal syntactic criterion which checks that the dependencies between words in the window are respected, and an external criterion which defines boundaries, making it possible to ensure that the window is well positioned in the sentence. The use of these criteria improves filtering of the variants and assists the expert in validating the indexing.",2007,Terminology,5,@ recognition and extraction of term and @ variant in text @ crucial process in text mining @ @ use @ ilc platform @ automatic controlled indexing platform to perform @ linguistic process @ @ @ a methodology @ enhancing @ recognition of syntactic term variation in english @ syntactic and morpho-syntactic feature @ principal spurious variant of term @ ascribed to incorrect word dependency @ to overcome @ problem @ consider @ term variant a a window on @ sentence and introduce @ criterion @ @ internal syntactic criterion @ check @ @ dependency @ word in @ window @ respected and @ external criterion @ defines boundary making @ possible to ensure @ @ window is well positioned in @ sentence @ @ use of @ criterion improves filtering of @ variant and assist @ expert in validating @ indexing @ 
4192,Semantic web: Revolutionizing knowledge discovery in the life sciences,"The Semantic Web is now a research discipline in its own right and commercial interest in applications of Semantic Web technologies is strong. The advantages of the Semantic Web lie in its ability to present and provide access to complex knowledge in a standardized form making interoperability between distributed databases and middleware achievable. Life Scientists have much to gain from the emergence of the Semantic Web since their work is strongly knowledge-based. Unambiguous, semantically-rich, structured declarations of information have long been a fundamental cornerstone of scientific discourse. To have such information available in machine-readable form makes a whole new generation of scientific software possible. The value that the Semantic Web offers to the Life Sciences is currently under appreciated. A pedagogical oasis is required for interested scientists and bioinformatics professionals, where they can learn about and draw inspiration from the Semantic Web and its component technologies. In this context this book seeks to offer students, researchers, and professionals a glimpse of the technology, its capabilities and the reach of its current implementation in the Life Sciences. This collection of representative topics, written by leading experts, documents important and encouraging first steps showing the utility of the Semantic Web to Life Science research. Semantic Web: Revolutionizing Knowledge Discovery in Life Sciences is divided into six parts that cover the topics of: knowledge integration, knowledge representation, knowledge visualization, utilization of formal knowledge representations, and access to distributed knowledge. The final part considers the viability of the semantic web in life science and the legal challenges that will impact on its establishment. This book may be approached from technical, scientific or application specific perspectives. Component technologies of the Semantic Web (including RDF databases, ontologies, ontological languages, agent systems and web services) are described throughout the book. They are the basic building blocks for creating the Semantic Web infrastructure. Other technologies, such as natural language processing and text mining, which are becoming increasingly important to the Semantic Web, are also discussed. Scientists reading the book will see that the complex needs of biology and medicine are being addressed. Moreover, pioneering Life Scientists have joined forces with Semantic Web developers to build valuable semantic resources for the scientific community. Different areas of computer science (e.g., artificial intelligence, database integration, and visualization) are also being recruited to advance this vision. The ongoing synergy between the Life Sciences and Computer Science is poised to deliver revolutionary discovery tools and new capabilities. As well as providing the background material and critical evaluation criteria for the design and use of meaningful Semantic Web implementations a multitude of examples are provided. These illustrate the diversity of life science tasks that are benefiting from the use of Semantic Web infrastructure and serve to demonstrate the great potential of the Semantic Web in the Life Sciences. © 2007 Springer Science+Business Media, LLC. All rights reserved.",2007,Semantic Web: Revolutionizing Knowledge Discovery in the Life Sciences,46,@ semantic web is now a research discipline in @ @ right and commercial interest in application of semantic web technology is strong @ @ advantage of @ semantic web lie in @ ability to @ and provide access to complex knowledge in a standardized form making interoperability @ distributed database and middleware achievable @ life scientist @ much to gain @ @ emergence of @ semantic web since @ work is strongly knowledge-based @ unambiguous semantically-rich structured declaration of information @ long @ a fundamental cornerstone of scientific discourse @ to @ @ information available in machine-readable form make a whole @ generation of scientific software possible @ @ value @ @ semantic web offer to @ life science is currently @ appreciated @ a pedagogical oasis is required @ interested scientist and bioinformatics professional @ @ @ learn @ and draw inspiration @ @ semantic web and @ component technology @ in @ context @ book seek to offer student researcher and professional a glimpse of @ technology @ capability and @ reach of @ current implementation in @ life science @ @ collection of representative topic written by leading expert document important and encouraging first step showing @ utility of @ semantic web to life science research @ semantic web @ revolutionizing knowledge discovery in life science is divided @ six part @ cover @ topic of @ knowledge integration knowledge representation knowledge visualization utilization of formal knowledge representation and access to distributed knowledge @ @ final part considers @ viability of @ semantic web in life science and @ legal challenge @ @ impact on @ establishment @ @ book may @ approached @ technical scientific @ application specific perspective @ component technology of @ semantic web @ including rdf database ontology ontological language agent system and web service @ @ described throughout @ book @ @ @ @ basic building block @ creating @ semantic web infrastructure @ @ technology @ a natural language processing and text mining @ @ becoming increasingly important to @ semantic web @ @ discussed @ scientist reading @ book @ see @ @ complex need of biology and medicine @ @ addressed @ moreover pioneering life scientist @ joined force @ semantic web developer to build valuable semantic resource @ @ scientific community @ different area of computer science @ e @ g @ artificial intelligence database integration and visualization @ @ @ @ recruited to advance @ vision @ @ ongoing synergy @ @ life science and computer science is poised to deliver revolutionary discovery tool and @ capability @ a well a providing @ background material and critical evaluation criterion @ @ design and use of meaningful semantic web implementation a multitude of example @ provided @ @ illustrate @ diversity of life science task @ @ benefiting @ @ use of semantic web infrastructure and serve to demonstrate @ great potential of @ semantic web in @ life science @ @ science @ medium llc @ @ right reserved @ 
4193,Mining text with the prototype-matching method,"Text documents are the most common means for exchanging formal knowledge among people. Text is a rich medium that can contain a vast range of information, but text can be difficult to decipher automatically. Many organizations have vast repositories of textual data but with few means of automatically mining that text. Text mining methods seek to use an understanding of natural language text to extract information relevant to user needs. This article evaluates a new text mining methodology: prototype-matching for text clustering, developed by the authors' research group. The methodology was applied to four applications: clustering documents based on their abstracts, analyzing financial data, distinguishing authorship, and evaluating multiple translation similarity. The results are discussed in terms of common business applications and possible future research. © 2007, IGI Global.",2007,Information Resources Management Journal,2,text document @ @ @ common mean @ exchanging formal knowledge among people @ text is a rich medium @ @ contain a vast range of information @ text @ @ difficult to decipher automatically @ many organization @ vast repository of textual data @ @ @ mean of automatically mining @ text @ text mining method seek to use @ understanding of natural language text to extract information relevant to user need @ @ article evaluates a @ text mining methodology @ prototype-matching @ text clustering developed by @ author @ research group @ @ methodology wa applied to four application @ clustering document based on @ abstract analyzing financial data distinguishing authorship and evaluating multiple translation similarity @ @ @ @ discussed in term of common @ application and possible future research @ igi global @ 
4207,Ontology based text indexing and querying for the semantic web,"This publication shows how the gap between the HTML based internet and the RDF based vision of the semantic web might be bridged, by linking words in texts to concepts of ontologies. Most current search engines use indexes that are built at the syntactical level and return hits based on simple string comparisons. However, the indexes do not contain synonyms, cannot differentiate between homonyms ('mouse' as a pointing vs. 'mouse' as an animal) and users receive different search results when they use different conjugation forms of the same word. In this publication, we present a system that uses ontologies and Natural Language Processing techniques to index texts, and thus supports word sense disambiguation and the retrieval of texts that contain equivalent words, by indexing them to concepts of ontologies. For this purpose, we developed fully automated methods for mapping equivalent concepts of imported RDF ontologies (for this prototype WordNet, SUMO and OpenCyc). These methods will thus allow the seamless integration of domain specific ontologies for concept based information retrieval in different domains. To demonstrate the practical workability of this approach, a set of web pages that contain synonyms and homonyms were indexed and can be queried via a search engine like query frontend. However, the ontology based indexing approach can also be used for other data mining applications such text clustering, relation mining and for searching free text fields in biological databases. The ontology alignment methods and some of the text mining principles described in this publication are now incorporated into the ONDEX system http://ondex.sourceforge.net/. © 2006 Elsevier B.V. All rights reserved.",2006,Knowledge-Based Systems,66,@ publication @ @ @ gap @ @ html based internet and @ rdf based vision of @ semantic web might @ bridged by linking word in text to concept of ontology @ @ current search engine use index @ @ built at @ syntactical level and return hit based on simple string comparison @ however @ index @ not contain synonym cannot differentiate @ homonym @ @ mouse @ a a pointing v @ @ mouse @ a @ animal @ and user receive different search @ @ @ use different conjugation form of @ @ word @ in @ publication @ @ a system @ us ontology and natural language processing technique to index text and thus support word sense disambiguation and @ retrieval of text @ contain equivalent word by indexing @ to concept of ontology @ @ @ purpose @ developed fully automated method @ mapping equivalent concept of imported rdf ontology @ @ @ prototype wordnet sumo and opencyc @ @ @ method @ thus allow @ seamless integration of domain specific ontology @ concept based information retrieval in different domain @ to demonstrate @ practical workability of @ approach a set of web page @ contain synonym and homonym @ indexed and @ @ queried via a search engine like query frontend @ however @ ontology based indexing approach @ @ @ used @ @ data mining application @ text clustering relation mining and @ searching free text field in biological database @ @ ontology alignment method and some of @ text mining principle described in @ publication @ now incorporated @ @ ondex system http @ ondex @ sourceforge @ net @ @ b @ v @ @ right reserved @ 
4218,The development of a schema for the annotation of terms in the BioCaster disease detecting/tracking system,"Amid growing public concern about the spread of infectious diseases such as avian influenza and SARS, there is an increasing need for collecting timely and reliable information about disease outbreaks from natural language data such as online news articles. In this paper we introduce BioCaster, a text mining-based system for infectious disease detection and tracking currently being developed, and discuss the development of a domain ontology and schema for the annotation of terms. In particular we focus on the comparison between two approaches, 1) a traditional task-oriented approach with a simple schema that does not strictly follow ontological principles, and 2) a formal approach which is ontologically well-founded but adds extra requirements to the annotation schema. We report on several critical problems that were highlighted by an entity annotation experiment, attributable to the purely task-oriented ontology design. A second experiment based on a formally constructed ontology produced improved annotation results despite the apparent complexity of the annotation schema.",2006,CEUR Workshop Proceedings,15,amid growing public concern @ @ spread of infectious disease @ a avian influenza and sars @ is @ increasing need @ collecting timely and reliable information @ disease outbreak @ natural language data @ a online news article @ in @ @ @ introduce biocaster a text mining-based system @ infectious disease detection and tracking currently @ developed and discus @ development of a domain ontology and schema @ @ annotation of term @ in particular @ focus on @ comparison @ @ approach @ a traditional task-oriented approach @ a simple schema @ doe not strictly follow ontological principle and @ a formal approach @ is ontologically well-founded @ add extra requirement to @ annotation schema @ @ report on several critical problem @ @ highlighted by @ entity annotation experiment attributable to @ purely task-oriented ontology design @ a second experiment based on a formally constructed ontology produced improved annotation @ despite @ apparent complexity of @ annotation schema @ 
4219,"Ontology learning and population from text: Algorithms, evaluation and applications","Standard formalisms for knowledge representation such as RDFS or OWL have been recently developed by the semantic web community and are now in place. However, the crucial question still remains: how will we acquire all the knowledge available in people's heads to feed our machines? Natural language is THE means of communication for humans, and consequently texts are massively available on the Web. Terabytes and terabytes of texts containing opinions, ideas, facts and information of all sorts are waiting to be mined for interesting patterns and relationships, or used to annotate documents to facilitate their retrieval. A semantic web which ignores the massive amount of information encoded in text, might actually be a semantic, but not a very useful, web. Knowledge acquisition, and in particular ontology learning from text, actually has to be regarded as a crucial step within the vision of a semantic web. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications presents approaches for ontology learning from text and will be relevant for researchers working on text mining, natural language processing, information retrieval, semantic web and ontologies. Containing introductory material and a quantity of related work on the one hand, but also detailed descriptions of algorithms, evaluation procedures etc. on the other, this book is suitable for novices, and experts in the field, as well as lecturers. Datasets, algorithms and course material can be downloaded at http://www.cimiano.de/olp. Ontology Learning and Population from Text: Algorithms, Evaluation and Applications is designed for practitioners in industry, as well researchers and graduate-level students in computer science. © 2006 Springer Science+Business Media, LLC. All rights reserved.",2006,"Ontology Learning and Population from Text: Algorithms, Evaluation and Applications",320,standard formalism @ knowledge representation @ a rdfs @ owl @ @ recently developed by @ semantic web community and @ now in place @ however @ crucial question still remains @ @ @ @ acquire @ @ knowledge available in people @ s head to feed @ machine @ natural language is @ mean of communication @ human and consequently text @ massively available on @ web @ terabyte and terabyte of text containing opinion idea fact and information of @ sort @ waiting to @ mined @ interesting pattern and relationship @ used to annotate document to facilitate @ retrieval @ a semantic web @ ignores @ massive amount of information encoded in text might actually @ a semantic @ not a @ useful web @ knowledge acquisition and in particular ontology learning @ text actually ha to @ regarded a a crucial step within @ vision of a semantic web @ ontology learning and population @ text @ algorithm evaluation and application @ approach @ ontology learning @ text and @ @ relevant @ researcher working on text mining natural language processing information retrieval semantic web and ontology @ containing introductory material and a quantity of related work on @ @ hand @ @ detailed description of algorithm evaluation procedure etc @ on @ @ @ book is suitable @ novice and expert in @ field a well a lecturer @ datasets algorithm and course material @ @ downloaded at http @ www @ cimiano @ de olp @ ontology learning and population @ text @ algorithm evaluation and application is designed @ practitioner in industry a well researcher and graduate-level student in computer science @ @ science @ medium llc @ @ right reserved @ 
4224,Sentence similarity based on semantic nets and corpus statistics,"Sentence similarity measures play an increasingly Important role in text-related research and applications In areas such as text mining, Web page retrieval, and dialogue systems. Existing methods for computing sentence similarity have been adopted from approaches used for long text documents. These methods process sentences in a very high-dimensional space and are consequently Inefficient, require human Input, and are not adaptable to some application domains. This paper focuses directiy on computing the similarity between very short texts of sentence length. It presents an algorithm that takes account of semantic Information and word order Information Implied In the sentences. The semantic similarity of two sentences Is calculated using Information from a structured lexical database and from corpus statistics. The use of a lexical database enables our method to model human common sense knowledge and the Incorporation of corpus statistics allows our method to be adaptable to different domains. The proposed method can be used In a variety of applications that Involve text knowledge representation and discovery. Experiments on two sets of selected sentence pairs demonstrate that the proposed method provides a similarity measure that shows a significant correlation to human Intuition. © 2000 IEEE.",2006,IEEE Transactions on Knowledge and Data Engineering,561,sentence similarity measure play @ increasingly important role in text-related research and application in area @ a text mining web page retrieval and dialogue system @ existing method @ computing sentence similarity @ @ adopted @ approach used @ long text document @ @ method process sentence in a @ high-dimensional space and @ consequently inefficient require human input and @ not adaptable to some application domain @ @ @ focus directiy on computing @ similarity @ @ short text of sentence length @ @ @ @ algorithm @ take account of semantic information and word order information implied in @ sentence @ @ semantic similarity of @ sentence is calculated @ information @ a structured lexical database and @ corpus statistic @ @ use of a lexical database enables @ method to model human common sense knowledge and @ incorporation of corpus statistic allows @ method to @ adaptable to different domain @ @ proposed method @ @ used in a variety of application @ involve text knowledge representation and discovery @ experiment on @ set of selected sentence pair demonstrate @ @ proposed method provides a similarity measure @ @ a significant correlation to human intuition @ @ @ 
4229,From words to concepts in text mining,In the text mining process the interface between the natural language text and content reflecting terms is essential for the successful indexing process. The traditional interfaces such as the sauri and lexicons have limitations for web content processing. The recent lexical nets have promises; but their efficiency needs to be tested. The current work measures the use of word relations specified by lexical nets in a large test bed. The results correlate with similar recent past studies as not all specified word relations in the lexical nets are semantically rich in expressing the conceptual relations between given words. The results call for applying more heuristic approaches for text mining.,2006,Journal of Digital Information Management,0,in @ text mining process @ interface @ @ natural language text and content reflecting term is essential @ @ successful indexing process @ @ traditional interface @ a @ sauri and lexicon @ limitation @ web content processing @ @ recent lexical net @ promise @ @ @ efficiency need to @ tested @ @ current work measure @ use of word relation specified by lexical net in a @ test bed @ @ @ correlate @ similar recent past study a not @ specified word relation in @ lexical net @ semantically rich in expressing @ conceptual relation @ given word @ @ @ call @ applying more heuristic approach @ text mining @ 
4235,Arabic proper names: Extraction and classification,"Extracting and classifying proper names represent an important key element for improving the efficiency and the performance of many applications in the area of natural language processing and text mining. The valuable information in the text usually is located around proper names, to collect this information it should be found first. By extracting proper names from the text we provide thus applications with both the proper name found in the text, some information about it and where it was found. The proper names in Arabic do not start with capital letter as in many other languages so special treatment is needed to find them in a text. Little research has been conducted in this area; most efforts have been based on a number of heuristic rules used to find names in the text, some used graphs to represent the words that might form a name and the relationships between them, and some they use statistical methods for this reason. In this paper we present a new technique to extract names from text by using a hybrid system built based on both statistical methods and predefined rules. First we mark proper name phrases in the text that might include names, second we use some statistical methods to extract proper name from them, and third we classify the proper name with respect to its major class and its subclass. We have generated different rules and we put different assumptions to accomplish the goal of this research.",2006,WSEAS Transactions on Computers,0,extracting and classifying proper name represent @ important key element @ improving @ efficiency and @ performance of many application in @ area of natural language processing and text mining @ @ valuable information in @ text usually is located around proper name to collect @ information @ @ @ found first @ by extracting proper name @ @ text @ provide thus application @ @ @ proper name found in @ text some information @ @ and @ @ wa found @ @ proper name in arabic @ not start @ capital letter a in many @ language @ special treatment is needed to find @ in a text @ little research ha @ conducted in @ area @ @ effort @ @ based on a number of heuristic rule used to find name in @ text some used graph to represent @ word @ might form a name and @ relationship @ @ and some @ use statistical method @ @ reason @ in @ @ @ @ a @ technique to extract name @ text by @ a hybrid system built based on @ statistical method and predefined rule @ first @ mark proper name phrase in @ text @ might include name second @ use some statistical method to extract proper name @ @ and third @ classify @ proper name @ respect to @ major class and @ subclass @ @ @ generated different rule and @ put different assumption to accomplish @ goal of @ research @ 
4239,Improving text mining with controlled natural language: A case study for protein interactions,"Linking the biomedical literature to other data resources is notoriously difficult and requires text mining. Text mining aims to automatically extract facts from literature. Since authors write in natural language, text mining is a great natural language processing challenge, which is far from being solved. We propose an alternative: If authors and editors summarize the main facts in a controlled natural language, text mining will become easier and more powerful. To demonstrate this approach, we use the language Attempto Controlled English (ACE). We define a simple model to capture the main aspects of protein interactions. To evaluate our approach, we collected a dataset of 459 paragraph headings about protein interaction from literature. 56% of these headings can be represented exactly in ACE and another 23% partially. These results indicate that our approach is feasible. © Springer-Verlag Berlin Heidelberg 2006.",2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),18,linking @ biomedical literature to @ data resource is notoriously difficult and requires text mining @ text mining aim to automatically extract fact @ literature @ since author write in natural language text mining is a great natural language processing challenge @ is far @ @ solved @ @ propose @ alternative @ if author and editor summarize @ main fact in a controlled natural language text mining @ become easier and more powerful @ to demonstrate @ approach @ use @ language attempto controlled english @ ace @ @ @ define a simple model to capture @ main aspect of protein interaction @ to evaluate @ approach @ collected a dataset of paragraph heading @ protein interaction @ literature @ of @ heading @ @ represented exactly in ace and another partially @ @ @ indicate @ @ approach is feasible @ springer-verlag @ @ @ 
4240,"Advances in Natural Language Processing 5th International Conference on NLP, FinTAL 2006, Proceedings",The proceedings contain 75 papers. The topics discussed include: recursion in natural languages; the explanatory combinatorial dictionary as the key tool in machine translation; a computational implementation of internally headed relative clause constructions; a straightforward method for automatic identification of marginalized languages; a text mining approach for definition question answering; applying latent Dirichlet allocation to automatic essay grading; automatic feature extraction for question classification based on dissimilarity of probability distributions; document clustering based on maximal frequent sequences; enriching thesauri with hierarchical relationships by pattern matching in dictionaries; experiments in passage selection and answer identification for question answering; extracting term collocations for directing users to informative Web pages; and improving term extraction with terminological resources.,2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proceeding contain @ @ @ topic discussed include @ recursion in natural language @ @ explanatory combinatorial dictionary a @ key tool in machine translation @ a computational implementation of internally headed relative clause construction @ a straightforward method @ automatic identification of marginalized language @ a text mining approach @ definition question answering @ applying latent dirichlet allocation to automatic essay grading @ automatic feature extraction @ question classification based on dissimilarity of probability distribution @ document clustering based on maximal frequent sequence @ enriching thesaurus @ hierarchical relationship by pattern matching in dictionary @ experiment in passage selection and answer identification @ question answering @ extracting term collocation @ directing user to informative web page @ and improving term extraction @ terminological resource @ 
4243,Semantic scoring based on small-world phenomenon for feature selection in text mining,"This paper proposes an effective scoring scheme for feature selection in Text Mining, using characteristics of Small-World Phenomenon on the semantic networks of documents. Our focus is on the reservation of both syntactic and statistical information of words, rather than solely simple frequency summarization in prevailing scoring schemes, such as TFIDF. Experimental results on TREC dataset show that our scoring scheme outperforms the prevailing schemes. © Springer-Verlag Berlin Heidelberg 2006.",2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,@ @ proposes @ effective scoring scheme @ feature selection in text mining @ characteristic of small-world phenomenon on @ semantic network of document @ @ focus is on @ reservation of @ syntactic and statistical information of word rather @ solely simple frequency summarization in prevailing scoring scheme @ a tfidf @ experimental @ on trec dataset @ @ @ scoring scheme outperforms @ prevailing scheme @ springer-verlag @ @ @ 
4244,Automatic keyphrases extraction from document using neural network,"Keyphrase extraction is a task with many applications in information retrieval, text mining, and natural language processing. In this paper, a keyphrase extraction approach based on neural network is proposed. To determine whether a phrase is a keyphrase, the following features of a phrase in a given document arc adopted: its term frequency and inverted document frequency, whether to appear in the title or headings (subheadings) of the given document, and its frequency appearing in the paragraphs of the given document. The algorithm is evaluated by the standard information retrieval metrics of precision and recall, mid human assessment. © Springer-Verlag Berlin Heidelberg 2006.",2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),28,keyphrase extraction is a task @ many application in information retrieval text mining and natural language processing @ in @ @ a keyphrase extraction approach based on neural network is proposed @ to determine whether a phrase is a keyphrase @ following feature of a phrase in a given document arc adopted @ @ term frequency and inverted document frequency whether to appear in @ title @ heading @ subheading @ of @ given document and @ frequency appearing in @ paragraph of @ given document @ @ algorithm is evaluated by @ standard information retrieval metric of precision and recall mid human assessment @ springer-verlag @ @ @ 
4245,Ensemble learning for keyphrases extraction from scientific document,"Keyphrase extraction is a task with many applications in information retrieval, text mining, and natural language processing. In this paper, a keyphrase extraction approach based on neural network ensemble is proposed. To determine whether a phrase is a keyphrase, the following features of a phrase in a given document are adopted: its term frequency, whether to appear in the title, abstract or headings (subheadings), and its frequency appearing in the paragraphs of the given document. The approach is evaluated by the standard information retrieval metrics of precision and recall. Experiment results show that the ensemble learning can significantly increase the precision and recall. © Springer-Verlag Berlin Heidelberg 2006.",2006,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,keyphrase extraction is a task @ many application in information retrieval text mining and natural language processing @ in @ @ a keyphrase extraction approach based on neural network ensemble is proposed @ to determine whether a phrase is a keyphrase @ following feature of a phrase in a given document @ adopted @ @ term frequency whether to appear in @ title abstract @ heading @ subheading @ and @ frequency appearing in @ paragraph of @ given document @ @ approach is evaluated by @ standard information retrieval metric of precision and recall @ experiment @ @ @ @ ensemble learning @ significantly increase @ precision and recall @ springer-verlag @ @ @ 
4251,INAOE at CLEF 2006: Experiments in Spanish Question Answering,"This paper describes the system developed by the Language Technologies Lab at INAOE for the Spanish Question Answering task at CLEF 2006. The presented system is centered in a full datadriven architecture that uses machine learning and text mining techniques to identify the most probable answers to factoid and definition questions respectively. Its major quality is that it mainly relies on the use of lexical information and avoids applying any complex language processing resource such as named entity classifiers, parsers or ontologies. Experimental results show that the proposed architecture can be a practical solution for monolingual question answering reaching an answer precision as high as 51%.",2006,CEUR Workshop Proceedings,1,@ @ describes @ system developed by @ language technology lab at inaoe @ @ spanish question answering task at clef @ @ presented system is centered in a full datadriven architecture @ us machine learning and text mining technique to identify @ @ probable answer to factoid and definition question respectively @ @ major quality is @ @ mainly relies on @ use of lexical information and avoids applying @ complex language processing resource @ a named entity classifier parser @ ontology @ experimental @ @ @ @ proposed architecture @ @ a practical solution @ monolingual question answering reaching @ answer precision a high a @ 
4256,Sequential patterns for text categorization,"Text categorization is a well-known task based essentially on statistical approaches using neural networks, Support Vector Machines and other machine learning algorithms. Texts are generally considered as bags of words without any order. Although these approaches have proven to be efficient, they do not provide users with comprehensive and reusable rules about their data. Such rules are, however, very important for users to describe trends in the data they have to analyze. In this framework, an association-rule based approach has been proposed by Bing Liu (CBA). We propose, in this paper, to extend this approach by using sequential patterns in the SPaC method (Sequential Patterns for Classification) for text categorization. Taking order into account allows us to represent the succession of words through a document without complex and time-consuming representations and treatments such as those performed in natural language and grammatical methods. The original method we propose here consists in mining sequential patterns in order to build a classifier. We experimentally show that our proposal is relevant, and that it is very interesting compared to other methods. In particular, our method outperforms CBA and provides better results than SVM on some corpus. © 2006-IOS Press and the authors.",2006,Intelligent Data Analysis,29,text categorization is a well-known task based essentially on statistical approach @ neural network support vector machine and @ machine learning algorithm @ text @ generally considered a bag of word without @ order @ although @ approach @ proven to @ efficient @ @ not provide user @ comprehensive and reusable rule @ @ data @ @ rule @ however @ important @ user to describe trend in @ data @ @ to analyze @ in @ framework @ association-rule based approach ha @ proposed by bing liu @ cba @ @ @ propose in @ @ to extend @ approach by @ sequential pattern in @ spac method @ sequential pattern @ classification @ @ text categorization @ taking order @ account allows u to represent @ succession of word @ a document without complex and time-consuming representation and treatment @ a @ performed in natural language and grammatical method @ @ original method @ propose @ consists in mining sequential pattern in order to build a classifier @ @ experimentally @ @ @ proposal is relevant and @ @ is @ interesting compared to @ method @ in particular @ method outperforms cba and provides better @ @ svm on some corpus @ io @ and @ author @ 
4263,Ontology driven text mining for cost management processes,"In this article a semantic based software system for the management and monitoring of enterprise purchase processes is described and a paradigmatic case study (the Creactive Consulting S.p.A company that have developed the system) is presented. The system enables purchaser officers to search products through a semantic based engine, and navigate a semantic based catalogue in order to electronically buy the more suitable (less expensive) products. This system is based on a domain-specific ontological model, developed according to a structured representation of purchasable items. In the following paragraphs some of the difficulties that has been overcame will be described. In particular the pre-analysis - through text-mining techniques - of a system of documents written in natural language (that it is used to unveil concepts), and the definition of the notion of ""functional equivalence"" between items (that it is used to effectively compare products) will be deeply analyzed.",2005,CEUR Workshop Proceedings,0,in @ article a semantic based software system @ @ management and monitoring of enterprise purchase process is described and a paradigmatic case study @ @ creactive consulting s @ p @ a company @ @ developed @ system @ is presented @ @ system enables purchaser officer to search product @ a semantic based engine and navigate a semantic based catalogue in order to electronically buy @ more suitable @ le expensive @ product @ @ system is based on a domain-specific ontological model developed according to a structured representation of purchasable item @ in @ following paragraph some of @ difficulty @ ha @ overcame @ @ described @ in particular @ pre-analysis @ text-mining technique of a system of document written in natural language @ @ @ is used to unveil concept @ and @ definition of @ notion of @ functional equivalence @ @ item @ @ @ is used to effectively compare product @ @ @ deeply analyzed @ 
4270,Detecting the emergence of new concepts in web communities,"This paper describes a methodology to detect the emergence (or the disappearance) of concepts through the observation of natural language communications (NLC). NLC are the documents, e-mails, written communications of any kind, that the members of a web community produce, access, and exchange for their purposes. The emergence of a new concept is suggested by the repetitive and consistent use of certain terms, while its intended meaning and appropriate conceptualization is obtained through a combination of text mining and algebraic methods.",2005,CEUR Workshop Proceedings,0,@ @ describes a methodology to detect @ emergence @ @ @ disappearance @ of concept @ @ observation of natural language communication @ nlc @ @ nlc @ @ document e-mail written communication of @ kind @ @ member of a web community produce access and exchange @ @ purpose @ @ emergence of a @ concept is suggested by @ repetitive and consistent use of certain term @ @ intended meaning and appropriate conceptualization is obtained @ a combination of text mining and algebraic method @ 
4271,Recognizing noun phrases in biomedical text: An evaluation of lab prototypes and commercial chunkers,"In the biomedical domain, many systems for text mining and information extraction rely on basic morphological and syntactic analysis such as part-of-speech tagging or noun phrase (NP) chunking. Due to the lack of sufficient in-domain resources these systems often make use of NLP tools trained and evaluated on newspaper-language training sets. Scientific texts in the life sciences, however, differ from general language in the structure and complexity of noun phrases. Therefore, we tested the effects this domain change has on the performance of these systems. For this purpose, we compared three prototype chunking systems developed in research labs (all based on statistical learning methods) and one chunking system which is part of a commercial information extraction toolkit (based on manually supplied grammar specifications). Trained on PENN TREEBANK tagging and chunking annotations for newspapers, we ran these systems on the GENIA treebank which contains such annotations for biological abstracts taken from MED-LINE. We, first, observed a significant over-all loss in performance (on the order of 4%) and, second, found (with the exception of the SVM-based system) no significant difference between the performance of lab prototypes and the commerical chunker on GENIA data. Fortunately, the performance loss can also be partly remedied by few biomedical domain-specific adaptations.",2005,CEUR Workshop Proceedings,0,in @ biomedical domain many system @ text mining and information extraction rely on basic morphological and syntactic analysis @ a part-of-speech tagging @ noun phrase @ np @ chunking @ due to @ lack of sufficient in-domain resource @ system often make use of nlp tool trained and evaluated on newspaper-language training set @ scientific text in @ life science however differ @ general language in @ structure and complexity of noun phrase @ therefore @ tested @ effect @ domain change ha on @ performance of @ system @ @ @ purpose @ compared three prototype chunking system developed in research lab @ @ based on statistical learning method @ and @ chunking system @ is part of a commercial information extraction toolkit @ based on manually supplied grammar specification @ @ trained on penn treebank tagging and chunking annotation @ newspaper @ ran @ system on @ genia treebank @ contains @ annotation @ biological abstract taken @ med-line @ @ first observed a significant over-all loss in performance @ on @ order of @ and second found @ @ @ exception of @ svm-based system @ no significant difference @ @ performance of lab prototype and @ commerical chunker on genia data @ fortunately @ performance loss @ @ @ partly remedied by @ biomedical domain-specific adaptation @ 
4288,Evolving text classification rules with genetic programming,We describe a novel method for using genetic programming to create compact classification rules using combinations of N-grams (character strings). Genetic programs acquire fitness by producing rules that are effective classifiers in terms of precision and recall when evaluated against a set of training documents. We describe a set of functions and terminals and provide results from a classification task using the Reuters 21578 dataset. We also suggest that the rules may have a number of other uses beyond classification and provide a basis for text mining applications. Copyright © 2005 Taylor & Francis Inc.,2005,Applied Artificial Intelligence,6,@ describe a novel method @ @ genetic programming to create compact classification rule @ combination of n-grams @ character string @ @ genetic program acquire fitness by producing rule @ @ effective classifier in term of precision and recall @ evaluated @ a set of training document @ @ describe a set of function and terminal and provide @ @ a classification task @ @ reuters dataset @ @ @ suggest @ @ rule may @ a number of @ us beyond classification and provide a basis @ text mining application @ @ taylor francis inc @ 
4290,A hybrid approach to protein name identification in biomedical texts,"This paper presents a hybrid approach to identifying protein names in biomedical texts, which is regarded as a crucial step for text mining. Our approach employs a set of simple heuristics for initial detection of protein names and uses a probabilistic model for locating complete protein names. In addition, a protein name dictionary is complementarily consulted. In contrast to previously proposed methods, our proposed method avoids the use of natural language processing tools such as part-of-speech taggers and syntactic parsers and solely relies on surface clues, so as to reduce the processing overhead. Moreover, we propose a framework to automatically create a large-scale corpus annotated with protein names, which can be then used for training our probabilistic model. We implemented a protein name identification system, named PROTEX, based on our proposed method and evaluated it by comparing with a system developed by other researchers on a common test set. The experiments showed that the automatically constructed corpus is equally useful in training as compared with manually annotated corpora and that effective performance can be achieved in identifying compound protein names with PROTEX. © 2004 Elsevier Ltd. All rights reserved.",2005,Information Processing and Management,19,@ @ @ a hybrid approach to identifying protein name in biomedical text @ is regarded a a crucial step @ text mining @ @ approach employ a set of simple heuristic @ initial detection of protein name and us a probabilistic model @ locating complete protein name @ in addition a protein name dictionary is complementarily consulted @ in contrast to @ proposed method @ proposed method avoids @ use of natural language processing tool @ a part-of-speech tagger and syntactic parser and solely relies on surface clue @ a to reduce @ processing overhead @ moreover @ propose a framework to automatically create a large-scale corpus annotated @ protein name @ @ @ @ used @ training @ probabilistic model @ @ implemented a protein name identification system named protex based on @ proposed method and evaluated @ by comparing @ a system developed by @ researcher on a common test set @ @ experiment showed @ @ automatically constructed corpus is equally useful in training a compared @ manually annotated corpus and @ effective performance @ @ achieved in identifying compound protein name @ protex @ @ ltd @ @ right reserved @ 
4296,Newsmap: A knowledge map for online news,"Information technology has made possible the capture and accessing of a large number of data and knowledge bases, which in turn has brought about the problem of information overload. Text mining to turn textual information into knowledge has become a very active research area, but much of the research remains restricted to the English language. Due to the differences in linguistic characteristics and methods of natural language processing, many existing text analysis approaches have yet to be shown to be useful for the Chinese language. This research focuses on the automatic generation of a hierarchical knowledge map NewsMap, based on online Chinese news, particularly the finance and health sections. Whether in print or online, news still represents one important knowledge source that people produce and consume on a daily basis. The hierarchical knowledge map can be used as a tool for browsing business intelligence and medical knowledge hidden in news articles. In order to assess the quality of the map, an empirical study was conducted which shows that the categories of the hierarchical knowledge map generated by NewsMap are better than those generated by regular news readers, both in terms of recall and precision, on the sub-level categories but not on the top-level categories. NewsMap employs an improved interface combining a 1D alphabetical hierarchical list and a 2D Self-Organizing Map (SOM) island display. Another empirical study compared the two visualization displays and found that users' performances can be improved by taking advantage of the visual cues of the 2D SOM display. © 2004 Elsevier B.V. All rights reserved.",2005,Decision Support Systems,99,information technology ha made possible @ capture and accessing of a @ number of data and knowledge base @ in turn ha brought @ @ problem of information overload @ text mining to turn textual information @ knowledge ha become a @ active research area @ much of @ research remains restricted to @ english language @ due to @ difference in linguistic characteristic and method of natural language processing many existing text analysis approach @ yet to @ @ to @ useful @ @ chinese language @ @ research focus on @ automatic generation of a hierarchical knowledge map newsmap based on online chinese news particularly @ finance and health section @ whether in print @ online news still represents @ important knowledge source @ people produce and consume on a daily basis @ @ hierarchical knowledge map @ @ used a a tool @ browsing @ intelligence and medical knowledge hidden in news article @ in order to ass @ quality of @ map @ empirical study wa conducted @ @ @ @ category of @ hierarchical knowledge map generated by newsmap @ better @ @ generated by regular news reader @ in term of recall and precision on @ sub-level category @ not on @ top-level category @ newsmap employ @ improved interface combining a @ alphabetical hierarchical list and a @ self-organizing map @ som @ island display @ another empirical study compared @ @ visualization display and found @ user @ performance @ @ improved by taking advantage of @ visual cue of @ @ som display @ @ b @ v @ @ right reserved @ 
4304,Survey of text mining,"Text mining, also known as text data mining or text knowledge discovery, means discovering implicit, previously unknown, and potentially useful patterns in large amounts of text. In this paper, the text mining is first introduced including its definition, characteristics, and the relationship with other research fields such as data mining, information retrieval, information extraction, computational linguistics etc. Then, text mining models, text characteristics extracting and intermediate representation forms, and the technology of text mining's classification and realization are presented. Finally, some products of text mining are introduced.",2005,Moshi Shibie yu Rengong Zhineng/Pattern Recognition and Artificial Intelligence,2,text mining @ known a text data mining @ text knowledge discovery mean discovering implicit @ unknown and potentially useful pattern in @ amount of text @ in @ @ @ text mining is first introduced including @ definition characteristic and @ relationship @ @ research field @ a data mining information retrieval information extraction computational linguistics etc @ @ text mining model text characteristic extracting and intermediate representation form and @ technology of text mining @ s classification and realization @ presented @ finally some product of text mining @ introduced @ 
4307,"Text mining, names and security","A Process Query System, a new approach to representing and querying multiple hypotheses, is proposed for cross-document co-reference and linking based on existing entity extraction, coreference and database name-matching technologies. A crucial component of linking entities across documents is the ability to recognize when different name strings are potential references to the same entity. Given the extraordinary range of variation international names can take when rendered in the Roman alphabet, this is a daunting task. The extension of name variant matching to free text will add important text mining functionality for intelligence and security informatics' toolkits.",2005,Journal of Database Management,6,a process query system a @ approach to representing and querying multiple hypothesis is proposed @ cross-document co-reference and linking based on existing entity extraction coreference and database name-matching technology @ a crucial component of linking entity across document is @ ability to recognize @ different name string @ potential reference to @ @ entity @ given @ extraordinary range of variation international name @ take @ rendered in @ roman alphabet @ is a daunting task @ @ extension of name variant matching to free text @ add important text mining functionality @ intelligence and security informatics @ toolkits @ 
4310,Data mining method from text database,"Recently, various types of data are expected to get in information processing according to multi-media technology. Especially, linguistic data are employed in fuzzy systems as well as fuzzy numerical values. In this paper we propose a text minig method based on fuzzy quantification model. In the process of text mining, we will pursue the following steps: 1) Sentences included in a text in Japanese are broken down into words. 2) It is possible to realize common understanding using fuzzy thesaurus that enables us to translate words into synonyms or into upper concepts. In this paper, we employ the method to translate words using Chinese characters or continuous letters of Katakana more then one katakana letter (Japanese alphabet letter) into keywords. The method realizes the high speed of processing without any dictionary for separating words. Fuzzy multivariate analysis is employed to analyze such processed data and to abstract a latent mutual related structure under the data. In other words, we abstract the knowledge from the given text data. At the end we apply the method to mining the text information of libraries and Web pages distributed over a web network and discussing about the application to Kansei engineering. © Springer-Verlag Berlin Heidelberg 2005.",2005,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,recently various type of data @ expected to get in information processing according to multi-media technology @ especially linguistic data @ employed in fuzzy system a well a fuzzy numerical value @ in @ @ @ propose a text minig method based on fuzzy quantification model @ in @ process of text mining @ @ pursue @ following step @ @ sentence included in a text in japanese @ broken down @ word @ @ @ is possible to realize common understanding @ fuzzy thesaurus @ enables u to translate word @ synonym @ @ upper concept @ in @ @ @ employ @ method to translate word @ chinese character @ continuous letter of katakana more @ @ katakana letter @ japanese alphabet letter @ @ keywords @ @ method realizes @ high speed of processing without @ dictionary @ separating word @ fuzzy multivariate analysis is employed to analyze @ processed data and to abstract a latent mutual related structure @ @ data @ in @ word @ abstract @ knowledge @ @ given text data @ at @ end @ apply @ method to mining @ text information of library and web page distributed @ a web network and discussing @ @ application to kansei engineering @ springer-verlag @ @ @ 
4313,Word segmentation and POS tagging for Chinese keyphrase extraction,"Keyphrases are essential for many text mining applications. In order to automatically extracting keyphrases from Chinese text, an extraction system is proposed in this paper. To access a particular problem of Chinese information processing, a lexicon-based word segmentation approach is presented. For this purpose, a verb lexicon, a functional word lexicon and a stop word lexicon are constructed. A predefined keyphrase lexicon is applied to improve the performance of extraction. The approach uses a small Part-Of-Speech(POS) tagset to index phrases simply according to these lexicons. It is especially effective for identifying phrases in form of combinations of nouns, adjectives and verbs. Keyphrases are sifted by their weighted TF-IDF (Term occurrence Frequency-Inverse Document Frequency) values. New keyphrases are added into the keyphrase lexicon. © Springer-Verlag Berlin Heidelberg 2005.",2005,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,keyphrases @ essential @ many text mining application @ in order to automatically extracting keyphrases @ chinese text @ extraction system is proposed in @ @ @ to access a particular problem of chinese information processing a lexicon-based word segmentation approach is presented @ @ @ purpose a verb lexicon a functional word lexicon and a stop word lexicon @ constructed @ a predefined keyphrase lexicon is applied to improve @ performance of extraction @ @ approach us a small part-of-speech @ po @ tagset to index phrase simply according to @ lexicon @ @ is especially effective @ identifying phrase in form of combination of noun adjective and verb @ keyphrases @ sifted by @ weighted tf-idf @ term occurrence frequency-inverse document frequency @ value @ @ keyphrases @ added @ @ keyphrase lexicon @ springer-verlag @ @ @ 
4334,An optimization criterion for generalized discriminant analysis on undersampled problems,"An optimization criterion is presented for discriminant analysis. The criterion extends the optimization criteria of the classical Linear Discriminant Analysis (LDA) through the use of the pseudoinverse when the scatter matrices are singular. It is applicable regardless of the relative sizes of the data dimension and sample size, overcoming a limitation of classical LDA. The optimization problem can be solved analytically by applying the Generalized Singular Value Decomposition (GSVD) technique. The pseudoinverse has been suggested and used for undersampled problems in the past, where the data dimension exceeds the number of data points. The criterion proposed in this paper provides a theoretical justification for this procedure. An approximation algorithm for the GSVD-based approach is also presented. It reduces the computational complexity by finding subclusters of each cluster and uses their centroids to capture the structure of each cluster. This reduced problem yields much smaller matrices to which the GSVD can be applied efficiently. Experiments on text data, with up to 7,000 dimensions, show that the approximation algorithm produces results that are close to those produced by the exact algorithm. © 2004 IEEE.",2004,IEEE Transactions on Pattern Analysis and Machine Intelligence,211,@ optimization criterion is presented @ discriminant analysis @ @ criterion extends @ optimization criterion of @ classical linear discriminant analysis @ lda @ @ @ use of @ pseudoinverse @ @ scatter matrix @ singular @ @ is applicable regardless of @ relative size of @ data dimension and sample size overcoming a limitation of classical lda @ @ optimization problem @ @ solved analytically by applying @ generalized singular value decomposition @ gsvd @ technique @ @ pseudoinverse ha @ suggested and used @ undersampled problem in @ past @ @ data dimension exceeds @ number of data point @ @ criterion proposed in @ @ provides a theoretical justification @ @ procedure @ @ approximation algorithm @ @ gsvd-based approach is @ presented @ @ reduces @ computational complexity by finding subclusters of @ cluster and us @ centroid to capture @ structure of @ cluster @ @ reduced problem yield much smaller matrix to @ @ gsvd @ @ applied efficiently @ experiment on text data @ up to dimension @ @ @ approximation algorithm produce @ @ @ close to @ produced by @ exact algorithm @ @ @ 
4335,A Chinese word segmentation based on language situation in processing ambiguous words,"While the processing of natural language is beneficial to the text mining, Chinese word segmentation is an important step in the processing of Chinese natural language. In this paper, the convergence essence of the segmentation process is analyzed, and a theory of Chinese word segmentation based on language situation is deducted. Based on the segmentation theory, an algorithm of Chinese word segmentation is presented. Both in theory and from the experiment results, the algorithm is efficient. © 2003 Elsevier Inc. All rights reserved.",2004,Information Sciences,22,@ @ processing of natural language is beneficial to @ text mining chinese word segmentation is @ important step in @ processing of chinese natural language @ in @ @ @ convergence essence of @ segmentation process is analyzed and a theory of chinese word segmentation based on language situation is deducted @ based on @ segmentation theory @ algorithm of chinese word segmentation is presented @ @ in theory and @ @ experiment @ @ algorithm is efficient @ @ inc @ @ right reserved @ 
4338,Natural language processing of patents and technical documentation,"Natural Language Processing techniques for text-mining and information retrieval are finding application in the analysis of many kinds of documentation, from technical documentation to World Wide Web. Particularly, Functional Analysis techniques are based on the extraction of the interactions between the entities described in the document: these interactions are expressed as Subject-Action-Object (SAO) triples (obtainable using a suitable syntactic parser) which represent a concept in its most synthesizing form. In this work, the techniques developed for a functional analysis of patents and their implementation in the PAT-Analyzer tool are presented. The same technique has been properly tailored and applied to the analysis of software requirements documents. Current work in the direction of the development of a SAO-based Content Analysis of technical documentation is presented. © Springer-Verlag 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),51,natural language processing technique @ text-mining and information retrieval @ finding application in @ analysis of many kind of documentation @ technical documentation to world wide web @ particularly functional analysis technique @ based on @ extraction of @ interaction @ @ entity described in @ document @ @ interaction @ expressed a subject-action-object @ sao @ triple @ obtainable @ a suitable syntactic parser @ @ represent a concept in @ @ synthesizing form @ in @ work @ technique developed @ a functional analysis of patent and @ implementation in @ pat-analyzer tool @ presented @ @ @ technique ha @ properly tailored and applied to @ analysis of software requirement document @ current work in @ direction of @ development of a sao-based content analysis of technical documentation is presented @ springer-verlag @ 
4343,A flexible workbench for document analysis and text mining,"Document analysis and text mining techniques are used to preprocess documents in information retrieval systems, to extract concepts in ontology construction processes, and to discover and classify knowledge along several dimensions. In most cases it is not obvious how the techniques should be configured and combined, and it is a time-consuming process to set up and test various combinations of techniques. In this paper, we present a workbench that makes it easy to plug in new document analysis and text mining techniques and experiment with different constellations of techniques. We explain the architecture of the workbench and show how the workbench has been used to extract ontological concepts and relationships for a document collection published by the Norwegian Center for Medical Informatics. © Springer-Verlag 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,document analysis and text mining technique @ used to preprocess document in information retrieval system to extract concept in ontology construction process and to discover and classify knowledge along several dimension @ in @ case @ is not obvious @ @ technique @ @ configured and combined and @ is a time-consuming process to set up and test various combination of technique @ in @ @ @ @ a workbench @ make @ easy to plug in @ document analysis and text mining technique and experiment @ different constellation of technique @ @ explain @ architecture of @ workbench and @ @ @ workbench ha @ used to extract ontological concept and relationship @ a document collection published by @ norwegian center @ medical informatics @ springer-verlag @ 
4345,Applying passage in Web text mining,"Textual information on the Web is very huge, varied, and useful. Although traditional text mining treats a text document as a single piece of information, this approach may not be suitable for Web documents that are long and heterogeneous in their contents. This article presents a new approach that applies the concept of a passage to Web text mining. In this approach, a single Web text document is considered as several passages instead of a single text. To investigate the effectiveness of the approach, Thai Web documents taken from the Internet are used. As our preliminary experiment, we explore the influence of using passages on the construction of association rules by comparing them with a version that does not use passages. © 2004 Wiley Periodicals, Inc.",2004,International Journal of Intelligent Systems,6,textual information on @ web is @ huge varied and useful @ although traditional text mining treat a text document a a single piece of information @ approach may not @ suitable @ web document @ @ long and heterogeneous in @ content @ @ article @ a @ approach @ applies @ concept of a passage to web text mining @ in @ approach a single web text document is considered a several passage instead of a single text @ to investigate @ effectiveness of @ approach thai web document taken @ @ internet @ used @ a @ preliminary experiment @ explore @ influence of @ passage on @ construction of association rule by comparing @ @ a version @ doe not use passage @ wiley periodical inc @ 
4347,A text-mining system for knowledge discovery from biomedical documents,"This paper describes the application of IBM TAKMI® for Biomedical Documents to facilitate knowledge discovery from the very large text databases characteristic of life science and healthcare applications. This set of tools, designated MedTAKMI, is an extension of the TAKMI (Text Analysis and Knowledge Mining) system originally developed for text mining in customer-relationship-management applications. MedTAKMI dynamically and interactively mines a collection of documents to obtain characteristic features within them. By using multifaceted mining of these documents together with biomedically motivated categories for term extraction and a series of drill-down queries, users can obtain knowledge about a specific topic after seeing only a few key documents. In addition, the use of natural language techniques makes it possible to extract deeper relationships among biomedical concepts. The MedTAKMI system is capable of mining the entire MEDLINE® database of 11 million biomedical journal abstracts. It is currently running at a customer site. © 2004 IBM.",2004,IBM Systems Journal,59,@ @ describes @ application of ibm takmi @ biomedical document to facilitate knowledge discovery @ @ @ @ text database characteristic of life science and healthcare application @ @ set of tool designated medtakmi is @ extension of @ takmi @ text analysis and knowledge mining @ system originally developed @ text mining in customer-relationship-management application @ medtakmi dynamically and interactively mine a collection of document to obtain characteristic feature within @ @ by @ multifaceted mining of @ document together @ biomedically motivated category @ term extraction and a series of drill-down query user @ obtain knowledge @ a specific topic @ seeing only a @ key document @ in addition @ use of natural language technique make @ possible to extract deeper relationship among biomedical concept @ @ medtakmi system is capable of mining @ entire medline database of million biomedical journal abstract @ @ is currently running at a customer site @ ibm @ 
4350,Discovering unexpected information for technology watch,"The purpose of technology watch is to gather, process and integrate the scientific and technical information that is useful to economic players. In this article, we propose to use text mining techniques to automate processing of data found in scientific text databases. The watch activity introduces an unusual difficulty compared with conventional areas of application for text mining techniques since, instead of searching for frequent knowledge hidden in the texts, the target is unexpected knowledge. As a result, the usual measures used for knowledge discovery have to be revised. For that purpose, we have developed the UnexpectedMiner system using new measures for to estimate the unexpectedness of a document. Our system is evaluated using a base that contains articles relating to the field of machine learning. © Springer-Verlag Berlin Heidelberg 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),4,@ purpose of technology watch is to gather process and integrate @ scientific and technical information @ is useful to economic player @ in @ article @ propose to use text mining technique to automate processing of data found in scientific text database @ @ watch activity introduces @ unusual difficulty compared @ conventional area of application @ text mining technique since instead of searching @ frequent knowledge hidden in @ text @ target is unexpected knowledge @ a a @ @ usual measure used @ knowledge discovery @ to @ revised @ @ @ purpose @ @ developed @ unexpectedminer system @ @ measure @ to estimate @ unexpectedness of a document @ @ system is evaluated @ a base @ contains article relating to @ field of machine learning @ springer-verlag @ @ @ 
4351,Evaluation of information access technologies at the NTCIR workshop,"This paper introduces the NTCIR Workshop, a series of evaluation workshops that are designed to enhance research in information access technologies, such as information retrieval, cross-lingual information retrieval, text summarization, question answering and text mining, by providing infrastructure for large-scale evaluations. A brief history, the test collections, and recent progress after the previous CLEF Workshop are described, highlighting the differences from CLEF. To conclude, some thoughts on future directions are suggested. © Springer-Verlag 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ @ introduces @ ntcir workshop a series of evaluation workshop @ @ designed to enhance research in information access technology @ a information retrieval cross-lingual information retrieval text summarization question answering and text mining by providing infrastructure @ large-scale evaluation @ a brief history @ test collection and recent progress @ @ previous clef workshop @ described highlighting @ difference @ clef @ to conclude some thought on future direction @ suggested @ springer-verlag @ 
4352,A method of extracting and evaluating popularity and unpopularity for natural language expressions,"Although a user's opinion, or a live voice, is very useful information for text mining of the business, it is difficult to extract popularity and unpopularity impressions of users from texts written in natural language. The popularity and unpopularity impressions discussed here depend on user's claims, interests and demands. This paper presents a method of determining these impressions in commodity review sentences. Multi-attribute rule is introduced to extract the impressions from sentences, and four-stage-rules are defined in order to evaluate popularity and unpopularity impressions step by step. A deterministic multi-attribute pattern matching algorithm is utilized to determine the impressions efficiently. From simulation results for 2,240 review comments, it is verified that the multi-attribute pattern matching algorithm is 44.5 times faster than the Aho and Corasick method. The precision and recall of extracted impressions for each commodity are 94% and 93%. Moreover, the precision and recall of the resulting impressions for each rule are 95% and 95%, respectively. © Springer-Verlag 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,although a user @ s opinion @ a live voice is @ useful information @ text mining of @ @ @ is difficult to extract popularity and unpopularity impression of user @ text written in natural language @ @ popularity and unpopularity impression discussed @ depend on user @ s claim interest and demand @ @ @ @ a method of determining @ impression in commodity review sentence @ multi-attribute rule is introduced to extract @ impression @ sentence and four-stage-rules @ defined in order to evaluate popularity and unpopularity impression step by step @ a deterministic multi-attribute pattern matching algorithm is utilized to determine @ impression efficiently @ @ simulation @ @ review comment @ is verified @ @ multi-attribute pattern matching algorithm is @ time faster @ @ aho and corasick method @ @ precision and recall of extracted impression @ @ commodity @ and @ moreover @ precision and recall of @ resulting impression @ @ rule @ and respectively @ springer-verlag @ 
4353,Centroid-based language identification using letter feature set,"In recent years, an unexpected amount of growth of the text documents volume has been observed on the internet, intranet, in digital libraries and newsgroups. To obtain useful information and meaningful patterns from these documents, a great many researchers known under the term ""text mining"" have been carried out. Among them text categorization is to be mentioned that covers the problem of classifying documents relative to their similarities. One of techniques applied in this area is called centroid-based document classification method. All researchers on text categorization use the notion of frequency somehow or other. In this study, letter frequencies (LF) have been used for text categorization. By making use of letter frequencies information, the centroid-based document classification has been carried out. An experiment has been done on language detection for text documents. Its results allow propose that the letter-based text categorization should be done prior to term based text categorization. © Springer-Verlag 2004.",2004,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8,in recent year @ unexpected amount of growth of @ text document volume ha @ observed on @ internet intranet in digital library and newsgroups @ to obtain useful information and meaningful pattern @ @ document a great many researcher known @ @ term @ text mining @ @ @ carried @ @ among @ text categorization is to @ mentioned @ cover @ problem of classifying document relative to @ similarity @ @ of technique applied in @ area is called centroid-based document classification method @ @ researcher on text categorization use @ notion of frequency somehow @ @ @ in @ study letter frequency @ lf @ @ @ used @ text categorization @ by making use of letter frequency information @ centroid-based document classification ha @ carried @ @ @ experiment ha @ done on language detection @ text document @ @ @ allow propose @ @ letter-based text categorization @ @ done prior to term based text categorization @ springer-verlag @ 
4356,"Information extraction, multilinguality and portability","The growing availability of on-line textual sources and the potential number of applications of knowledge acquisition approaches from textual data, such as Information Extraction (IE), has lead to an increase in IE research. Some examples of these applications are the generation of data bases from documents, as well as the acquisition of knowledge useful for emerging technologies like question answering and information integration, among others related to text mining. However, one of the main drawbacks of the application of IE refers to the intrinsic language and domain dependence. For the sake of reducing the high cost of manually adapting IE applications to new domains and languages, different Machine Learning (ML) techniques have been applied by the research community. This survey describes and compares the main approaches to IE and the different ML techniques used to achieve adaptable IE technology, as of today.",2004,Inteligencia Artificial,1,@ growing availability of on-line textual source and @ potential number of application of knowledge acquisition approach @ textual data @ a information extraction @ ie @ ha lead to @ increase in ie research @ some example of @ application @ @ generation of data base @ document a well a @ acquisition of knowledge useful @ emerging technology like question answering and information integration among others related to text mining @ however @ of @ main drawback of @ application of ie refers to @ intrinsic language and domain dependence @ @ @ sake of reducing @ high cost of manually adapting ie application to @ domain and language different machine learning @ ml @ technique @ @ applied by @ research community @ @ survey describes and compare @ main approach to ie and @ different ml technique used to achieve adaptable ie technology a of today @ 
4359,A semantically guided and domain-independent evolutionary model for knowledge discovery from texts,"We present a novel evolutionary model for knowledge discovery from texts (KDTs), which deals with issues concerning shallow text representation and processing for mining purposes in an integrated way. Its aims is to look for novel and interesting explanatory knowledge across text documents. The approach uses natural language technology and genetic algorithms to produce explanatory novel hypotheses. The proposed approach is interdisciplinary, involving concepts not only from evolutionary algorithms but also from many kinds of text mining methods. Accordingly, new kinds of genetic operations suitable for text mining are proposed. The principles behind the representation and a new proposal for using multiobjective evaluation at the semantic level are described. Some promising results and their assessment by human experts are also discussed which indicate the plausibility of the model for effective KDT.",2003,IEEE Transactions on Evolutionary Computation,17,@ @ a novel evolutionary model @ knowledge discovery @ text @ kdts @ @ deal @ issue concerning shallow text representation and processing @ mining purpose in @ integrated way @ @ aim is to look @ novel and interesting explanatory knowledge across text document @ @ approach us natural language technology and genetic algorithm to produce explanatory novel hypothesis @ @ proposed approach is interdisciplinary involving concept not only @ evolutionary algorithm @ @ @ many kind of text mining method @ accordingly @ kind of genetic operation suitable @ text mining @ proposed @ @ principle behind @ representation and a @ proposal @ @ multiobjective evaluation at @ semantic level @ described @ some promising @ and @ assessment by human expert @ @ discussed @ indicate @ plausibility of @ model @ effective kdt @ 
4367,Text mining neuroscience journal articles to populate neuroscience databases,"We have developed a program NeuroText to populate the neuroscience databases in SenseLab (http://senselab.med.yale.edu/senselab) by mining the natural language text of neuroscience articles. NeuroText uses a two-step approach to identify relevant articles. The first step (pre-processing), aimed at 100% sensitivity, identifies abstracts containing database keywords. In the second step, potentially relevant abstracts identified in the first step are processed for specificity dictated by database architecture, and neuroscience, lexical and semantic contexts. NeuroText results were presented to the experts for validation using a dynamically generated interface that also allows expert-validated articles to be automatically deposited into the databases. Of the test set of 912 articles, 735 were rejected at the pre-processing step. For the remaining articles, the accuracy of predicting database-relevant articles was 85%. Twenty-two articles were erroneously identified. NeuroText deferred decisions on 29 articles to the expert. A comparison of NeuroText results versus the experts' analyses revealed that the program failed to correctly identify articles' relevance due to concepts that did not yet exist in the knowledgebase or due to vaguely presented information in the abstracts. NeuroText uses two ""evolution"" techniques (supervised and unsupervised) that play an important role in the continual improvement of the retrieval results. Software that uses the NeuroText approach can facilitate the creation of curated, special-interest, bibliography databases.",2003,Neuroinformatics,15,@ @ developed a program neurotext to populate @ neuroscience database in senselab @ http @ senselab @ med @ yale @ edu senselab @ by mining @ natural language text of neuroscience article @ neurotext us a two-step approach to identify relevant article @ @ first step @ pre-processing @ aimed at sensitivity identifies abstract containing database keywords @ in @ second step potentially relevant abstract identified in @ first step @ processed @ specificity dictated by database architecture and neuroscience lexical and semantic context @ neurotext @ @ presented to @ expert @ validation @ a dynamically generated interface @ @ allows expert-validated article to @ automatically deposited @ @ database @ of @ test set of article @ rejected at @ pre-processing step @ @ @ remaining article @ accuracy of predicting database-relevant article wa @ twenty-two article @ erroneously identified @ neurotext deferred decision on article to @ expert @ a comparison of neurotext @ versus @ expert @ analysis revealed @ @ program failed to correctly identify article @ relevance due to concept @ @ not yet exist in @ knowledgebase @ due to vaguely presented information in @ abstract @ neurotext us @ @ evolution @ technique @ supervised and unsupervised @ @ play @ important role in @ continual improvement of @ retrieval @ @ software @ us @ neurotext approach @ facilitate @ creation of curated special-interest bibliography database @ 
4377,Is shallow parsing useful for unsupervised learning of semantic clusters?,"The context of this paper is the application of unsupervised Machine Learning techniques to building ontology extraction tools for Natural Language Processing. Our method relies on exploiting large amounts of linguistically annotated text, and on linguistic concepts such as selectional restrictions and co-composition. We work with a corpus of medical texts in English. First we apply a shallow parser to the corpus to get subject-verb-object structures. We then extract verb-noun relations, and apply a clustering algorithm to them to build semantic classes of nouns. We have evaluated the adequacy of the clustering method when applied to a syntactically tagged corpus, and the relevance of the semantic content of the resulting clusters. © Springer-Verlag Berlin Heidelberg 2003.",2003,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,@ context of @ @ is @ application of unsupervised machine learning technique to building ontology extraction tool @ natural language processing @ @ method relies on exploiting @ amount of linguistically annotated text and on linguistic concept @ a selectional restriction and co-composition @ @ work @ a corpus of medical text in english @ first @ apply a shallow parser to @ corpus to get subject-verb-object structure @ @ @ extract verb-noun relation and apply a clustering algorithm to @ to build semantic class of noun @ @ @ evaluated @ adequacy of @ clustering method @ applied to a syntactically tagged corpus and @ relevance of @ semantic content of @ resulting cluster @ springer-verlag @ @ @ 
4378,The Web as a Parallel Corpus,"Parallel corpora have become an essential resource for work in multilingual natural language processing. In this article, we report on our work using the STRAND system for mining parallel text on the World Wide Web, first reviewing the original algorithm and results and then presenting a set of significant enhancements. These enhancements include the use of supervised learning based on structural features of documents to improve classification performance, a new content-based measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair.",2003,Computational Linguistics,295,parallel corpus @ become @ essential resource @ work in multilingual natural language processing @ in @ article @ report on @ work @ @ strand system @ mining parallel text on @ world wide web first reviewing @ original algorithm and @ and @ presenting a set of significant enhancement @ @ enhancement include @ use of supervised learning based on structural feature of document to improve classification performance a @ content-based measure of translational equivalence and adaptation of @ system to take advantage of @ internet archive @ mining parallel text @ @ web on a @ scale @ finally @ value of @ technique is demonstrated in @ construction of a significant parallel corpus @ a low-density language pair @ 
4383,Evaluation of information access technologies at NTCIR workshop,"This paper introduces the NTCIR Workshops, a series of evaluation workshops that are designed to enhance research in information access technologies, such as information retrieval, text summarization, question answering, text mining, etc., by providing infrastructure of large-scale evaluation. A brief history, test collections, and recent progress after the previous CLEF Workshop are described with highlighting the difference from CLEF in this paper. To conclude, some thoughts on future directions are suggested.",2003,CEUR Workshop Proceedings,0,@ @ introduces @ ntcir workshop a series of evaluation workshop @ @ designed to enhance research in information access technology @ a information retrieval text summarization question answering text mining etc @ by providing infrastructure of large-scale evaluation @ a brief history test collection and recent progress @ @ previous clef workshop @ described @ highlighting @ difference @ clef in @ @ @ to conclude some thought on future direction @ suggested @ 
4384,Mining generalized character n-grams in large corpora,"In this paper, we study the computational cost of extracting character n-grams from a corpus. We propose an approach for reducing this cost which is relevant especially for text mining and natural language applications. The underlying idea is to take under consideration only n-grams occurring above a given frequency in a corpus. This approach is applied to three different corpora, allowing the extraction of all frequent n-grams in those corpora in reasonable time. © Springer-Verlag Berlin Heidelberg 2003.",2003,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ study @ computational cost of extracting character n-grams @ a corpus @ @ propose @ approach @ reducing @ cost @ is relevant especially @ text mining and natural language application @ @ underlying idea is to take @ consideration only n-grams occurring @ a given frequency in a corpus @ @ approach is applied to three different corpus allowing @ extraction of @ frequent n-grams in @ corpus in reasonable time @ springer-verlag @ @ @ 
4385,CLIR at NTCIR workshop 3: Cross-language and cross-genre retrieval,"This paper introduces the NTCIR Workshops, a series of evaluation workshops that are designed to enhance research in information access technologies, such as information retrieval, text summarization, question answering, information extraction, and text mining, by providing large-scale test collections and a forum for researchers. A brief history and descriptions of tasks, participants, test collections and CLIR evaluation at the workshops, and a brief overview of the third NTCIR Workshop are given. To conclude, some thoughts on future directions are suggested. © Springer-Verlag Berlin Heidelberg 2003.",2003,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),3,@ @ introduces @ ntcir workshop a series of evaluation workshop @ @ designed to enhance research in information access technology @ a information retrieval text summarization question answering information extraction and text mining by providing large-scale test collection and a forum @ researcher @ a brief history and description of task participant test collection and clir evaluation at @ workshop and a brief overview of @ third ntcir workshop @ given @ to conclude some thought on future direction @ suggested @ springer-verlag @ @ @ 
4390,Embedding Web-Based Statistical Translation Models in Cross-Language Information Retrieval,"Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CUR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.",2003,Computational Linguistics,76,although more and more language pair @ covered by machine translation @ mt @ service @ @ still many pair @ lack translation resource @ cross-language information retrieval @ cur @ is @ application @ need translation functionality of a relatively low level of sophistication since current model @ information retrieval @ ir @ @ still based on a bag of word @ @ web provides a vast resource @ @ automatic construction of parallel corpus @ @ @ used to train statistical translation model automatically @ @ resulting translation model @ @ embedded in several way in a retrieval model @ in @ article @ @ investigate @ problem of automatically mining parallel text @ @ web and different way of integrating @ translation model within @ retrieval process @ @ experiment on standard test collection @ clir @ @ @ web-based translation model @ surpass commercial mt system in clir task @ @ @ open @ perspective of constructing a fully automatic query translation device @ clir at a @ low cost @ 
4391,Mathematical sciences in the nineties,"In the last decade of the twentieth century, we saw great progress in the mathematical sciences as well as changes in the activities of the mathematical scientist. These included the resolution of some well-known conjectures, the introduction of new areas of study, as well as the adoption of new tools and new methods of operation. The IBM Research Division was an active participant in many of these events. We discuss here a selection of these, focusing on some to which contributions were made by mathematicians at IBM Research.",2003,IBM Journal of Research and Development,1,in @ last decade of @ twentieth century @ saw great progress in @ mathematical science a well a change in @ activity of @ mathematical scientist @ @ included @ resolution of some well-known conjecture @ introduction of @ area of study a well a @ adoption of @ tool and @ method of operation @ @ ibm research division wa @ active participant in many of @ event @ @ discus @ a selection of @ focusing on some to @ contribution @ made by mathematician at ibm research @ 
4394,"25th European Conference on Information Retrieval Research, ECIR 2003","The proceedings contain 49 papers. The special focus in this conference is on Web, Retrieval of Structured Documents, Collaborative Filtering, Text Mining, Text Representation and Natural Language Processing. The topics include: Annotation and retrieval of structured video documents; improving the evaluation of web search systems; hierarchical classification of html documents with webclass II; hierarchical indexing and flexible element retrieval for structured document; construction of a test collection for the focussed retrieval of structured documents; user behaviour in the context of structured documents; attaining fast and successful searches in e-commerce environments; learning user similarity and rating style for collaborative recommendation; spoken information extraction from Italian broadcast news; stemming and decompounding for German text retrieval; question answering system for incomplete and noisy data; term proximity scoring for keyword-based retrieval systems; propositional logic representations for documents and queries; from uncertain inference to probability of relevance for advanced IR applications; topic detection and tracking with spatio-temporal evidence; clustering and visualization in a multi-lingual multi-document summarization system; a hybrid relevance-feedback approach to text retrieval; experiments with document archive size detection; using kullback-leibler distance for text categorization; discretizing continuous attributes in adaboost for text categorization; a graphical visualization tool for web search results; relevance feedback for content-based image retrieval; representative sampling for text classification using support vector machines and Chinese text categorization based on the binary weighting model with non-binary smoothing.",2003,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,@ proceeding contain @ @ @ special focus in @ conference is on web retrieval of structured document collaborative filtering text mining text representation and natural language processing @ @ topic include @ annotation and retrieval of structured video document @ improving @ evaluation of web search system @ hierarchical classification of html document @ webclass ii @ hierarchical indexing and flexible element retrieval @ structured document @ construction of a test collection @ @ focussed retrieval of structured document @ user behaviour in @ context of structured document @ attaining fast and successful search in e-commerce environment @ learning user similarity and rating style @ collaborative recommendation @ spoken information extraction @ italian broadcast news @ stemming and decompounding @ german text retrieval @ question answering system @ incomplete and noisy data @ term proximity scoring @ keyword-based retrieval system @ propositional logic representation @ document and query @ @ uncertain inference to probability of relevance @ advanced ir application @ topic detection and tracking @ spatio-temporal evidence @ clustering and visualization in a multi-lingual multi-document summarization system @ a hybrid relevance-feedback approach to text retrieval @ experiment @ document archive size detection @ @ kullback-leibler distance @ text categorization @ discretizing continuous attribute in adaboost @ text categorization @ a graphical visualization tool @ web search @ @ relevance feedback @ content-based image retrieval @ representative sampling @ text classification @ support vector machine and chinese text categorization based on @ binary weighting model @ non-binary smoothing @ 
4397,Morpho-syntactic parsing for a text mining environment: An NP recognition model for knowledge visualization and information retrieval,"Sidhom and Hassoun discuss the crucial role of NLP tools in Knowledge Extraction and Management as well as in the design of Information Retrieval Systems. The authors focus more specifically on the morphosyntactic issues by describing their morpho-syntactic analysis platform, which has been implemented to cover the automatic indexing and information retrieval topics. To this end they implemented the Cascaded ""Augmented Transition Network (ATN)"". They used this formalism in order to analyse French text descriptions of Multimedia documents. An implementation of an ATN parsing automaton is briefly described. The Platform in its logical operation is considered as an investigative tool towards the knowledge organization (based on an NP recognition model) and management of multiform e-documents (text, multimedia, audio, image) using their text descriptions.",2002,Knowledge Organization,1,sidhom and hassoun discus @ crucial role of nlp tool in knowledge extraction and management a well a in @ design of information retrieval system @ @ author focus more specifically on @ morphosyntactic issue by describing @ morpho-syntactic analysis platform @ ha @ implemented to cover @ automatic indexing and information retrieval topic @ to @ end @ implemented @ cascaded @ augmented transition network @ atn @ @ @ @ used @ formalism in order to analyse french text description of multimedia document @ @ implementation of @ atn parsing automaton is briefly described @ @ platform in @ logical operation is considered a @ investigative tool towards @ knowledge organization @ based on @ np recognition model @ and management of multiform e-documents @ text multimedia audio image @ @ @ text description @ 
4399,A scalable and efficient probabilistic information retrieval and text mining system,A system for probabilistic information retrieval and text mining that is both scalable and efficient is presented. Separate feature extraction or stop-word lists are not needed since the system can remove unneeded parameters dynamically based on a local mutual information measure. This is shown to be as effective as using a global measure. A novel way ofstoring system parameters eliminates the need for a ranking step during information retrieval from queries. Probability models over word contexts provide a method to suggest related words that can be added to a query. Test results are presented on a categorization task and screen shots from a live system are shown to demonstrate its capabilities. © Springer-Verlag Berlin Heidelberg 2002.,2002,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),1,a system @ probabilistic information retrieval and text mining @ is @ scalable and efficient is presented @ separate feature extraction @ stop-word list @ not needed since @ system @ remove unneeded parameter dynamically based on a local mutual information measure @ @ is @ to @ a effective a @ a global measure @ a novel way ofstoring system parameter eliminates @ need @ a ranking step @ information retrieval @ query @ probability model @ word context provide a method to suggest related word @ @ @ added to a query @ test @ @ presented on a categorization task and screen shot @ a live system @ @ to demonstrate @ capability @ springer-verlag @ @ @ 
4400,A comparative study of information extraction strategies,"The availability of online text documents exposes readers to a vast amount of potentially valuable knowledge buried therein. The sheer scale of material has created the pressing need for automated methods of discovering relevant information without having to read it all. Hence the growing interest in recent years in Text Mining. A common approach to Text Mining is Information Extraction (IE), extracting specific types (or templates) of information from a document collection. Although many works on IE have been published, researchers have not paid much attention to evaluate the contribution of syntactic and semantic analysis using Natural Language Processing (NLP) techniques to the quality of IE results. In this work we try to quantify the contribution of NLP techniques, by comparing three strategies for IE: naïve co-occurrence, ordered co-occurrence, and the structure-driven method - a rule-based strategy that relies on syntactic analysis followed by the extraction of suitable semantic templates. We use the three strategies for the extraction of two templates from financial news stories. We show that the structure-driven strategy provides significantly better precision results than the two other strategies (80-90% for the structure-driven compared with about only 60% for the co-occurrence and ordered co-occurrence). These results indicate that a syntactical and semantic analysis is necessary if one wishes to obtain high accuracy. © Springer-Verlag Berlin Heidelberg 2002.",2002,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),18,@ availability of online text document expose reader to a vast amount of potentially valuable knowledge buried therein @ @ sheer scale of material ha created @ pressing need @ automated method of discovering relevant information without @ to read @ @ @ hence @ growing interest in recent year in text mining @ a common approach to text mining is information extraction @ ie @ extracting specific type @ @ template @ of information @ a document collection @ although many work on ie @ @ published researcher @ not paid much attention to evaluate @ contribution of syntactic and semantic analysis @ natural language processing @ nlp @ technique to @ quality of ie @ @ in @ work @ try to quantify @ contribution of nlp technique by comparing three strategy @ ie @ naïve co-occurrence ordered co-occurrence and @ structure-driven method a rule-based strategy @ relies on syntactic analysis followed by @ extraction of suitable semantic template @ @ use @ three strategy @ @ extraction of @ template @ financial news story @ @ @ @ @ structure-driven strategy provides significantly better precision @ @ @ @ @ strategy @ @ @ structure-driven compared @ @ only @ @ co-occurrence and ordered co-occurrence @ @ @ @ indicate @ a syntactical and semantic analysis is necessary if @ wish to obtain high accuracy @ springer-verlag @ @ @ 
4401,Detecting deviations in text collections: An approach using conceptual graphs,"Deviation detection is an important problem of both data and text mining. In this paper we consider the detection of deviations in a set of texts represented as conceptual graphs. In contrast with statistical and distance-based approaches, the method we propose is based on the concept of generalization and regularity. Among its main characteristics are the detection of rare patterns (that attempt to give a generalized description of rare texts) and the ability to discover local deviations (deviations at different contexts and generalization levels). The method is illustrated with the analysis of a set of computer science papers. © Springer-Verlag Berlin Heidelberg 2002.",2002,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2,deviation detection is @ important problem of @ data and text mining @ in @ @ @ consider @ detection of deviation in a set of text represented a conceptual graph @ in contrast @ statistical and distance-based approach @ method @ propose is based on @ concept of generalization and regularity @ among @ main characteristic @ @ detection of rare pattern @ @ attempt to give a generalized description of rare text @ and @ ability to discover local deviation @ deviation at different context and generalization level @ @ @ method is illustrated @ @ analysis of a set of computer science @ @ springer-verlag @ @ @ 
4406,Text mining of bilingual parallel corpora with a measure of semantic similarity,"This paper describes a new application of a text-mining algorithm to the text sources of bilingual parallel corpora. The ultimate task, being undertaken in the context of a Chinese-English machine translation project, will be to develop a language-neutral method to discovery similar documents from multilingual text collections. Using a variation of automatic clustering techniques, which apply a neural net approach namely the Self-Organizing Maps (SOM), we have conducted several experiments to uncover associated documents based on Chinese-English bilingual parallel corpora, and a hybrid Chinese-English corpus. The experiments show some interesting results and a couple of potential ways for future work towards the field of multilingual information discovery. Besides, for exploring the impacts on linguistics issues with the machine learning approach to mining sensible linguistics elements from multilingual texts, we have examined the resulting term associations and text associations from the view of cross-lingual text similarity. To evaluate semantic relatedness of the mined bilingual texts, we applied a measure technique of semantic similarity in the resulting bilingual document clusters and word clusters. This paper presents algorithms that enable multilingual text mining based on the self-organizing map (SOM) for automatically grouping similar multilingual texts (i.e. Chinese and English texts), along with a means in measuring their semantic similarity to resolve the difficulties of syntactic and semantic ambiguity in multilingual information access.",2001,"Proceedings of the IEEE International Conference on Systems, Man and Cybernetics",6,@ @ describes a @ application of a text-mining algorithm to @ text source of bilingual parallel corpus @ @ ultimate task @ undertaken in @ context of a chinese-english machine translation project @ @ to develop a language-neutral method to discovery similar document @ multilingual text collection @ @ a variation of automatic clustering technique @ apply a neural net approach namely @ self-organizing map @ som @ @ @ conducted several experiment to uncover associated document based on chinese-english bilingual parallel corpus and a hybrid chinese-english corpus @ @ experiment @ some interesting @ and a couple of potential way @ future work towards @ field of multilingual information discovery @ besides @ exploring @ impact on linguistics issue @ @ machine learning approach to mining sensible linguistics element @ multilingual text @ @ examined @ resulting term association and text association @ @ view of cross-lingual text similarity @ to evaluate semantic relatedness of @ mined bilingual text @ applied a measure technique of semantic similarity in @ resulting bilingual document cluster and word cluster @ @ @ @ algorithm @ enable multilingual text mining based on @ self-organizing map @ som @ @ automatically grouping similar multilingual text @ i @ e @ chinese and english text @ along @ a mean in measuring @ semantic similarity to resolve @ difficulty of syntactic and semantic ambiguity in multilingual information access @ 
4407,Affect analysis of text using fuzzy semantic typing,"We propose a novel, convenient fusion of natural language processing and fuzzy logic techniques for analyzing the affect content in free text. Our main goals are fast analysis and visualization of affect content for decision making. The main linguistic resource for fuzzy semantic typing is the fuzzy-affect lexicon, from which other important resources-the fuzzy thesaurus and affect category groups-are generated. Free text is tagged with affect categories from the lexicon and the affect categories' centralities and intensities are combined using techniques from fuzzy logic to produce affect sets-fuzzy sets representing the affect quality of a document. We show different aspects of affect analysis using news content and movie reviews. Our experiments show a good correspondence between affect sets and human judgments of affect content. We ascribe this to the representation of ambiguity in our fuzzy affect lexicon and the ability of fuzzy logic to deal successfully with the ambiguity of words in a natural language. Planned extensions of the system include personalized profiles for Web-based content dissemination, fuzzy retrieval, clustering, and classification.",2001,IEEE Transactions on Fuzzy Systems,196,@ propose a novel convenient fusion of natural language processing and fuzzy logic technique @ analyzing @ affect content in free text @ @ main goal @ fast analysis and visualization of affect content @ decision making @ @ main linguistic resource @ fuzzy semantic typing is @ fuzzy-affect lexicon @ @ @ important resources-the fuzzy thesaurus and affect category groups-are generated @ free text is tagged @ affect category @ @ lexicon and @ affect category @ centrality and intensity @ combined @ technique @ fuzzy logic to produce affect sets-fuzzy set representing @ affect quality of a document @ @ @ different aspect of affect analysis @ news content and movie review @ @ experiment @ a good correspondence @ affect set and human judgment of affect content @ @ ascribe @ to @ representation of ambiguity in @ fuzzy affect lexicon and @ ability of fuzzy logic to deal successfully @ @ ambiguity of word in a natural language @ planned extension of @ system include personalized profile @ web-based content dissemination fuzzy retrieval clustering and classification @ 
4409,Technology of text mining,"A large amount of information is stored in databases, in intranets or in Internet. This information is organised in documents or in text documents. The difference depends on the fact if pictures, tables, figures, and formulas are included or not. The common problem is to find the desired piece of information, a trend, or an undiscovered pattern from these sources. The problem is not a new one. Traditionally the problem has been considered under the title of information seeking, this means the science how to find a book in the library. Traditionally the problem has been solved either by classifying and accessing documents by Dewey Decimal Classification system or by giving a number of characteristic keywords. The problem is that nowadays there are lots of unclassified documents in company databases and in intranet or in Internet. First one defines some terms. Text filtering means an information seeking process in which documents are selected from a dynamic text stream. Text mining is a process of analysing text to extract information from it for particular purposes. Text categorisation means the process of clustering similar documents from a large document set. All these terms have a certain degree of overlapping. Text mining, also know as document information mining, text data mining, or knowledge discovery in textual databases is an merging technology for analysing large collections of unstructured documents for the purposes of extracting interesting and non-trivial patterns or knowledge. Typical subproblems that have been solved are language identification, feature selection/extraction, clustering, natural language processing, summarisation, categorisation, search, indexing, and visualisation. These subproblems are discussed in detail and the most common approaches are given. Finally some examples of current uses of text mining are given and some potential application areas are mentioned. © Springer-Verlag Berlin Heidelberg 2001.",2001,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10,a @ amount of information is stored in database in intranet @ in internet @ @ information is organised in document @ in text document @ @ difference depends on @ fact if picture table figure and formula @ included @ not @ @ common problem is to find @ desired piece of information a trend @ @ undiscovered pattern @ @ source @ @ problem is not a @ @ @ traditionally @ problem ha @ considered @ @ title of information seeking @ mean @ science @ to find a book in @ library @ traditionally @ problem ha @ solved either by classifying and accessing document by dewey decimal classification system @ by giving a number of characteristic keywords @ @ problem is @ nowadays @ @ lot of unclassified document in company database and in intranet @ in internet @ first @ defines some term @ text filtering mean @ information seeking process in @ document @ selected @ a dynamic text stream @ text mining is a process of analysing text to extract information @ @ @ particular purpose @ text categorisation mean @ process of clustering similar document @ a @ document set @ @ @ term @ a certain degree of overlapping @ text mining @ know a document information mining text data mining @ knowledge discovery in textual database is @ merging technology @ analysing @ collection of unstructured document @ @ purpose of extracting interesting and non-trivial pattern @ knowledge @ typical subproblems @ @ @ solved @ language identification feature selection extraction clustering natural language processing summarisation categorisation search indexing and visualisation @ @ subproblems @ discussed in detail and @ @ common approach @ given @ finally some example of current us of text mining @ given and some potential application area @ mentioned @ springer-verlag @ @ @ 
4410,Text mining with conceptual graphs,"A method for conceptual clustering of a collection of texts represented with conceptual graphs is presented. It uses the incremental strategy to construct the cluster hierarchy and incorporates some characteristics attractive for text mining processes. For instance, it considers the structural information of the graphs, uses domain knowledge to detect the cluster with generalized descriptions, and uses a user-defined similarity measure between the graphs.",2001,"Proceedings of the IEEE International Conference on Systems, Man and Cybernetics",6,a method @ conceptual clustering of a collection of text represented @ conceptual graph is presented @ @ us @ incremental strategy to construct @ cluster hierarchy and incorporates some characteristic attractive @ text mining process @ @ instance @ considers @ structural information of @ graph us domain knowledge to detect @ cluster @ generalized description and us a user-defined similarity measure @ @ graph @ 
4413,Flexible comparison of conceptual graphs,"Conceptual graphs allow for powerful and computationally affordable representation of the semantic contents of natural language texts. We propose a method of comparison (approximate matching) of conceptual graphs. The method takes into account synonymy and subtype/supertype relationships between the concepts and relations used in the conceptual graphs, thus allowing for greater flexibility of approximate matching. The method also allows the user to choose the desirable aspect of similarity in the cases when the two graphs can be generalized in different ways. The algorithm and examples of its application are presented. The results are potentially useful in a range of tasks requiring approximate semantic or another structural matching – among them, information retrieval and text mining. © Springer-Verlag Berlin Heidelberg 2001.",2001,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),43,conceptual graph allow @ powerful and computationally affordable representation of @ semantic content of natural language text @ @ propose a method of comparison @ approximate matching @ of conceptual graph @ @ method take @ account synonymy and subtype supertype relationship @ @ concept and relation used in @ conceptual graph thus allowing @ greater flexibility of approximate matching @ @ method @ allows @ user to choose @ desirable aspect of similarity in @ case @ @ @ graph @ @ generalized in different way @ @ algorithm and example of @ application @ presented @ @ @ @ potentially useful in a range of task requiring approximate semantic @ another structural matching among @ information retrieval and text mining @ springer-verlag @ @ @ 
4416,Improving information retrieval system performance by combining different text-mining techniques,"WordNet, a hand-made, general-purpose, and machine-readable thesaurus, has been used in information retrieval research by many researchers, but failed to improve the performance of their retrieval system. Thereby in this paper we investigate why the use of WordNet has not been successful. Based on this analysis we propose a method of making WordNet more useful in information retrieval applications by combining it with other knowledge resources. A simple word sense disambiguation is performed to avoid misleading expansion terms. Experiments using several standard information retrieval test collections show that our method results in a significant improvement of information retrieval performance. Failure analysis were done on the cases in which the proposed method fail to improve the retrieval effectiveness. We found that queries containing negative statements and multiple aspects might cause problems in the proposed method and we also investigated the solution to these problems. © 2000-IOS Press. All rights reserved.",2000,Intelligent Data Analysis,1,wordnet a hand-made general-purpose and machine-readable thesaurus ha @ used in information retrieval research by many researcher @ failed to improve @ performance of @ retrieval system @ thereby in @ @ @ investigate @ @ use of wordnet ha not @ successful @ based on @ analysis @ propose a method of making wordnet more useful in information retrieval application by combining @ @ @ knowledge resource @ a simple word sense disambiguation is performed to avoid misleading expansion term @ experiment @ several standard information retrieval test collection @ @ @ method @ in a significant improvement of information retrieval performance @ failure analysis @ done on @ case in @ @ proposed method fail to improve @ retrieval effectiveness @ @ found @ query containing negative statement and multiple aspect might cause problem in @ proposed method and @ @ investigated @ solution to @ problem @ io @ @ @ right reserved @ 
4417,Tool to discover the main themes in a Spanish or English document,"While most work on Knowledge Discovery in databases has been concerned with structured databases, there has been little work on handling the huge amount of information that is available only in unstructured textual form. In this paper a system based on information retrieval and text mining methods is presented. In addition, it is shown how the system analyzes a document containing natural language sentences in order to recognize its main topics or themes. The knowledge base used for the system is conformed by trees of concept. The architecture and the main algorithms of the system are discussed in this work.",2000,Expert Systems with Applications,6,@ @ work on knowledge discovery in database ha @ concerned @ structured database @ ha @ little work on handling @ huge amount of information @ is available only in unstructured textual form @ in @ @ a system based on information retrieval and text mining method is presented @ in addition @ is @ @ @ system analyzes a document containing natural language sentence in order to recognize @ main topic @ theme @ @ knowledge base used @ @ system is conformed by tree of concept @ @ architecture and @ main algorithm of @ system @ discussed in @ work @ 
4419,Preserving text categorization through translation,"In this paper, we treat natural language documents as we treat strongly-typed functional programming languages by introducing semantic categories as types. We use axioms to define primitive semantic categories and inference rules to capture the meaningful relationships among the primitive semantic categories. Primitive categories are nodes of a sense type decision tree. Axioms and inference rules are used to construct compound categories, validate category hypotheses, and eliminate ambiguities. The same categorization is obtained when this approach is applied to a text in a given natural language or to its translation into another language if an one-to-one mapping can be defined between the axioms and inference rules associated to the initial language and the axioms and inference rules associated to the other language.",1999,"Proceedings of the IEEE International Conference on Systems, Man and Cybernetics",2,in @ @ @ treat natural language document a @ treat strongly-typed functional programming language by introducing semantic category a type @ @ use axiom to define primitive semantic category and inference rule to capture @ meaningful relationship among @ primitive semantic category @ primitive category @ node of a sense type decision tree @ axiom and inference rule @ used to construct compound category validate category hypothesis and eliminate ambiguity @ @ @ categorization is obtained @ @ approach is applied to a text in a given natural language @ to @ translation @ another language if @ one-to-one mapping @ @ defined @ @ axiom and inference rule associated to @ initial language and @ axiom and inference rule associated to @ @ language @ 
4422,Using LocalMaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units,"The availability of contiguous and non-contiguous multiword lexical units (MWUs) in Natural Language Processing (NLP) lexica enhances parsing precision, helps attachment decisions, improves indexing in information retrieval (IR) systems, reinforces information extraction (IE) and text mining, among other applications. Unfortunately, their acquisition has long been a significant problem in NLP, IR and IE. In this paper we propose two new association measures, the Symmetric Conditional Probability (SCP) and the Mutual Expectation (ME) for the extraction of contiguous and non-contiguous MWUs. Both measures are used by a new algorithm, the LocalMaxs, that requires neither empirically obtained thresholds nor complex linguistic filters. We assess the results obtained by both measures by comparing them with reference association measures (Specific Mutual Information, ϕ2, Dice and Log- Likelihood coefficients) over a multilingual parallel corpus. An additional experiment has been carried out over a part-of-speech tagged Portuguese corpus for extracting contiguous compound verbs. © Springer-Verlag Berlin Heidelberg 1999.",1999,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),65,@ availability of contiguous and non-contiguous multiword lexical unit @ mwus @ in natural language processing @ nlp @ lexica enhances parsing precision help attachment decision improves indexing in information retrieval @ ir @ system reinforces information extraction @ ie @ and text mining among @ application @ unfortunately @ acquisition ha long @ a significant problem in nlp ir and ie @ in @ @ @ propose @ @ association measure @ symmetric conditional probability @ scp @ and @ mutual expectation @ me @ @ @ extraction of contiguous and non-contiguous mwus @ @ measure @ used by a @ algorithm @ localmaxs @ requires neither empirically obtained threshold @ complex linguistic filter @ @ ass @ @ obtained by @ measure by comparing @ @ reference association measure @ specific mutual information ϕ dice and log likelihood coefficient @ @ a multilingual parallel corpus @ @ additional experiment ha @ carried @ @ a part-of-speech tagged portuguese corpus @ extracting contiguous compound verb @ springer-verlag @ @ @ 
4424,Projet hypermap: Pour un environnement complet de génération automatique d’hypertexte,"In this paper, we try to argue for the design options we have chosen in the setting up of an integrated hypertext development environment. Three steps are involved: Document import, automatic and/or assisted indexing, thematic clustering. Two approaches have been used: On one hand, a linguistic approch for word stemming and irrelevant word eliminating; On the other hand, a statistic approach for noun phrase detection, word filtering, query expansion and document ranking, and thematic clustering for documents and word stems. © Springer-Verlag Berlin Heidelberg 1998.",1998,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),0,in @ @ @ try to argue @ @ design option @ @ chosen in @ setting up of @ integrated hypertext development environment @ three step @ involved @ document import automatic and @ assisted indexing thematic clustering @ @ approach @ @ used @ on @ hand a linguistic approch @ word stemming and irrelevant word eliminating @ on @ @ hand a statistic approach @ noun phrase detection word filtering query expansion and document ranking and thematic clustering @ document and word stem @ springer-verlag @ @ @ 
4425,Mining in the phrasal frontier,"Data mining methods have been applied to a wide variety of domains. Surprisingly enough, only a few examples of data mining in text are available. However, considering the amount of existing document collections, text mining would be most useful. Traditionally, texts have been analysed using various information retrieval related methods and natural language processing. In this paper, we present our first experiments in applying general methods of data mining to discovering phrases and co-occurring terms. We also describe the text mining process developed. Our results show that data mining methods — with appropriate preprocessing — can be used in text processing, and that by shifting the focus the process can be used to obtain results for various purposes. © Springer-Vertag Berlin Heidelberg 1997.",1997,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6,data mining method @ @ applied to a wide variety of domain @ surprisingly enough only a @ example of data mining in text @ available @ however considering @ amount of existing document collection text mining would @ @ useful @ traditionally text @ @ analysed @ various information retrieval related method and natural language processing @ in @ @ @ @ @ first experiment in applying general method of data mining to discovering phrase and co-occurring term @ @ @ describe @ text mining process developed @ @ @ @ @ data mining method @ appropriate preprocessing @ @ used in text processing and @ by shifting @ focus @ process @ @ used to obtain @ @ various purpose @ springer-vertag @ @ @ 
